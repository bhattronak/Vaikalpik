{"1": {"Title": "Introduction", "Sub Topics": {"1.1": {"Title": "Machine learning: what and why?", "Sub Topics": {"1.1.1": {"Title": "Types of machine learning"}}}, "1.2": {"Title": "Supervised learning", "Sub Topics": {"1.2.1": {"Title": "Classi\ufb01cation"}, "1.2.2": {"Title": "Regression"}}}, "1.3": {"Title": "Unsupervised learning", "Sub Topics": {"1.3.1": {"Title": "Discovering clusters 101.3.2 Discovering latent factors 111.3.3 Discovering graph structure 131.3.4 Matrix completion"}}}, "1.4": {"Title": "Some basic concepts in machine learning", "Sub Topics": {"1.4.1": {"Title": "Parametric vs non-parametric models 161.4.2 A simple non-parametric classi\ufb01er: K-nearest neighbors"}, "1.4.3": {"Title": "The curse of dimensionality 181.4.4 Parametric models for classi\ufb01cation and regression 191.4.5 Linear regression 191.4.6 Logistic regression 211.4.7 Over\ufb01tting 221.4.8 Model selection 221.4.9 No free lunch theorem"}}}}}, "2": {"Title": "Probability", "Sub Topics": {"2.1": {"Title": "Introduction 272.2 A brief review of probability theory", "Sub Topics": {}}, "2.3": {"Title": "Some common discrete distributions", "Sub Topics": {"2.3.1": {"Title": "The binomial and Bernoulli distributions 342.3.2 The multinomial and multinoulli distributions 352.3.3 The Poisson distribution 372.3.4 The empirical distribution"}}}, "2.4": {"Title": "Some common continuous distributions", "Sub Topics": {"2.4.1": {"Title": "Gaussian (normal) distribution 382.4.2 Degenerate pdf 392.4.3 The Laplace distribution 412.4.4 The gamma distribution 412.4.5 The beta distribution 422.4.6 Pareto distribution"}}}, "2.5": {"Title": "Joint probability distributions", "Sub Topics": {"2.5.1": {"Title": "Covariance and correlation 442.5.2 The multivariate Gaussian 462.5.3 Multivariate Student tdistribution"}, "2.5.4": {"Title": "Dirichlet distribution"}}}, "2.6": {"Title": "Transformations of random variables", "Sub Topics": {"2.6.1": {"Title": "Linear transformations 492.6.2 General transformations 502.6.3 Central limit theorem"}}}, "2.7": {"Title": "Monte Carlo approximation", "Sub Topics": {"2.7.1": {"Title": "Example: change of variables, the MC way 532.7.2 Example: estimating \u03c0by Monte Carlo integration"}, "2.7.3": {"Title": "Accuracy of Monte Carlo approximation"}}}, "2.8": {"Title": "Information theory", "Sub Topics": {"2.8.1": {"Title": "Entropy 562.8.2 KL divergence 572.8.3 Mutual information"}}}}}, "3": {"Title": "Generative models for discrete data", "Sub Topics": {"3.1": {"Title": "Introduction 653.2 Bayesian concept learning", "Sub Topics": {}}, "3.3": {"Title": "The beta-binomial model", "Sub Topics": {"3.3.1": {"Title": "Likelihood 733.3.2 Prior 743.3.3 Posterior 753.3.4 Posterior predictive distribution"}}}, "3.4": {"Title": "The Dirichlet-multinomial model", "Sub Topics": {"3.4.1": {"Title": "Likelihood"}, "3.4.2": {"Title": "Prior 793.4.3 Posterior 793.4.4 Posterior predictive"}}}, "3.5": {"Title": "Naive Bayes classi\ufb01ers", "Sub Topics": {"3.5.1": {"Title": "Model \ufb01tting 833.5.2 Using the model for prediction 853.5.3 The log-sum-exp trick 863.5.4 Feature selection using mutual information 863.5.5 Classifying documents using bag of words"}}}}}, "4": {"Title": "Gaussian models", "Sub Topics": {"4.1": {"Title": "Introduction", "Sub Topics": {"4.1.1": {"Title": "Notation 974.1.2 Basics 974.1.3 MLE for an MVN 994.1.4 Maximum entropy derivation of the Gaussian *"}}}, "4.2": {"Title": "Gaussian discriminant analysis", "Sub Topics": {"4.2.1": {"Title": "Quadratic discriminant analysis (QDA) 1024.2.2 Linear discriminant analysis (LDA) 1034.2.3 Two-class LDA 1044.2.4 MLE for discriminant analysis 1064.2.5 Strategies for preventing over\ufb01tting 1064.2.6 Regularized LDA * 1074.2.7 Diagonal LDA 1084.2.8 Nearest shrunken centroids classi\ufb01er *"}}}, "4.3": {"Title": "Inference in jointly Gaussian distributions", "Sub Topics": {"4.3.1": {"Title": "Statement of the result 1114.3.2 Examples 1114.3.3 Information form 1154.3.4 Proof of the result *"}}}, "4.4": {"Title": "Linear Gaussian systems", "Sub Topics": {"4.4.1": {"Title": "Statement of the result 1194.4.2 Examples 1204.4.3 Proof of the result *"}}}, "4.5": {"Title": "Digression: The Wishart distribution *", "Sub Topics": {"4.5.1": {"Title": "Inverse Wishart distribution 1264.5.2 Visualizing the Wishart distribution *"}}}, "4.6": {"Title": "Inferring the parameters of an MVN", "Sub Topics": {"4.6.2": {"Title": "Posterior distribution of \u03a3*"}, "4.6.3": {"Title": "Posterior distribution of \u03bcand\u03a3*"}, "4.6.4": {"Title": "Sensor fusion with unknown precisions *"}}}}}, "5": {"Title": "Bayesian statistics", "Sub Topics": {"5.1": {"Title": "Introduction", "Sub Topics": {}}, "5.2": {"Title": "Summarizing posterior distributions", "Sub Topics": {"5.2.1": {"Title": "MAP estimation 1495.2.2 Credible intervals 1525.2.3 Inference for a difference in proportions"}}}, "5.3": {"Title": "Bayesian model selection", "Sub Topics": {"5.3.1": {"Title": "Bayesian Occam\u2019s razor 1565.3.2 Computing the marginal likelihood (evidence) 1585.3.3 Bayes factors 1635.3.4 Jeffreys-Lindley paradox *"}}}, "5.4": {"Title": "Priors", "Sub Topics": {"5.4.1": {"Title": "Uninformative priors 1655.4.2 Jeffreys priors * 1665.4.3 Robust priors 1685.4.4 Mixtures of conjugate priors"}}}, "5.5": {"Title": "Hierarchical Bayes", "Sub Topics": {"5.5.1": {"Title": "Example: modeling related cancer rates"}}}, "5.6": {"Title": "Empirical Bayes", "Sub Topics": {"5.6.1": {"Title": "Example: beta-binomial model 1735.6.2 Example: Gaussian-Gaussian model"}}}, "5.7": {"Title": "Bayesian decision theory", "Sub Topics": {"5.7.1": {"Title": "Bayes estimators for common loss functions 1775.7.2 The false positive vs false negative tradeoff 1805.7.3 Other topics *"}}}}}, "6": {"Title": "Frequentist statistics", "Sub Topics": {"6.1": {"Title": "Introduction 1916.2 Sampling distribution of an estimator", "Sub Topics": {}}, "6.3": {"Title": "Frequentist decision theory", "Sub Topics": {"6.3.1": {"Title": "Bayes risk 1956.3.2 Minimax risk 1966.3.3 Admissible estimators"}}}, "6.4": {"Title": "Desirable properties of estimators", "Sub Topics": {"6.4.1": {"Title": "Consistent estimators 2006.4.2 Unbiased estimators 2006.4.3 Minimum variance estimators 2016.4.4 The bias-variance tradeoff"}}}, "6.5": {"Title": "Empirical risk minimization", "Sub Topics": {"6.5.1": {"Title": "Regularized risk minimization 2056.5.2 Structural risk minimization 2066.5.3 Estimating the risk using cross validation 2066.5.4 Upper bounding the risk using statistical learning theory *"}, "6.5.5": {"Title": "Surrogate loss functions"}}}, "6.6": {"Title": "Pathologies of frequentist statistics *", "Sub Topics": {"6.6.1": {"Title": "Counter-intuitive behavior of con\ufb01dence intervals"}, "6.6.2": {"Title": "p-values considered harmful 2136.6.3 The likelihood principle 2146.6.4 Why isn\u2019t everyone a Bayesian?"}}}}}, "7": {"Title": "Linear regression", "Sub Topics": {"7.1": {"Title": "Introduction 2177.2 Model speci\ufb01cation 2177.3 Maximum likelihood estimation (least squares)", "Sub Topics": {}}, "7.4": {"Title": "Robust linear regression * 2237.5 Ridge regression", "Sub Topics": {}}, "7.6": {"Title": "Bayesian linear regression", "Sub Topics": {"7.6.4": {"Title": "EB for linear regression (evidence procedure)"}}}}}, "8": {"Title": "Logistic regression", "Sub Topics": {"8.1": {"Title": "Introduction 2458.2 Model speci\ufb01cation 2458.3 Model \ufb01tting", "Sub Topics": {}}, "8.4": {"Title": "Bayesian logistic regression", "Sub Topics": {"8.4.1": {"Title": "Laplace approximation 2558.4.2 Derivation of the BIC 2558.4.3 Gaussian approximation for logistic regression 2568.4.4 Approximating the posterior predictive 2568.4.5 Residual analysis (outlier detection) *"}}}, "8.5": {"Title": "Online learning and stochastic optimization", "Sub Topics": {"8.5.1": {"Title": "Online learning and regret minimization"}, "8.5.2": {"Title": "Stochastic optimization and risk minimization"}, "8.5.3": {"Title": "The LMS algorithm 2648.5.4 The perceptron algorithm 2658.5.5 A Bayesian view"}}}, "8.6": {"Title": "Generative vs discriminative classi\ufb01ers", "Sub Topics": {"8.6.1": {"Title": "Pros and cons of each approach 2688.6.2 Dealing with missing data 2698.6.3 Fisher\u2019s linear discriminant analysis (FLDA) *"}}}}}, "9": {"Title": "Generalized linear models and the exponential family", "Sub Topics": {"9.1": {"Title": "Introduction 2819.2 The exponential family", "Sub Topics": {}}, "9.3": {"Title": "Generalized linear models (GLMs)", "Sub Topics": {"9.3.1": {"Title": "Basics 2909.3.2 ML and MAP estimation 2929.3.3 Bayesian inference"}}}, "9.4": {"Title": "Probit regression", "Sub Topics": {"9.4.1": {"Title": "ML/MAP estimation using gradient-based optimization 2949.4.2 Latent variable interpretation 2949.4.3 Ordinal probit regression * 2959.4.4 Multinomial probit models *"}}}, "9.5": {"Title": "Multi-task learning", "Sub Topics": {"9.5.1": {"Title": "Hierarchical Bayes for multi-task learning 2969.5.2 Application to personalized email spam \ufb01ltering 2969.5.3 Application to domain adaptation 2979.5.4 Other kinds of prior"}}}, "9.6": {"Title": "Generalized linear mixed models *", "Sub Topics": {"9.6.1": {"Title": "Example: semi-parametric GLMMs for medical data 2989.6.2 Computational issues"}}}, "9.7": {"Title": "Learning to rank *", "Sub Topics": {"9.7.1": {"Title": "The pointwise approach 3019.7.2 The pairwise approach 3019.7.3 The listwise approach 3029.7.4 Loss functions for ranking"}}}}}, "10": {"Title": "Directed graphical models (Bayes nets)", "Sub Topics": {"10.1": {"Title": "Introduction", "Sub Topics": {"10.1.1": {"Title": "Chain rule 30710.1.2 Conditional independence"}, "10.1.3": {"Title": "Graphical models"}, "10.1.4": {"Title": "Graph terminology 30910.1.5 Directed graphical models"}}}, "10.2": {"Title": "Examples", "Sub Topics": {"10.2.1": {"Title": "Naive Bayes classi\ufb01ers 31110.2.2 Markov and hidden Markov models 31210.2.3 Medical diagnosis 31310.2.4 Genetic linkage analysis * 31510.2.5 Directed Gaussian graphical models *"}}}, "10.3": {"Title": "Inference 31910.4 Learning", "Sub Topics": {}}, "10.5": {"Title": "Conditional independence properties of DGMs", "Sub Topics": {"10.5.2": {"Title": "Other Markov properties of DGMs 32710.5.3 Markov blanket and full conditionals"}}}, "10.6": {"Title": "In\ufb02uence (decision) diagrams *", "Sub Topics": {}}}}, "11": {"Title": "Mixture models and the EM algorithm", "Sub Topics": {"11.1": {"Title": "Latent variable models 33711.2 Mixture models", "Sub Topics": {}}, "11.3": {"Title": "Parameter estimation for mixture models", "Sub Topics": {"11.3.1": {"Title": "Unidenti\ufb01ability 34611.3.2 Computing a MAP estimate is non-convex"}}}, "11.4": {"Title": "The EM algorithm", "Sub Topics": {"11.4.1": {"Title": "Basic idea 34911.4.2 EM for GMMs 35011.4.3 EM for mixture of experts 35711.4.4 EM for DGMs with hidden variables 35811.4.5 EM for the Student distribution * 35911.4.6 EM for probit regression * 36211.4.7 Theoretical basis for EM * 36311.4.8 Online EM 36511.4.9 Other EM variants *"}}}, "11.5": {"Title": "Model selection for latent variable models", "Sub Topics": {"11.5.1": {"Title": "Model selection for probabilistic models 37011.5.2 Model selection for non-probabilistic methods"}}}, "11.6": {"Title": "Fitting models with missing data", "Sub Topics": {"11.6.1": {"Title": "EM for the MLE of an MVN with missing data"}}}}}, "12": {"Title": "Latent linear models", "Sub Topics": {"12.1": {"Title": "Factor analysis", "Sub Topics": {"12.1.1": {"Title": "FA is a low rank parameterization of an MVN"}, "12.1.2": {"Title": "Inference of the latent factors 38212.1.3 Unidenti\ufb01ability 38312.1.4 Mixtures of factor analysers 38512.1.5 EM for factor analysis models 38612.1.6 Fitting FA models with missing data"}}}, "12.2": {"Title": "Principal components analysis (PCA)", "Sub Topics": {"12.2.1": {"Title": "Classical PCA: statement of the theorem 38712.2.2 Proof * 38912.2.3 Singular value decomposition (SVD) 39212.2.4 Probabilistic PCA 39512.2.5 EM algorithm for PCA"}}}, "12.3": {"Title": "Choosing the number of latent dimensions", "Sub Topics": {"12.3.1": {"Title": "Model selection for FA/PPCA 39812.3.2 Model selection for PCA"}}}, "12.4": {"Title": "PCA for categorical data 40212.5 PCA for paired and multi-view data", "Sub Topics": {}}, "12.6": {"Title": "Independent Component Analysis (ICA)", "Sub Topics": {"12.6.1": {"Title": "Maximum likelihood estimation 41012.6.2 The FastICA algorithm 41112.6.3 Using EM 41412.6.4 Other estimation principles *"}}}}}, "13": {"Title": "Sparse linear models", "Sub Topics": {"13.1": {"Title": "Introduction 42113.2 Bayesian variable selection", "Sub Topics": {}}, "13.6": {"Title": "Non-convex regularizers", "Sub Topics": {"13.6.1": {"Title": "Bridge regression 45813.6.2 Hierarchical adaptive lasso 45813.6.3 Other hierarchical priors"}}}, "13.7": {"Title": "Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)", "Sub Topics": {"13.7.1": {"Title": "ARD for linear regression 46313.7.2 Whence sparsity? 46513.7.3 Connection to MAP estimation 46513.7.4 Algorithms for ARD * 46613.7.5 ARD for logistic regression"}}}, "13.8": {"Title": "Sparse coding *", "Sub Topics": {"13.8.1": {"Title": "Learning a sparse coding dictionary 46913.8.2 Results of dictionary learning from image patches 47013.8.3 Compressed sensing 47213.8.4 Image inpainting and denoising"}}}}}, "14": {"Title": "Kernels", "Sub Topics": {"14.1": {"Title": "Introduction 47914.2 Kernel functions", "Sub Topics": {}}, "14.3": {"Title": "Using kernels inside GLMs", "Sub Topics": {"14.3.1": {"Title": "Kernel machines 48614.3.2 L1VMs, RVMs, and other sparse vector machines"}}}, "14.4": {"Title": "The kernel trick", "Sub Topics": {"14.4.1": {"Title": "Kernelized nearest neighbor classi\ufb01cation 48914.4.2 Kernelized K-medoids clustering 48914.4.3 Kernelized ridge regression 49214.4.4 Kernel PCA"}}}, "14.5": {"Title": "Support vector machines (SVMs)", "Sub Topics": {"14.5.1": {"Title": "SVMs for regression 49714.5.2 SVMs for classi\ufb01cation"}, "14.5.4": {"Title": "Summary of key points"}, "14.5.5": {"Title": "A probabilistic interpretation of SVMs"}}}, "14.6": {"Title": "Comparison of discriminative kernel methods 50514.7 Kernels for building generative models", "Sub Topics": {}}}}, "15": {"Title": "Gaussian processes", "Sub Topics": {"15.1": {"Title": "Introduction 51515.2 GPs for regression", "Sub Topics": {}}, "15.3": {"Title": "GPs meet GLMs", "Sub Topics": {"15.3.1": {"Title": "Binary classi\ufb01cation 52515.3.2 Multi-class classi\ufb01cation 52815.3.3 GPs for Poisson regression"}}}, "15.4": {"Title": "Connection with other methods", "Sub Topics": {"15.4.1": {"Title": "Linear models compared to GPs 53215.4.2 Linear smoothers compared to GPs 53315.4.3 SVMs compared to GPs 53415.4.4 L1VM and RVMs compared to GPs 53415.4.5 Neural networks compared to GPs 53515.4.6 Smoothing splines compared to GPs * 53615.4.7 RKHS methods compared to GPs *"}}}, "15.5": {"Title": "GP latent variable model 54015.6 Approximation methods for large datasets", "Sub Topics": {}}}}, "16": {"Title": "Adaptive basis function models", "Sub Topics": {"16.1": {"Title": "Introduction 54316.2 Classi\ufb01cation and regression trees (CART)", "Sub Topics": {}}, "16.3": {"Title": "Generalized additive models", "Sub Topics": {"16.3.1": {"Title": "Back\ufb01tting"}, "16.3.2": {"Title": "Computational efficiency 55316.3.3 Multivariate adaptive regression splines (MARS)"}}}, "16.4": {"Title": "Boosting", "Sub Topics": {"16.4.1": {"Title": "Forward stagewise additive modeling 55516.4.2 L2boosting 55716.4.3 AdaBoost 55816.4.4 LogitBoost 55916.4.5 Boosting as functional gradient descent 56016.4.6 Sparse boosting 56116.4.7 Multivariate adaptive regression trees (MART) 56216.4.8 Why does boosting work so well? 56216.4.9 A Bayesian view"}}}, "16.5": {"Title": "Feedforward neural networks (multilayer perceptrons)", "Sub Topics": {"16.5.1": {"Title": "Convolutional neural networks 56416.5.2 Other kinds of neural networks 56816.5.3 A brief history of the \ufb01eld 56816.5.4 The backpropagation algorithm 56916.5.5 Identi\ufb01ability 57216.5.6 Regularization 57216.5.7 Bayesian inference *"}}}, "16.6": {"Title": "Ensemble learning", "Sub Topics": {"16.6.1": {"Title": "Stacking 58016.6.2 Error-correcting output codes 58116.6.3 Ensemble learning is not equivalent to Bayes model averaging"}}}, "16.7": {"Title": "Experimental comparison", "Sub Topics": {"16.7.1": {"Title": "Low-dimensional features 58216.7.2 High-dimensional features"}}}, "16.8": {"Title": "Interpreting black-box models", "Sub Topics": {}}}}, "17": {"Title": "Markov and hidden Markov models", "Sub Topics": {"17.1": {"Title": "Introduction 58917.2 Markov models", "Sub Topics": {}}, "17.3": {"Title": "Hidden Markov models", "Sub Topics": {"17.3.1": {"Title": "Applications of HMMs"}}}, "17.4": {"Title": "Inference in HMMs", "Sub Topics": {"17.4.1": {"Title": "Types of inference problems for temporal models 60617.4.2 The forwards algorithm 60917.4.3 The forwards-backwards algorithm 61017.4.4 The Viterbi algorithm 61217.4.5 Forwards \ufb01ltering, backwards sampling"}}}, "17.5": {"Title": "Learning for HMMs", "Sub Topics": {"17.5.1": {"Title": "Training with fully observed data"}, "17.5.2": {"Title": "EM for HMMs (the Baum-Welch algorithm) 61817.5.3 Bayesian methods for \u201c\ufb01tting\u201d HMMs * 62017.5.4 Discriminative training 62017.5.5 Model selection"}}}, "17.6": {"Title": "Generalizations of HMMs", "Sub Topics": {"17.6.1": {"Title": "Variable duration (semi-Markov) HMMs 62217.6.2 Hierarchical HMMs 62417.6.3 Input-output HMMs 62517.6.4 Auto-regressive and buried HMMs 62617.6.5 Factorial HMM 62717.6.6 Coupled HMM and the in\ufb02uence model 62817.6.7 Dynamic Bayesian networks (DBNs)"}}}}}, "18": {"Title": "State space models", "Sub Topics": {"18.1": {"Title": "Introduction 63118.2 Applications of SSMs", "Sub Topics": {}}, "18.3": {"Title": "Inference in LG-SSM", "Sub Topics": {"18.3.1": {"Title": "The Kalman \ufb01ltering algorithm 64018.3.2 The Kalman smoothing algorithm"}}}, "18.4": {"Title": "Learning for LG-SSM", "Sub Topics": {"18.4.1": {"Title": "Identi\ufb01ability and numerical stability 64618.4.2 Training with fully observed data 64718.4.3 EM for LG-SSM 64718.4.4 Subspace methods 64718.4.5 Bayesian methods for \u201c\ufb01tting\u201d LG-SSMs"}}}, "18.5": {"Title": "Approximate online inference for non-linear, non-Gaussian SSMs", "Sub Topics": {"18.5.1": {"Title": "Extended Kalman \ufb01lter (EKF) 64818.5.2 Unscented Kalman \ufb01lter (UKF) 65018.5.3 Assumed density \ufb01ltering (ADF)"}}}, "18.6": {"Title": "Hybrid discrete/continuous SSMs", "Sub Topics": {"18.6.1": {"Title": "Inference 65618.6.2 Application: data association and multi-target tracking 65818.6.3 Application: fault diagnosis 65918.6.4 Application: econometric forecasting"}}}}}, "19": {"Title": "Undirected graphical models (Markov random \ufb01elds)", "Sub Topics": {"19.1": {"Title": "Introduction 66119.2 Conditional independence properties of UGMs", "Sub Topics": {}}, "19.3": {"Title": "Parameterization of MRFs", "Sub Topics": {"19.3.1": {"Title": "The Hammersley-Clifford theorem 66519.3.2 Representing potential functions"}}}, "19.4": {"Title": "Examples of MRFs", "Sub Topics": {"19.4.1": {"Title": "Ising model 66819.4.2 Hop\ufb01eld networks 66919.4.3 Potts model 67119.4.4 Gaussian MRFs 67219.4.5 Markov logic networks *"}}}, "19.5": {"Title": "Learning", "Sub Topics": {"19.5.1": {"Title": "Training maxent models using gradient methods 67619.5.2 Training partially observed maxent models 67719.5.3 Approximate methods for computing the MLEs of MRFs 67819.5.4 Pseudo likelihood 67819.5.5 Stochastic maximum likelihood 67919.5.6 Feature induction for maxent models * 68019.5.7 Iterative proportional \ufb01tting (IPF) *"}}}, "19.6": {"Title": "Conditional random \ufb01elds (CRFs)", "Sub Topics": {"19.6.1": {"Title": "Chain-structured CRFs, MEMMs and the label-bias problem 68419.6.2 Applications of CRFs 68619.6.3 CRF training"}}}, "19.7": {"Title": "Structural SVMs", "Sub Topics": {"19.7.1": {"Title": "SSVMs: a probabilistic view 69319.7.2 SSVMs: a non-probabilistic view 69519.7.3 Cutting plane methods for \ufb01tting SSVMs 69819.7.4 Online algorithms for \ufb01tting SSVMs 70019.7.5 Latent structural SVMs"}}}}}, "20": {"Title": "Exact inference for graphical models", "Sub Topics": {"20.1": {"Title": "Introduction 70720.2 Belief propagation for trees", "Sub Topics": {}}, "20.3": {"Title": "The variable elimination algorithm", "Sub Topics": {"20.3.1": {"Title": "The generalized distributive law * 71720.3.2 Computational complexity of VE 71720.3.3 A weakness of VE"}}}, "20.4": {"Title": "The junction tree algorithm *", "Sub Topics": {"20.4.1": {"Title": "Creating a junction tree 72020.4.2 Message passing on a junction tree 72220.4.3 Computational complexity of JTA"}, "20.4.4": {"Title": "JTA generalizations *"}}}, "20.5": {"Title": "Computational intractability of exact inference in the worst case", "Sub Topics": {"20.5.1": {"Title": "Approximate inference"}}}}}, "21": {"Title": "Variational inference", "Sub Topics": {"21.1": {"Title": "Introduction", "Sub Topics": {}}, "21.2": {"Title": "Variational inference", "Sub Topics": {"21.2.1": {"Title": "Alternative interpretations of the variational objective 73321.2.2 Forward or reverse KL? *"}}}, "21.3": {"Title": "The mean \ufb01eld method", "Sub Topics": {"21.3.1": {"Title": "Derivation of the mean \ufb01eld update equations 73621.3.2 Example: mean \ufb01eld for the Ising model"}}}, "21.4": {"Title": "Structured mean \ufb01eld *", "Sub Topics": {"21.4.1": {"Title": "Example: factorial HMM"}}}, "21.5": {"Title": "Variational Bayes", "Sub Topics": {"21.5.1": {"Title": "Example: VB for a univariate Gaussian 74221.5.2 Example: VB for linear regression"}}}, "21.6": {"Title": "Variational Bayes EM", "Sub Topics": {"21.6.1": {"Title": "Example: VBEM for mixtures of Gaussians *"}}}, "21.7": {"Title": "Variational message passing and VIBES 75621.8 Local variational bounds *", "Sub Topics": {}}}}, "22": {"Title": "More variational inference", "Sub Topics": {"22.1": {"Title": "Introduction 76722.2 Loopy belief propagation: algorithmic issues", "Sub Topics": {}}, "22.3": {"Title": "Loopy belief propagation: theoretical issues *", "Sub Topics": {"22.3.1": {"Title": "UGMs represented in exponential family form 77622.3.2 The marginal polytope 77722.3.3 Exact inference as a variational optimization problem 77822.3.4 Mean \ufb01eld as a variational optimization problem 77922.3.5 LBP as a variational optimization problem 77922.3.6 Loopy BP vs mean \ufb01eld"}}}, "22.4": {"Title": "Extensions of belief propagation *", "Sub Topics": {"22.4.1": {"Title": "Generalized belief propagation"}, "22.4.2": {"Title": "Convex belief propagation"}}}, "22.5": {"Title": "Expectation propagation", "Sub Topics": {"22.5.1": {"Title": "EP as a variational inference problem"}, "22.5.2": {"Title": "Optimizing the EP objective using moment matching 78922.5.3 EP for the clutter problem 79122.5.4 LBP is a special case of EP 79222.5.5 Ranking players using TrueSkill 79322.5.6 Other applications of EP"}}}, "22.6": {"Title": "MAP state estimation", "Sub Topics": {"22.6.1": {"Title": "Linear programming relaxation 79922.6.2 Max-product belief propagation 80022.6.3 Graphcuts 80122.6.4 Experimental comparison of graphcuts and BP 80422.6.5 Dual decomposition"}}}}}, "23": {"Title": "Monte Carlo inference", "Sub Topics": {"23.1": {"Title": "Introduction 81523.2 Sampling from standard distributions", "Sub Topics": {}}, "23.3": {"Title": "Rejection sampling", "Sub Topics": {"23.3.1": {"Title": "Basic idea 81723.3.2 Example 81823.3.3 Application to Bayesian statistics 81923.3.4 Adaptive rejection sampling 81923.3.5 Rejection sampling in high dimensions"}}}, "23.4": {"Title": "Importance sampling", "Sub Topics": {"23.4.1": {"Title": "Basic idea 82023.4.2 Handling unnormalized distributions 82123.4.3 Importance sampling for a DGM: likelihood weighting 82223.4.4 Sampling importance resampling (SIR)"}}}, "23.5": {"Title": "Particle \ufb01ltering", "Sub Topics": {"23.5.1": {"Title": "Sequential importance sampling 82423.5.2 The degeneracy problem 82523.5.3 The resampling step 82523.5.4 The proposal distribution 82723.5.5 Application: robot localization 82823.5.6 Application: visual object tracking 82823.5.7 Application: time series forecasting"}}}, "23.6": {"Title": "Rao-Blackwellised particle \ufb01ltering (RBPF)", "Sub Topics": {"23.6.1": {"Title": "RBPF for switching LG-SSMs 83123.6.2 Application: tracking a maneuvering target 83223.6.3 Application: Fast SLAM"}}}}}, "24": {"Title": "Markov chain Monte Carlo (MCMC) inference", "Sub Topics": {"24.1": {"Title": "Introduction", "Sub Topics": {}}, "24.2": {"Title": "Gibbs sampling", "Sub Topics": {"24.2.1": {"Title": "Basic idea 83824.2.2 Example: Gibbs sampling for the Ising model 83824.2.3 Example: Gibbs sampling for inferring the parameters of a GMM 84024.2.4 Collapsed Gibbs sampling * 84124.2.5 Gibbs sampling for hierarchical GLMs 84424.2.6 BUGS and JAGS 84624.2.7 The Imputation Posterior (IP) algorithm 84724.2.8 Blocking Gibbs sampling"}}}, "24.3": {"Title": "Metropolis Hastings algorithm", "Sub Topics": {"24.3.1": {"Title": "Basic idea 84824.3.2 Gibbs sampling is a special case of MH 84924.3.3 Proposal distributions 85024.3.4 Adaptive MCMC 85324.3.5 Initialization and mode hopping 85424.3.6 Why MH works * 85424.3.7 Reversible jump (trans-dimensional) MCMC *"}}}, "24.4": {"Title": "Speed and accuracy of MCMC", "Sub Topics": {"24.4.1": {"Title": "The burn-in phase 85624.4.2 Mixing rates of Markov chains * 85724.4.3 Practical convergence diagnostics 85824.4.4 Accuracy of MCMC 86024.4.5 How many chains?"}}}, "24.5": {"Title": "Auxiliary variable MCMC *", "Sub Topics": {"24.5.1": {"Title": "Auxiliary variable sampling for logistic regression 86324.5.2 Slice sampling 86424.5.3 Swendsen Wang 86624.5.4 Hybrid/Hamiltonian MCMC *"}}}, "24.6": {"Title": "Annealing methods", "Sub Topics": {"24.6.1": {"Title": "Simulated annealing 86924.6.2 Annealed importance sampling 87124.6.3 Parallel tempering"}}}, "24.7": {"Title": "Approximating the marginal likelihood", "Sub Topics": {"24.7.1": {"Title": "The candidate method 87224.7.2 Harmonic mean estimate 87224.7.3 Annealed importance sampling"}}}}}, "25": {"Title": "Clustering", "Sub Topics": {"25.1": {"Title": "Introduction", "Sub Topics": {"25.1.1": {"Title": "Measuring (dis)similarity 87525.1.2 Evaluating the output of clustering methods *"}}}, "25.2": {"Title": "Dirichlet process mixture models", "Sub Topics": {"25.2.1": {"Title": "From \ufb01nite to in\ufb01nite mixture models 87925.2.2 The Dirichlet process"}, "25.2.3": {"Title": "Applying Dirichlet processes to mixture modeling"}, "25.2.4": {"Title": "Fitting a DP mixture model"}}}, "25.3": {"Title": "Affinity propagation 88725.4 Spectral clustering", "Sub Topics": {}}, "25.5": {"Title": "Hierarchical clustering", "Sub Topics": {"25.5.1": {"Title": "Agglomerative clustering 89525.5.2 Divisive clustering 89825.5.3 Choosing the number of clusters 89925.5.4 Bayesian hierarchical clustering"}}}, "25.6": {"Title": "Clustering datapoints and features", "Sub Topics": {"25.6.1": {"Title": "Biclustering 90325.6.2 Multi-view clustering"}}}}}, "26": {"Title": "Graphical model structure learning", "Sub Topics": {"26.1": {"Title": "Introduction 90726.2 Structure learning for knowledge discovery", "Sub Topics": {}}, "26.3": {"Title": "Learning tree structures", "Sub Topics": {"26.3.1": {"Title": "Directed or undirected tree? 91126.3.2 Chow-Liu algorithm for \ufb01nding the ML tree structure 91226.3.3 Finding the MAP forest 91226.3.4 Mixtures of trees"}}}, "26.4": {"Title": "Learning DAG structures", "Sub Topics": {"26.4.1": {"Title": "Markov equivalence 91426.4.2 Exact structural inference 91626.4.3 Scaling up to larger graphs"}}}, "26.5": {"Title": "Learning DAG structure with latent variables", "Sub Topics": {"26.5.1": {"Title": "Approximating the marginal likelihood when we have missing data 92226.5.2 Structural EM 92526.5.3 Discovering hidden variables 92626.5.4 Case study: Google\u2019s Rephil 92826.5.5 Structural equation models *"}}}, "26.6": {"Title": "Learning causal DAGs", "Sub Topics": {"26.6.1": {"Title": "Causal interpretation of DAGs 93126.6.2 Using causal DAGs to resolve Simpson\u2019s paradox 93326.6.3 Learning causal DAG structures"}}}, "26.7": {"Title": "Learning undirected Gaussian graphical models", "Sub Topics": {"26.7.1": {"Title": "MLE for a GGM 93826.7.2 Graphical lasso 93926.7.3 Bayesian inference for GGM structure * 94126.7.4 Handling non-Gaussian data using copulas *"}}}, "26.8": {"Title": "Learning undirected discrete graphical models", "Sub Topics": {"26.8.1": {"Title": "Graphical lasso for MRFs/CRFs"}, "26.8.2": {"Title": "Thin junction trees"}}}}}, "27": {"Title": "Latent variable models for discrete data", "Sub Topics": {"27.1": {"Title": "Introduction 94527.2 Distributed state LVMs for discrete data", "Sub Topics": {}}, "27.3": {"Title": "Latent Dirichlet allocation (LDA)", "Sub Topics": {"27.3.1": {"Title": "Basics 95027.3.2 Unsupervised discovery of topics 95327.3.3 Quantitatively evaluating LDA as a language model 95327.3.4 Fitting using (collapsed) Gibbs sampling 95527.3.5 Example 95627.3.6 Fitting using batch variational inference 95727.3.7 Fitting using online variational inference 95927.3.8 Determining the number of topics"}}}, "27.4": {"Title": "Extensions of LDA", "Sub Topics": {"27.4.1": {"Title": "Correlated topic model 96127.4.2 Dynamic topic model 96227.4.3 LDA-HMM 96327.4.4 Supervised LDA"}}}, "27.5": {"Title": "LVMs for graph-structured data", "Sub Topics": {"27.5.1": {"Title": "Stochastic block model 97127.5.2 Mixed membership stochastic block model 97327.5.3 Relational topic model"}}}, "27.6": {"Title": "LVMs for relational data", "Sub Topics": {"27.6.1": {"Title": "In\ufb01nite relational model 97627.6.2 Probabilistic matrix factorization for collaborative \ufb01ltering"}}}, "27.7": {"Title": "Restricted Boltzmann machines (RBMs)", "Sub Topics": {"27.7.1": {"Title": "Varieties of RBMs 98527.7.2 Learning RBMs 98727.7.3 Applications of RBMs"}}}}}, "28": {"Title": "Deep learning", "Sub Topics": {"28.1": {"Title": "Introduction 99528.2 Deep generative models", "Sub Topics": {}}, "28.3": {"Title": "Deep neural networks", "Sub Topics": {"28.3.1": {"Title": "Deep multi-layer perceptrons"}, "28.3.2": {"Title": "Deep auto-encoders 100028.3.3 Stacked denoising auto-encoders"}}}, "28.4": {"Title": "Applications of deep networks", "Sub Topics": {"28.4.1": {"Title": "Handwritten digit classi\ufb01cation using DBNs 100128.4.2 Data visualization and feature discovery using deep auto-encoders 100228.4.3 Information retrieval using deep auto-encoders (semantic hashing) 100328.4.4 Learning audio features using 1d convolutional DBNs 100428.4.5 Learning image features using 2d convolutional DBNs"}}}, "28.5": {"Title": "Discussion", "Sub Topics": {}}}}}