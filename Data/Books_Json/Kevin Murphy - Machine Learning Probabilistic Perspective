{"Name": "Kevin Murphy - Machine Learning Probabilistic Perspective.pdf", "Pages": {"0": "Machine Learning\nA Probabilistic Perspective\nKevin P. Murphy\n\u201c An astonishing machine learning book: intuitive, full  \nof examples, fun to read but still comprehensive, \nstrong, and deep! A great starting point for any univer-\nsity student\u2014and a must-have for anybody in the field. \u201d\nJan Peters, Darmstadt University of Technology;  \nMax-Planck Institute for Intelligent Systems\n\u201cKevin Murphy excels at unraveling the complexities  \nof machine learning methods while motivating the \nreader with a stream of illustrated examples and \nreal-world case studies. The accompanying software \npackage includes source code for many of the figures, \nmaking it both easy and very tempting to dive in and \nexplore these methods for yourself. A must-buy for \nanyone interested in machine learning or curious \nabout how to extract useful knowledge from big data. \u201d\nJohn Winn, Microsoft Research\n\u201cThis is a wonderful book that starts with basic topics \nin statistical modeling, culminating in the most ad-\nvanced topics. It provides both the theoretical foun-\ndations of probabilistic machine learning as well as \npractical tools, in the form of MATLAB code. The book \nshould be on the shelf of any student interested in the \ntopic, and any practitioner working in the field. \u201d\nY oram Singer, Google Research\n\u201cThis book will be an essential reference for practitio-\nners of modern machine learning. It covers the basic \nconcepts needed to understand the field as a whole, \nand the powerful modern methods that build on those \nconcepts. In Machine Learning , the language of prob-\nability and statistics reveals important connections be-\ntween seemingly disparate algorithms and strategies. \nThus, its readers will become articulate in a holistic \nview of the state-of-the-art and poised to build the next \ngeneration of machine learning algorithms. \u201d\nDavid Blei, Princeton University\n machine learning\nMachine Learning\nA Probabilistic Perspective\nKevin P . Murphy\nToday\u2019s Web-enabled deluge of electronic data calls for \nautomated methods of data analysis. Machine learning \nprovides these, developing methods that can automatically \ndetect patterns in data and use the uncovered patterns to \npredict future data. This textbook offers a comprehensive \nand self-contained introduction to the field of machine \nlearning, a unified, probabilistic approach. \nThe coverage combines breadth and depth, offering \nnecessary background material on such topics as probabili-\nty, optimization, and linear algebra as well as discussion of \nrecent developments in the field, including conditional ran-\ndom fields, L1 regularization, and deep learning. The book \nis written in an informal, accessible style, complete with \npseudo-code for the most important algorithms. All topics \nare copiously illustrated with color images and worked \nexamples drawn from such application domains as biology, \ntext processing, computer vision, and robotics. Rather than \nproviding a cookbook of different heuristic methods, the \nbook stresses a principled model-based approach, often \nusing the language of graphical models to specify mod-\nels in a concise and intuitive way. Almost all the models \ndescribed have been implemented in a MATLAB software \npackage\u2014PMTK (probabilistic modeling toolkit)\u2014that is \nfreely available online. The book is suitable for upper-level \nundergraduates with an introductory-level college math \nbackground and beginning graduate students.  \nKevin P . Murphy is a Research Scientist at Google. Previ-\nously, he was Associate Professor of Computer Science \nand Statistics at the University of British Columbia.\nAdaptive Computation and Machine Learning seriesThe MIT Press\nMassachusetts Institute of Technology\nCambridge, Massachusetts 02142\nhttp://mitpress.mit.edu978-0-262-01802-9\nThe cover image is based on sequential Bayesian updating  \nof a 2 D Gaussian distribution. See Figure 7 .11 for details.", "1": "Machine Learning: A Probabilistic Perspective", "2": "", "3": "Machine Learning\nA Probabilistic Perspective\nKevin P. Murphy\nThe MIT Press\nCambridge, MassachusettsLondon, England", "4": "\u00a9 2012 Massachusetts Institute of Technology\nAll rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical\nmeans (including photocopying, recording, or information storage and retrieval) without permission inwriting from the publisher.\nFor information about special quantity discounts, please email special_sales@mitpress.mit.eduThis book was set in the L\nATEX programming language by the author. Printed and bound in the United\nStates of America.Library of Congress Cataloging-in-Publication Information\nMurphy, Kevin P.\nMachine learning : a probabilistic perspective / Kevin P. Murphy.p. cm. \u2014 (Adaptive computation and machine learning series)Includes bibliographical references and index.ISBN 978-0-262-01802-9 (hardcover : alk. paper)1. Machine learning. 2. Probabilities. I. Title.Q325.5.M87 2012006.3\u20191\u2014dc232012004558\n1 0987654321", "5": "This book is dedicated to Alessandro, Michael and Stefano,\nand to the memory of Gerard Joseph Murphy.", "6": "", "7": "Contents\nPreface xxvii\n1 Introduction 1\n1.1 Machine learning: what and why? 1\n1.1.1 Types of machine learning 2\n1.2 Supervised learning 3\n1.2.1 Classi\ufb01cation 3\n1.2.2 Regression 8\n1.3 Unsupervised learning 9\n1.3.1 Discovering clusters 101.3.2 Discovering latent factors 111.3.3 Discovering graph structure 131.3.4 Matrix completion 14\n1.4 Some basic concepts in machine learning 16\n1.4.1 Parametric vs non-parametric models 161.4.2 A simple non-parametric classi\ufb01er: K-nearest neighbors 16\n1.4.3 The curse of dimensionality 181.4.4 Parametric models for classi\ufb01cation and regression 191.4.5 Linear regression 191.4.6 Logistic regression 211.4.7 Over\ufb01tting 221.4.8 Model selection 221.4.9 No free lunch theorem 24\n2 Probability 27\n2.1 Introduction 272.2 A brief review of probability theory 28\n2.2.1 Discrete random variables 282.2.2 Fundamental rules 282.2.3 Bayes rule 292.2.4 Independence and conditional independence 302.2.5 Continuous random variables 32", "8": "viii CONTENTS\n2.2.6 Quantiles 33\n2.2.7 Mean and variance 33\n2.3 Some common discrete distributions 34\n2.3.1 The binomial and Bernoulli distributions 342.3.2 The multinomial and multinoulli distributions 352.3.3 The Poisson distribution 372.3.4 The empirical distribution 37\n2.4 Some common continuous distributions 38\n2.4.1 Gaussian (normal) distribution 382.4.2 Degenerate pdf 392.4.3 The Laplace distribution 412.4.4 The gamma distribution 412.4.5 The beta distribution 422.4.6 Pareto distribution 43\n2.5 Joint probability distributions 44\n2.5.1 Covariance and correlation 442.5.2 The multivariate Gaussian 462.5.3 Multivariate Student tdistribution 46\n2.5.4 Dirichlet distribution 47\n2.6 Transformations of random variables 49\n2.6.1 Linear transformations 492.6.2 General transformations 502.6.3 Central limit theorem 51\n2.7 Monte Carlo approximation 52\n2.7.1 Example: change of variables, the MC way 532.7.2 Example: estimating \u03c0by Monte Carlo integration 54\n2.7.3 Accuracy of Monte Carlo approximation 54\n2.8 Information theory 56\n2.8.1 Entropy 562.8.2 KL divergence 572.8.3 Mutual information 59\n3 Generative models for discrete data 65\n3.1 Introduction 653.2 Bayesian concept learning 65\n3.2.1 Likelihood 673.2.2 Prior 673.2.3 Posterior 683.2.4 Posterior predictive distribution 713.2.5 A more complex prior 72\n3.3 The beta-binomial model 72\n3.3.1 Likelihood 733.3.2 Prior 743.3.3 Posterior 753.3.4 Posterior predictive distribution 77", "9": "CONTENTS ix\n3.4 The Dirichlet-multinomial model 78\n3.4.1 Likelihood 79\n3.4.2 Prior 793.4.3 Posterior 793.4.4 Posterior predictive 81\n3.5 Naive Bayes classi\ufb01ers 82\n3.5.1 Model \ufb01tting 833.5.2 Using the model for prediction 853.5.3 The log-sum-exp trick 863.5.4 Feature selection using mutual information 863.5.5 Classifying documents using bag of words 87\n4 Gaussian models 97\n4.1 Introduction 97\n4.1.1 Notation 974.1.2 Basics 974.1.3 MLE for an MVN 994.1.4 Maximum entropy derivation of the Gaussian * 101\n4.2 Gaussian discriminant analysis 101\n4.2.1 Quadratic discriminant analysis (QDA) 1024.2.2 Linear discriminant analysis (LDA) 1034.2.3 Two-class LDA 1044.2.4 MLE for discriminant analysis 1064.2.5 Strategies for preventing over\ufb01tting 1064.2.6 Regularized LDA * 1074.2.7 Diagonal LDA 1084.2.8 Nearest shrunken centroids classi\ufb01er * 109\n4.3 Inference in jointly Gaussian distributions 110\n4.3.1 Statement of the result 1114.3.2 Examples 1114.3.3 Information form 1154.3.4 Proof of the result * 116\n4.4 Linear Gaussian systems 119\n4.4.1 Statement of the result 1194.4.2 Examples 1204.4.3 Proof of the result * 124\n4.5 Digression: The Wishart distribution * 125\n4.5.1 Inverse Wishart distribution 1264.5.2 Visualizing the Wishart distribution * 127\n4.6 Inferring the parameters of an MVN 127\n4.6.1 Posterior distribution of \u03bc128\n4.6.2 Posterior distribution of \u03a3* 128\n4.6.3 Posterior distribution of \u03bcand\u03a3* 132\n4.6.4 Sensor fusion with unknown precisions * 138", "10": "x CONTENTS\n5 Bayesian statistics 149\n5.1 Introduction 149\n5.2 Summarizing posterior distributions 149\n5.2.1 MAP estimation 1495.2.2 Credible intervals 1525.2.3 Inference for a difference in proportions 154\n5.3 Bayesian model selection 155\n5.3.1 Bayesian Occam\u2019s razor 1565.3.2 Computing the marginal likelihood (evidence) 1585.3.3 Bayes factors 1635.3.4 Jeffreys-Lindley paradox * 164\n5.4 Priors 165\n5.4.1 Uninformative priors 1655.4.2 Jeffreys priors * 1665.4.3 Robust priors 1685.4.4 Mixtures of conjugate priors 168\n5.5 Hierarchical Bayes 171\n5.5.1 Example: modeling related cancer rates 171\n5.6 Empirical Bayes 172\n5.6.1 Example: beta-binomial model 1735.6.2 Example: Gaussian-Gaussian model 173\n5.7 Bayesian decision theory 176\n5.7.1 Bayes estimators for common loss functions 1775.7.2 The false positive vs false negative tradeoff 1805.7.3 Other topics * 184\n6 Frequentist statistics 191\n6.1 Introduction 1916.2 Sampling distribution of an estimator 191\n6.2.1 Bootstrap 1926.2.2 Large sample theory for the MLE * 193\n6.3 Frequentist decision theory 194\n6.3.1 Bayes risk 1956.3.2 Minimax risk 1966.3.3 Admissible estimators 197\n6.4 Desirable properties of estimators 200\n6.4.1 Consistent estimators 2006.4.2 Unbiased estimators 2006.4.3 Minimum variance estimators 2016.4.4 The bias-variance tradeoff 202\n6.5 Empirical risk minimization 204\n6.5.1 Regularized risk minimization 2056.5.2 Structural risk minimization 2066.5.3 Estimating the risk using cross validation 2066.5.4 Upper bounding the risk using statistical learning theory * 209", "11": "CONTENTS xi\n6.5.5 Surrogate loss functions 210\n6.6 Pathologies of frequentist statistics * 211\n6.6.1 Counter-intuitive behavior of con\ufb01dence intervals 212\n6.6.2 p-values considered harmful 2136.6.3 The likelihood principle 2146.6.4 Why isn\u2019t everyone a Bayesian? 215\n7 Linear regression 217\n7.1 Introduction 2177.2 Model speci\ufb01cation 2177.3 Maximum likelihood estimation (least squares) 217\n7.3.1 Derivation of the MLE 2197.3.2 Geometric interpretation 2207.3.3 Convexity 221\n7.4 Robust linear regression * 2237.5 Ridge regression 225\n7.5.1 Basic idea 2257.5.2 Numerically stable computation * 2277.5.3 Connection with PCA * 2287.5.4 Regularization effects of big data 230\n7.6 Bayesian linear regression 231\n7.6.1 Computing the posterior 2327.6.2 Computing the posterior predictive 2337.6.3 Bayesian inference when \u03c3\n2is unknown * 234\n7.6.4 EB for linear regression (evidence procedure) 238\n8 Logistic regression 245\n8.1 Introduction 2458.2 Model speci\ufb01cation 2458.3 Model \ufb01tting 245\n8.3.1 MLE 2468.3.2 Steepest descent 2478.3.3 Newton\u2019s method 2498.3.4 Iteratively reweighted least squares (IRLS) 2508.3.5 Quasi-Newton (variable metric) methods 2518.3.6 /lscript\n2regularization 252\n8.3.7 Multi-class logistic regression 252\n8.4 Bayesian logistic regression 254\n8.4.1 Laplace approximation 2558.4.2 Derivation of the BIC 2558.4.3 Gaussian approximation for logistic regression 2568.4.4 Approximating the posterior predictive 2568.4.5 Residual analysis (outlier detection) * 260\n8.5 Online learning and stochastic optimization 261\n8.5.1 Online learning and regret minimization 262", "12": "xii CONTENTS\n8.5.2 Stochastic optimization and risk minimization 262\n8.5.3 The LMS algorithm 2648.5.4 The perceptron algorithm 2658.5.5 A Bayesian view 266\n8.6 Generative vs discriminative classi\ufb01ers 267\n8.6.1 Pros and cons of each approach 2688.6.2 Dealing with missing data 2698.6.3 Fisher\u2019s linear discriminant analysis (FLDA) * 271\n9 Generalized linear models and the exponential family 281\n9.1 Introduction 2819.2 The exponential family 281\n9.2.1 De\ufb01nition 2829.2.2 Examples 2829.2.3 Log partition function 2849.2.4 MLE for the exponential family 2869.2.5 Bayes for the exponential family * 2879.2.6 Maximum entropy derivation of the exponential family * 289\n9.3 Generalized linear models (GLMs) 290\n9.3.1 Basics 2909.3.2 ML and MAP estimation 2929.3.3 Bayesian inference 293\n9.4 Probit regression 293\n9.4.1 ML/MAP estimation using gradient-based optimization 2949.4.2 Latent variable interpretation 2949.4.3 Ordinal probit regression * 2959.4.4 Multinomial probit models * 295\n9.5 Multi-task learning 296\n9.5.1 Hierarchical Bayes for multi-task learning 2969.5.2 Application to personalized email spam \ufb01ltering 2969.5.3 Application to domain adaptation 2979.5.4 Other kinds of prior 297\n9.6 Generalized linear mixed models * 298\n9.6.1 Example: semi-parametric GLMMs for medical data 2989.6.2 Computational issues 300\n9.7 Learning to rank * 300\n9.7.1 The pointwise approach 3019.7.2 The pairwise approach 3019.7.3 The listwise approach 3029.7.4 Loss functions for ranking 303\n10 Directed graphical models (Bayes nets) 307\n10.1 Introduction 307\n10.1.1 Chain rule 30710.1.2 Conditional independence 308", "13": "CONTENTS xiii\n10.1.3 Graphical models 308\n10.1.4 Graph terminology 30910.1.5 Directed graphical models 310\n10.2 Examples 311\n10.2.1 Naive Bayes classi\ufb01ers 31110.2.2 Markov and hidden Markov models 31210.2.3 Medical diagnosis 31310.2.4 Genetic linkage analysis * 31510.2.5 Directed Gaussian graphical models * 318\n10.3 Inference 31910.4 Learning 320\n10.4.1 Plate notation 32010.4.2 Learning from complete data 32210.4.3 Learning with missing and/or latent variables 323\n10.5 Conditional independence properties of DGMs 324\n10.5.1 d-separation and the Bayes Ball algorithm (global Markov\nproperties) 324\n10.5.2 Other Markov properties of DGMs 32710.5.3 Markov blanket and full conditionals 327\n10.6 In\ufb02uence (decision) diagrams * 328\n11 Mixture models and the EM algorithm 337\n11.1 Latent variable models 33711.2 Mixture models 337\n11.2.1 Mixtures of Gaussians 33911.2.2 Mixture of multinoullis 34011.2.3 Using mixture models for clustering 34011.2.4 Mixtures of experts 342\n11.3 Parameter estimation for mixture models 345\n11.3.1 Unidenti\ufb01ability 34611.3.2 Computing a MAP estimate is non-convex 347\n11.4 The EM algorithm 348\n11.4.1 Basic idea 34911.4.2 EM for GMMs 35011.4.3 EM for mixture of experts 35711.4.4 EM for DGMs with hidden variables 35811.4.5 EM for the Student distribution * 35911.4.6 EM for probit regression * 36211.4.7 Theoretical basis for EM * 36311.4.8 Online EM 36511.4.9 Other EM variants * 367\n11.5 Model selection for latent variable models 370\n11.5.1 Model selection for probabilistic models 37011.5.2 Model selection for non-probabilistic methods 370\n11.6 Fitting models with missing data 372", "14": "xiv CONTENTS\n11.6.1 EM for the MLE of an MVN with missing data 373\n12 Latent linear models 381\n12.1 Factor analysis 381\n12.1.1 FA is a low rank parameterization of an MVN 381\n12.1.2 Inference of the latent factors 38212.1.3 Unidenti\ufb01ability 38312.1.4 Mixtures of factor analysers 38512.1.5 EM for factor analysis models 38612.1.6 Fitting FA models with missing data 387\n12.2 Principal components analysis (PCA) 387\n12.2.1 Classical PCA: statement of the theorem 38712.2.2 Proof * 38912.2.3 Singular value decomposition (SVD) 39212.2.4 Probabilistic PCA 39512.2.5 EM algorithm for PCA 396\n12.3 Choosing the number of latent dimensions 398\n12.3.1 Model selection for FA/PPCA 39812.3.2 Model selection for PCA 399\n12.4 PCA for categorical data 40212.5 PCA for paired and multi-view data 404\n12.5.1 Supervised PCA (latent factor regression) 40512.5.2 Partial least squares 40612.5.3 Canonical correlation analysis 407\n12.6 Independent Component Analysis (ICA) 407\n12.6.1 Maximum likelihood estimation 41012.6.2 The FastICA algorithm 41112.6.3 Using EM 41412.6.4 Other estimation principles * 415\n13 Sparse linear models 421\n13.1 Introduction 42113.2 Bayesian variable selection 422\n13.2.1 The spike and slab model 42413.2.2 From the Bernoulli-Gaussian model to /lscript\n0regularization 425\n13.2.3 Algorithms 426\n13.3/lscript1regularization: basics 429\n13.3.1 Why does /lscript1regularization yield sparse solutions? 430\n13.3.2 Optimality conditions for lasso 43113.3.3 Comparison of least squares, lasso, ridge and subset selection 43513.3.4 Regularization path 43613.3.5 Model selection 43913.3.6 Bayesian inference for linear models with Laplace priors 440\n13.4/lscript\n1regularization: algorithms 441\n13.4.1 Coordinate descent 441", "15": "CONTENTS xv\n13.4.2 LARS and other homotopy methods 441\n13.4.3 Proximal and gradient projection methods 44213.4.4 EM for lasso 447\n13.5/lscript\n1regularization: extensions 449\n13.5.1 Group Lasso 44913.5.2 Fused lasso 45413.5.3 Elastic net (ridge and lasso combined) 455\n13.6 Non-convex regularizers 457\n13.6.1 Bridge regression 45813.6.2 Hierarchical adaptive lasso 45813.6.3 Other hierarchical priors 462\n13.7 Automatic relevance determination (ARD)/sparse Bayesian learning (SBL) 463\n13.7.1 ARD for linear regression 46313.7.2 Whence sparsity? 46513.7.3 Connection to MAP estimation 46513.7.4 Algorithms for ARD * 46613.7.5 ARD for logistic regression 468\n13.8 Sparse coding * 468\n13.8.1 Learning a sparse coding dictionary 46913.8.2 Results of dictionary learning from image patches 47013.8.3 Compressed sensing 47213.8.4 Image inpainting and denoising 472\n14 Kernels 479\n14.1 Introduction 47914.2 Kernel functions 479\n14.2.1 RBF kernels 48014.2.2 Kernels for comparing documents 48014.2.3 Mercer (positive de\ufb01nite) kernels 48114.2.4 Linear kernels 48214.2.5 Matern kernels 48214.2.6 String kernels 48314.2.7 Pyramid match kernels 48414.2.8 Kernels derived from probabilistic generative models 485\n14.3 Using kernels inside GLMs 486\n14.3.1 Kernel machines 48614.3.2 L1VMs, RVMs, and other sparse vector machines 487\n14.4 The kernel trick 488\n14.4.1 Kernelized nearest neighbor classi\ufb01cation 48914.4.2 Kernelized K-medoids clustering 48914.4.3 Kernelized ridge regression 49214.4.4 Kernel PCA 493\n14.5 Support vector machines (SVMs) 496\n14.5.1 SVMs for regression 49714.5.2 SVMs for classi\ufb01cation 498", "16": "xvi CONTENTS\n14.5.3 Choosing C504\n14.5.4 Summary of key points 504\n14.5.5 A probabilistic interpretation of SVMs 505\n14.6 Comparison of discriminative kernel methods 50514.7 Kernels for building generative models 507\n14.7.1 Smoothing kernels 50714.7.2 Kernel density estimation (KDE) 50814.7.3 From KDE to KNN 50914.7.4 Kernel regression 51014.7.5 Locally weighted regression 512\n15 Gaussian processes 515\n15.1 Introduction 51515.2 GPs for regression 516\n15.2.1 Predictions using noise-free observations 51715.2.2 Predictions using noisy observations 51815.2.3 Effect of the kernel parameters 51915.2.4 Estimating the kernel parameters 52115.2.5 Computational and numerical issues * 52415.2.6 Semi-parametric GPs * 524\n15.3 GPs meet GLMs 525\n15.3.1 Binary classi\ufb01cation 52515.3.2 Multi-class classi\ufb01cation 52815.3.3 GPs for Poisson regression 531\n15.4 Connection with other methods 532\n15.4.1 Linear models compared to GPs 53215.4.2 Linear smoothers compared to GPs 53315.4.3 SVMs compared to GPs 53415.4.4 L1VM and RVMs compared to GPs 53415.4.5 Neural networks compared to GPs 53515.4.6 Smoothing splines compared to GPs * 53615.4.7 RKHS methods compared to GPs * 538\n15.5 GP latent variable model 54015.6 Approximation methods for large datasets 542\n16 Adaptive basis function models 543\n16.1 Introduction 54316.2 Classi\ufb01cation and regression trees (CART) 544\n16.2.1 Basics 54416.2.2 Growing a tree 54516.2.3 Pruning a tree 54916.2.4 Pros and cons of trees 55016.2.5 Random forests 55016.2.6 CART compared to hierarchical mixture of experts * 551\n16.3 Generalized additive models 552", "17": "CONTENTS xvii\n16.3.1 Back\ufb01tting 552\n16.3.2 Computational efficiency 55316.3.3 Multivariate adaptive regression splines (MARS) 553\n16.4 Boosting 554\n16.4.1 Forward stagewise additive modeling 55516.4.2 L2boosting 55716.4.3 AdaBoost 55816.4.4 LogitBoost 55916.4.5 Boosting as functional gradient descent 56016.4.6 Sparse boosting 56116.4.7 Multivariate adaptive regression trees (MART) 56216.4.8 Why does boosting work so well? 56216.4.9 A Bayesian view 563\n16.5 Feedforward neural networks (multilayer perceptrons) 563\n16.5.1 Convolutional neural networks 56416.5.2 Other kinds of neural networks 56816.5.3 A brief history of the \ufb01eld 56816.5.4 The backpropagation algorithm 56916.5.5 Identi\ufb01ability 57216.5.6 Regularization 57216.5.7 Bayesian inference * 576\n16.6 Ensemble learning 580\n16.6.1 Stacking 58016.6.2 Error-correcting output codes 58116.6.3 Ensemble learning is not equivalent to Bayes model averaging 581\n16.7 Experimental comparison 582\n16.7.1 Low-dimensional features 58216.7.2 High-dimensional features 583\n16.8 Interpreting black-box models 585\n17 Markov and hidden Markov models 589\n17.1 Introduction 58917.2 Markov models 589\n17.2.1 Transition matrix 58917.2.2 Application: Language modeling 59117.2.3 Stationary distribution of a Markov chain * 59617.2.4 Application: Google\u2019s PageRank algorithm for web page ranking * 600\n17.3 Hidden Markov models 603\n17.3.1 Applications of HMMs 604\n17.4 Inference in HMMs 606\n17.4.1 Types of inference problems for temporal models 60617.4.2 The forwards algorithm 60917.4.3 The forwards-backwards algorithm 61017.4.4 The Viterbi algorithm 61217.4.5 Forwards \ufb01ltering, backwards sampling 616", "18": "xviii CONTENTS\n17.5 Learning for HMMs 617\n17.5.1 Training with fully observed data 617\n17.5.2 EM for HMMs (the Baum-Welch algorithm) 61817.5.3 Bayesian methods for \u201c\ufb01tting\u201d HMMs * 62017.5.4 Discriminative training 62017.5.5 Model selection 621\n17.6 Generalizations of HMMs 621\n17.6.1 Variable duration (semi-Markov) HMMs 62217.6.2 Hierarchical HMMs 62417.6.3 Input-output HMMs 62517.6.4 Auto-regressive and buried HMMs 62617.6.5 Factorial HMM 62717.6.6 Coupled HMM and the in\ufb02uence model 62817.6.7 Dynamic Bayesian networks (DBNs) 628\n18 State space models 631\n18.1 Introduction 63118.2 Applications of SSMs 632\n18.2.1 SSMs for object tracking 63218.2.2 Robotic SLAM 63318.2.3 Online parameter learning using recursive least squares 63618.2.4 SSM for time series forecasting * 637\n18.3 Inference in LG-SSM 640\n18.3.1 The Kalman \ufb01ltering algorithm 64018.3.2 The Kalman smoothing algorithm 643\n18.4 Learning for LG-SSM 646\n18.4.1 Identi\ufb01ability and numerical stability 64618.4.2 Training with fully observed data 64718.4.3 EM for LG-SSM 64718.4.4 Subspace methods 64718.4.5 Bayesian methods for \u201c\ufb01tting\u201d LG-SSMs 647\n18.5 Approximate online inference for non-linear, non-Gaussian SSMs 647\n18.5.1 Extended Kalman \ufb01lter (EKF) 64818.5.2 Unscented Kalman \ufb01lter (UKF) 65018.5.3 Assumed density \ufb01ltering (ADF) 652\n18.6 Hybrid discrete/continuous SSMs 655\n18.6.1 Inference 65618.6.2 Application: data association and multi-target tracking 65818.6.3 Application: fault diagnosis 65918.6.4 Application: econometric forecasting 660\n19 Undirected graphical models (Markov random \ufb01elds) 661\n19.1 Introduction 66119.2 Conditional independence properties of UGMs 661\n19.2.1 Key properties 661", "19": "CONTENTS xix\n19.2.2 An undirected alternative to d-separation 663\n19.2.3 Comparing directed and undirected graphical models 664\n19.3 Parameterization of MRFs 665\n19.3.1 The Hammersley-Clifford theorem 66519.3.2 Representing potential functions 667\n19.4 Examples of MRFs 668\n19.4.1 Ising model 66819.4.2 Hop\ufb01eld networks 66919.4.3 Potts model 67119.4.4 Gaussian MRFs 67219.4.5 Markov logic networks * 674\n19.5 Learning 676\n19.5.1 Training maxent models using gradient methods 67619.5.2 Training partially observed maxent models 67719.5.3 Approximate methods for computing the MLEs of MRFs 67819.5.4 Pseudo likelihood 67819.5.5 Stochastic maximum likelihood 67919.5.6 Feature induction for maxent models * 68019.5.7 Iterative proportional \ufb01tting (IPF) * 681\n19.6 Conditional random \ufb01elds (CRFs) 684\n19.6.1 Chain-structured CRFs, MEMMs and the label-bias problem 68419.6.2 Applications of CRFs 68619.6.3 CRF training 692\n19.7 Structural SVMs 693\n19.7.1 SSVMs: a probabilistic view 69319.7.2 SSVMs: a non-probabilistic view 69519.7.3 Cutting plane methods for \ufb01tting SSVMs 69819.7.4 Online algorithms for \ufb01tting SSVMs 70019.7.5 Latent structural SVMs 701\n20 Exact inference for graphical models 707\n20.1 Introduction 70720.2 Belief propagation for trees 707\n20.2.1 Serial protocol 70720.2.2 Parallel protocol 70920.2.3 Gaussian BP * 71020.2.4 Other BP variants * 712\n20.3 The variable elimination algorithm 714\n20.3.1 The generalized distributive law * 71720.3.2 Computational complexity of VE 71720.3.3 A weakness of VE 720\n20.4 The junction tree algorithm * 720\n20.4.1 Creating a junction tree 72020.4.2 Message passing on a junction tree 72220.4.3 Computational complexity of JTA 725", "20": "xx CONTENTS\n20.4.4 JTA generalizations * 726\n20.5 Computational intractability of exact inference in the worst case 726\n20.5.1 Approximate inference 727\n21 Variational inference 731\n21.1 Introduction 731\n21.2 Variational inference 732\n21.2.1 Alternative interpretations of the variational objective 73321.2.2 Forward or reverse KL? * 733\n21.3 The mean \ufb01eld method 735\n21.3.1 Derivation of the mean \ufb01eld update equations 73621.3.2 Example: mean \ufb01eld for the Ising model 737\n21.4 Structured mean \ufb01eld * 739\n21.4.1 Example: factorial HMM 740\n21.5 Variational Bayes 742\n21.5.1 Example: VB for a univariate Gaussian 74221.5.2 Example: VB for linear regression 746\n21.6 Variational Bayes EM 749\n21.6.1 Example: VBEM for mixtures of Gaussians * 750\n21.7 Variational message passing and VIBES 75621.8 Local variational bounds * 756\n21.8.1 Motivating applications 75621.8.2 Bohning\u2019s quadratic bound to the log-sum-exp function 75821.8.3 Bounds for the sigmoid function 76021.8.4 Other bounds and approximations to the log-sum-exp function * 76221.8.5 Variational inference based on upper bounds 763\n22 More variational inference 767\n22.1 Introduction 76722.2 Loopy belief propagation: algorithmic issues 767\n22.2.1 A brief history 76722.2.2 LBP on pairwise models 76822.2.3 LBP on a factor graph 76922.2.4 Convergence 77122.2.5 Accuracy of LBP 77422.2.6 Other speedup tricks for LBP * 775\n22.3 Loopy belief propagation: theoretical issues * 776\n22.3.1 UGMs represented in exponential family form 77622.3.2 The marginal polytope 77722.3.3 Exact inference as a variational optimization problem 77822.3.4 Mean \ufb01eld as a variational optimization problem 77922.3.5 LBP as a variational optimization problem 77922.3.6 Loopy BP vs mean \ufb01eld 783\n22.4 Extensions of belief propagation * 783\n22.4.1 Generalized belief propagation 783", "21": "CONTENTS xxi\n22.4.2 Convex belief propagation 785\n22.5 Expectation propagation 787\n22.5.1 EP as a variational inference problem 788\n22.5.2 Optimizing the EP objective using moment matching 78922.5.3 EP for the clutter problem 79122.5.4 LBP is a special case of EP 79222.5.5 Ranking players using TrueSkill 79322.5.6 Other applications of EP 799\n22.6 MAP state estimation 799\n22.6.1 Linear programming relaxation 79922.6.2 Max-product belief propagation 80022.6.3 Graphcuts 80122.6.4 Experimental comparison of graphcuts and BP 80422.6.5 Dual decomposition 806\n23 Monte Carlo inference 815\n23.1 Introduction 81523.2 Sampling from standard distributions 815\n23.2.1 Using the cdf 81523.2.2 Sampling from a Gaussian (Box-Muller method) 817\n23.3 Rejection sampling 817\n23.3.1 Basic idea 81723.3.2 Example 81823.3.3 Application to Bayesian statistics 81923.3.4 Adaptive rejection sampling 81923.3.5 Rejection sampling in high dimensions 820\n23.4 Importance sampling 820\n23.4.1 Basic idea 82023.4.2 Handling unnormalized distributions 82123.4.3 Importance sampling for a DGM: likelihood weighting 82223.4.4 Sampling importance resampling (SIR) 822\n23.5 Particle \ufb01ltering 823\n23.5.1 Sequential importance sampling 82423.5.2 The degeneracy problem 82523.5.3 The resampling step 82523.5.4 The proposal distribution 82723.5.5 Application: robot localization 82823.5.6 Application: visual object tracking 82823.5.7 Application: time series forecasting 831\n23.6 Rao-Blackwellised particle \ufb01ltering (RBPF) 831\n23.6.1 RBPF for switching LG-SSMs 83123.6.2 Application: tracking a maneuvering target 83223.6.3 Application: Fast SLAM 834\n24 Markov chain Monte Carlo (MCMC) inference 837", "22": "xxii CONTENTS\n24.1 Introduction 837\n24.2 Gibbs sampling 838\n24.2.1 Basic idea 83824.2.2 Example: Gibbs sampling for the Ising model 83824.2.3 Example: Gibbs sampling for inferring the parameters of a GMM 84024.2.4 Collapsed Gibbs sampling * 84124.2.5 Gibbs sampling for hierarchical GLMs 84424.2.6 BUGS and JAGS 84624.2.7 The Imputation Posterior (IP) algorithm 84724.2.8 Blocking Gibbs sampling 847\n24.3 Metropolis Hastings algorithm 848\n24.3.1 Basic idea 84824.3.2 Gibbs sampling is a special case of MH 84924.3.3 Proposal distributions 85024.3.4 Adaptive MCMC 85324.3.5 Initialization and mode hopping 85424.3.6 Why MH works * 85424.3.7 Reversible jump (trans-dimensional) MCMC * 855\n24.4 Speed and accuracy of MCMC 856\n24.4.1 The burn-in phase 85624.4.2 Mixing rates of Markov chains * 85724.4.3 Practical convergence diagnostics 85824.4.4 Accuracy of MCMC 86024.4.5 How many chains? 862\n24.5 Auxiliary variable MCMC * 863\n24.5.1 Auxiliary variable sampling for logistic regression 86324.5.2 Slice sampling 86424.5.3 Swendsen Wang 86624.5.4 Hybrid/Hamiltonian MCMC * 868\n24.6 Annealing methods 868\n24.6.1 Simulated annealing 86924.6.2 Annealed importance sampling 87124.6.3 Parallel tempering 871\n24.7 Approximating the marginal likelihood 872\n24.7.1 The candidate method 87224.7.2 Harmonic mean estimate 87224.7.3 Annealed importance sampling 873\n25 Clustering 875\n25.1 Introduction 875\n25.1.1 Measuring (dis)similarity 87525.1.2 Evaluating the output of clustering methods * 876\n25.2 Dirichlet process mixture models 879\n25.2.1 From \ufb01nite to in\ufb01nite mixture models 87925.2.2 The Dirichlet process 882", "23": "CONTENTS xxiii\n25.2.3 Applying Dirichlet processes to mixture modeling 885\n25.2.4 Fitting a DP mixture model 886\n25.3 Affinity propagation 88725.4 Spectral clustering 890\n25.4.1 Graph Laplacian 89125.4.2 Normalized graph Laplacian 89225.4.3 Example 893\n25.5 Hierarchical clustering 893\n25.5.1 Agglomerative clustering 89525.5.2 Divisive clustering 89825.5.3 Choosing the number of clusters 89925.5.4 Bayesian hierarchical clustering 899\n25.6 Clustering datapoints and features 901\n25.6.1 Biclustering 90325.6.2 Multi-view clustering 903\n26 Graphical model structure learning 907\n26.1 Introduction 90726.2 Structure learning for knowledge discovery 908\n26.2.1 Relevance networks 90826.2.2 Dependency networks 909\n26.3 Learning tree structures 910\n26.3.1 Directed or undirected tree? 91126.3.2 Chow-Liu algorithm for \ufb01nding the ML tree structure 91226.3.3 Finding the MAP forest 91226.3.4 Mixtures of trees 914\n26.4 Learning DAG structures 914\n26.4.1 Markov equivalence 91426.4.2 Exact structural inference 91626.4.3 Scaling up to larger graphs 920\n26.5 Learning DAG structure with latent variables 922\n26.5.1 Approximating the marginal likelihood when we have missing data 92226.5.2 Structural EM 92526.5.3 Discovering hidden variables 92626.5.4 Case study: Google\u2019s Rephil 92826.5.5 Structural equation models * 929\n26.6 Learning causal DAGs 931\n26.6.1 Causal interpretation of DAGs 93126.6.2 Using causal DAGs to resolve Simpson\u2019s paradox 93326.6.3 Learning causal DAG structures 935\n26.7 Learning undirected Gaussian graphical models 938\n26.7.1 MLE for a GGM 93826.7.2 Graphical lasso 93926.7.3 Bayesian inference for GGM structure * 94126.7.4 Handling non-Gaussian data using copulas * 942", "24": "xxiv CONTENTS\n26.8 Learning undirected discrete graphical models 942\n26.8.1 Graphical lasso for MRFs/CRFs 942\n26.8.2 Thin junction trees 944\n27 Latent variable models for discrete data 945\n27.1 Introduction 94527.2 Distributed state LVMs for discrete data 946\n27.2.1 Mixture models 94627.2.2 Exponential family PCA 94727.2.3 LDA and mPCA 94827.2.4 GaP model and non-negative matrix factorization 949\n27.3 Latent Dirichlet allocation (LDA) 950\n27.3.1 Basics 95027.3.2 Unsupervised discovery of topics 95327.3.3 Quantitatively evaluating LDA as a language model 95327.3.4 Fitting using (collapsed) Gibbs sampling 95527.3.5 Example 95627.3.6 Fitting using batch variational inference 95727.3.7 Fitting using online variational inference 95927.3.8 Determining the number of topics 960\n27.4 Extensions of LDA 961\n27.4.1 Correlated topic model 96127.4.2 Dynamic topic model 96227.4.3 LDA-HMM 96327.4.4 Supervised LDA 967\n27.5 LVMs for graph-structured data 970\n27.5.1 Stochastic block model 97127.5.2 Mixed membership stochastic block model 97327.5.3 Relational topic model 974\n27.6 LVMs for relational data 975\n27.6.1 In\ufb01nite relational model 97627.6.2 Probabilistic matrix factorization for collaborative \ufb01ltering 979\n27.7 Restricted Boltzmann machines (RBMs) 983\n27.7.1 Varieties of RBMs 98527.7.2 Learning RBMs 98727.7.3 Applications of RBMs 991\n28 Deep learning 995\n28.1 Introduction 99528.2 Deep generative models 995\n28.2.1 Deep directed networks 99628.2.2 Deep Boltzmann machines 99628.2.3 Deep belief networks 99728.2.4 Greedy layer-wise learning of DBNs 998\n28.3 Deep neural networks 999", "25": "CONTENTS xxv\n28.3.1 Deep multi-layer perceptrons 999\n28.3.2 Deep auto-encoders 100028.3.3 Stacked denoising auto-encoders 1001\n28.4 Applications of deep networks 1001\n28.4.1 Handwritten digit classi\ufb01cation using DBNs 100128.4.2 Data visualization and feature discovery using deep auto-encoders 100228.4.3 Information retrieval using deep auto-encoders (semantic hashing) 100328.4.4 Learning audio features using 1d convolutional DBNs 100428.4.5 Learning image features using 2d convolutional DBNs 1005\n28.5 Discussion 1005\nNotation 1009\nBibliography 1015\nIndexes 1047\nIndex to code 1047Index to keywords 1050", "26": "", "27": "Preface\nIntroduction\nWith the ever increasing amounts of data in electronic form, the need for automated methods\nfor data analysis continues to grow. The goal of machine learning is to develop methods thatcan automatically detect patterns in data, and then to use the uncovered patterns to predictfuture data or other outcomes of interest. Machine learning is thus closely related to the \ufb01eldsof statistics and data mining, but differs slightly in terms of its emphasis and terminology. Thisbook provides a detailed introduction to the \ufb01eld, and includes worked examples drawn fromapplication domains such as molecular biology, text processing, computer vision, and robotics.\nTarget audience\nThis book is suitable for upper-level undergraduate students and beginning graduate students incomputer science, statistics, electrical engineering, econometrics, or any one else who has theappropriate mathematical background. Speci\ufb01cally, the reader is assumed to already be familiarwith basic multivariate calculus, probability, linear algebra, and computer programming. Priorexposure to statistics is helpful but not necessary.\nA probabilistic approach\nThis books adopts the view that the best way to make machines that can learn from data is touse the tools of probability theory, which has been the mainstay of statistics and engineering forcenturies. Probability theory can be applied to any problem involving uncertainty. In machinelearning, uncertainty comes in many forms: what is the best prediction (or decision) given somedata? what is the best model given some data? what measurement should I perform next? etc.\nThe systematic application of probabilistic reasoning to all inferential problems, including\ninferring parameters of statistical models, is sometimes called a Bayesian approach. However,this term tends to elicit very strong reactions (either positive or negative, depending on whoyou ask), so we prefer the more neutral term \u201cprobabilistic approach\u201d. Besides, we will oftenuse techniques such as maximum likelihood estimation, which are not Bayesian methods, butcertainly fall within the probabilistic paradigm.\nRather than describing a cookbook of different heuristic methods, this book stresses a princi-\npled model-based approach to machine learning. For any given model, a variety of algorithms", "28": "xxviii Preface\ncan often be applied. Conversely, any given algorithm can often be applied to a variety of\nmodels. This kind of modularity, where we distinguish model from algorithm, is good pedagogyand good engineering.\nWe will often use the language of graphical models to specify our models in a concise and\nintuitive way. In addition to aiding comprehension, the graph structure aids in developingefficient algorithms, as we will see. However, this book is not primarily about graphical models;it is about probabilistic modeling in general.\nA practical approach\nNearly all of the methods described in this book have been implemented in a MATLABsoftware\npackage called PMTK, which stands for probabilistic modeling toolkit. This is freely available\nfrom pmtk3.googlecode .com(the digit 3 refers to the third edition of the toolkit, which is the\none used in this version of the book). There are also a variety of supporting \ufb01les, written by otherpeople, available at pmtksupport .googlecode .com. These will be downloaded automatically,\nif you follow the setup instructions described on the PMTK website.\nMATLABis a high-level, interactive scripting language ideally suited to numerical computation\nand data visualization, and can be purchased from www.mathworks .com. Some of the code\nrequires the Statistics toolbox, which needs to be purchased separately. There is also a freeversion of Matlab called Octave, available at http://www .gnu.org/software/octave/ , which\nsupports most of the functionality of MATLAB. Some (but not all) of the code in this book also\nworks in Octave. See the PMTK website for details.\nPMTK was used to generate many of the \ufb01gures in this book; the source code for these \ufb01gures\nis included on the PMTK website, allowing the reader to easily see the effects of changing thedata or algorithm or parameter settings. The book refers to \ufb01les by name, e.g., naiveBayesFit .\nIn order to \ufb01nd the corresponding \ufb01le, you can use two methods: within Matlab you can typewhich naiveBayesFit and it will return the full path to the \ufb01le; or, if you do not have Matlab\nbut want to read the source code anyway, you can use your favorite search engine, which shouldreturn the corresponding \ufb01le from the pmtk3.googlecode .comwebsite.\nDetails on how to use PMTK can be found on the website, which will be udpated over time.\nDetails on the underlying theory behind these methods can be found in this book.\nAcknowledgments\nA book this large is obviously a team effort. I would especially like to thank the following people:my wife Margaret, for keeping the home \ufb01res burning as I toiled away in my office for the last sixyears; Matt Dunham, who created many of the \ufb01gures in this book, and who wrote much of thecode in PMTK; Baback Moghaddam, who gave extremely detailed feedback on every page of anearlier draft of the book; Chris Williams, who also gave very detailed feedback; Cody Severinskiand Wei-Lwun Lu, who assisted with \ufb01gures; generations of UBC students, who gave helpfulcomments on earlier drafts; Daphne Koller, Nir Friedman, and Chris Manning, for letting me usetheir latex style \ufb01les; Stanford University, Google Research and Skyline College for hosting meduring part of my sabbatical; and various Canadian funding agencies (NSERC, CRC and CIFAR)who have supported me \ufb01nancially over the years.\nIn addition, I would like to thank the following people for giving me helpful feedback on", "29": "Preface xxix\nparts of the book, and/or for sharing \ufb01gures, code, exercises or even (in some cases) text: David\nBlei, Hannes Bretschneider, Greg Corrado, Arnaud Doucet, Mario Figueiredo, Nando de Freitas,Mark Girolami, Gabriel Goh, Tom Griffiths, Katherine Heller, Geoff Hinton, Aapo Hyvarinen,Tommi Jaakkola, Mike Jordan, Charles Kemp, Emtiyaz Khan, Bonnie Kirkpatrick, Daphne Koller,Zico Kolter, Honglak Lee, Julien Mairal, Andrew McPherson, Tom Minka, Ian Nabney, ArthurPope, Carl Rassmussen, Ryan Rifkin, Ruslan Salakhutdinov, Mark Schmidt, Daniel Selsam, DavidSontag, Erik Sudderth, Josh Tenenbaum, Kai Yu, Martin Wainwright, Yair Weiss.\nKevin Patrick Murphy\nPalo Alto, CaliforniaJune 2012", "30": "", "31": "1Introduction\n1.1 Machine learning: what and why?\nWe are drowning in information and starving for knowledge. \u2014 John Naisbitt.\nWe are entering the era of big data. For example, there are about 1 trillion web pages1; one\nhour of video is uploaded to YouTube every second, amounting to 10 years of content every\nday2; the genomes of 1000s of people, each of which has a length of 3.8\u00d7109base pairs, have\nbeen sequenced by various labs; Walmart handles more than 1M transactions per hour and hasdatabases containing more than 2.5 petabytes (2.5 \u00d710\n15) of information (Cukier 2010); and so\non.\nThis deluge of data calls for automated methods of data analysis, which is what machine\nlearning provides. In particular, we de\ufb01ne machine learning as a set of methods that can\nautomatically detect patterns in data, and then use the uncovered patterns to predict futuredata, or to perform other kinds of decision making under uncertainty (such as planning how tocollect more data!).\nThis books adopts the view that the best way to solve such problems is to use the tools\nof probability theory. Probability theory can be applied to any problem involving uncertainty.In machine learning, uncertainty comes in many forms: what is the best prediction about thefuture given some past data? what is the best model to explain some data? what measurementshould I perform next? etc. The probabilistic approach to machine learning is closely related tothe \ufb01eld of statistics, but differs slightly in terms of its emphasis and terminology\n3.\nWe will describe a wide variety of probabilistic models, suitable for a wide variety of data and\ntasks. We will also describe a wide variety of algorithms for learning and using such models.The goal is not to develop a cook book of ad hoc techiques, but instead to present a uni\ufb01edview of the \ufb01eld through the lens of probabilistic modeling and inference. Although we will payattention to computational efficiency, details on how to scale these methods to truly massivedatasets are better described in other books, such as (Rajaraman and Ullman 2011; Bekkermanet al. 2011).\n1.http://googleblog .blogspot .com/2008/07/we-knew-web-was-big .html\n2. Source: http://www .youtube .com/t/press_statistics .\n3. Rob Tibshirani, a statistician at Stanford university, has created an amusing comparison between machine learning\nand statistics, available at http://www-stat .stanford .edu/~tibs/stat315a/glossary .pdf.", "32": "2 Chapter 1. Introduction\nIt should be noted, however, that even when one has an apparently massive data set, the\neffective number of data points for certain cases of interest might be quite small. In fact, data\nacross a variety of domains exhibits a property known as the long tail, which means that a\nfew things (e.g., words) are very common, but most things are quite rare (see Section 2.4.6 fordetails). For example, 20% of Google searches each day have never been seen before\n4. This\nmeans that the core statistical issues that we discuss in this book, concerning generalizing fromrelatively small samples sizes, are still very relevant even in the big data era.\n1.1.1 Types of machine learning\nMachine learning is usually divided into two main types. In the predictive orsupervised\nlearning approach, the goal is to learn a mapping from inputs xto outputs y, given a labeled\nset of input-output pairs D={(xi,yi)}N\ni=1.H e r eDis called the training set, and Nis the\nnumber of training examples.\nIn the simplest setting, each training input xiis aD-dimensional vector of numbers, rep-\nresenting, say, the height and weight of a person. These are called features, attributes or\ncovariates. In general, however, xicould be a complex structured object, such as an image, a\nsentence, an email message, a time series, a molecular shape, a graph, etc.\nSimilarly the form of the output or response variable can in principle be anything, but\nmost methods assume that yiis a categorical ornominal variable from some \ufb01nite set,\nyi\u2208{1,...,C}(such as male or female), or that yiis a real-valued scalar (such as income\nlevel). When yiis categorical, the problem is known as classi\ufb01cation orpattern recognition,\nand when yiis real-valued, the problem is known as regression. Another variant, known as\nordinal regression , occurs where label space Yhas some natural ordering, such as grades A\u2013F.\nThe second main type of machine learning is the descriptive orunsupervised learning\napproach. Here we are only given inputs, D={xi}Ni=1, and the goal is to \ufb01nd \u201cinteresting\npatterns\u201d in the data. This is sometimes called knowledge discovery. This is a much less\nwell-de\ufb01ned problem, since we are not told what kinds of patterns to look for, and there is no\nobvious error metric to use (unlike supervised learning, where we can compare our predictionofyfor a given xto the observed value).\nThere is a third type of machine learning, known as reinforcement learning, which is\nsomewhat less commonly used. This is useful for learning how to act or behave when givenoccasional reward or punishment signals. (For example, consider how a baby learns to walk.)Unfortunately, RL is beyond the scope of this book, although we do discuss decision theoryin Section 5.7, which is the basis of RL. See e.g., (Kaelbling et al. 1996; Sutton and Barto 1998;Russell and Norvig 2010; Szepesvari 2010; Wiering and van Otterlo 2012) for more informationon RL.\n4.\nhttp://certifiedknowledge .org/blog/are-search-queries-becoming-even-more-unique-statistic\ns-from-google .", "33": "1.2. Supervised learning 3\n(a)\n (b)\nFigure 1.1 Left: Some labeled training examples of colored shapes, along with 3 unlabeled test cases.\nRight: Representing the training data as an N\u00d7Ddesign matrix. Row irepresents the feature vector xi.\nThe last column is the label, yi\u2208{0,1}. Based on a \ufb01gure by Leslie Kaelbling.\n1.2 Supervised learning\nWe begin our investigation of machine learning by discussing supervised learning, which is the\nform of ML most widely used in practice.\n1.2.1 Classi\ufb01cation\nIn this section, we discuss classi\ufb01cation. Here the goal is to learn a mapping from inputs x\nto outputs y,w h e r ey\u2208{1,...,C}, withCbeing the number of classes. If C=2, this is\ncalled binary classi\ufb01cation (in which case we often assume y\u2208{0,1}); ifC>2, this is called\nmulticlass classi\ufb01cation . If the class labels are not mutually exclusive (e.g., somebody may be\nclassi\ufb01ed as tall and strong), we call it multi-label classi\ufb01cation , but this is best viewed as\npredicting multiple related binary class labels (a so-called multiple output model ). When we\nuse the term \u201cclassi\ufb01cation\u201d, we will mean multiclass classi\ufb01cation with a single output, unless\nwe state otherwise.\nOne way to formalize the problem is as function approximation . We assume y=f(x)for\nsome unknown function f, and the goal of learning is to estimate the function fgiven a labeled\ntraining set, and then to make predictions using \u02c6y=\u02c6f(x). (We use the hat symbol to denote\nan estimate.) Our main goal is to make predictions on novel inputs, meaning ones that we have\nnot seen before (this is called generalization ), since predicting the response on the training set\nis easy (we can just look up the answer).\n1.2.1.1 Example\nAs a simple toy example of classi\ufb01cation, consider the problem illustrated in Figure 1.1(a). We\nhave two classes of object which correspond to labels 0 and 1. The inputs are colored shapes.\nThese have been described by a set of Dfeatures or attributes, which are stored in an N\u00d7D\ndesign matrix X, shown in Figure 1.1(b). The input features xcan be discrete, continuous or a\ncombination of the two. In addition to the inputs, we have a vector of training labels y.\nIn Figure 1.1, the test cases are a blue crescent, a yellow circle and a blue arrow. None of\nthese have been seen before. Thus we are required to generalize beyond the training set. A", "34": "4 Chapter 1. Introduction\nreasonable guess is that blue crescent should be y=1, since all blue shapes are labeled 1 in the\ntraining set. The yellow circle is harder to classify, since some yellow things are labeled y=1\nand some are labeled y=0, and some circles are labeled y=1and some y=0. Consequently\nit is not clear what the right label should be in the case of the yellow circle. Similarly, the correct\nlabel for the blue arrow is unclear.\n1.2.1.2 The need for probabilistic predictions\nTo handle ambiguous cases, such as the yellow circle above, it is desirable to return a probability.The reader is assumed to already have some familiarity with basic concepts in probability. Ifnot, please consult Chapter 2 for a refresher, if necessary.\nWe will denote the probability distribution over possible labels, given the input vector xand\ntraining set Dbyp(y|x,D). In general, this represents a vector of length C. (If there are just two\nclasses, it is sufficient to return the single number p(y=1|x,D), sincep(y=1|x,D)+p(y=\n0|x,D)=1.) In our notation, we make explicit that the probability is conditional on the test\ninputx, as well as the training set D, by putting these terms on the right hand side of the\nconditioning bar |. We are also implicitly conditioning on the form of model that we use to make\npredictions. When choosing between different models, we will make this assumption explicit bywritingp(y|x,D,M),w h e r eMdenotes the model. However, if the model is clear from context,\nwe will drop Mfrom our notation for brevity.\nGiven a probabilistic output, we can always compute our \u201cbest guess\u201d as to the \u201ctrue label\u201d\nusing\n\u02c6y=\u02c6f(x)=\nCargmax\nc=1p(y=c|x,D) (1.1)\nThis corresponds to the most probable class label, and is called the modeof the distribution\np(y|x,D); it is also known as a MAP estimate (MAP stands for maximum a posteriori). Using\nthe most probable label makes intuitive sense, but we will give a more formal justi\ufb01cation forthis procedure in Section 5.7.\nNow consider a case such as the yellow circle, where p(\u02c6y|x,D)is far from 1.0. In such a\ncase we are not very con\ufb01dent of our answer, so it might be better to say \u201cI don\u2019t know\u201d insteadof returning an answer that we don\u2019t really trust. This is particularly important in domainssuch as medicine and \ufb01nance where we may be risk averse, as we explain in Section 5.7.Another application where it is important to assess risk is when playing TV game shows, suchas Jeopardy. In this game, contestants have to solve various word puzzles and answer a varietyof trivia questions, but if they answer incorrectly, they lose money. In 2011, IBM unveiled acomputer system called Watson which beat the top human Jeopardy champion. Watson uses avariety of interesting techniques (Ferrucci et al. 2010), but the most pertinent one for our presentpurposes is that it contains a module that estimates how con\ufb01dent it is of its answer. The systemonly chooses to \u201cbuzz in\u201d its answer if sufficiently con\ufb01dent it is correct. Similarly, Google has asystem known as SmartASS (ad selection system) that predicts the probability you will click onan ad based on your search history and other user and ad-speci\ufb01c features (Metz 2010). Thisprobability is known as the click-through rate orCTR, and can be used to maximize expected\npro\ufb01t. We will discuss some of the basic principles behind systems such as SmartASS later inthis book.", "35": "1.2. Supervised learning 5\nwordsdocuments\n10 20 30 40 50 60 70 80 90 100100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nFigure 1.2 Subset of size 16242 x 100 of the 20-newsgroups data. We only show 1000 rows, for clarity.\nEach row is a document (represented as a bag-of-words bit vector), each column is a word. The red\nlines separate the 4 classes, which are (in descending order) comp, rec, sci, talk (these are the titles of\nUSENET groups). We can see that there are subsets of words whose presence or absence is indicative\nof the class. The data is available from http://cs .nyu.edu/~roweis/data .html. Figure generated by\nnewsgroupsVisualize .\n1.2.1.3 Real-world applications\nClassi\ufb01cation is probably the most widely used form of machine learning, and has been used\nto solve many interesting and often difficult real-world problems. We have already mentioned\nsome important applciations. We give a few more examples below.\nDocument classi\ufb01cation and email spam \ufb01ltering\nIndocument classi\ufb01cation , the goal is to classify a document, such as a web page or email\nmessage, into one of Cclasses, that is, to compute p(y=c|x,D),w h e r exis some represen-\ntation of the text. A special case of this is email spam \ufb01ltering , where the classes are spam\ny=1or hamy=0.\nMost classi\ufb01ers assume that the input vector xhas a \ufb01xed size. A common way to represent\nvariable-length documents in feature-vector format is to use a bag of words representation.\nThis is explained in detail in Section 3.4.4.1, but the basic idea is to de\ufb01ne xij=1iff wordj\noccurs in document i. If we apply this transformation to every document in our data set, we get\na binary document \u00d7word co-occurrence matrix: see Figure 1.2 for an example. Essentially the\ndocument classi\ufb01cation problem has been reduced to one that looks for subtle changes in the\npattern of bits. For example, we may notice that most spam messages have a high probability of\ncontaining the words \u201cbuy\u201d, \u201ccheap\u201d, \u201cviagra\u201d, etc. In Exercise 8.1 and Exercise 8.2, you will get\nhands-on experience applying various classi\ufb01cation techniques to the spam \ufb01ltering problem.", "36": "6 Chapter 1. Introduction\n(a)\n (b)\n (c)\nFigure 1.3 Three types of iris \ufb02owers: setosa, versicolor and virginica. Source: http://www .statlab.u\nni-heidelberg .de/data/iris/ . Used with kind permission of Dennis Kramb and SIGNA.sepal lengthsepal lengthsepal width petal length petal widthsepal width petal length petal width\nFigure 1.4 Visualization of the Iris data as a pairwise scatter plot. The diagonal plots the marginal\nhistograms of the 4 features. The off diagonals contain scatterplots of all possible pairs of features. Red\ncircle = setosa, green diamond = versicolor, blue star = virginica. Figure generated by fisheririsDemo .\nClassifying \ufb02owers\nFigure 1.3 gives another example of classi\ufb01cation, due to the statistician Ronald Fisher. The goal\nis to learn to distinguish three different kinds of iris \ufb02ower, called setosa, versicolor and virginica.\nFortunately, rather than working directly with images, a botanist has already extracted 4 useful\nfeatures or characteristics: sepal length and width, and petal length and width. (Such feature\nextraction is an important, but difficult, task. Most machine learning methods use features\nchosen by some human. Later we will discuss some methods that can learn good features from\nthe data.) If we make a scatter plot of the iris data, as in Figure 1.4, we see that it is easy to\ndistinguish setosas (red circles) from the other two classes by just checking if their petal length", "37": "1.2. Supervised learning 7\ntrue class = 7\n true class = 2\n true class = 1\ntrue class = 0\n true class = 4\n true class = 1\ntrue class = 4\n true class = 9\n true class = 5\n(a)\ntrue class = 7\n true class = 2\n true class = 1\ntrue class = 0\n true class = 4\n true class = 1\ntrue class = 4\n true class = 9\n true class = 5\n(b)\nFigure 1.5 (a) First 9 test MNIST gray-scale images. (b) Same as (a), but with the features permuted\nrandomly. Classi\ufb01cation performance is identical on both versions of the data (assuming the training data\nis permuted in an identical way). Figure generated by shuffledDigitsDemo .\nor width is below some threshold. However, distinguishing versicolor from virginica is slightly\nharder; any decision will need to be based on at least two features. (It is always a good idea\nto perform exploratory data analysis , such as plotting the data, before applying a machine\nlearning method.)\nImage classi\ufb01cation and handwriting recognition\nNow consider the harder problem of classifying images directly, where a human has not pre-\nprocessed the data. We might want to classify the image as a whole, e.g., is it an indoors or\noutdoors scene? is it a horizontal or vertical photo? does it contain a dog or not? This is called\nimage classi\ufb01cation .\nIn the special case that the images consist of isolated handwritten letters and digits, for\nexample, in a postal or ZIP code on a letter, we can use classi\ufb01cation to perform handwriting\nrecognition . Astandarddatasetusedinthisareaisknownas MNIST,whichstandsfor\u201cModi\ufb01ed\nNational Institute of Standards\u201d5. (The term \u201cmodi\ufb01ed\u201d is used because the images have been\npreprocessed to ensure the digits are mostly in the center of the image.) This dataset contains\n60,000 training images and 10,000 test images of the digits 0 to 9, as written by various people.\nThe images are size 28\u00d728and have grayscale values in the range 0 : 255. See Figure 1.5(a) for\nsome example images.\nMany generic classi\ufb01cation methods ignore any structure in the input features, such as spatial\nlayout. Consequently, they can also just as easily handle data that looks like Figure 1.5(b), which\nis the same data except we have randomly permuted the order of all the features. (You will\nverify this in Exercise 1.1.) This \ufb02exibility is both a blessing (since the methods are general\npurpose) and a curse (since the methods ignore an obviously useful source of information). We\nwill discuss methods for exploiting structure in the input features later in the book.\n5. Available from http://yann .lecun .com/exdb/mnist/ .", "38": "8 Chapter 1. Introduction\n(a)\n (b)\nFigure 1.6 Example of face detection. (a) Input image (Murphy family, photo taken 5 August 2010). Used\nwith kind permission of Bernard Diedrich of Sherwood Studios. (b) Output of classi\ufb01er, which detected 5\nfaces at different poses. This was produced using the online demo at http://demo .pittpatt .com/. The\nclassi\ufb01er was trained on 1000s of manually labeled images of faces and non-faces, and then was applied\nto a dense set of overlapping patches in the test image. Only the patches whose probability of containing\na face was sufficiently high were returned. Used with kind permission of Pittpatt.com\nFace detection and recognition\nA harder problem is to \ufb01nd objects within an image; this is called object detection orobject\nlocalization . An important special case of this is face detection . One approach to this problem\nis to divide the image into many small overlapping patches at different locations, scales and\norientations, and to classify each such patch based on whether it contains face-like texture or\nnot. This is called a sliding window detector . The system then returns those locations where\nthe probability of face is sufficiently high. See Figure 1.6 for an example. Such face detection\nsystems are built-in to most modern digital cameras; the locations of the detected faces are\nused to determine the center of the auto-focus. Another application is automatically blurring\nout faces in Google\u2019s StreetView system.\nHaving found the faces, one can then proceed to perform face recognition , which means\nestimating the identity of the person (see Figure 1.10(a)). In this case, the number of class labels\nmight be very large. Also, the features one should use are likely to be different than in the face\ndetection problem: for recognition, subtle differences between faces such as hairstyle may be\nimportant for determining identity, but for detection, it is important to be invariant to such\ndetails, and to just focus on the differences between faces and non-faces. For more information\nabout visual object detection, see e.g., (Szeliski 2010).\n1.2.2 Regression\nRegression is just like classi\ufb01cation except the response variable is continuous. Figure 1.7 shows\na simple example: we have a single real-valued input xi\u2208R, and a single real-valued response\nyi\u2208R. We consider \ufb01tting two models to the data: a straight line and a quadratic function.\n(We explain how to \ufb01t such models below.) Various extensions of this basic problem can arise,\nsuch as having high-dimensional inputs, outliers, non-smooth responses, etc. We will discuss\nways to handle such problems later in the book.", "39": "1.3. Unsupervised learning 9\n0 5 10 15 20\u221210\u22125051015degree 1\n(a)0 5 10 15 20\u221210\u22125051015degree 2\n(b)\nFigure 1.7 (a) Linear regression on some 1d data. (b) Same data with polynomial regression (degree 2).\nFigure generated by linregPolyVsDegree .\nHere are some examples of real-world regression problems.\n\u2022 Predict tomorrow\u2019s stock market price given current market conditions and other possible\nside information.\n\u2022 Predict the age of a viewer watching a given video on YouTube.\n\u2022 Predict the location in 3d space of a robot arm end effector, given control signals (torques)\nsent to its various motors.\n\u2022 Predict the amount of prostate speci\ufb01c antigen (PSA) in the body as a function of a number\nof different clinical measurements.\n\u2022 Predict the temperature at any location inside a building using weather data, time, door\nsensors, etc.\n1.3 Unsupervised learning\nWe now consider unsupervised learning, where we are just given output data, without any\ninputs. The goal is to discover \u201cinteresting structure\u201d in the data; this is sometimes called\nknowledge discovery. Unlike supervised learning, we are not told what the desired output isfor each input. Instead, we will formalize our task as one of density estimation, that is, we\nwant to build models of the form p(x\ni|\u03b8). There are two differences from the supervised case.\nFirst, we have written p(xi|\u03b8)instead of p(yi|xi,\u03b8); that is, supervised learning is conditional\ndensity estimation, whereas unsupervised learning is unconditional density estimation. Second,x\niis a vector of features, so we need to create multivariate probability models. By contrast,\nin supervised learning, yiis usually just a single variable that we are trying to predict. This\nmeans that for most supervised learning problems, we can use univariate probability models(with input-dependent parameters), which signi\ufb01cantly simpli\ufb01es the problem. (We will discussmulti-output classi\ufb01cation in Chapter 19, where we will see that it also involves multivariateprobability models.)\nUnsupervised learning is arguably more typical of human and animal learning. It is also\nmore widely applicable than supervised learning, since it does not require a human expert to", "40": "10 Chapter 1. Introduction\n55 60 65 70 75 8080100120140160180200220240260280\nheightweight\n(a)55 60 65 70 75 8080100120140160180200220240260280\nheightweightK=2\n(b)\nFigure 1.8 (a) The height and weight of some people. (b) A possible clustering using K=2clusters.\nFigure generated by kmeansHeightWeight .\nmanually label the data. Labeled data is not only expensive to acquire6, but it also contains\nrelatively little information, certainly not enough to reliably estimate the parameters of complex\nmodels. Geoff Hinton, who is a famous professor of ML at the University of Toronto, has said:\nWhen we\u2019re learning to see, nobody\u2019s telling us what the right answers are \u2014 we justlook. Every so often, your mother says \u201cthat\u2019s a dog\u201d, but that\u2019s very little information.You\u2019d be lucky if you got a few bits of information \u2014 even one bit per second \u2014 thatway. The brain\u2019s visual system has 10\n14neural connections. And you only live for 109\nseconds. So it\u2019s no use learning one bit per second. You need more like 105bits per\nsecond. And there\u2019s only one place you can get that much information: from the inputitself. \u2014 Geoffrey Hinton, 1996 (quoted in (Gorder 2006)).\nBelow we describe some canonical examples of unsupervised learning.\n1.3.1 Discovering clusters\nAs a canonical example of unsupervised learning, consider the problem of clustering data into\ngroups. For example, Figure 1.8(a) plots some 2d data, representing the height and weight ofa group of 210 people. It seems that there might be various clusters, or subgroups, althoughit is not clear how many. Let Kdenote the number of clusters. Our \ufb01rst goal is to estimate\nthe distribution over the number of clusters, p(K|D); this tells us if there are subpopulations\nwithin the data. For simplicity, we often approximate the distribution p(K|D)by its mode,\nK\n\u2217= argmax Kp(K|D). In the supervised case, we were told that there are two classes (male\nand female), but in the unsupervised case, we are free to choose as many or few clusters as welike. Picking a model of the \u201cright\u201d complexity is called model selection, and will be discussedin detail below.\nOur second goal is to estimate which cluster each point belongs to. Let z\ni\u2208{1,...,K}\nrepresent the cluster to which data point iis assigned. (z iis an example of a hiddenor\n6. The advent of crowd sourcing web sites such as Mechanical Turk, (https://www .mturk .com/mturk/welcome ),\nwhich outsource data processing tasks to humans all over the world, has reduced the cost of labeling data. Nevertheless,\nthe amount of unlabeled data is still orders of magnitude larger than the amount of labeled data.", "41": "1.3. Unsupervised learning 11\n\u22128\u22126\u22124\u2212202468\n\u22124\u22122024\u2212202\n(a)\u2212505\n\u22124\u22122024\n(b)\nFigure 1.9 (a) A set of points that live on a 2d linear subspace embedded in 3d. The solid red line is the\n\ufb01rst principal component direction. The dotted black line is the second PC direction. (b) 2D representation\nof the data. Figure generated by pcaDemo3d .\nlatentvariable, since it is never observed in the training set.) We can infer which cluster each\ndata point belongs to by computing z\u2217\ni= argmaxkp(zi=k|xi,D). This is illustrated in\nFigure 1.8(b), where we use different colors to indicate the assignments, assuming K=2.\nIn this book, we focus on model based clustering, which means we \ufb01t a probabilistic model\nto the data, rather than running some ad hoc algorithm. The advantages of the model-based\napproach are that one can compare different kinds of models in an objective way (in terms ofthe likelihood they assign to the data), we can combine them together into larger systems, etc.\nHere are some real world applications of clustering.\n\u2022 In astronomy, the autoclass system (Cheeseman et al. 1988) discovered a new type of star,\nbased on clustering astrophysical measurements.\n\u2022 In e-commerce, it is common to cluster users into groups, based on their purchasing or\nweb-sur\ufb01ng behavior, and then to send customized targeted advertising to each group (seee.g., (Berkhin 2006)).\n\u2022 In biology, it is common to cluster \ufb02ow-cytometry data into groups, to discover different\nsub-populations of cells (see e.g., (Lo et al. 2009)).\n1.3.2 Discovering latent factors\nWhen dealing with high dimensional data, it is often useful to reduce the dimensionality byprojecting the data to a lower dimensional subspace which captures the \u201cessence\u201d of the data.This is called dimensionality reduction. A simple example is shown in Figure 1.9, where we\nproject some 3d data down to a 2d plane. The 2d approximation is quite good, since most pointslie close to this subspace. Reducing to 1d would involve projecting points onto the red line inFigure 1.9(a); this would be a rather poor approximation. (We will make this notion precise inChapter 12.)\nThe motivation behind this technique is that although the data may appear high dimensional,\nthere may only be a small number of degrees of variability, corresponding to latent factors.F o r\nexample, when modeling the appearance of face images, there may only be a few underlyinglatent factors which describe most of the variability, such as lighting, pose, identity, etc, asillustrated in Figure 1.10.", "42": "12 Chapter 1. Introduction\n(a)\n (b)\nFigure 1.10 a) 25 randomly chosen 64\u00d764pixel images from the Olivetti face database. (b) The mean\nand the \ufb01rst three principal component basis vectors (eigenfaces). Figure generated by pcaImageDemo .\nWhen used as input to other statistical models, such low dimensional representations often\nresult in better predictive accuracy, because they focus on the \u201cessence\u201d of the object, \ufb01ltering\nout inessential features. Also, low dimensional representations are useful for enabling fast\nnearest neighbor searches and two dimensional projections are very useful for visualizing high\ndimensional data.\nThe most common approach to dimensionality reduction is called principal components\nanalysis orPCA. This can be thought of as an unsupervised version of (multi-output) linear\nregression, where we observe the high-dimensional response y, but not the low-dimensional\n\u201ccause\u201dz. Thus the model has the form z\u2192y; we have to \u201cinvert the arrow\u201d, and infer the\nlatent low-dimensional zfrom the observed high-dimensional y. See Section 12.1 for details.\nDimensionality reduction, and PCA in particular, has been applied in many different areas.\nSome examples include the following:\n\u2022 In biology, it is common to use PCA to interpret gene microarray data, to account for the\nfact that each measurement is usually the result of many genes which are correlated in their\nbehavior by the fact that they belong to different biological pathways.\n\u2022 In natural language processing, it is common to use a variant of PCA called latent semantic\nanalysis for document retrieval (see Section 27.2.2).\n\u2022 In signal processing (e.g., of acoustic or neural signals), it is common to use ICA (which is a\nvariant of PCA) to separate signals into their different sources (see Section 12.6).\n\u2022 In computer graphics, it is common to project motion capture data to a low dimensional\nspace, and use it to create animations. See Section 15.5 for one way to tackle such problems.", "43": "1.3. Unsupervised learning 13\nFigure 1.11 A sparse undirected Gaussian graphical model learned using graphical lasso (Section 26.7.2)\napplied to some \ufb02ow cytometry data (from (Sachs et al. 2005)), which measures the phosphorylation status\nof 11 proteins. Figure generated by ggmLassoDemo .\n1.3.3 Discovering graph structure\nSometimes we measure a set of correlated variables, and we would like to discover which ones\nare most correlated with which others. This can be represented by a graph G, in which nodes\nrepresent variables, and edges represent direct dependence between variables (we will make\nthis precise in Chapter 10, when we discuss graphical models). We can then learn this graph\nstructure from data, i.e., we compute \u02c6G=a r g m a x p(G|D).\nAs with unsupervised learning in general, there are two main applications for learning sparse\ngraphs: to discover new knowledge, and to get better joint probability density estimators. We\nnow give somes example of each.\n\u2022 Much of the motivation for learning sparse graphical models comes from the systems biology\ncommunity. For example, suppose we measure the phosphorylation status of some proteins\nin a cell (Sachs et al. 2005). Figure 1.11 gives an example of a graph structure that was learned\nfrom this data (using methods discussed in Section 26.7.2). As another example, Smith et al.\n(2006) showed that one can recover the neural \u201cwiring diagram\u201d of a certain kind of bird\nfrom time-series EEG data. The recovered structure closely matched the known functional\nconnectivity of this part of the bird brain.\n\u2022 In some cases, we are not interested in interpreting the graph structure, we just want to\nuse it to model correlations and to make predictions. One example of this is in \ufb01nancial\nportfolio management, where accurate models of the covariance between large numbers of\ndifferent stocks is important. Carvalho and West (2007) show that by learning a sparse graph,\nand then using this as the basis of a trading strategy, it is possible to outperform (i.e., make\nmore money than) methods that do not exploit sparse graphs. Another example is predicting\ntraffic jams on the freeway. Horvitz et al. (2005) describe a deployed system called JamBayes\nfor predicting traffic \ufb02ow in the Seattle area; predictions are made using a graphical model\nwhose structure was learned from data.", "44": "14 Chapter 1. Introduction\n(a)\n (b)\nFigure 1.12 (a) A noisy image with an occluder. (b) An estimate of the underlying pixel intensities, based\non a pairwise MRF model. Source: Figure 8 of (Felzenszwalb and Huttenlocher 2006). Used with kind\npermission of Pedro Felzenszwalb.\n1.3.4 Matrix completion\nSometimes we have missing data, that is, variables whose values are unknown. For example, we\nmight have conducted a survey, and some people might not have answered certain questions.\nOr we might have various sensors, some of which fail. The corresponding design matrix will\nthen have \u201choles\u201d in it; these missing entries are often represented by NaN, which stands for\n\u201cnot a number\u201d. The goal of imputation is to infer plausible values for the missing entries. This\nis sometimes called matrix completion . Below we give some example applications.\n1.3.4.1 Image inpainting\nAn interesting example of an imputation-like task is known as image inpainting . The goal is\nto \u201c\ufb01ll in\u201d holes (e.g., due to scratches or occlusions) in an image with realistic texture. This is\nillustrated in Figure 1.12, where we denoise the image, as well as impute the pixels hidden behind\nthe occlusion. This can be tackled by building a joint probability model of the pixels, given a\nset of clean images, and then inferring the unknown variables (pixels) given the known variables\n(pixels). This is somewhat like masket basket analysis, except the data is real-valued and spatially\nstructured, so the kinds of probability models we use are quite different. See Sections 19.6.2.7\nand 13.8.4 for some possible choices.\n1.3.4.2 Collaborative \ufb01ltering\nAnother interesting example of an imputation-like task is known as collaborative \ufb01ltering .A\ncommon example of this concerns predicting which movies people will want to watch based\non how they, and other people, have rated movies which they have already seen. The key idea\nis that the prediction is not based on features of the movie or user (although it could be), but\nmerely on a ratings matrix. More precisely, we have a matrix XwhereX(m,u)is the rating", "45": "1.3. Unsupervised learning 15\n\u0003\u0003XVHUV\nPRYLHV\u0014\u0003 \"\u0016\u0018\"\n\"\u0014\u0003\u0003\u0003 \u0015\n\u0003\u0017 \u0003\u0017\u0018\"\nFigure 1.13 Example of movie-rating data. Training data is in red, test data is denoted by ?, empty cells\nare unknown.\n(say an integer between 1 and 5, where 1 is dislike and 5 is like) by user uof movie m.N o t e\nthat most of the entries in Xwill be missing or unknown, since most users will not have rated\nmost movies. Hence we only observe a tiny subset of the Xmatrix, and we want to predict\na different subset. In particular, for any given user u, we might want to predict which of the\nunrated movies he/she is most likely to want to watch.\nIn order to encourage research in this area, the DVD rental company Net\ufb02ix created a com-\npetition, launched in 2006, with a $1M USD prize (see http://netflixprize .com/). In\nparticular, they provided a large matrix of ratings, on a scale of 1 to 5, for \u223c18kmovies\ncreated by \u223c500kusers. The full matrix would have \u223c9\u00d7109entries, but only about 1%\nof the entries are observed, so the matrix is extremely sparse. A subset of these are used for\ntraining, and the rest for testing, as shown in Figure 1.13. The goal of the competition was to\npredict more accurately than Net\ufb02ix\u2019s existing system. On 21 September 2009, the prize wasawarded to a team of researchers known as \u201cBellKor\u2019s Pragmatic Chaos\u201d. Section 27.6.2 discussessome of their methodology. Further details on the teams and their methods can be found athttp://www .netflixprize .com/community/viewtopic .php?id=1537 .\n1.3.4.3 Market basket analysis\nIn commercial data mining, there is much interest in a task called market basket analysis. The\ndata consists of a (typically very large but sparse) binary matrix, where each column representsan item or product, and each row represents a transaction. We set x\nij=1if itemjwas\npurchased on the i\u2019th transaction. Many items are purchased together (e.g., bread and butter),\nso there will be correlations amongst the bits. Given a new partially observed bit vector,representing a subset of items that the consumer has bought, the goal is to predict which otherbits are likely to turn on, representing other items the consumer might be likely to buy. (Unlikecollaborative \ufb01ltering, we often assume there is no missing data in the training data, since weknow the past shopping behavior of each customer.)\nThis task arises in other domains besides modeling purchasing patterns. For example, similar\ntechniques can be used to model dependencies between \ufb01les in complex software systems. Inthis case, the task is to predict, given a subset of \ufb01les that have been changed, which other onesneed to be updated to ensure consistency (see e.g., (Hu et al. 2010)).\nIt is common to solve such tasks using frequent itemset mining, which create association\nrules (see e.g., (Hastie et al. 2009, sec 14.2) for details). Alternatively, we can adopt a probabilisticapproach, and \ufb01t a joint density model p(x\n1,...,x D)to the bit vectors, see e.g., (Hu et al.", "46": "16 Chapter 1. Introduction\n(a)\n (b)\nFigure 1.14 (a) Illustration of a K-nearest neighbors classi\ufb01er in 2d for K=3. The 3 nearest neighbors\nof test point x1have labels 1, 1 and 0, so we predict p(y=1|x1,D,K=3 )=2 /3. The 3 nearest\nneighbors of test point x2have labels 0, 0, and 0, so we predict p(y=1|x2,D,K=3 )=0 /3. (b)\nIllustration of the Voronoi tesselation induced by 1-NN. Based on Figure 4.13 of (Duda et al. 2001). Figure\ngenerated by knnVoronoi .\n2010). Such models often have better predictive acccuracy than association rules, although they\nmay be less interpretible. This is typical of the difference between data mining and machine\nlearning: in data mining, there is more emphasis on interpretable models, whereas in machine\nlearning, there is more emphasis on accurate models.\n1.4 Some basic concepts in machine learning\nIn this Section, we provide an introduction to some key ideas in machine learning. We will\nexpand on these concepts later in the book, but we introduce them brie\ufb02y here, to give a \ufb02avor\nof things to come.\n1.4.1 Parametric vs non-parametric models\nIn this book, we will be focussing on probabilistic models of the form p(y|x)orp(x), depending\non whether we are interested in supervised or unsupervised learning respectively. There are\nmany ways to de\ufb01ne such models, but the most important distinction is this: does the model\nhave a \ufb01xed number of parameters, or does the number of parameters grow with the amount\nof training data? The former is called a parametric model , and the latter is called a non-\nparametric model . Parametric models have the advantage of often being faster to use, but the\ndisadvantage of making stronger assumptions about the nature of the data distributions. Non-\nparametric models are more \ufb02exible, but often computationally intractable for large datasets.\nWe will give examples of both kinds of models in the sections below. We focus on supervised\nlearning for simplicity, although much of our discussion also applies to unsupervised learning.\n1.4.2 A simple non-parametric classi\ufb01er: K-nearest neighbors\nA simple example of a non-parametric classi\ufb01er is the Knearest neighbor (KNN) classi\ufb01er.\nThis simply \u201clooks at\u201d the Kpoints in the training set that are nearest to the test input x,", "47": "1.4. Some basic concepts in machine learning 17\n\u22123 \u22122 \u22121 0 1 2 3\u22122\u22121012345train\n(a)\np(y=1|data,K=10)\n  \n20 40 60 80 10020406080100120\n00.10.20.30.40.50.60.70.80.91\n(b)\np(y=2|data,K=10)\n  \n20 40 60 80 10020406080100120\n00.10.20.30.40.50.60.70.80.91\n(c)\u22123 \u22122 \u22121 0 1 2 3\u22122\u22121012345predicted label,  K=10\n  \nc1\nc2\nc3\n(d)\nFigure 1.15 (a) Some synthetic 3-class training data in 2d. (b) Probability of class 1 for KNN with K=1 0.\n(c) Probability of class 2. (d) MAP estimate of class label. Figure generated by knnClassifyDemo .\ncounts how many members of each class are in this set, and returns that empirical fraction as\nthe estimate, as illustrated in Figure 1.14. More formally,\np(y=c|x,D,K)=1\nK/summationdisplay\ni\u2208NK(x,D)I(yi=c) (1.2)\nwhereNK(x,D)are the (indices of the) Knearest points to xinDandI(e)is the indicator\nfunction de\ufb01ned as follows:\nI(e)=/braceleftbigg\n1ifeis true\n0ifeis false(1.3)\nThis method is an example of memory-based learning orinstance-based learning . It can\nbe derived from a probabilistic framework as explained in Section 14.7.3. The most common", "48": "18 Chapter 1. Introduction\ns\n11\n0\n(a)0 0.2 0.4 0.6 0.8 100.10.20.30.40.50.60.70.80.91\nFraction of data in neighborhoodEdge length of cube\nd=1d=3d=5d=7d=10\n(b)\nFigure 1.16 Illustration of the curse of dimensionality. (a) We embed a small cube of side sinside a larger\nunit cube. (b) We plot the edge length of a cube needed to cover a given volume of the unit cube as a\nfunction of the number of dimensions. Based on Figure 2.6 from (Hastie et al. 2009). Figure generated bycurseDimensionality .\ndistance metric to use is Euclidean distance (which limits the applicability of the technique to\ndata which is real-valued), although other metrics can be used.\nFigure 1.15 gives an example of the method in action, where the input is two dimensional, we\nhave three classes, and K=1 0. (We discuss the effect of Kbelow.) Panel (a) plots the training\ndata. Panel (b) plots p(y=1|x,D)wherexis evaluated on a grid of points. Panel (c) plots\np(y=2|x,D). We do not need to plot p(y=3|x,D), since probabilities sum to one. Panel (d)\nplots the MAP estimate \u02c6y(x) = argmaxc(y=c|x,D).\nA KNN classi\ufb01er with K=1induces a Voronoi tessellation of the points (see Figure 1.14(b)).\nThis is a partition of space which associates a region V(xi)with each point xiin such a way\nthat all points in V(xi)are closer to xithan to any other point. Within each cell, the predicted\nlabel is the label of the corresponding training point.\n1.4.3 The curse of dimensionality\nThe KNN classi\ufb01er is simple and can work quite well, provided it is given a good distance metricand has enough labeled training data. In fact, it can be shown that the KNN classi\ufb01er can comewithin a factor of 2 of the best possible performance if N\u2192\u221e(Cover and Hart 1967).\nHowever, the main problem with KNN classi\ufb01ers is that they do not work well with high\ndimensional inputs. The poor performance in high dimensional settings is due to the curse of\ndimensionality.\nTo explain the curse, we give some examples from (Hastie et al. 2009, p22). Consider applying\na KNN classi\ufb01er to data where the inputs are uniformly distributed in the D-dimensional unit\ncube. Suppose we estimate the density of class labels around a test point xby \u201cgrowing\u201d a\nhyper-cube around xuntil it contains a desired fraction fof the data points. The expected edge\nlength of this cube will be e\nD(f)=f1/D.I fD=1 0, and we want to base our estimate on 10%", "49": "1.4. Some basic concepts in machine learning 19\n\u22123 \u22122 \u22121 0 1 2 300.050.10.150.20.250.30.350.4PDF\n(a)\n (b)\nFigure 1.17 (a) A Gaussian pdf with mean 0 and variance 1. Figure generated by gaussPlotDemo . (b)\nVisualization of the conditional density model p(y|x,\u03b8)=N(y|w0+w1x,\u03c32). The density falls off\nexponentially fast as we move away from the regression line. Figure generated by linregWedgeDemo2 .\nof the data, we have e10(0.1) = 0.8, so we need to extend the cube 80% along each dimension\naroundx. Even if we only use 1% of the data, we \ufb01nd e10(0.01) = 0.63: see Figure 1.16. Since\nthe entire range of the data is only 1 along each dimension, we see that the method is no longer\nvery local, despite the name \u201cnearest neighbor\u201d. The trouble with looking at neighbors that are\nso far away is that they may not be good predictors about the behavior of the input-output\nfunction at a given point.\n1.4.4 Parametric models for classi\ufb01cation and regression\nThe main way to combat the curse of dimensionality is to make some assumptions about\nthe nature of the data distribution (either p(y|x)for a supervised problem or p(x)for an\nunsupervised problem). These assumptions, known as inductive bias , are often embodied in\nthe form of a parametric model , which is a statistical model with a \ufb01xed number of parameters.\nBelow we brie\ufb02y describe two widely used examples; we will revisit these and other models in\nmuch greater depth later in the book.\n1.4.5 Linear regression\nOne of the most widely used models for regression is known as linear regression . This asserts\nthat the response is a linear function of the inputs. This can be written as follows:\ny(x)=wTx+/epsilon1=D/summationdisplay\nj=1wjxj+/epsilon1 (1.4)\nwherewTxrepresents the inner or scalar product between the input vector xand the model\u2019s\nweight vector w7, and/epsilon1is the residual error between our linear predictions and the true\nresponse.\n7. In statistics, it is more common to denote the regression weights by \u03b2.", "50": "20 Chapter 1. Introduction\n0 5 10 15 20\u221210\u22125051015degree 14\n(a)0 5 10 15 20\u221210\u22125051015degree 20\n(b)\nFigure 1.18 Polynomial of degrees 14 and 20 \ufb01t by least squares to 21 data points. Figure generated by\nlinregPolyVsDegree .\nWe often assume that /epsilon1has a Gaussian8ornormaldistribution. We denote this by /epsilon1\u223c\nN(\u03bc,\u03c32),w h e r e\u03bcis the mean and \u03c32is the variance (see Chapter 2 for details). When we plot\nthis distribution, we get the well-known bell curve shown in Figure 1.17(a).\nTo make the connection between linear regression and Gaussians more explicit, we can rewrite\nthe model in the following form:\np(y|x,\u03b8)=N(y|\u03bc(x),\u03c32(x)) (1.5)\nThis makes it clear that the model is a conditional probability density. In the simplest case, we\nassume\u03bcis a linear function of x,s o\u03bc=wTx, and that the noise is \ufb01xed, \u03c32(x)=\u03c32.I n\nthis case, \u03b8=(w,\u03c32)are the parameters of the model.\nFor example, suppose the input is 1 dimensional. We can represent the expected response as\nfollows:\n\u03bc(x)=w0+w1x=wTx (1.6)\nwherew0is the intercept or biasterm,w1is the slope, and where we have de\ufb01ned the vector\nx=( 1,x). (Prepending a constant 1 term to an input vector is a common notational trick which\nallows us to combine the intercept term with the other terms in the model.) If w1is positive,\nit means we expect the output to increase as the input increases. This is illustrated in 1d inFigure 1.17(b); a more conventional plot, of the mean response vs x, is shown in Figure 1.7(a).\nLinear regression can be made to model non-linear relationships by replacing xwith some\nnon-linear function of the inputs, \u03c6(x). That is, we use\np(y|x,\u03b8)=N(y|w\nT\u03c6(x),\u03c32) (1.7)\nThis is known as basis function expansion. For example, Figure 1.18 illustrates the case where\n\u03c6(x)=[ 1,x,x2,...,xd],f o rd=1 4andd=2 0; this is known as polynomial regression.\nWe will consider other kinds of basis functions later in the book. In fact, many popularmachine learning methods \u2014 such as support vector machines, neural networks, classi\ufb01cationand regression trees, etc. \u2014 can be seen as just different ways of estimating basis functionsfrom data, as we discuss in Chapters 14 and 16.\n8. Carl Friedrich Gauss (1777\u20131855) was a German mathematician and physicist.", "51": "1.4. Some basic concepts in machine learning 21\n\u221210 \u22125 0 5 1000.10.20.30.40.50.60.70.80.91\n(a)460 480 500 520540560 580 600 62064000.10.20.30.40.50.60.70.80.91\n(b)\nFigure 1.19 (a) The sigmoid or logistic function. We have sigm(\u2212\u221e)=0 ,sigm(0) = 0.5 , and\nsigm(\u221e )=1. Figure generated by sigmoidPlot . (b) Logistic regression for SAT scores. Solid black dots\nare the data. The open red circles are the predicted probabilities. The green crosses denote two students\nwith the same SAT score of 525 (and hence same input representation x) but with different training labels\n(one student passed, y=1, the other failed, y=0). Hence this data is not perfectly separable using just\nthe SAT feature. Figure generated by logregSATdemo .\n1.4.6 Logistic regression\nWe can generalize linear regression to the (binary) classi\ufb01cation setting by making two changes.\nFirst we replace the Gaussian distribution for ywith a Bernoulli distribution9,which is more\nappropriate for the case when the response is binary, y\u2208{0,1}. That is, we use\np(y|x,w) = Ber(y|\u03bc(x)) (1.8)\nwhere\u03bc(x)=E[y|x]=p(y=1|x). Second, we compute a linear combination of the inputs,\nas before, but then we pass this through a function that ensures 0\u2264\u03bc(x)\u22641by de\ufb01ning\n\u03bc(x) = sigm(wTx) (1.9)\nwheresigm(\u03b7)refers to the sigmoid function, also known as the logisticorlogitfunction.\nThis is de\ufb01ned as\nsigm(\u03b7)/defines1\n1+exp(\u2212\u03b7)=e\u03b7\ne\u03b7+1(1.10)\nThe term \u201csigmoid\u201d means S-shaped: see Figure 1.19(a) for a plot. It is also known as a squashing\nfunction, since it maps the whole real line to [0,1], which is necessary for the output to be\ninterpreted as a probability.\nPutting these two steps together we get\np(y|x,w) = Ber(y|sigm(wTx)) (1.11)\nThis is called logistic regression due to its similarity to linear regression (although it is a form\nof classi\ufb01cation, not regression!).\n9. Daniel Bernoulli (1700\u20131782) was a Dutch-Swiss mathematician and physicist.", "52": "22 Chapter 1. Introduction\nA simple example of logistic regression is shown in Figure 1.19(b), where we plot\np(yi=1|xi,w) = sigm(w 0+w1xi) (1.12)\nwherexiis the SAT10score of student iandyiis whether they passed or failed a class. The\nsolid black dots show the training data, and the red circles plot p(y=1|xi,\u02c6w),w h e r e\u02c6ware\nthe parameters estimated from the training data (we discuss how to compute these estimates in\nSection 8.3.4).\nIf we threshold the output probability at 0.5, we can induce a decision rule of the form\n\u02c6y(x)=1\u21d0\u21d2p(y=1|x)>0.5 (1.13)\nBy looking at Figure 1.19(b), we see that sigm(w0+w1x)=0.5forx\u2248545 =x\u2217. We can\nimagine drawing a vertical line at x=x\u2217; this is known as a decision boundary. Everything to\nthe left of this line is classi\ufb01ed as a 0, and everything to the right of the line is classi\ufb01ed as a 1.\nWe notice that this decision rule has a non-zero error rate even on the training set. This\nis because the data is not linearly separable, i.e., there is no straight line we can draw to\nseparate the 0s from the 1s. We can create models with non-linear decision boundaries usingbasis function expansion, just as we did with non-linear regression. We will see many examplesof this later in the book.\n1.4.7 Over\ufb01tting\nWhen we \ufb01t highly \ufb02exible models, we need to be careful that we do not over\ufb01tthe data, that\nis, we should avoid trying to model every minor variation in the input, since this is more likelyto be noise than true signal. This is illustrated in Figure 1.18(b), where we see that using a highdegree polynomial results in a curve that is very \u201cwiggly\u201d. It is unlikely that the true functionhas such extreme oscillations. Thus using such a model might result in accurate predictions offuture outputs.\nAs another example, consider the KNN classi\ufb01er. The value of Kcan have a large effect on\nthe behavior of this model. When K=1, the method makes no errors on the training set (since\nwe just return the labels of the original training points), but the resulting prediction surface isvery \u201cwiggly\u201d (see Figure 1.20(a)). Therefore the method may not work well at predicting futuredata. In Figure 1.20(b), we see that using K=5results in a smoother prediction surface,\nbecause we are averaging over a larger neighborhood. As Kincreases, the predictions becomes\nsmoother until, in the limit of K=N, we end up predicting the majority label of the whole\ndata set. Below we discuss how to pick the \u201cright\u201d value of K.\n1.4.8 Model selection\nWhen we have a variety of models of different complexity (e.g., linear or logistic regressionmodels with different degree polynomials, or KNN classi\ufb01ers with different values of K), how\nshould we pick the right one? A natural approach is to compute the misclassi\ufb01cation rate on\n10. SAT stands for \u201cScholastic Aptitude Test\u201d. This is a standardized test for college admissions used in the United States\n(the data in this example is from (Johnson and Albert 1999, p87)).", "53": "1.4. Some basic concepts in machine learning 23\n\u22123 \u22122 \u22121 0 1 2 3\u22122\u22121012345predicted label,  K=1\n  \nc1\nc2\nc3\n(a)\u22123 \u22122 \u22121 0 1 2 3\u22122\u22121012345predicted label,  K=5\n  \nc1\nc2\nc3\n(b)\nFigure 1.20 Prediction surface for KNN on the data in Figure 1.15(a). (a) K=1. (b) K=5. Figure generated by\nknnClassifyDemo .\nthe training set for each method. This is de\ufb01ned as follows:\nerr(f,D)=1\nNN/summationdisplay\ni=1I(f(xi)/negationslash=yi) (1.14)\nwheref(x)is our classi\ufb01er. In Figure 1.21(a), we plot this error rate vs Kfor a KNN classi\ufb01er\n(dotted blue line). We see that increasing Kincreasesour error rate on the training set, because\nwe are over-smoothing. As we said above, we can get minimal error on the training set by using\nK=1, since this model is just memorizing the data.\nHowever, what we care about is generalization error, which is the expected value of the\nmisclassi\ufb01cation rate when averaged over future data (see Section 6.3 for details). This can beapproximated by computing the misclassi\ufb01cation rate on a large independent test set, not used\nduring model training. We plot the test error vs Kin Figure 1.21(a) in solid red (upper curve).\nNow we see a U-shaped curve: for complex models (small K), the method over\ufb01ts, and for\nsimple models (big K), the method under\ufb01ts. Therefore, an obvious way to pick Kis to pick\nthe value with the minimum error on the test set (in this example, any value between 10 and100 should be \ufb01ne).\nUnfortunately, when training the model, we don\u2019t have access to the test set (by assumption),\nso we cannot use the test set to pick the model of the right complexity.\n11However, we can create\na test set by partitioning the training set into two: the part used for training the model, and asecond part, called the validation set, used for selecting the model complexity. We then \ufb01t all\nthe models on the training set, and evaluate their performance on the validation set, and pickthe best. Once we have picked the best, we can re\ufb01t it to all the available data. If we have aseparate test set, we can evaluate performance on this, in order to estimate the accuracy of ourmethod. (We discuss this in more detail in Section 6.5.3.)\nOften we use about 80% of the data for the training set, and 20% for the validation set. But\nif the number of training cases is small, this technique runs into problems, because the model\n11. In academic settings, we usually do have access to the test set, but we should not use it for model \ufb01tting or model\nselection, otherwise we will get an unrealistically optimistic estimate of performance of our method. This is one of the\n\u201cgolden rules\u201d of machine learning research.", "54": "24 Chapter 1. Introduction\n0 20 40 60 80 100 12000.050.10.150.20.250.30.35\nKmisclassification ratetrain\ntest\n(a)\n (b)\nFigure 1.21 (a) Misclassi\ufb01cation rate vs Kin a K-nearest neighbor classi\ufb01er. On the left, where Kis\nsmall, the model is complex and hence we over\ufb01t. On the right, where Kis large, the model is simple\nand we under\ufb01t. Dotted blue line: training set (size 200). Solid red line: test set (size 500). (b) Schematic\nof 5-fold cross validation. Figure generated by knnClassifyDemo .\nwon\u2019t have enough data to train on, and we won\u2019t have enough data to make a reliable estimate\nof the future performance.\nA simple but popular solution to this is to use cross validation (CV). The idea is simple: we\nsplit the training data into Kfolds; then, for each fold k\u2208{1,...,K}, we train on all the\nfolds but the k\u2019th, and test on the k\u2019th, in a round-robin fashion, as sketched in Figure 1.21(b).\nWe then compute the error averaged over all the folds, and use this as a proxy for the test error.\n(Note that each point gets predicted only once, although it will be used for training K\u22121times.)\nIt is common to use K=5; this is called 5-fold CV. If we set K=N, then we get a method\ncalled leave-one out cross validation ,o rLOOCV, since in fold i, we train on all the data cases\nexcept for i, and then test on i. Exercise 1.3 asks you to compute the 5-fold CV estimate of the\ntest error vs K, and to compare it to the empirical test error in Figure 1.21(a).\nChoosing Kfor a KNN classi\ufb01er is a special case of a more general problem known as model\nselection , where we have to choose between models with different degrees of \ufb02exibility. Cross-\nvalidation is widely used for solving such problems, although we will discuss other approaches\nlater in the book.\n1.4.9 No free lunch theorem\nAll models are wrong, but some models are useful. \u2014 George Box (Box and Draper 1987,\np424).12\nMuch of machine learning is concerned with devising different models, and different algorithms\nto \ufb01t them. We can use methods such as cross validation to empirically choose the best method\nfor our particular problem. However, there is no universally best model \u2014 this is sometimes\ncalled the no free lunch theorem (Wolpert 1996). The reason for this is that a set of assumptions\nthat works well in one domain may work poorly in another.\n12. George Box is a retired statistics professor at the University of Wisconsin.", "55": "1.4. Some basic concepts in machine learning 25\nAs a consequence of the no free lunch theorem, we need to develop many different types of\nmodels, to cover the wide variety of data that occurs in the real world. And for each model,\nthere may be many different algorithms we can use to train the model, which make differentspeed-accuracy-complexity tradeoffs. It is this combination of data, models and algorithms thatwe will be studying in the subsequent chapters.\nExercises\nExercise 1.1 KNN classi\ufb01er on shuffled MNIST data\nRunmnist1NNdemo and verify that the misclassi\ufb01cation rate (on the \ufb01rst 1000 test cases) of MNIST of a\n1-NN classi\ufb01er is 3.8%. (If you run it all on all 10,000 test cases, the error rate is 3.09%.) Modify the code\nso that you \ufb01rst randomly permute the features (columns of the training and test design matrices), as inshuffledDigitsDemo , and then apply the classi\ufb01er. Verify that the error rate is not changed.\nExercise 1.2 Approximate KNN classi\ufb01ers\nUse the Matlab/C++ code at http://people .cs.ubc.ca/~mariusm/index .php/FLANN/FLANN to per-\nform approximate nearest neighbor search, and combine it with mnist1NNdemo to classify the MNIST data\nset. How much speedup do you get, and what is the drop (if any) in accuracy?\nExercise 1.3 CV for KNN\nUseknnClassifyDemo to plot the CV estimate of the misclassi\ufb01cation rate on the test set. Compare this\nto Figure 1.21(a). Discuss the similarities and differences to the test error rate.", "56": "", "57": "2 Probability\n2.1 Introduction\nProbability theory is nothing but common sense reduced to calculation. \u2014 Pierre Laplace,\n1812\nIn the previous chapter, we saw how probability can play a useful role in machine learning. Inthis chapter, we discuss probability theory in more detail. We do not have to space to go intogreat detail \u2014 for that, you are better off consulting some of the excellent textbooks availableon this topic, such as (Jaynes 2003; Bertsekas and Tsitsiklis 2008; Wasserman 2004). But we willbrie\ufb02y review many of the key ideas you will need in later chapters.\nBefore we start with the more technical material, let us pause and ask: what is probability?\nWe are all familiar with the phrase \u201cthe probability that a coin will land heads is 0.5\u201d. But whatdoes this mean? There are actually at least two different interpretations of probability. One iscalled the frequentist interpretation. In this view, probabilities represent long run frequencies\nof events. For example, the above statement means that, if we \ufb02ip the coin many times, weexpect it to land heads about half the time.\n1\nThe other interpretation is called the Bayesian interpretation of probability. In this view,\nprobability is used to quantify our uncertainty about something; hence it is fundamentally\nrelated to information rather than repeated trials (Jaynes 2003). In the Bayesian view, the abovestatement means we believe the coin is equally likely to land heads or tails on the next toss.\nOne big advantage of the Bayesian interpretation is that it can be used to model our uncer-\ntainty about events that do not have long term frequencies. For example, we might want tocompute the probability that the polar ice cap will melt by 2020 CE. This event will happen zeroor one times, but cannot happen repeatedly. Nevertheless, we ought to be able to quantify ouruncertainty about this event; based on how probable we think this event is, we will (hopefully!)take appropriate actions (see Section 5.7 for a discussion of optimal decision making underuncertainty). To give some more machine learning oriented examples, we might have receiveda speci\ufb01c email message, and want to compute the probability it is spam. Or we might haveobserved a \u201cblip\u201d on our radar screen, and want to compute the probability distribution overthe location of the corresponding target (be it a bird, plane, or missile). In all these cases, theidea of repeated trials does not make sense, but the Bayesian interpretation is valid and indeed\n1. Actually, the Stanford statistician (and former professional magician) Persi Diaconis has shown that a coin is about\n51% likely to land facing the same way up as it started, due to the physics of the problem (Diaconis et al. 2007).", "58": "28 Chapter2. Probability\n0 1 2 3 4 500.250.50.751\n(a)0 1 2 3 4 500.250.50.751\n(b)\nFigure 2.1 (A) a uniform distribution on {1,2,3,4}, withp(x=k)=1/4. (b) a degenerate distribution\np(x)=1ifx=1andp(x)=0ifx\u2208{2,3,4}. Figure generated by discreteProbDistFig .\nquite natural. We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the\nbasic rules of probability theory are the same, no matter which interpretation is adopted.\n2.2 A brief review of probability theory\nThis section is a very brief review of the basics of probability theory, and is merely meant asa refresher for readers who may be \u201crusty\u201d. Readers who are already familiar with these basicsmay safely skip this section.\n2.2.1 Discrete random variables\nThe expression p(A)denotes the probability that the event Ais true. For example, Amight\nbe the logical expression \u201cit will rain tomorrow\u201d. We require that 0\u2264p(A)\u22641,w h e r e\np(A)=0means the event de\ufb01nitely will not happen, and p(A)=1means the event de\ufb01nitely\nwill happen. We write p(A)to denote the probability of the event not A; this is de\ufb01ned to\np(A)=1\u2212p(A). We will often write A=1to mean the event Ais true, and A=0to mean\nthe event Ais false.\nWe can extend the notion of binary events by de\ufb01ning a discrete random variable X, which\ncan take on any value from a \ufb01nite or countably in\ufb01nite set X. We denote the probability of\nthe event that X=xbyp(X=x), or justp(x)for short. Here p()is called a probability\nmass function orpmf. This satis\ufb01es the properties 0\u2264p(x)\u22641and/summationtext\nx\u2208Xp(x)=1.\nFigure 2.1 shows two pmf\u2019s de\ufb01ned on the \ufb01nite state space X={1,2,3,4,5}. On the left we\nhave a uniform distribution, p(x)=1/5, and on the right, we have a degenerate distribution,\np(x)= I(x=1 ),w h e r e I()is the binary indicator function. This distribution represents the\nfact thatXis always equal to the value 1, in other words, it is a constant.\n2.2.2 Fundamental rules\nIn this section, we review the basic rules of probability.", "59": "2.2. Abriefreviewofprobabilitytheory 29\n2.2.2.1 Probability of a union of two events\nGiven two events, AandB, we de\ufb01ne the probability of AorBas follows:\np(A\u2228B)=p( A)+p(B)\u2212p(A\u2227B) (2.1)\n=p(A)+p(B)ifAandBare mutually exclusive (2.2)\n2.2.2.2 Joint probabilities\nWe de\ufb01ne the probability of the joint event AandBas follows:\np(A,B)=p(A\u2227B)=p(A|B)p(B) (2.3)\nThis is sometimes called the product rule. Given a joint distribution on two events p(A,B),\nwe de\ufb01ne the marginal distribution as follows:\np(A)=/summationdisplay\nbp(A,B)=/summationdisplay\nbp(A|B=b)p(B=b) (2.4)\nwhere we are summing over all possible states of B. We can de\ufb01ne p(B)similarly. This is\nsometimes called the sum rule or therule of total probability.\nThe product rule can be applied multiple times to yield the chain rule of probability:\np(X1:D)=p(X1)p(X2|X1)p(X3|X2,X1)p(X4|X1,X2,X3)...p(XD|X1:D\u22121)(2.5)\nwhere we introduce the Matlab-like notation 1:Dto denote the set {1,2,...,D}.\n2.2.2.3 Conditional probability\nWe de\ufb01ne the conditional probability of eventA, given that event Bis true, as follows:\np(A|B)=p(A,B)\np(B)ifp(B)>0 (2.6)\n2.2.3 Bayes rule\nCombining the de\ufb01nition of conditional probability with the product and sum rules yields Bayes\nrule, also called Bayes Theorem2:\np(X=x|Y=y)=p(X=x,Y=y)\np(Y=y)=p(X=x)p(Y=y|X=x)/summationtext\nx/primep(X=x/prime)p(Y=y|X=x/prime)(2.7)\n2.2.3.1 Example: medical diagnosis\nAs an example of how to use this rule, consider the following medical diagonsis problem.\nSuppose you are a woman in your 40s, and you decide to have a medical test for breast cancercalled amammogram. If the test is positive, what is the probability you have cancer? That\nobviously depends on how reliable the test is. Suppose you are told the test has a sensitivity\n2. Thomas Bayes (1702\u20131761) was an English mathematician and Presbyterian minister.", "60": "30 Chapter2. Probability\nof 80%, which means, if you have cancer, the test will be positive with probability 0.8. In other\nwords,\np(x=1|y=1 )=0 .8 (2.8)\nwherex=1is the event the mammogram is positive, and y=1is the event you have breast\ncancer. Many people conclude they are therefore 80% likely to have cancer. But this is false! Itignores the prior probability of having breast cancer, which fortunately is quite low:\np(y=1 )=0 .004 (2.9)\nIgnoring this prior is called the base rate fallacy. We also need to take into account the fact\nthat the test may be a false positive orfalse alarm. Unfortunately, such false positives are\nquite likely (with current screening technology):\np(x=1|y=0 )=0 .1 (2.10)\nCombining these three terms using Bayes rule, we can compute the correct answer as follows:\np(y=1|x=1 ) =p(x=1|y=1 )p(y=1 )\np(x=1|y=1 )p(y=1 )+p(x=1|y=0 )p(y=0 )(2.11)\n=0.8\u00d70.004\n0.8\u00d70.004+0.1\u00d70.996=0.031 (2.12)\nwherep(y=0 )=1 \u2212p(y=1 )=0 .996. In other words, if you test positive, you only have\nabout a 3% chance of actually having breast cancer!3\n2.2.3.2 Example: Generative classi\ufb01ers\nWe can generalize the medical diagonosis example to classify feature vectors xof arbitrary type\nas follows:\np(y=c|x,\u03b8)=p(y=c|\u03b8)p(x|y=c,\u03b8)/summationtext\nc/primep(y=c/prime|\u03b8)p(x|y=c/prime,\u03b8)(2.13)\nThis is called a generative classi\ufb01er, since it speci\ufb01es how to generate the data using the class-\nconditional density p(x|y=c)and the class prior p(y=c). We discuss such models in detail\nin Chapters 3 and 4. An alternative approach is to directly \ufb01t the class posterior, p(y=c|x);\nthis is known as a discriminative classi\ufb01er. We discuss the pros and cons of the two approachesin Section 8.6.\n2.2.4 Independence and conditional independence\nWe sayXandYareunconditionally independent ormarginally independent, denoted\nX\u22a5Y, if we can represent the joint as the product of the two marginals (see Figure 2.2), i.e.,\nX\u22a5Y\u21d0\u21d2p(X,Y)=p(X)p(Y) (2.14)", "61": "2.2. Abriefreviewofprobabilitytheory 31\n\u0006\u0002\u0007\u0004\u0001\b\u0003\u0006\u0002\b\u0003\n\u0006\u0002\u0007\u0003\u0005\nFigure 2.2 Computing p(x,y)=p(x)p(y),w h e r eX\u22a5Y.H e r eXandYare discrete random variables;\nXhas 6 possible states (values) and Yhas 5 possible states. A general joint distribution on two such\nvariables would require (6\u00d75)\u22121=2 9parameters to de\ufb01ne it (we subtract 1 because of the sum-to-one\nconstraint). By assuming (unconditional) independence, we only need (6\u22121)+(5\u22121) = 9parameters\nto de\ufb01ne p(x,y).\nIn general, we say a set of variables is mutually independent if the joint can be written as a\nproduct of marginals.\nUnfortunately, unconditional independence is rare, because most variables can in\ufb02uence most\nother variables. However, usually this in\ufb02uence is mediated via other variables rather than beingdirect. We therefore say XandYareconditionally independent (CI) given Ziff the conditional\njoint can be written as a product of conditional marginals:\nX\u22a5Y|Z\u21d0\u21d2p(X,Y|Z)=p(X|Z)p(Y|Z) (2.15)\nWhen we discuss graphical models in Chapter 10, we will see that we can write this assumptionas a graph X\u2212Z\u2212Y, which captures the intuition that all the dependencies between XandY\nare mediated via Z. For example, the probability it will rain tomorrow (event X) is independent\nof whether the ground is wet today (event Y), given knowledge of whether it is raining today\n(eventZ). Intuitively, this is because Z\u201ccauses\u201d both XandY, so if we know Z,w ed on o t\nneed to know about Yin order to predict Xor vice versa. We shall expand on this concept in\nChapter 10.\nAnother characterization of CI is this:\nTheorem 2.2.1. X\u22a5Y|Ziffthereexistfunction gandhsuchthat\np(x,y|z)=g(x,z)h(y,z) (2.16)\nforallx,y,zsuch\n thatp(z)>0.\n3. These numbers are from (McGrayne 2011, p257). Based on this analysis, the US government decided not to recommend\nannual mammogram screening to women in their 40s: the number of false alarms would cause needless worry and\nstress amongst women, and result in unnecesssary, expensive, and potentially harmful followup tests. See Section 5.7\nfor the optimal way to trade off risk reverse reward in the face of uncertainty.", "62": "32 Chapter2. Probability\nSee Exercise 2.8 for the proof.\nCI assumptions allow us to build large probabilistic models from small pieces. We will see\nmany examples of this throughout the book. In particular, in Section 3.5, we discuss naive Bayesclassi\ufb01ers, in Section 17.2, we discuss Markov models, and in Chapter 10 we discuss graphicalmodels; all of these models heavily exploit CI properties.\n2.2.5 Continuous random variables\nSo far, we have only considered reasoning about uncertain discrete quantities. We will now show(following (Jaynes 2003, p107)) how to extend probability to reason about uncertain continuousquantities.\nSupposeXis some uncertain continuous quantity. The probability that Xlies in any interval\na\u2264X\u2264bcan be computed as follows. De\ufb01ne the events A=(X\u2264a),B=(X\u2264b)and\nW=(a<X\u2264b). We have that B=A\u2228W, and since AandWare mutually exclusive, the\nsum rules gives\np(B)=p(A)+p(W) (2.17)\nand hence\np(W)=p(B)\u2212p(A) (2.18)\nDe\ufb01ne the function F(q)/definesp(X\u2264q). This is called the cumulative distribution function\norcdfofX. This is obviously a monotonically increasing function. See Figure 2.3(a) for an\nexample. Using this notation we have\np(a<X\u2264b)=F(b)\u2212F(a) (2.19)\nNow \nde\ufb01nef(x)=d\ndxF(x)(we assume this derivative exists); this is called the probability\ndensity function orpdf. See Figure 2.3(b) for an example. Given a pdf, we can compute the\nprobability of a continuous variable being in a \ufb01nite interval as follows:\nP(a<X\u2264b)=/integraldisplayb\naf(x)dx (2.20)\nAs the size of the interval gets smaller, we can write\nP(x\u2264X\u2264x+dx)\u2248p(x)dx (2.21)\nWe require p(x)\u22650, but it is possible for p(x)>1for any given x, so long as the density\nintegrates to 1. As an example, consider the uniform distribution Unif(a,b):\nUnif(x|a,b)=1\nb\u2212aI(a\u2264x\u2264b) (2.22)\nIf we seta=0andb=1\n2,w eh a v ep( x)=2for anyx\u2208[0,1\n2].", "63": "2.2. Abriefreviewofprobabilitytheory 33\n\u22123 \u22122 \u22121 0 1 2 3020406080100CDF\n(a)\u03a6\u22121(\u03b1/2) 0 \u03a6\u22121(1\u2212\u03b1/2)\u03b1/2 \u03b1/2\n(b)\nFigure 2.3 (a) Plot of the cdf for the standard normal, N(0,1). (b) Corresponding pdf. The shaded\nregions each contain \u03b1/2of the probability mass. Therefore the nonshaded region contains 1\u2212\u03b1of the\nprobability mass. If the distribution is Gaussian N(0,1), then the leftmost cutoff point is \u03a6\u22121(\u03b1/2),w h e r e\n\u03a6is the cdf of the Gaussian. By symmetry, the rightost cutoff point is \u03a6\u22121(1\u2212\u03b1/2) =\u2212\u03a6\u22121(\u03b1/2).I f\n\u03b1=0.05, the central interval is 95%, and the left cutoff is -1.96 and the right is 1.96. Figure generated by\nquantileDemo.\n2.2.6 Quantiles\nSince the cdf Fis a monotonically increasing function, it has an inverse; let us denote this by\nF\u22121.I fFis the cdf of X, thenF\u22121(\u03b1)is the value of x\u03b1such that P(X\u2264x\u03b1)=\u03b1; this is\ncalled the \u03b1quantile ofF. The value F\u22121(0.5)is themedianof the distribution, with half of\nthe probability mass on the left, and half on the right. The values F\u22121(0.25)andF\u22121(0.75)\nare the lower and upper quartiles.\nWe can also use the inverse cdf to compute tail area probabilities. For example, if \u03a6is\nthe cdf of the Gaussian distribution N(0,1), then points to the left of \u03a6\u22121(\u03b1)/2)contain\u03b1/2\nprobability mass, as illustrated in Figure 2.3(b). By symmetry, points to the right of \u03a6\u22121(1\u2212\u03b1/2)\nalso contain \u03b1/2of the mass. Hence the central interval (\u03a6\u22121(\u03b1/2),\u03a6\u22121(1\u2212\u03b1/2))contains\n1\u2212\u03b1of the mass. If we set \u03b1=0.05, the central 95% interval is covered by the range\n(\u03a6\u22121(0.025),\u03a6\u22121(0.975)) = (\u22121. 96,1.96) (2.23)\nIf the distribution is N(\u03bc,\u03c32), then the 95% interval becomes (\u03bc\u22121.96\u03c3,\u03bc+1.96\u03c3). This is\nsometimes approximated by writing \u03bc\u00b12\u03c3.\n2.2.7 Mean and variance\nThe most familiar property of a distribution is its mean,o rexpected value, denoted by \u03bc.F o r\ndiscrete rv\u2019s, it is de\ufb01ned as E[X]/defines/summationtext\nx\u2208Xxp(x), and for continuous rv\u2019s, it is de\ufb01ned as\nE[X]/defines/integraltext\nXxp(x)dx. If this integral is not \ufb01nite, the mean is not de\ufb01ned (we will see some\nexamples of this later).\nThevariance is a measure of the \u201cspread\u201d of a distribution, denoted by \u03c32. This is de\ufb01ned", "64": "34 Chapter2. Probability\nas follows:\nvar[X]/definesE/bracketleftbig\n(X\u2212\u03bc)2/bracketrightbig\n=/integraldisplay\n(x\u2212\u03bc)2p(x)dx (2.24)\n=/integraldisplay\nx2p(x)dx+\u03bc2/integraldisplay\np(x)dx\u22122\u03bc/integraldisplay\nxp(x)dx=E/bracketleftbig\nX2/bracketrightbig\n\u2212\u03bc2(2.25)\nfrom which we derive the useful result\nE/bracketleftbig\nX2/bracketrightbig\n=\u03bc2+\u03c32(2.26)\nThestandard deviation is de\ufb01ned as\nstd[X]/defines/radicalbig\nvar[X] (2.27)\nThis is useful since it has the same units as Xitself.\n2.3 Some common discrete distributions\nIn this section, we review some commonly used parametric distributions de\ufb01ned on discrete\nstate spaces, both \ufb01nite and countably in\ufb01nite.\n2.3.1 The binomial and Bernoulli distributions\nSuppose we toss a coin ntimes. Let X\u2208{0,...,n}be the number of heads. If the probability\nof heads is \u03b8, then we say Xhas abinomial distribution, written as X\u223cBin(n,\u03b8). The pmf\nis given by\nBin(k|n,\u03b8)/defines/parenleftbiggn\nk/parenrightbigg\n\u03b8k(1\u2212\u03b8)n\u2212k(2.28)\nwhere/parenleftbiggn\nk/parenrightbigg\n/definesn!\n(n\u2212k)!k!(2.29)\nis the number of ways to choose kitems from n(this is known as the binomial coefficient,\nand is pronounced \u201cn choose k\u201d). See Figure 2.4 for some examples of the binomial distribution.This distribution has the following mean and variance:\nmean=\u03b8,var=n\u03b8(1\u2212\u03b8) (2.30)\nNow suppose we toss a coin only once. Let X\u2208{0,1}be a binary random variable, with\nprobability of \u201csuccess\u201d or \u201cheads\u201d of \u03b8. We say that Xhas aBernoulli distribution. This is\nwritten as X\u223cBer(\u03b8), where the pmf is de\ufb01ned as\nBer(x|\u03b8)=\u03b8\nI(x=1)(1\u2212\u03b8)I(x=0)(2.31)\nIn other words,\nBer(x|\u03b8)=/braceleftbigg\u03b8ifx=1\n1\u2212\u03b8ifx=0(2.32)\nThis is obviously just a special case of a Binomial distribution with n=1.", "65": "2.3. Somecommondiscretedistributions 35\n0 1 2 3 4 5 6 7 8 9 1000.050.10.150.20.250.30.35\u03b8=0.250\n(a)0 1 2 3 4 5 6 7 8 9 1000.050.10.150.20.250.30.350.4\u03b8=0.900\n(b)\nFigure 2.4 Illustration of the binomial distribution with n=1 0and\u03b8\u2208{0.25,0.9}. Figure generated\nbybinomDistPlot.\n2.3.2 The multinomial and multinoulli distributions\nThe binomial distribution can be used to model the outcomes of coin tosses. To model the\noutcomes of tossing a K-sided die, we can use the multinomial distribution. This is de\ufb01ned as\nfollows: let x=(x1,...,x K)be a random vector, where xjis the number of times side jof\nthe die occurs. Then xhas the following pmf:\nMu(x|n,\u03b8)/defines/parenleftbiggn\nx1...xK/parenrightbiggK/productdisplay\nj=1\u03b8xj\nj (2.33)\nwhere\u03b8jis the probability that side jshows up, and\n/parenleftbiggn\nx1...xK/parenrightbigg\n/definesn!\nx1!x2!\u00b7\u00b7\u00b7xK!(2.34)\nis themultinomial coefficient (the number of ways to divide a set of size n=/summationtextK\nk=1xkinto\nsubsets with sizes x1up toxK).\nNow suppose n=1. This is like rolling a K-sided dice once, so xwill be a vector of 0s\nand 1s (a bit vector), in which only one bit can be turned on. Speci\ufb01cally, if the dice shows\nup as face k, then the k\u2019th bit will be on. In this case, we can think of xas being a scalar\ncategorical random variable with Kstates (values), and xis itsdummy encoding, that is,\nx=[I(x=1 ),..., I(x=K)]. For example, if K=3, we encode the states 1, 2 and 3 as\n(1,0,0),(0,1,0), and(0,0,1). This is also called a one-hot encoding, since we imagine that\nonly one of the K\u201cwires\u201d is \u201chot\u201d or on. In this case, the pmf becomes\nMu(x|1,\u03b8)=K/productdisplay\nj=1\u03b8I(xj=1)\nj (2.35)\nSee Figure 2.1(b-c) for an example. This very common special case is known as a categorical\nordiscretedistribution. (Gustavo Lacerda suggested we call it the multinoulli distribution,b y\nanalogy with the Binomial/ Bernoulli distinction, a term which we shall adopt in this book.) We", "66": "36 Chapter2. Probability\nName nKx\nMultinomial - - x\u2208{0,1,...,n}K,/summationtextK\nk=1xk=n\nMultinoulli 1 - x\u2208{0,1}K,/summationtextK\nk=1xk=1(1-of-Kencoding)\nBinomial - 1 x\u2208{0,1,...,n}\nBernoulli 1 1 x\u2208{0,1}\nTable 2.1 Summary of the multinomial and related distributions.\na t a g c c g g t a c g g c a \nt t a g c t g c a a c c g c a \nt c a g c c a c t a g a g c a \na t a a c c g c g a c c g c a \nt t a g c c g c t a a g g t a \nt a a g c c t c g t a c g t a \nt t a g c c g t t a c g g c c \na t a t c c g g t a c a g t a \na t a g c a g g t a c c g a a \na c a t c c g t g a c g g a a\n(a)123456789101112131415012\nSequence PositionBits\n(b)\nFigure 2.5 (a) Some aligned DNA sequences. (b) The corresponding sequence logo. Figure generated by\nseqlogoDemo .\nwill use the following notation for this case:\nCat(x|\u03b8)/definesMu(x|1,\u03b8) (2.36)\nIn otherwords, if x\u223cCat(\u03b8), thenp(x=j|\u03b8)=\u03b8j. See Table 2.1 for a summary.\n2.3.2.1 Application: DNA sequence motifs\nAn interesting application of multinomial models arises in biosequence analysis . Suppose\nwe have a set of (aligned) DNA sequences, such as in Figure 2.5(a), where there are 10 rows\n(sequences) and 15 columns (locations along the genome). We see that several locations are con-\nserved by evolution (e.g., because they are part of a gene coding region), since the corresponding\ncolumns tend to be \u201cpure\u201d. For example, column 7 is all G\u2019s.\nOne way to visually summarize the data is by using a sequence logo : see Figure 2.5(b). We\nplot the letters A, C, G and T with a fontsize proportional to their empirical probability, and with\nthe most probable letter on the top. The empirical probability distribution at location t,\u02c6\u03b8t,i s\ngotten by normalizing the vector of counts (see Equation 3.48 ):\nNt=/parenleftBiggN/summationdisplay\ni=1I(Xit=1 ),N/summationdisplay\ni=1I(Xit=2 ),N/summationdisplay\ni=1I(Xit=3 ),N/summationdisplay\ni=1I(Xit=4 )/parenrightBigg\n(2.37)\n\u02c6\u03b8t=Nt/N (2.38)\nThis distribution is known as a motif. We can also compute the most probable letter in each\nlocation; this is called the consensus sequence .", "67": "2.3. Somecommondiscretedistributions 37\n0 5 10 15 20 25 3000.050.10.150.20.250.30.350.4Poi(\u03bb=1.000)\n(a)0 5 10 15 20 25 3000.020.040.060.080.10.120.14Poi(\u03bb=10.000)\n(b)\nFigure 2.6 Illustration of some Poisson distributions for \u03bb\u2208{1,10}. We have truncated the x-axis to\n25 for clarity, but the support of the distribution is over all the non-negative integers. Figure generated by\npoissonPlotDemo.\n2.3.3 The Poisson distribution\nWe say that X\u2208{0,1,2,...}has aPoisson distribution with parameter \u03bb>0, written\nX\u223cPoi(\u03bb), if its pmf is\nPoi(x|\u03bb)=e\u2212\u03bb\u03bbx\nx!(2.39)\nThe \ufb01rst term is just the normalization constant, required to ensure the distribution sums to 1.\nThe Poisson distribution is often used as a model for counts of rare events like radioactive\ndecay and traffic accidents. See Figure 2.6 for some plots.\n2.3.4 The empirical distribution\nGiven a set of data, D={x1,...,x N}, we de\ufb01ne the empirical distribution, also called the\nempirical measure, as follows:\npemp(A)/defines1\nNN/summationdisplay\ni=1\u03b4xi(A) (2.40)\nwhere\u03b4x(A)is theDirac measure, de\ufb01ned by\n\u03b4x(A)=/braceleftbigg0ifx/negationslash\u2208A\n1ifx\u2208A(2.41)\nIn general, we can associate \u201cweights\u201d with each sample:\np(x)=N/summationdisplay\ni=1wi\u03b4xi(x) (2.42)\nwhere we require 0\u2264wi\u22641and/summationtextN\ni=1wi=1. We can think of this as a histogram, with\n\u201cspikes\u201d at the data points xi,w h e r ewidetermines the height of spike i. This distribution\nassigns 0 probability to any point not in the data set.", "68": "38 Chapter2. Probability\n2.4 Some common continuous distributions\nIn this section we present some commonly used univariate (one-dimensional) continuous prob-\nability distributions.\n2.4.1 Gaussian (normal) distribution\nThe most widely used distribution in statistics and machine learning is the Gaussian or normaldistribution. Its pdf is given by\nN(x|\u03bc,\u03c3\n2)/defines1\u221a\n2\u03c0\u03c32e\u22121\n2\u03c32(x\u2212\u03bc)2(2.43)\nHere\u03bc=E[X]is the mean (and mode), and \u03c32= var[X]is the variance.\u221a\n2\u03c0\u03c32is the\nnormalization constant needed to ensure the density integrates to 1 (see Exercise 2.11).\nWe write X\u223cN(\u03bc,\u03c32)to denote that p(X=x)=N(x|\u03bc,\u03c32).I fX\u223cN(0,1),w e\nsayXfollows a standard normal distribution. See Figure 2.3(b) for a plot of this pdf; this is\nsometimes called the bell curve.\nWe will often talk about the precision of a Gaussian, by which we mean the inverse variance:\n\u03bb=1/\u03c32. A high precision means a narrow distribution (low variance) centered on \u03bc.4\nNote that, since this is a pdf, we can have p(x)>1. To see this, consider evaluating the\ndensity at its center, x=\u03bc.W eh a v e N(\u03bc|\u03bc,\u03c32)=(\u03c3\u221a\n2\u03c0)\u22121e0,so if\u03c3<1/\u221a\n2\u03c0,w eh a v e\np(x)>1.\nThe cumulative distribution function or cdf of the Gaussian is de\ufb01ned as\n\u03a6(x;\u03bc,\u03c32)/defines/integraldisplayx\n\u2212\u221eN(z|\u03bc,\u03c32)dz (2.44)\nSee Figure 2.3(a) for a plot of this cdf when \u03bc=0,\u03c32=1. This integral has no closed form\nexpression, but is built in to most software packages. In particular, we can compute it in termsof theerror function (erf):\n\u03a6(x;\u03bc,\u03c3)=1\n2[1+erf(z/\u221a\n2)] (2.45)\nwherez=(x\u2212\u03bc)/\u03c3and\nerf(x)/defines2\u221a\u03c0/integraldisplayx\n0e\u2212t2dt (2.46)\nThe Gaussian distribution is the most widely used distribution in statistics. There are several\nreasons for this. First, it has two parameters which are easy to interpret, and which capturesome of the most basic properties of a distribution, namely its mean and variance. Second,the central limit theorem (Section 2.6.3) tells us that sums of independent random variableshave an approximately Gaussian distribution, making it a good choice for modeling residualerrors or \u201cnoise\u201d. Third, the Gaussian distribution makes the least number of assumptions (has\n4. The symbol \u03bbwill have many different meanings in this book, in order to be consistent with the rest of the literature.\nThe intended meaning should be clear from context.", "69": "2.4. Somecommoncontinuousdistributions 39\nmaximum entropy), subject to the constraint of having a speci\ufb01ed mean and variance, as we\nshow in Section 9.2.6; this makes it a good default choice in many cases. Finally, it has a simplemathematical form, which results in easy to implement, but often highly effective, methods, aswe will see. See (Jaynes 2003, ch 7) for a more extensive discussion of why Gaussians are sowidely used.\n2.4.2 Degenerate pdf\nIn the limit that \u03c32\u21920, the Gaussian becomes an in\ufb01nitely tall and in\ufb01nitely thin \u201cspike\u201d\ncentered at \u03bc:\nlim\n\u03c32\u21920N(x|\u03bc,\u03c32)=\u03b4(x\u2212\u03bc) (2.47)\nwhere\u03b4is called a Dirac delta function, and is de\ufb01ned as\n\u03b4(x)=/braceleftbigg\u221eifx=0\n0ifx/negationslash=0(2.48)\nsuch that\n/integraldisplay\u221e\n\u2212\u221e\u03b4(x)dx=1 (2.49)\nA useful property of delta functions is the sifting property, which selects out a single term\nfrom a sum or integral:\n/integraldisplay\u221e\n\u2212\u221ef(x)\u03b4(x\u2212\u03bc)dx=f(\u03bc) (2.50)\nsince the integrand is only non-zero if x\u2212\u03bc=0.\nOne problem with the Gaussian distribution is that it is sensitive to outliers, since the log-\nprobability only decays quadratically with distance from the center. A more robust distributionis theStudenttdistribution\n5Its pdf is as follows:\nT(x|\u03bc,\u03c32,\u03bd)\u221d/bracketleftBigg\n1+1\n\u03bd/parenleftbiggx\u2212\u03bc\n\u03c3/parenrightbigg2/bracketrightBigg\u2212(\u03bd+1\n2)\n(2.51)\nwhere\u03bcis the mean, \u03c32>0is the scale parameter, and \u03bd>0is called the degrees of\nfreedom. See Figure 2.7 for some plots. For later reference, we note that the distribution hasthe following properties:\nmean=\u03bc,mode=\u03bc,var=\u03bd\u03c3\n2\n(\u03bd\u22122)(2.52)\n5. This distribution has a colourful etymology. It was \ufb01rst published in 1908 by William Sealy Gosset, who worked at the\nGuinness brewery in Dublin. Since his employer would not allow him to use his own name, he called it the \u201cStudent\u201d\ndistribution. The origin of the term tseems to have arisen in the context of Tables of the Student distribution, used by\nFisher when developing the basis of classical statistical inference. See http://jeff560.tripod.com/s.html for more\nhistorical details.", "70": "40 Chapter2. Probability\n\u22124 \u22123 \u22122 \u22121 0 1 2 3 400.10.20.30.40.50.60.70.8\n  \nGauss\nStudent\nLaplace\n(a)\u22124 \u22123 \u22122 \u22121 0 1 2 3 4\u22129\u22128\u22127\u22126\u22125\u22124\u22123\u22122\u221210\n  \nGauss\nStudent\nLaplace\n(b)\nFigure 2.7 (a) The pdf\u2019s for a N(0,1),T(0,1,1)andLap(0,1/\u221a\n2). The mean is 0 and the variance\nis 1 for both the Gaussian and Laplace. The mean and variance of the Student is unde\ufb01ned when \u03bd=1.\n(b) Log of these pdf\u2019s. Note that the Student distribution is not log-concave for any parameter value, unlike\nthe Laplace distribution, which is always log-concave (and log-convex...) Nevertheless, both are unimodal.Figure generated by studentLaplacePdfPlot .\n\u22125 0 5 1000.10.20.30.40.5\n  \ngaussian\nstudent T\nlaplace\n(a)\u22125 0 5 1000.10.20.30.40.5\n  \ngaussian\nstudent T\nlaplace\n(b)\nFigure 2.8 Illustration of the effect of outliers on \ufb01tting Gaussian, Student and Laplace distributions. (a)\nNo outliers (the Gaussian and Student curves are on top of each other). (b) With outliers. We see that theGaussian is more affected by outliers than the Student and Laplace distributions. Based on Figure 2.16 of(Bishop 2006a). Figure generated by robustDemo.\nThe variance is only de\ufb01ned if \u03bd>2. The mean is only de\ufb01ned if \u03bd>1.\nAs an illustration of the robustness of the Student distribution, consider Figure 2.8. On the\nleft, we show a Gaussian and a Student \ufb01t to some data with no outliers. On the right, we\nadd some outliers. We see that the Gaussian is affected a lot, whereas the Student distributionhardly changes. This is because the Student has heavier tails, at least for small \u03bd(see Figure 2.7).\nIf\u03bd=1, this distribution is known as the CauchyorLorentzdistribution. This is notable\nfor having such heavy tails that the integral that de\ufb01nes the mean does not converge.\nTo ensure \ufb01nite variance, we require \u03bd>2. It is common to use \u03bd=4, which gives good\nperformance in a range of problems (Lange et al. 1989). For \u03bd/greatermuch5, the Student distribution\nrapidly approaches a Gaussian distribution and loses its robustness properties.", "71": "2.4. Somecommoncontinuousdistributions 41\n1 2 3 4 5 6 70.10.20.30.40.50.60.70.80.9Gamma distributions\n  \na=1.0,b=1.0\na=1.5,b=1.0\na=2.0,b=1.0\n(a)0 0.5 1 1.5 2 2.500.511.522.533.5\n(b)\nFigure 2.9 (a) Some Ga(a,b=1 )distributions. If a\u22641, the mode is at 0, otherwise it is >0.A s\nwe increase the rate b, we reduce the horizontal scale, thus squeezing everything leftwards and upwards.\nFigure generated by gammaPlotDemo. (b) An empirical pdf of some rainfall data, with a \ufb01tted Gamma\ndistribution superimposed. Figure generated by gammaRainfallDemo .\n2.4.3 The Laplace distribution\nAnother distribution with heavy tails is the Laplace distribution6, also known as the double\nsided exponential distribution. This has the following pdf:\nLap(x|\u03bc,b)/defines1\n2bexp/parenleftbigg\n\u2212|x\u2212\u03bc|\nb/parenrightbigg\n(2.53)\nHere\u03bcis a location parameter and b>0is a scale parameter. See Figure 2.7 for a plot. This\ndistribution has the following properties:\nmean=\u03bc,mode=\u03bc,var=2b2(2.54)\nIts robustness to outliers is illustrated in Figure 2.8. It also put mores probability density at 0\nthan the Gaussian. This property is a useful way to encourage sparsity in a model, as we willsee in Section 13.3.\n2.4.4 The gamma distribution\nThegamma distribution is a \ufb02exible distribution for positive real valued rv\u2019s, x>0.I t i s\nde\ufb01ned in terms of two parameters, called the shape a>0and the rate b>0:7\nGa(T|shape =a,rate =b)/definesba\n\u0393(a)Ta\u22121e\u2212Tb(2.55)\n6. Pierre-Simon Laplace (1749\u20131827) was a French mathematician, who played a key role in creating the \ufb01eld of Bayesian\nstatistics.\n7. There is an alternative parameterization, where we use the scale parameter instead of the rate: Gas(T |a,b)/defines\nGa(T|a,1/b). This version is the one used by Matlab\u2019s gampdf, although in this book will use the rate parameterization\nunless otherwise speci\ufb01ed.", "72": "42 Chapter2. Probability\nwhere\u0393(a)is the gamma function:\n\u0393(x)/defines/integraldisplay\u221e\n0ux\u22121e\u2212udu (2.56)\nSee Figure 2.9 for some plots. For later reference, we note that the distribution has the following\nproperties:\nmean=a\nb,mode=a\u22121\nb,var=a\nb2(2.57)\nThere are several distributions which are just special cases of the Gamma, which we discuss\nbelow.\n\u2022Exponential distribution This is de\ufb01ned by Expon(x|\u03bb)/definesGa(x|1,\u03bb),w h e r e\u03bbis the rate\nparameter. This distribution describes the times between events in a Poisson process, i.e. a\nprocess in which events occur continuously and independently at a constant average rate \u03bb.\n\u2022Erlang distribution This is the same as the Gamma distribution where ais an integer. It\nis common to \ufb01x a=2, yielding the one-parameter Erlang distribution, Erlang(x|\u03bb)=\nGa(x|2,\u03bb),w h e r e\u03bb is the rate parameter.\n\u2022Chi-squared distribution This is de\ufb01ned by \u03c72(x|\u03bd)/definesGa(x|\u03bd\n2,1\n2). This is the distribution\nof the sum of squared Gaussian random variables. More precisely, if Zi\u223cN(0,1), and\nS=/summationtext\u03bd\ni=1Z2\ni, thenS\u223c\u03c72\n\u03bd.\nAnother useful result is the following: If X\u223cGa(a,b), then one can show (Exercise 2.10)\nthat1\nX\u223cIG(a,b),w h e r eIG is theinverse gamma distribution de\ufb01ned by\nIG(x|shape =a,scale =b)/definesba\n\u0393(a)x\u2212(a+1)e\u2212b/x(2.58)\nThe distribution has these properties\nmean=b\na\u22121,mode=b\na+1,var=b2\n(a\u22121)2(a\u22122), (2.59)\nThe mean only exists if a>1. The variance only exists if a>2.\nWe will see applications of these distributions later on.\n2.4.5 The beta distribution\nThebeta distribution has support over the interval [0,1]and is de\ufb01ned as follows:\nBeta(x|a,b)=1\nB(a,b)xa\u22121(1\u2212x)b\u22121(2.60)\nHereB(p,q)is the beta function,\nB(a,b)/defines\u0393(a)\u0393(b)\n\u0393(a+b)(2.61)\nSee Figure 2.10 for plots of some beta distributions. We require a,b >0to ensure the distribution\nis integrable (i.e., to ensure B(a,b)exists). If a=b=1, we get the uniform distirbution. If", "73": "2.4. Somecommoncontinuousdistributions 43\n0 0.2 0.4 0.6 0.8 100.511.522.53beta distributions\n  \na=0.1, b=0.1\na=1.0, b=1.0\na=2.0, b=3.0\na=8.0, b=4.0\nFigure 2.10 Some beta distributions. Figure generated by betaPlotDemo.\naandbare both less than 1, we get a bimodal distribution with \u201cspikes\u201d at 0 and 1; if aand\nbare both greater than 1, the distribution is unimodal. For later reference, we note that the\ndistribution has the following properties (Exercise 2.16):\nmean=a\na+b,mode=a\u22121\na+b\u22122,var=ab\n(a+b)2(a+b+1)(2.62)\n2.4.6 Pareto distribution\nThePareto distribution is used to model the distribution of quantities that exhibit long tails,\nalso called heavy tails. For example, it has been observed that the most frequent word in\nEnglish (\u201cthe\u201d) occurs approximately twice as often as the second most frequent word (\u201cof\u201d),\nwhich occurs twice as often as the fourth most frequent word, etc. If we plot the frequency ofwords vs their rank, we will get a power law; this is known as Zipf\u2019s law. Wealth has a similarly\nskewed distribution, especially in plutocracies such as the USA.\n8\nThe Pareto pdf is de\ufb01ned as follow:\nPareto(x|k,m)=kmkx\u2212(k+1)I(x\u2265m) (2.63)\nThis density asserts that xmust be greater than some constant m, but not too much greater,\nwherekcontrols what is \u201ctoo much\u201d. As k\u2192\u221e, the distribution approaches \u03b4(x\u2212m). See\nFigure 2.11(a) for some plots. If we plot the distibution on a log-log scale, it forms a straightline, of the form logp(x)=alogx+cfor some constants aandc. See Figure 2.11(b) for an\nillustration (this is known as a power law). This distribution has the following properties\nmean=km\nk\u22121ifk>1,mode=m,var=m2k\n(k\u22121)2(k\u22122)ifk>2 (2.64)\n8. In the USA, 400 Americans have more wealth than half of all Americans combined. (Source:\nhttp://www.politifact.com/wisconsin/statements/2011/mar/10/michael-moore/michael-moore-s\nays-400-americans-have-more-wealth- .) See (Hacker and Pierson 2010) for a political analysis of how such an\nextreme distribution of income has arisen in a democratic country.", "74": "44 Chapter2. Probability\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 500.20.40.60.811.21.41.61.82Pareto distribution\n  \nm=0.01, k=0.10\nm=0.00, k=0.50\nm=1.00, k=1.00\n(a)10010\u2212310\u2212210\u22121100101Pareto(m=1, k) on log scale\n  \nk=1.0\nk=2.0\nk=3.0\n(b)\nFigure 2.11 (a) The Pareto distribution Pareto(x |m,k)form=1. (b) The pdf on a log-log scale. Figure\ngenerated by paretoPlot.\n2.5 Joint probability distributions\nSo far, we have been mostly focusing on modeling univariate probability distributions. In this\nsection, we start our discussion of the more challenging problem of building joint probabilitydistributions on multiple related random variables; this will be a central topic in this book.\nAjoint probability distribution has the form p(x\n1,...,x D)f o ras e to fD> 1variables,\nand models the (stochastic) relationships between the variables. If all the variables are discrete,we can represent the joint distribution as a big multi-dimensional array, with one variable perdimension. However, the number of parameters needed to de\ufb01ne such a model is O(K\nD),\nwhereKis the number of states for each variable.\nWe can de\ufb01ne high dimensional joint distributions using fewer parameters by making con-\nditional independence assumptions, as we explain in Chapter 10. In the case of continuousdistributions, an alternative approach is to restrict the form of the pdf to certain functionalforms, some of which we will examine below.\n2.5.1 Covariance and correlation\nThecovariance between two rv\u2019s XandYmeasures the degree to which XandYare (linearly)\nrelated. Covariance is de\ufb01ned as\ncov[X,Y ]/definesE[(X\u2212E[X])(Y\u2212E[Y])] = E[XY]\u2212E[X]E[Y] (2.65)", "75": "2.5. Jointprobabilitydistributions 45\nFigure 2.12 Several sets of (x,y)points, with the correlation coefficient of xandyfor each set. Note\nthat the correlation re\ufb02ects the noisiness and direction of a linear relationship (top row), but not the slope\nof that relationship (middle), nor many aspects of nonlinear relationships (bottom). N.B.: the \ufb01gure in the\ncenter has a slope of 0 but in that case the correlation coefficient is unde\ufb01ned because the variance of Y\nis zero. Source: http://en .wikipedia .org/wiki/File:Correlation_examples .png\nIfxis ad-dimensional random vector, its covariance matrix is de\ufb01ned to be the following\nsymmetric, positive de\ufb01nite matrix:\ncov[x]/definesE/bracketleftBig\n(x\u2212E[x])(x\u2212E[x])T/bracketrightBig\n(2.66)\n=\u239b\n\u239c\u239c\u239c\u239dvar[X1]c o v [ X1,X2]\u00b7\u00b7\u00b7cov[X1,Xd]\ncov[X2,X1] var[ X2]\u00b7\u00b7\u00b7cov[X2,Xd]\n............\ncov[Xd,X1]c o v [Xd,X2]\u00b7\u00b7\u00b7var[Xd]\u239e\n\u239f\u239f\u239f\u23a0(2.67)\nCovariances can be between 0 and in\ufb01nity. Sometimes it is more convenient to work with a\nnormalized measure, with a \ufb01nite upper bound. The (Pearson) correlation coefficient between\nXandYis de\ufb01ned as\ncorr[X,Y]/definescov[X,Y]/radicalbig\nvar[X]var[Y](2.68)\nAcorrelation matrix has the form\nR=\u239b\n\u239c\u239dcorr[X1,X1] corr[X1,X2]\u00b7\u00b7\u00b7corr[X1,Xd]\n............\ncorr[Xd,X1] corr[Xd,X2]\u00b7\u00b7\u00b7corr[Xd,Xd]\u239e\n\u239f\u23a0 (2.69)\nOne can show (Exercise 4.3) that \u22121\u2264corr[X,Y]\u22641. Hence in a correlation matrix, each\nentry on the diagonal is 1, and the other entries are between -1 and 1.\nOne can also show that corr[X,Y]=1if and only if Y=aX+bfor some parameters a\nandb, i.e., if there is a linearrelationship between XandY(see Exercise 4.4). Intuitively one", "76": "46 Chapter2. Probability\nmight expect the correlation coefficient to be related to the slope of the regression line, i.e., the\ncoefficient ain the expression Y=aX+b. However, as we show in Equation 7.99 later, the\nregression coefficient is in fact given by a=c o v[X,Y]/var[X]. A better way to think of the\ncorrelation coefficient is as a degree of linearity: see Figure 2.12.\nIfXandYare independent, meaning p(X,Y)=p(X)p(Y)(see Section 2.2.4), then\ncov[X,Y ]=0, and hence corr[X,Y ]=0so they are uncorrelated. However, the con-\nv e r s ei sn o tt r u e : uncorrelateddoesnotimplyindependent. For example, let X\u223cU(\u22121,1)and\nY=X2. Clearly Yis dependent on X(in fact,Yis uniquely determined by X), yet one\ncan show (Exercise 4.1) that corr[X,Y ]=0. Some striking examples of this fact are shown in\nFigure 2.12. This shows several data sets where there is clear dependendence between XandY,\nand yet the correlation coefficient is 0. A more general measure of dependence between randomvariables is mutual information, discussed in Section 2.8.3. This is only zero if the variables trulyare independent.\n2.5.2 The multivariate Gaussian\nThemultivariate Gaussian ormultivariate normal (MVN) is the most widely used joint prob-\nability density function for continuous variables. We discuss MVNs in detail in Chapter 4; herewe just give some de\ufb01nitions and plots.\nThe pdf of the MVN in Ddimensions is de\ufb01ned by the following:\nN(x|\u03bc,\u03a3)/defines1\n(2\u03c0)D/2|\u03a3|1/2exp/bracketleftbigg\n\u22121\n2(x\u2212\u03bc)T\u03a3\u22121(x\u2212\u03bc)/bracketrightbigg\n(2.70)\nwhere\u03bc=E[x]\u2208RDis the mean vector, and \u03a3=c o v [x]is theD\u00d7Dcovariance\nmatrix. Sometimes we will work in terms of the precision matrix orconcentration matrix\ninstead. This is just the inverse covariance matrix, \u039b=\u03a3\u22121. The normalization constant\n(2\u03c0)\u2212D/2|\u039b|1/2just ensures that the pdf integrates to 1 (see Exercise 4.5).\nFigure 2.13 plots some MVN densities in 2d for three different kinds of covariance matrices.\nA full covariance matrix has D(D+1)/2parameters (we divide by 2 since \u03a3is symmetric). A\ndiagonal covariance matrix has Dparameters, and has 0s in the off-diagonal terms. A spherical\norisotropic covariance, \u03a3=\u03c32ID, has one free parameter.\n2.5.3 Multivariate Student tdistribution\nA more robust alternative to the MVN is the multivariate Student t distribution, whose pdf is\ngiven by\nT(x|\u03bc,\u03a3,\u03bd)=\u0393(\u03bd/2+D/2)\n\u0393(\u03bd/2)|\u03a3|\u22121/2\n\u03bdD/2\u03c0D/2\u00d7/bracketleftbigg\n1+1\n\u03bd(x\u2212\u03bc)T\u03a3\u22121(x\u2212\u03bc)/bracketrightbigg\u2212(\u03bd+D\n2)\n(2.71)\n=\u0393(\u03bd/2+D/2)\n\u0393(\u03bd/2)|\u03c0V|\u22121/2\u00d7/bracketleftbig\n1+(x\u2212\u03bc)TV\u22121(x\u2212\u03bc)/bracketrightbig\u2212(\u03bd+D\n2)(2.72)\nwhere\u03a3is called the scale matrix (since it is not exactly the covariance matrix) and V=\u03bd\u03a3.\nThis has fatter tails than a Gaussian. The smaller \u03bdis, the fatter the tails. As \u03bd\u2192\u221e, the", "77": "2.5. Jointprobabilitydistributions 47\nfull\n\u22126 \u22124 \u22122 0 2 4 6\u22126\u22124\u221220246\n(a)diagonal\n\u22125 \u22124 \u22123 \u22122 \u22121 0 1 2 3 4 5\u221210\u22128\u22126\u22124\u221220246810\n(b)\nspherical\n\u22124 \u22122 0 2 4 6\u22125\u22124\u22123\u22122\u22121012345\n(c)\u2212505\n\u221250500.050.10.150.2spherical\n(d)\nFigure 2.13 We show the level sets for 2d Gaussians. (a) A full covariance matrix has elliptical contours.\n(b) A diagonal covariance matrix is an axis aligned ellipse. (c) A spherical covariance matrix has a circular\nshape. (d) Surface plot for the spherical Gaussian in (c). Figure generated by gaussPlot2Ddemo.\ndistribution tends towards a Gaussian. The distribution has the following properties\nmean=\u03bc,mode=\u03bc,Cov=\u03bd\n\u03bd\u22122\u03a3 (2.73)\n2.5.4 Dirichlet distribution\nA multivariate generalization of the beta distribution is the Dirichlet distribution9, which has\nsupport over the probability simplex, de\ufb01ned by\nSK={x:0\u2264xk\u22641,K/summationdisplay\nk=1xk=1} (2.74)\nThe pdf is de\ufb01ned as follows:\nDir(x|\u03b1)/defines1\nB(\u03b1)K/productdisplay\nk=1x\u03b1k\u22121\nkI(x\u2208SK) (2.75)\n9. Johann Dirichlet was a German mathematician, 1805\u20131859.", "78": "48 Chapter2. Probability\n(a)\n (b)\n(c)00.51\n00.51051015\n\u03b1=0.10\np\n(d)\nFigure 2.14 (a) The Dirichlet distribution when K=3de\ufb01nes a distribution over the simplex, which\ncan be represented by the triangular surface. Points on this surface satisfy 0\u2264\u03b8k\u22641and/summationtext3\nk=1\u03b8k=\n1. (b) Plot of the Dirichlet density when \u03b1=( 2,2,2). (c) \u03b1= (20,2,2). Figure generated by\nvisDirichletGui , by Jonathan Huang. (d) \u03b1=( 0.1,0.1,0.1). (The comb-like structure on the edges is\na plotting artifact.) Figure generated by dirichlet3dPlot .\n1 2 3 4 500.51Samples from Dir (alpha=0.1)\n1 2 3 4 500.51\n1 2 3 4 500.51\n1 2 3 4 500.51\n1 2 3 4 500.51\n(a)1 2 3 4 500.51Samples from Dir (alpha=1)\n1 2 3 4 500.51\n1 2 3 4 500.51\n1 2 3 4 500.51\n1 2 3 4 500.51\n(b)\nFigure 2.15 Samples from a 5-dimensional symmetric Dirichlet distribution for different parameter values.\n(a)\u03b1=( 0.1,...,0.1). This results in very sparse distributions, with many 0s. (b) \u03b1=( 1,...,1). This\nresults in more uniform (and dense) distributions. Figure generated by dirichletHistogramDemo .", "79": "2.6. Transformationsofrandomvariables 49\nwhereB(\u03b11,...,\u03b1 K)is the natural generalization of the beta function to Kvariables:\nB(\u03b1)/defines/producttextK\nk=1\u0393(\u03b1k)\n\u0393(\u03b10)(2.76)\nwhere\u03b10/defines/summationtextKk=1\u03b1k.\nFigure 2.14 shows some plots of the Dirichlet when K=3, and Figure 2.15 for some sampled\nprobability vectors. We see that \u03b10=/summationtextKk=1\u03b1kcontrols the strength of the distribution (how\npeaked it is), and the \u03b1kcontrol where the peak occurs. For example, Dir(1,1,1)is a uniform\ndistribution, Dir(2,2,2)is a broad distribution centered at (1/3,1/3,1/3), andDir(20,20,20)\nis a narrow distribution centered at (1/3,1/3,1/3).I f\u03b1k<1for allk, we get \u201cspikes\u201d at the\ncorner of the simplex.\nFor future reference, the distribution has these properties\nE[xk]=\u03b1k\n\u03b10,mode[xk]=\u03b1k\u22121\n\u03b10\u2212K,var[xk]=\u03b1k(\u03b10\u2212\u03b1k)\n\u03b12\n0(\u03b10+1)(2.77)\nwhere\u03b10=/summationtext\nk\u03b1k. Often we use a symmetric Dirichlet prior of the form \u03b1k=\u03b1/K. In this\ncase, the mean becomes 1/K, and the variance becomes var[xk]=K\u22121\nK2(\u03b1+1). So increasing \u03b1\nincreases the precision (decreases the variance) of the distribution.\n2.6 Transformations of random variables\nIfx\u223cp()is some random variable, and y=f(x), what is the distribution of y? This is the\nquestion we address in this section.\n2.6.1 Linear transformations\nSupposef()is a linear function:\ny=f(x)=Ax+b (2.78)\nIn this case, we can easily derive the mean and covariance of yas follows. First, for the mean,\nwe have\nE[y]=E[Ax+b]=A\u03bc+b (2.79)\nwhere\u03bc=E[x]. This is called the linearity of expectation.I f f()is a scalar-valued function,\nf(x)=aTx+b, the corresponding result is\nE/bracketleftbig\naTx+b/bracketrightbig\n=aT\u03bc+b (2.80)\nFor the covariance, we have\ncov[y]=c o v[Ax+b]=A\u03a3AT(2.81)\nwhere\u03a3=c o v[x]. We leave the proof of this as an exercise. If f()is scalar valued, the result\nbecomes\nvar[y]=v a r/bracketleftbig\naTx+b/bracketrightbig\n=aT\u03a3a (2.82)", "80": "50 Chapter2. Probability\nWe will use both of these results extensively in later chapters. Note, however, that the mean\nand covariance only completely de\ufb01ne the distribution of yifxis Gaussian. In general we must\nuse the techniques described below to derive the full distribution of y, as opposed to just its\n\ufb01rst two moments.\n2.6.2 General transformations\nIfXis a discrete rv, we can derive the pmf for yby simply summing up the probability mass\nfor all the x\u2019s such that f(x)=y:\npy(y)=/summationdisplay\nx:f(x)=ypx(x) (2.83)\nFor example, if f(X)=1ifXis even and f(X)=0otherwise, and px(X)is uniform on the\nset{1,...,10}, then py(1) =/summationtext\nx\u2208{2,4,6,8,10} px(x)=0.5, andpy(0) = 0.5 similarly. Note\nthat in this example, fis a many-to-one function.\nIfXis continuous, we cannot use Equation 2.83 since px(x)is a density, not a pmf, and we\ncannot sum up densities. Instead, we work with cdf\u2019s, and write\nPy(y)/definesP(Y\u2264y)=P(f(X)\u2264y)=P(X\u2208{x|f(x)\u2264y}) (2.84)\nWe can derive the pdf of yby differentiating the cdf.\nIn the case of monotonic and hence invertible functions, we can write\nPy(y)=P(f(X)\u2264y)=P(X\u2264f\u22121(y)) =Px(f\u22121(y)) (2.85)\nTaking derivatives we get\npy(y)/definesd\ndyPy(y)=d\ndyPx(f\u22121(y)) =dx\ndyd\ndxPx(x)=dx\ndypx(x) (2.86)\nwherex=f\u22121(y). We can think of dxas a measure of volume in the x-space; similarly dy\nmeasures volume in yspace. Thusdx\ndymeasures the change in volume. Since the sign of this\nchange is not important, we take the absolute value to get the general expression:\npy(y)=px(x)/vextendsingle/vextendsingledx\ndy/vextendsingle/vextendsingle (2.87)\nThis is called change of variables formula. We can understand this result more intuitively as\nfollows. Observations falling in the range (x,x+\u03b4x)will get transformed into (y,y+\u03b4y),w h e r e\np\nx(x)\u03b4x\u2248py(y)\u03b4y. Hence py(y)\u2248px(x)|\u03b4x\n\u03b4y|. For example, suppose X\u223cU(\u22121,1), and\nY=X2. Thenpy(y)=1\n2y\u22121\n2. See also Exercise 2.10.\n2.6.2.1 Multivariate change of variables *\nWe can extend the previous results to multivariate distributions as follows. Let fbe a function\nthat maps RntoRn, and lety=f(x). Then its Jacobian matrix Jis given by\nJx\u2192y/defines\u2202(y1,...,y n)\n\u2202(x1,...,x n)/defines\u239b\n\u239c\u239d\u2202y1\n\u2202x1\u00b7\u00b7\u00b7\u2202y1\n\u2202xn.........\n\u2202yn\n\u2202x1\u00b7\u00b7\u00b7\u2202yn\n\u2202xn\u239e\n\u239f\u23a0 (2.88)", "81": "2.6. Transformationsofrandomvariables 51\n|detJ|measures how much a unit cube changes in volume when we apply f.\nIffis an invertible mapping, we can de\ufb01ne the pdf of the transformed variables using the\nJacobian of the inverse mapping y\u2192x:\npy(y)=px(x)/vextendsingle/vextendsingledet/parenleftbigg\u2202x\n\u2202y/parenrightbigg/vextendsingle/vextendsingle=p\nx(x)|detJy\u2192x| (2.89)\nIn Exercise 4.5 you will use this formula to derive the normalization constant for a multivariate\nGaussian.\nAs a simple example, consider transforming a density from Cartesian coordinates x=(x1,x2)\nto polar coordinates y=(r,\u03b8),w h e r ex 1=rcos\u03b8andx2=rsin\u03b8. Then\nJy\u2192x=/parenleftbigg\u2202x1\n\u2202r\u2202x1\n\u2202\u03b8\u2202x2\n\u2202r\u2202x2\n\u2202\u03b8/parenrightbigg\n=/parenleftbiggcos\u03b8\u2212rsin\u03b8\nsin\u03b8rcos\u03b8/parenrightbigg\n(2.90)\nand\n|detJ|=|rcos2\u03b8+rsin2\u03b8|=|r| (2.91)\nHence\npy(y)=p x(x)|detJ| (2.92)\npr,\u03b8(r,\u03b8)=p x1,x2(x1,x2)r=px1,x2(rcos\u03b8,rsin\u03b8)r (2.93)\nTo see this geometrically, notice that the area of the shaded patch in Figure 2.16 is given by\nP(r\u2264R\u2264r+dr,\u03b8\u2264\u0398\u2264\u03b8+d\u03b8)=p r,\u03b8(r,\u03b8)drd\u03b8 (2.94)\nIn the limit, this is equal to the density at the center of the patch, p(r,\u03b8), times the size of the\npatch,rd rd \u03b8. Hence\npr,\u03b8(r,\u03b8)drd\u03b8=px1,x2(rcos\u03b8,rsin\u03b8)rd rd \u03b8 (2.95)\n2.6.3 Central limit theorem\nNow consider Nrandom variables with pdf\u2019s (not necessarily Gaussian) p(xi), each with mean\n\u03bcand variance \u03c32. We assume each variable is independent and identically distributed\noriidfor short. Let SN=/summationtextN\ni=1Xibe the sum of the rv\u2019s. This is a simple but widely\nused transformation of rv\u2019s. One can show that, as Nincreases, the distribution of this sum\napproaches\np(SN=s)=1\u221a\n2\u03c0N\u03c32exp/parenleftbigg\n\u2212(s\u2212N\u03bc)2\n2N\u03c32/parenrightbigg\n(2.96)\nHence the distribution of the quantity\nZN/definesSN\u2212N\u03bc\n\u03c3\u221a\nN=X\u2212\u03bc\n\u03c3/\u221a\nN(2.97)\nconverges to the standard normal, where X=1\nN/summationtextNi=1xiis the sample mean. This is called\nthecentral limit theorem. See e.g., (Jaynes 2003, p222) or (Rice 1995, p169) for a proof.\nIn Figure 2.17 we give an example in which we compute the mean of rv\u2019s drawn from a beta\ndistribution. We see that the sampling distribution of the mean value rapidly converges to a\nGaussian distribution.", "82": "52 Chapter2. Probability\nFigure 2.16 Change of variables from polar to Cartesian. The area of the shaded patch is rd rd \u03b8. Based\non (Rice 1995) Figure 3.16.\n0 0.5 10123N = 1\n(a)0 0.5 10123N = 5\n(b)\nFigure 2.17 The central limit theorem in pictures. We plot a histogram of1\nN/summationtextN\ni=1xij,w h e r exij\u223c\nBeta(1,5),f o rj= 1 : 10000 .A sN\u2192\u221e, the distribution tends towards a Gaussian. (a) N=1. (b)\nN=5. Based on Figure 2.6 of (Bishop 2006a). Figure generated by centralLimitDemo .\n2.7 Monte Carlo approximation\nIn general, computing the distribution of a function of an rv using the change of variables\nformula can be difficult. One simple but powerful alternative is as follows. First we generate\nSsamples from the distribution, call them x1,...,x S. (There are many ways to generate such\nsamples; one popular method, for high dimensional distributions, is called Markov chain Monte\nCarlo or MCMC; this will be explained in Chapter 24.) Given the samples, we can approximate\nthe distribution of f(X)by using the empirical distribution of {f(xs)}S\ns=1. This is called a\nMonte Carlo approximation, named after a city in Europe known for its plush gambling casinos.\nMonte Carlo techniques were \ufb01rst developed in the area of statistical physics \u2014 in particular,\nduring development of the atomic bomb \u2014 but are now widely used in statistics and machine\nlearning as well.\nWe can use Monte Carlo to approximate the expected value of any function of a random", "83": "2.7. MonteCarloapproximation 53\n\u22121 0 1\u22120.500.511.5\n0 0.5 10246\n0 0.5 100.050.10.150.20.25\nFigure 2.18 Computing the distribution of y=x2,w h e r ep(x)is uniform (left). The analytic result is\nshown in the middle, and the Monte Carlo approximation is shown on the right. Figure generated by\nchangeOfVarsDemo1d .\nvariable. We simply draw samples, and then compute the arithmetic mean of the function\napplied to the samples. This can be written as follows:\nE[f(X)] =/integraldisplay\nf(x)p(x)dx\u22481\nSS/summationdisplay\ns=1f(xs) (2.98)\nwherexs\u223cp(X). This is called Monte Carlo integration, and has the advantage over numerical\nintegration (which is based on evaluating the function at a \ufb01xed grid of points) that the functionis only evaluated in places where there is non-negligible probability.\nBy varying the function f(), we can approximate many quantities of interest, such as\n\u2022\nx=1\nS/summationtextS\ns=1xs\u2192E[X]\n\u20221\nS/summationtextSs=1(xs\u2212x)2\u2192var[X]\n\u20221\nS#{xs\u2264c}\u2192P(X\u2264c)\n\u2022 median{x 1,...,x S}\u2192median( X)\nWe give some examples below, and will see many more in later chapters.\n2.7.1 Example: change of variables, the MC way\nIn Section 2.6.2, we discussed how to analytically compute the distribution of a function of a\nrandom variable, y=f(x). A much simpler approach is to use a Monte Carlo approximation.\nFor example, suppose x\u223cUnif(\u22121,1)andy=x2. We can approximate p(y)by drawing\nmany samples from p(x), squaring them, and computing the resulting empirical distribution.\nSee Figure 2.18 for an illustration. We will use this technique extensively in later chapters. Seealso Figure 5.2.", "84": "54 Chapter2. Probability\n\u22122 \u22121 0 1 2\u22122\u22121.5\u22121\u22120.500.511.52\nFigure 2.19 Estimating \u03c0by Monte Carlo integration. Blue points are inside the circle, red crosses are\noutside. Figure generated by mcEstimatePi.\n2.7.2 Example: estimating \u03c0by Monte Carlo integration\nMC approximation can be used for many applications, not just statistical ones. Suppose we want\nto estimate \u03c0. We know that the area of a circle with radius ris\u03c0r2, but it is also equal to the\nfollowing de\ufb01nite integral:\nI=/integraldisplayr\n\u2212r/integraldisplayr\n\u2212rI(x2+y2\u2264r2)dxdy (2.99)\nHence\u03c0=I/(r2). Let us approximate this by Monte Carlo integration. Let f(x,y)=\nI(x2+y2\u2264r2)be an indicator function that is 1 for points inside the circle, and 0 outside,\nand letp(x)andp(y)be uniform distributions on [\u2212r,r],s op(x)=p(y)=1/(2r). Then\nI=( 2r)(2r)/integraldisplay/integraldisplay\nf(x,y)p(x)p(y)dxdy (2.100)\n=4r2/integraldisplay/integraldisplay\nf(x,y)p(x)p(y)dxdy (2.101)\n\u22484r21\nSS/summationdisplay\ns=1f(xs,ys) (2.102)\nWe \ufb01nd\u02c6\u03c0=3.1416with standard error 0.09 (see Section 2.7.3 for a discussion of standard\nerrors). We can plot the points that are accepted/ rejected as in Figure 2.19.\n2.7.3 Accuracy of Monte Carlo approximation\nThe accuracy of an MC approximation increases with sample size. This is illustrated in Fig-ure 2.20, On the top line, we plot a histogram of samples from a Gaussian distribution. Onthe bottom line, we plot a smoothed version of these samples, created using a kernel densityestimate (Section 14.7.2). This smoothed distribution is then evaluated on a dense grid of points", "85": "2.7. MonteCarloapproximation 55\n0.5 1 1.5 2 2.5012345610 samples\n(a)0.5 1 1.5 2 2.500.20.40.60.811.21.41.61.82100 samples\n(b)\n0.5 1 1.5 2 2.500.511.522.510 samples\n(c)0.5 1 1.5 2 2.500.20.40.60.811.21.41.61.8100 samples\n(d)\nFigure 2.20 10 and 100 samples from a Gaussian distribution, N(\u03bc=1.5,\u03c32=0.25). Solid red\nline is true pdf. Top line: histogram of samples. Bottom line: kernel density estimate derived from\nsamples in dotted blue, solid red line is true pdf. Based on Figure 4.1 of (Hoff 2009). Figure generated bymcAccuracyDemo.\nand plotted. Note that this smoothing is just for the purposes of plotting, it is not used for the\nMonte Carlo estimate itself.\nIf we denote the exact mean by \u03bc=E[f(X)], and the MC approximation by \u02c6\u03bc, one can\nshow that, with independent samples,\n(\u02c6\u03bc\u2212\u03bc)\u2192N(0,\u03c32\nS) (2.103)\nwhere\n\u03c32= var[f(X)] =E/bracketleftbig\nf(X)2/bracketrightbig\n\u2212E[f(X)]2(2.104)\nThis is a consequence of the central-limit theorem. Of course, \u03c32is unknown in the above\nexpression, but it can also be estimated by MC:\n\u02c6\u03c32=1\nSS/summationdisplay\ns=1(f(xs)\u2212\u02c6\u03bc)2(2.105)\nThen we have\nP/braceleftbigg\n\u03bc\u22121.96\u02c6\u03c3\u221a\nS\u2264\u02c6\u03bc\u2264\u03bc+1.96\u02c6\u03c3\u221a\nS/bracerightbigg\n\u22480.95 (2.106)", "86": "56 Chapter2. Probability\nThe term/radicalBig\n\u02c6\u03c32\nSis called the (numerical or empirical) standard error, and is an estimate of our\nuncertainty about our estimate of \u03bc. (See Section 6.2 for more discussion on standard errors.)\nIf we want to report an answer which is accurate to within \u00b1/epsilon1with probability at least 95%,\nwe need to use a number of samples Swhich satis\ufb01es 1.96/radicalbig\n\u02c6\u03c32/S\u2264/epsilon1. We can approximate\nthe 1.96 factor by 2, yielding S\u22654\u02c6\u03c32\n/epsilon12.\n2.8 Information theory\ninformation theory is concerned with representing data in a compact fashion (a task known as\ndata compression orsource coding), as well as with transmitting and storing it in a way that\nis robust to errors (a task known as error correction orchannel coding). At \ufb01rst, this seems\nfar removed from the concerns of probability theory and machine learning, but in fact there is\nan intimate connection. To see this, note that compactly representing data requires allocatingshort codewords to highly probable bit strings, and reserving longer codewords to less probablebit strings. This is similar to the situation in natural language, where common words (such as\u201ca\u201d, \u201cthe\u201d, \u201cand\u201d) are generally much shorter than rare words. Also, decoding messages sent overnoisy channels requires having a good probability model of the kinds of messages that peopletend to send. In both cases, we need a model that can predict which kinds of data are likelyand which unlikely, which is also a central problem in machine learning (see (MacKay 2003) formore details on the connection between information theory and machine learning).\nObviously we cannot go into the details of information theory here (see e.g., (Cover and\nThomas 2006) if you are interested to learn more). However, we will introduce a few basicconcepts that we will need later in the book.\n2.8.1 Entropy\nTheentropy of a random variable Xwith distribution p, denoted by H(X)or sometimes\nH(p), is a measure of its uncertainty. In particular, for a discrete variable with Kstates, it is\nde\ufb01ned by\nH(X)/defines\u2212K/summationdisplay\nk=1p(X=k)log2p(X=k) (2.107)\nUsually we use log base 2, in which case the units are called bits(short for binary digits). If\nwe use log base e, the units are called nats. For example, if X\u2208{1,...,5}with histogram\ndistribution p=[ 0.25,0.25,0.2,0.15,0.15], we \ufb01ndH=2.2855. The discrete distribution with\nmaximum entropy is the uniform distribution (see Section 9.2.6 for a proof). Hence for a K-ary\nrandom variable, the entropy is maximized if p(x=k)=1/K; in this case, H(X)=l o g2K.\nConversely, the distribution with minimum entropy (which is zero) is any delta-function thatputs all its mass on one state. Such a distribution has no uncertainty. In Figure 2.5(b), wherewe plotted a DNA sequence logo, the height of each bar is de\ufb01ned to be 2\u2212H,w h e r eHis\nthe entropy of that distribution, and 2 is the maximum possible entropy. Thus a bar of height 0corresponds to a uniform distribution, whereas a bar of height 2 corresponds to a deterministicdistribution.", "87": "2.8. Informationtheory 57\n0 0.5 100.51\np(X = 1)H(X)\nFigure 2.21 Entropy of a Bernoulli random variable as a function of \u03b8. The maximum entropy is\nlog22=1. Figure generated by bernoulliEntropyFig .\nFor the special case of binary random variables, X\u2208{0,1}, we can write p(X=1 )=\u03b8\nandp(X=0 )=1 \u2212\u03b8. Hence the entropy becomes\nH(X)=\u2212[ p(X=1 )l o g2p(X=1 )+p(X=0 )l o g2p(X=0 ) ] (2.108)\n=\u2212[\u03b8log2\u03b8+(1\u2212\u03b8)log2(1\u2212\u03b8)] (2.109)\nThis is called the binary entropy function, and is also written H(\u03b8). We plot this in Figure 2.21.\nWe see that the maximum value of 1 occurs when the distribution is uniform, \u03b8=0.5.\n2.8.2 KL divergence\nOne way to measure the dissimilarity of two probability distributions, pandq, is known as the\nKullback-Leibler divergence (KL divergence)o rrelative entropy. This is de\ufb01ned as follows:\nKL(p||q)/definesK/summationdisplay\nk=1pklogpk\nqk(2.110)\nwhere the sum gets replaced by an integral for pdfs.10We can rewrite this as\nKL(p||q)=/summationdisplay\nkpklogpk\u2212/summationdisplay\nkpklogqk=\u2212H(p)+H(p,q) (2.111)\nwhere H(p,q)is called the cross entropy,\nH(p,q)/defines\u2212/summationdisplay\nkpklogqk (2.112)\nOne can show (Cover and Thomas 2006) that the cross entropy is the average number of bits\nneeded to encode data coming from a source with distribution pwhen we use model qto\n10. The KL divergence is not a distance, since it is asymmetric. One symmetric version of the KL divergence is the\nJensen-Shannon divergence, de\ufb01ned as JS(p1,p2)=0.5KL (p1||q)+0.5KL (p2||q),w h e r eq =0.5p1+0.5p2.", "88": "58 Chapter2. Probability\nde\ufb01ne our codebook. Hence the \u201cregular\u201d entropy H(p)= H(p,p), de\ufb01ned in Section 2.8.1, is\nthe expected number of bits if we use the true model, so the KL divergence is the difference\nbetween these. In other words, the KL divergence is the average number of extrabits needed to\nencode the data, due to the fact that we used distribution qto encode the data instead of the\ntrue distribution p.\nThe \u201cextra number of bits\u201d interpretation should make it clear that KL(p||q)\u22650, and that\nthe KL is only equal to zero iff q=p. We now give a proof of this important result.\nTheorem 2.8.1. (Information inequality) KL(p||q)\u22650withequalityiff p=q.\nProof.To prove the theorem, we need to use Jensen\u2019s inequality. This states that, for any\nconvex function f,w eh a v et h a t\nf/parenleftBiggn/summationdisplay\ni=1\u03bbixi/parenrightBigg\n\u2264n/summationdisplay\ni=1\u03bbif(xi) (2.113)\nwhere\u03bbi\u22650and/summationtextn\ni=1\u03bbi=1. This is clearly true for n=2(by de\ufb01nition of convexity), and\ncan be proved by induction for n>2.\nLet us now prove the main theorem, following (Cover and Thomas 2006, p28). Let A={x:\np(x)>0}be the support of p(x). Then\n\u2212KL(p||q)=\u2212/summationdisplay\nx\u2208Ap(x)logp(x)\nq(x)=/summationdisplay\nx\u2208Ap(x)logq(x)\np(x)(2.114)\n\u2264log/summationdisplay\nx\u2208Ap(x)q(x)\np(x)=l o g/summationdisplay\nx\u2208Aq(x) (2.115)\n\u2264log/summationdisplay\nx\u2208Xq(x)=l o g1=0 (2.116)\nwhere the \ufb01rst inequality follows from Jensen\u2019s. Since log(x)is a strictly concave function, we\nhave equality in Equation 2.115 iff p(x)=cq(x)for some c. We have equality in Equation 2.116\niff/summationtext\nx\u2208Aq(x)=/summationtext\nx\u2208Xq(x)=1, which implies c=1. Hence KL(p||q)=0iffp(x)=q(x)\nfor allx.\nOne important consequence of this result is that the discrete distribution with the maximum\nentropy is the uniform distribution. More precisely, H(X)\u2264log|X|,w h e r e |X|is the number\nof states for X, with equality iff p(x)is uniform. To see this, let u(x)=1/|X|. Then\n0\u2264KL(p||u)=/summationdisplay\nxp(x)logp(x)\nu(x)(2.117)\n=/summationdisplay\nxp(x)logp(x)\u2212/summationdisplay\nxp(x)logu(x)=\u2212 H(X)+log|X| (2.118)\nThis is a formulation of Laplace\u2019s principle of insufficient reason, which argues in favor of\nusing uniform distributions when there are no other reasons to favor one distribution over\nanother. See Section 9.2.6 for a discussion of how to create distributions that satisfy certainconstraints, but otherwise are as least-commital as possible. (For example, the Gaussian satis\ufb01es\ufb01rst and second moment constraints, but otherwise has maximum entropy.)", "89": "2.8. Informationtheory 59\n2.8.3 Mutual information\nConsider two random variables, XandY. Suppose we want to know how much knowing one\nvariable tells us about the other. We could compute the correlation coefficient, but this is only\nde\ufb01ned for real-valued random variables, and furthermore, this is a very limited measure ofdependence, as we saw in Figure 2.12. A more general approach is to determine how similar thejoint distribution p(X,Y)is to the factored distribution p(X)p(Y). This is called the mutual\ninformation orMI, and is de\ufb01ned as follows:\nI(X;Y)/definesKL(p(X,Y)||p(X)p(Y)) =/summationdisplay\nx/summationdisplay\nyp(x,y)logp(x,y)\np(x)p(y)(2.119)\nWe have I(X;Y)\u22650with equality iff p(X,Y)=p(X)p(Y). That is, the MI is zero iff the\nvariables are independent.\nTo gain insight into the meaning of MI, it helps to re-express it in terms of joint and conditional\nentropies. One can show (Exercise 2.12) that the above expression is equivalent to the following:\nI(X;Y)=H(X)\u2212H(X|Y)=H(Y)\u2212H(Y|X) (2.120)\nwhere H(Y|X)is theconditional entropy, de\ufb01ned as H(Y|X)=/summationtext\nxp(x)H(Y|X=x).\nThus we can interpret the MI between XandYas the reduction in uncertainty about Xafter\nobserving Y, or, by symmetry, the reduction in uncertainty about Yafter observing X. We will\nencounter several applications of MI later in the book. See also Exercises 2.13 and 2.14 for theconnection between MI and correlation coefficients.\nA quantity which is closely related to MI is the pointwise mutual information or PMI. For\ntwo events (not random variables) xandy, this is de\ufb01ned as\nPMI(x,y)/defineslogp(x,y)\np(x)p(y)=l o gp(x|y)\np(x)=l o gp(y|x)\np(y)(2.121)\nThis measures the discrepancy between these events occuring together compared to what wouldbe expected by chance. Clearly the MI of XandYis just the expected value of the PMI.\nInterestingly, we can rewrite the PMI as follows:\nPMI(x,y)=l o gp(x|y)\np(x)=l o gp(y|x)\np(y)(2.122)\nThis is the amount we learn from updating the prior p(x)into the posterior p(x|y), or equiva-\nlently, updating the prior p(y)into the posterior p(y|x).\n2.8.3.1 Mutual information for continuous random variables *\nThe above formula for MI is de\ufb01ned for discrete random variables. For continuous randomvariables, it is common to \ufb01rst discretize orquantize them, by dividing the ranges of each\nvariable into bins, and computing how many values fall in each histogram bin (Scott 1979). Wecan then easily compute the MI using the formula above (see mutualInfoAllPairsMixed for\nsome code, and miMixedDemo for a demo).\nUnfortunately, the number of bins used, and the location of the bin boundaries, can have\na signi\ufb01cant effect on the results. One way around this is to try to estimate the MI directly,", "90": "60 Chapter2. Probability\n/g22/g44/g52/g57/g47/g56/g57/g1/g22/g44/g52/g56/g47/g57/g61/g1/g4/g54/g44/g55/g1/g11/g10/g6/g10/g10/g10/g5-g13/g29/g47/g45/g44/g1/g29/g53/g56/g57/g1/g57/g53/g1/g27/g52/g48/g58/g55/g47/g44/g56/g1/g4/g3/g1/g61/g55/g56/g5/g12/g10/g13/g10\n/g10/g11/g10/g14/g10\n/g10 /g14 /g18 /g11/g12 /g11/g16-g15\n/g31/g58/g51/g41/g44/g55/g1/g53/g45/g1/g33/g46/g61/g56/g47/g42/g47/g40/g52/g56/g22/g44/g40/g57/g46/g56/g1/g43/g58/g44/g1/g57/g53/g1/g26/g27/g37/g9/g20/g27/g22/g35\n/g10/g1/g18/g10/g10/g1/g11/g16/g10/g10/g1\n/g10/g1 /g11/g60/g11/g10/g15/g12/g60/g11/g10/g16/g1 /g12/g60/g11/g10/g15\n/g26/g44/g40/g50/g57/g46/g1/g23/g60/g54/g8/g1/g9/g1/g33/g44/g55/g56/g53/g52/g1/g4/g36/g35/g2/g5/g1-g17\n/g30/g44/g40/g56/g50/g44/g56/g1/g27/g51/g51/g8/g1/g22/g47/g56/g54/g40/g55/g47/g57/g61/g1/g4/g3/g5/g1/g10/g13/g10/g16/g10\n/g10 /g11/g15/g10 /g13/g10/g10-g11\n\u0005\b\u0006\u000e\u000f\r\f\u0001\u0004\r\u000e\u000e\b\u000b\u0006\u0010\n\r\f\u0001\u0004\r\b\t\t\n\u0007\n\b\f\u0010\u0001\u0002\u0011\u0003\n/g30/g27/g21/g1/g35/g42/g53/g55/g44-g13-g18\n-g16\n-g17 -g15\n-g14\n/g10 /g10/g8/g12/g15 /g10/g8/g15 /g10/g8/g17/g15 /g11/g7/g11/g7/g10/g8/g15/g10/g10/g8/g15/g11\n-g2-g5-g3-g4\n-g8-g9-g3-g7-g16\n/g27/g52/g42/g53/g51/g44/g1/g9/g1/g33/g44/g55/g56/g53/g52/g1/g4/g27/g52/g57/g2/g5/g1/g20/g43/g58/g50/g57/g1/g4/g24/g44/g51/g40/g50/g44/g5/g1/g32/g41/g44/g56/g47/g57/g61/g1/g4/g3/g5/g1/g10/g12/g15/g15/g10/g17/g15\n/g10 /g12/g10/g6/g10/g10/g10 /g14/g10/g6/g10/g10/g10-g14\n/g21/g46/g47/g50/g43/g55/g44/g52/g1/g33/g44/g55/g1/g38/g53/g51/g40/g52/g1/g29/g47/g45/g44/g1/g23/g60/g54/g44/g42/g57/g40/g52/g42/g61/g1/g4/g39/g44/g40/g55/g56/g5/g1\n/g12/g14/g16/g16/g10\n/g13/g10/g19/g10\n-g18\n/g25/g55/g53/g56/g56/g1/g31/g40/g57/g62/g50/g1/g27/g52/g42/g1/g9/g1/g33/g44/g55/g56/g53/g52/g1/g4/g27/g52/g57/g2/g5/g1/g26/g44/g40/g50/g57/g46/g1/g23/g60/g54/g8/g1/g9/g1/g33/g44/g55/g56/g53/g52/g1/g4/g27/g52/g57/g1/g2/g5/g10/g12/g10/g10/g10/g14/g10/g10/g10/g16/g10/g10/g10\n/g10 /g12/g10/g6/g10/g10/g10 /g14/g10/g6/g10/g10/g10\nFigure 2.22 Left: Correlation coefficient vs maximal information criterion (MIC) for all pairwise relation-\nships in the WHO data. Right: scatter plots of certain pairs of variables. The red lines are non-parametric\nsmoothing regressions (Section 15.4.6) \ufb01t separately to each trend. Source: Figure 4 of (Reshed et al. 2011) .\nUsed with kind permission of David Reshef and the American Association for the Advancement of Science.\nwithout \ufb01rst performing density estimation (Learned-Miller 2004). Another approach is to try\nmany different bin sizes and locations, and to compute the maximum MI achieved. This\nstatistic, appropriately normalized, is known as the maximal information coefficient (MIC)\n(Reshed et al. 2011). More precisely, de\ufb01ne\nm(x,y)=maxG\u2208G(x,y)I(X(G);Y(G))\nlogmin(x,y)(2.123)\nwhereG(x,y)is the set of 2d grids of size x\u00d7y, andX(G),Y(G)represents a discretization of\nthe variables onto this grid. (The maximization over bin locations can be performed efficiently\nusing dynamic programming (Reshed et al. 2011).) Now de\ufb01ne the MIC as\nMIC/definesmax\nx,y:xy<Bm(x,y) (2.124)\nwhereBis some sample-size dependent bound on the number of bins we can use and still\nreliably estimate the distribution ((Reshed et al. 2011) suggest B=N0.6). It can be shown that\nthe MIC lies in the range [0,1], where 0 represents no relationship between the variables, and 1\nrepresents a noise-free relationship of any form, not just linear.\nFigure 2.22 gives an example of this statistic in action. The data consists of 357 variables\nmeasuring a variety of social, economic, health and political indicators, collected by the World\nHealth Organization (WHO). On the left of the \ufb01gure, we see the correlation coefficient (CC)\nplotted against the MIC for all 63,566 variable pairs. On the right of the \ufb01gure, we see scatter\nplots for particular pairs of variables, which we now discuss:\n\u2022 The point marked C has a low CC and a low MIC. The corresponding scatter plot makes it", "91": "2.8. Informationtheory 61\nclear that there is no relationship between these two variables (percentage of lives lost to\ninjury and density of dentists in the population).\n\u2022 The points marked D and H have high CC (in absolute value) and high MIC, because they\nrepresent nearly linear relationships.\n\u2022 The points marked E, F, and G have low CC but high MIC. This is because they correspond\nto non-linear (and sometimes, as in the case of E and F, non-functional, i.e., one-to-many)relationships between the variables.\nIn summary, we see that statistics (such as MIC) based on mutual information can be used\nto discover interesting relationships between variables in a way that simpler measures, such ascorrelation coefficients, cannot. For this reason, the MIC has been called \u201ca correlation for the21st century\u201d (Speed 2011).\nExercises\nExercise 2.1 Probabilities are sensitive to the form of the question that was used to generate the answer\n(Source: Minka.) My neighbor has two children. Assuming that the gender of a child is like a coin \ufb02ip,\nit is most likely, a priori, that my neighbor has one boy and one girl, with probability 1/2. The otherpossibilities\u2014two boys or two girls\u2014have probabilities 1/4 and 1/4.\na. Suppose I ask him whether he has any boys, and he says yes. What is the probability that one child is\na girl?\nb. Suppose instead that I happen to see one of his children run by, and it is a boy. What is the probability\nthat the other child is a girl?\nExercise 2.2 Legal reasoning\n(Source: Peter Lee.) Suppose a crime has been committed. Blood is found at the scene for which there isno innocent explanation. It is of a type which is present in 1% of the population.\na. The prosecutor claims: \u201cThere is a 1% chance that the defendant would have the crime blood type if he\nwere innocent. Thus there is a 99% chance that he guilty\u201d. This is known as the prosecutor\u2019s fallacy.\nWhat is wrong with this argument?\nb. The defender claims: \u201cThe crime occurred in a city of 800,000 people. The blood type would be\nfound in approximately 8000 people. The evidence has provided a probability of just 1 in 8000 that\nthe defendant is guilty, and thus has no relevance.\u201d This is known as the defender\u2019s fallacy. What is\nwrong with this argument?\nExercise 2.3 Variance of a sum\nShow that the variance of a sum is var[X+Y]=v a r[X]+v ar[Y]+2cov[X,Y ],wherecov[X,Y ]\nis the covariance between XandY\nExercise 2.4 Bayes rule for medical diagnosis\n(Source: Koller.) After your yearly checkup, the doctor has bad news and good news. The bad news is that\nyou tested positive for a serious disease, and that the test is 99% accurate (i.e., the probability of testingpositive given that you have the disease is 0.99, as is the probability of tetsing negative given that you don\u2019thave the disease). The good news is that this is a rare disease, striking only one in 10,000 people. What are\nthe chances that you actually have the disease? (Show your calculations as well as giving the \ufb01nal result.)", "92": "62 Chapter2. Probability\nExercise 2.5 The Monty Hall problem\n(Source: Mackay.) On a game show, a contestant is told the rules as follows:\nThere are three doors, labelled 1, 2, 3. A single prize has been hidden behind one of them. You\nget to select one door. Initially your chosen door will notbe opened. Instead, the gameshow host\nwill open one of the other two doors, and hewilldosoinsuchawayasnottorevealtheprize. For\nexample, if you \ufb01rst choose door 1, he will then open one of doors 2 and 3, and it is guaranteed\nthat he will choose which one to open so that the prize will not be revealed.\nAt this point, you will be given a fresh choice of door: you can either stick with your \ufb01rst choice,\nor you can switch to the other closed door. All the doors will then be opened and you will receivewhatever is behind your \ufb01nal choice of door.\nImagine that the contestant chooses door 1 \ufb01rst; then the gameshow host opens door 3, revealing nothingbehind the door, as promised. Should the contestant (a) stick with door 1, or (b) switch to door 2, or (c)\ndoes it make no difference? You may assume that initially, the prize is equally likely to be behind any of\nthe 3 doors. Hint: use Bayes rule.\nExercise 2.6 Conditional independence\n(Source: Koller.)\na. LetH\u2208{1,...,K}be a discrete random variable, and let e\n1ande2be the observed values of two\nother random variables E1andE2. Suppose we wish to calculate the vector\n/vectorP(H|e1,e2)=(P(H=1|e1,e2),...,P(H=K|e1,e2))\nWhich of the following sets of numbers are sufficient for the calculation?\ni.P(e1,e2),P(H),P(e1|H),P(e2|H)\nii.P(e1,e2),P(H),P(e1,e2|H)\niii.P(e1|H),P(e2|H),P(H)\nb. Now suppose we now assume E1\u22a5E2|H(i.e.,E1andE2are conditionally independent given H).\nWhich of the above 3 sets are sufficent now?\nShow your calculations as well as giving the \ufb01nal result. Hint: use Bayes rule.Exercise 2.7 Pairwise independence does not imply mutual independence\nWe say that two random variables are pairwise independent if\np(X\n2|X1)=p(X2) (2.125)\nand hence\np(X2,X1)=p(X1)p(X2|X1)=p(X1)p(X2) (2.126)\nWe say that nrandom variables are mutually independent if\np(Xi|XS)=p(Xi)\u2200S\u2286{1,...,n}\\{i} (2.127)\nand hence\np(X1:n)=n/productdisplay\ni=1p(Xi) (2.128)\nShow that pairwise independence between all pairs of variables does not necessarily imply mutual inde-\npendence. It suffices to give a counter example.", "93": "2.8. Informationtheory 63\nExercise 2.8 Conditional independence iff joint factorizes\nIn the text we said X\u22a5Y|Ziff\np(x,y|z)=p(x|z)p(y|z) (2.129)\nfor allx,y,zsuch that p(z)>0. Now prove the following alternative de\ufb01nition: X\u22a5Y|Ziff there exist\nfunctiongandhsuch that\np(x,y|z)=g(x,z)h(y,z) (2.130)\nfor allx,y,zsuch that p(z)>0.\nExercise 2.9 Conditional independence\n(Source: Koller.) Are the following properties true? Prove or disprove. Note that we are not restricting\nattention to distributions that can be represented by a graphical model.\na. True or false? (X\u22a5W|Z,Y)\u2227(X\u22a5Y|Z)\u21d2(X\u22a5Y,W|Z)\nb.T\nrue or false? (X\u22a5Y|Z)\u2227(X\u22a5Y|W)\u21d2(X\u22a5Y|Z,W)\nExercise 2.10 Deriving the inverse gamma density\nLetX\u223cGa(a,b), i.e.\nGa(x|a,b)=ba\n\u0393(a)xa\u22121e\u2212xb(2.131)\nLetY=1/X. Show that Y\u223cIG(a,b), i.e.,\nIG(x|shape = a,scale =b)=ba\n\u0393(a)x\u2212(a+1)e\u2212b/x(2.132)\nHint: use the change of variables formula.\nExercise 2.11 Normalization constant for a 1D Gaussian\nThe normalization constant for a zero-mean Gaussian is given by\nZ=/integraldisplayb\naexp/parenleftbigg\n\u2212x2\n2\u03c32/parenrightbigg\ndx (2.133)\nwherea=\u2212\u221eandb=\u221e. To compute this, consider its square\nZ2=/integraldisplayb\na/integraldisplayb\naexp/parenleftbigg\n\u2212x2+y2\n2\u03c32/parenrightbigg\ndxdy (2.134)\nLet us change variables from cartesian (x,y)to polar(r,\u03b8)usingx=rcos\u03b8andy=rsin\u03b8. Since\ndxdy=rdrd\u03b8, andcos2\u03b8+sin2\u03b8=1,w eh a v e\nZ2=/integraldisplay2\u03c0\n0/integraldisplay\u221e\n0rexp/parenleftbigg\n\u2212r2\n2\u03c32/parenrightbigg\ndrd\u03b8 (2.135)\nEvaluate this integral and hence show Z=\u03c3/radicalbig\n(2\u03c0). Hint 1: separate the integral into a product of\ntwo terms, the \ufb01rst of which (involving d\u03b8) is constant, so is easy. Hint 2: if u=e\u2212r2/2\u03c32then\ndu/dr=\u22121\n\u03c32re\u2212r2/2\u03c32, so the second integral is also easy (since/integraltext\nu/prime(r)dr=u(r)).", "94": "64 Chapter2. Probability\nExercise 2.12 Expressing mutual information in terms of entropies\nShow that\nI(X,Y)=H(X)\u2212H(X|Y)=H(Y)\u2212H(Y|X) (2.136)\nExercise 2.13 Mutual information for correlated normals\n(Source: (Cover and Thomas 1991, Q9.3).) Find the mutual information I(X1,X2)where Xhas a bivariate\nnormal distribution:\n/parenleftbiggX1\nX2/parenrightbigg\n\u223cN/parenleftbigg\n0,/parenleftbigg\u03c32\u03c1\u03c32\n\u03c1\u03c32\u03c32/parenrightbigg/parenrightbigg\n(2.137)\nEvaluateI(X1,X2)for\u03c1=1,\u03c1=0and\u03c1=\u22121and comment. Hint: The (differential) entropy of a\nd-dimensional Gaussian is\nh(X)=1\n2log2/bracketleftBig\n(2\u03c0e)ddet\u03a3/bracketrightBig\n(2.138)\nIn the 1d case, this becomes\nh(X)=1\n2log2/bracketleftbig\n2\u03c0e\u03c32/bracketrightbig\n(2.139)\nHint:log(0) = \u221e.\nExercise 2.14 A measure of correlation (normalized mutual information)\n(Source: (Cover and Thomas 1991, Q2.20).) Let XandYbe discrete random variables which are identically\ndistributed (so H(X)=H(Y)) but not necessarily independent. De\ufb01ne\nr=1\u2212H(Y|X)\nH(X)(2.140)\na. Showr=I(X,Y )\nH(X)\nb. Show0\u2264r\u22641\nc. When is r=0?\nd. When is r=1?\nExercise 2.15 MLE minimizes KL divergence to the empirical distribution\nLetpemp(x)betheempiricaldistribution, andlet q(x|\u03b8)besomemodel. Showthat argminqKL(pemp||q)\nis obtained by q(x)=q(x;\u02c6\u03b8),w h e r e\u02c6\u03b8is the MLE. Hint: use non-negativity of the KL divergence.\nExercise 2.16 Mean, mode, variance for the beta distribution\nSuppose\u03b8\u223cBeta(a,b) . Derive the mean, mode and variance.\nExercise 2.17 Expected value of the minimum\nSupposeX,Yare two points sampled independently and uniformly at random from the interval [0,1].\nWhat is the expected location of the left most point?", "95": "3 Generative models for discrete data\n3.1 Introduction\nIn Section 2.2.3.2, we discussed how to classify a feature vector xby applying Bayes rule to a\ngenerative classi\ufb01er of the form\np(y=c|x,\u03b8)\u221dp(x|y=c,\u03b8)p(y=c|\u03b8) (3.1)\nThe key to using such models is specifying a suitable form for the class-conditional density\np(x|y=c,\u03b8), which de\ufb01nes what kind of data we expect to see in each class. In this chapter,\nwe focus on the case where the observed data are discrete symbols. We also discuss how toinfer the unknown parameters \u03b8of such models.\n3.2 Bayesian concept learning\nConsider how a child learns to understand the meaning of a word, such as \u201cdog\u201d. Presumablythe child\u2019s parents point out positive examples of this concept, saying such things as, \u201clook atthe cute dog!\u201d, or \u201cmind the doggy\u201d, etc. However, it is very unlikely that they provide negativeexamples, by saying \u201clook at that non-dog\u201d. Certainly, negative examples may be obtained duringan active learning process \u2014 the child says \u201clook at the dog\u201d and the parent says \u201cthat\u2019s a cat,dear, not a dog\u201d \u2014 but psychological research has shown that people can learn concepts frompositive examples alone (Xu and Tenenbaum 2007).\nWe can think of learning the meaning of a word as equivalent to concept learning, which in\nturn is equivalent to binary classi\ufb01cation. To see this, de\ufb01ne f(x)=1ifxis an example of the\nconceptC, andf(x)=0otherwise. Then the goal is to learn the indicator function f, which\njust de\ufb01nes which elements are in the set C. By allowing for uncertainty about the de\ufb01nition\noff, or equivalently the elements of C, we can emulate fuzzy set theory, but using standard\nprobability calculus. Note that standard binary classi\ufb01cation techniques require positive andnegative examples. By contrast, we will devise a way to learn from positive examples alone.\nFor pedagogical purposes, we will consider a very simple example of concept learning called\nthenumber game , based on part of Josh Tenenbaum\u2019s PhD thesis (Tenenbaum 1999). The game\nproceeds as follows. I choose some simple arithmetical concept C, such as \u201cprime number\u201d or\n\u201ca number between 1 and 10\u201d. I then give you a series of randomly chosen positive examplesD={x\n1,...,x N}drawn from C, and ask you whether some new test case \u02dcxbelongs to C,\ni.e., I ask you to classify \u02dcx.", "96": "66 Chapter 3. Generative models for discrete data\n481216202428323640444852566064687276808488929610000.5116Examples\n481216202428323640444852566064687276808488929610000.5160\n481216202428323640444852566064687276808488929610000.5116   8   2  64\n481216202428323640444852566064687276808488929610000.5116  23  19  20\nFigure 3.1 Empirical predictive distribution averaged over 8 humans in the number game. First two\nrows: after seeing D={16}andD={60}. This illustrates diffuse similarity. Third row: after\nseeingD={16,8,2,64}. This illustrates rule-like behavior (powers of 2). Bottom row: after seeing\nD={16,23,19,20}. This illustrates focussed similarity (numbers near 20). Source: Figure 5.5 of\n(Tenenbaum 1999). Used with kind permission of Josh Tenenbaum.\nSuppose, for simplicity, that all numbers are integers between 1 and 100. Now suppose I tell\nyou \u201c16\u201d is a positive example of the concept. What other numbers do you think are positive?\n17? 6? 32? 99? It\u2019s hard to tell with only one example, so your predictions will be quite vague.Presumably numbers that are similar in some sense to 16 are more likely. But similar in whatway? 17 is similar, because it is \u201cclose by\u201d, 6 is similar because it has a digit in common,32 is similar because it is also even and a power of 2, but 99 does not seem similar. Thussome numbers are more likely than others. We can represent this as a probability distribution,p(\u02dcx|D), which is the probability that \u02dcx\u2208Cgiven the data Dfor any\u02dcx\u2208{1,...,100}. This\nis called the posterior predictive distribution. Figure 3.1(top) shows the predictive distribution\nof people derived from a lab experiment. We see that people predict numbers that are similarto 16, under a variety of kinds of similarity.\nNow suppose I tell you that 8, 2 and 64 are alsopositive examples. Now you may guess that\nthe hidden concept is \u201cpowers of two\u201d. This is an example of induction. Given this hypothesis,\nthe predictive distribution is quite speci\ufb01c, and puts most of its mass on powers of 2, as shownin Figure 3.1(third row). If instead I tell you the data is D={16,23,19,20}, you will get a\ndifferent kind of generalization gradient, as shown in Figure 3.1(bottom).\nHow can we explain this behavior and emulate it in a machine? The classic approach to\ninduction is to suppose we have a hypothesis space of concepts, H, such as: odd numbers,\neven numbers, all numbers between 1 and 100, powers of two, all numbers ending in j(for", "97": "3.2. Bayesian concept learning 67\n0\u2264j\u22649), etc. The subset of Hthat is consistent with the data Dis called the version space.\nAs we see more examples, the version space shrinks and we become increasingly certain about\nthe concept (Mitchell 1997).\nHowever, the version space is not the whole story. After seeing D={16}, there are many\nconsistent rules; how do you combine them to predict if \u02dcx\u2208C? Also, after seeing D=\n{16,8,2,64}, why did you choose the rule \u201cpowers of two\u201d and not, say, \u201call even numbers\u201d, or\n\u201cpowers of two except for 32\u201d, both of which are equally consistent with the evidence? We willnow provide a Bayesian explanation for this.\n3.2.1 Likelihood\nWe must explain why we chose htwo/defines\u201cpowers of two\u201d, and not, say, heven/defines\u201ceven numbers\u201d\nafter seeing D={16,8,2,64}, given that both hypotheses are consistent with the evidence.\nThe key intuition is that we want to avoid suspicious coincidences. If the true concept was\neven numbers, how come we only saw numbers that happened to be powers of two?\nTo formalize this, let us assume that examples are sampled uniformly at random from the\nextension of a concept. (The extension of a concept is just the set of numbers that belong\nto it, e.g., the extension of hevenis{2,4,6,...,98,100}; the extension of \u201cnumbers ending\nin 9\u201d is{9,19,...,99}.) Tenenbaum calls this the strong sampling assumption. Given this\nassumption, the probability of independently sampling Nitems (with replacement) from his\ngiven by\np(D|h)=\u23a8bracketleftbigg1\nsize(h)\u23a8bracketrightbiggN\n=\u23a8bracketleftbigg1\n|h|\u23a8bracketrightbiggN\n(3.2)\nThis crucial equation embodies what Tenenbaum calls the size principle, which means the\nmodel favors the simplest (smallest) hypothesis consistent with the data. This is more commonlyknown as Occam\u2019s razor.\n1\nTo see how it works, let D={16}. Then p(D|htwo)=1/6, since there are only 6 powers\nof two less than 100, but p(D|heven)=1/50, since there are 50 even numbers. So the\nlikelihood that h=htwois higher than if h=heven. After 4 examples, the likelihood of htwo\nis(1/6)4=7.7\u00d710\u22124, whereas the likelihood of hevenis(1/50)4=1.6\u00d710\u22127. This is\nalikelihood ratio of almost 5000:1 in favor of htwo. This quanti\ufb01es our earlier intuition that\nD={16,8,2,64}would be a very suspicious coincidence if generated by heven.\n3.2.2 Prior\nSupposeD={16,8,2,64}. Given this data, the concept h/prime=\u201cpowers of two except 32\u201d is\nmore likely than h=\u201cpowers of two\u201d, since h/primedoes not need to explain the coincidence that 32\nis missing from the set of examples.\nHowever, the hypothesis h/prime=\u201cpowers of two except 32\u201d seems \u201cconceptually unnatural\u201d. We\ncan capture such intution by assigning low prior probability to unnatural concepts. Of course,your prior might be different than mine. This subjective aspect of Bayesian reasoning is a\nsource of much controversy, since it means, for example, that a child and a math professor\n1. William of Occam (also spelt Ockham) was an English monk and philosopher, 1288\u20131348.", "98": "68 Chapter 3. Generative models for discrete data\nwill reach different answers. In fact, they presumably not only have different priors, but also\ndifferent hypothesis spaces. However, we can \ufb01nesse that by de\ufb01ning the hypothesis space ofthe child and the math professor to be the same, and then setting the child\u2019s prior weight to bezero on certain \u201cadvanced\u201d concepts. Thus there is no sharp distinction between the prior andthe hypothesis space.\nAlthough the subjectivity of the prior is controversial, it is actually quite useful. If you are\ntold the numbers are from some arithmetic rule, then given 1200, 1500, 900 and 1400, you maythink 400 is likely but 1183 is unlikely. But if you are told that the numbers are examples ofhealthy cholesterol levels, you would probably think 400 is unlikely and 1183 is likely. Thus wesee that the prior is the mechanism by which background knowledge can be brought to bear ona problem. Without this, rapid learning (i.e., from small samples sizes) is impossible.\nSo, what prior should we use? For illustration purposes, let us use a simple prior which\nputs uniform probability on 30 simple arithmetical concepts, such as \u201ceven numbers\u201d, \u201coddnumbers\u201d, \u201cprime numbers\u201d, \u201cnumbers ending in 9\u201d, etc. To make things more interesting, wemake the concepts even and odd more likely apriori. We also include two \u201cunnatural\u201d concepts,namely \u201cpowers of 2, plus 37\u201d and \u201cpowers of 2, except 32\u201d, but give them low prior weight. SeeFigure 3.2(a) for a plot of this prior. We will consider a slightly more sophisticated prior later on.\n3.2.3 Posterior\nThe posterior is simply the likelihood times the prior, normalized. In this context we have\np(h|D)=p(D|h)p(h)\u23a8summationtext\nh/prime\u2208Hp(D,h/prime)=p(h)I(D\u2208h)/|h|N\n\u23a8summationtext\nh/prime\u2208Hp(h/prime)I(D\u2208h/prime)/|h/prime|N(3.3)\nwhere I(D\u2208h)is 1iff(iff and only if) all the data are in the extension of the hypothesis\nh. Figure 3.2 plots the prior, likelihood and posterior after seeing D={16}. We see that the\nposterior is a combination of prior and likelihood. In the case of most of the concepts, the prioris uniform, so the posterior is proportional to the likelihood. However, the \u201cunnatural\u201d conceptsof \u201cpowers of 2, plus 37\u201d and \u201cpowers of 2, except 32\u201d have low posterior support, despite havinghigh likelihood, due to the low prior. Conversely, the concept of odd numbers has low posteriorsupport, despite having a high prior, due to the low likelihood.\nFigure 3.3 plots the prior, likelihood and posterior after seeing D={16,8,2,64}. Now the\nlikelihood is much more peaked on the powers of two concept, so this dominates the posterior.Essentially the learner has an ahamoment, and \ufb01gures out the true concept. (Here we see the\nneed for the low prior on the unnatural concepts, otherwise we would have over\ufb01t the data andpicked \u201cpowers of 2, except for 32\u201d.)\nIn general, when we have enough data, the posterior p(h|D)becomes peaked on a single\nconcept, namely the MAP estimate, i.e.,\np(h|D)\u2192\u03b4\n\u02c6hMAP(h) (3.4)\nwhere\u02c6hMAP=a r g m a xhp(h|D)is the posterior mode, and where \u03b4is theDirac measure\nde\ufb01ned by\n\u03b4x(A)=\u23a8braceleftbigg\n1ifx\u2208A\n0ifx/negationslash\u2208A(3.5)", "99": "3.2. Bayesian concept learning 69\n0 0.1 0.2powers of 2 \u2212 {32}powers of 2 + {37}allpowers of 10powers of 9powers of 8powers of 7powers of 6powers of 5powers of 4powers of 3powers of 2ends in 9ends in 8ends in 7ends in 6ends in 5ends in 4ends in 3ends in 2ends in 1mult of 10mult of 9mult of 8mult of 7mult of 6mult of 5mult of 4mult of 3squaresoddeven\nprior0 0.2 0.405101520253035\nlikdata = 16\n0 0.2 0.405101520253035\npost\nFigure 3.2 Prior, likelihood and posterior for D={16}. Based on (Tenenbaum 1999). Figure generated\nbynumbersGame .\nNote that the MAP estimate can be written as\n\u02c6hMAP= argmax\nhp(D|h)p(h) = argmax\nh[logp(D|h)+logp(h)] (3.6)\nSince the likelihood term depends exponentially on N, and the prior stays constant, as we get\nmore and more data, the MAP estimate converges towards the maximum likelihood estimate\norMLE:\n\u02c6hmle/definesargmax\nhp(D|h) = argmax\nhlogp(D|h) (3.7)\nIn other words, if we have enough data, we see that the data overwhelms the prior. In this", "100": "70 Chapter 3. Generative models for discrete data\n0 0.1 0.2powers of 2 \u2212 {32}powers of 2 + {37}allpowers of 10powers of 9powers of 8powers of 7powers of 6powers of 5powers of 4powers of 3powers of 2ends in 9ends in 8ends in 7ends in 6ends in 5ends in 4ends in 3ends in 2ends in 1mult of 10mult of 9mult of 8mult of 7mult of 6mult of 5mult of 4mult of 3squaresoddeven\nprior0 1 2\nx 10\u2212305101520253035\nlikdata = 16   8   2  64\n0 0.5 105101520253035\npost\nFigure 3.3 Prior, likelihood and posterior for D={16,8,2,64}. Based on (Tenenbaum 1999). Figure\ngenerated by numbersGame .\ncase, the MAP estimate converges towards the MLE.\nIf the true hypothesis is in the hypothesis space, then the MAP/ ML estimate will converge\nupon this hypothesis. Thus we say that Bayesian inference (and ML estimation) are consistent\nestimators (see Section 6.4.1 for details). We also say that the hypothesis space is identi\ufb01able in\nthe limit, meaning we can recover the truth in the limit of in\ufb01nite data. If our hypothesis classis not rich enough to represent the \u201ctruth\u201d (which will usually be the case), we will convergeon the hypothesis that is as close as possible to the truth. However, formalizing this notion of\u201ccloseness\u201d is beyond the scope of this chapter.", "101": "3.2. Bayesian concept learning 71\n4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 10000.51\npowers of 2 + {37}powers of 2 \u2212 {32}allmult of 4mult of 8evensquaresends in 6powers of 2powers of 4\n0 0.5 1\np(h | 16 )\nFigure 3.4 Posterior over hypotheses and the corresponding predictive distribution after seeing one\nexample, D={16}. A dot means this number is consistent with this hypothesis. The graph p(h|D)on\nthe right is the weight given to hypothesis h. By taking a weighed sum of dots, we get p(\u02dcx\u2208C|D)(top).\nBased on Figure 2.9 of (Tenenbaum 1999). Figure generated by numbersGame .\n3.2.4 Posterior predictive distribution\nThe posterior is our internal belief state about the world. The way to test if our beliefs are\njusti\ufb01ed is to use them to predict objectively observable quantities (this is the basis of the\nscienti\ufb01c method). Speci\ufb01cally, the posterior predictive distribution in this context is given by\np(\u02dcx\u2208C|D)=\u23a8summationdisplay\nhp(y=1|\u02dcx,h)p(h|D) (3.8)\nThis is just a weighted average of the predictions of each individual hypothesis and is calledBayes model averaging (Hoeting et al. 1999). This is illustrated in Figure 3.4. The dots at the\nbottom show the predictions from each hypothesis; the vertical curve on the right shows theweight associated with each hypothesis. If we multiply each row by its weight and add up, weget the distribution at the top.\nWhen we have a small and/or ambiguous dataset, the posterior p(h|D)is vague, which\ninduces a broad predictive distribution. However, once we have \u201c\ufb01gured things out\u201d, the posteriorbecomes a delta function centered at the MAP estimate. In this case, the predictive distribution", "102": "72 Chapter 3. Generative models for discrete data\nbecomes\np(\u02dcx\u2208C|D)=\u23a8summationdisplay\nhp(\u02dcx|h)\u03b4\u02c6h(h)=p(\u02dcx|\u02c6h) (3.9)\nThis is called a plug-in approximation to the predictive density and is very widely used, due\nto its simplicity. However, in general, this under-represents our uncertainty, and our predictions\nwill not be as \u201csmooth\u201d as when using BMA. We will see more examples of this later in the book.\nAlthough MAP learning is simple, it cannot explain the gradual shift from similarity-based\nreasoning (with uncertain posteriors) to rule-based reasoning (with certain posteriors). Forexample, suppose we observe D={16}. If we use the simple prior above, the minimal\nconsistent hypothesis is \u201call powers of 4\u201d, so only 4 and 16 get a non-zero probability of beingpredicted. This is of course an example of over\ufb01tting. Given D={16,8,2,64}, the MAP\nhypothesis is \u201call powers of two\u201d. Thus the plug-in predictive distribution gets broader (or staysthe same) as we see more data: it starts narrow, but is forced to broaden as it seems more data.In contrast, in the Bayesian approach, we start broad and then narrow down as we learn more,which makes more intuitive sense. In particular, given D={16}, there are many hypotheses\nwith non-negligible posterior support, so the predictive distribution is broad. However, when weseeD={16,8,2,64}, the posterior concentrates its mass on one hypothesis, so the predictive\ndistribution becomes narrower. So the predictions made by a plug-in approach and a Bayesianapproach are quite different in the small sample regime, although they converge to the sameanswer as we see more data.\n3.2.5 A more complex prior\nTo model human behavior, Tenenbaum used a slightly more sophisticated prior which was de-rived by analysing some experimental data of how people measure similarity between numbers;see (Tenenbaum 1999, p208) for details. The result is a set of arithmetical concepts similar tothose mentioned above, plus all intervals between nandmfor1\u2264n,m\u2264100. (Note that\nthese hypotheses are not mutually exclusive.) Thus the prior is a mixture of two priors, one\nover arithmetical rules, and one over intervals:\np(h)=\u03c0\n0prules(h)+(1\u2212 \u03c00)pinterval(h) (3.10)\nThe only free parameter in the model is the relative weight, \u03c00, given to these two parts of the\nprior. The results are not very sensitive to this value, so long as \u03c00>0.5, re\ufb02ecting the fact\nthat people are more likely to think of concepts de\ufb01ned by rules. The predictive distributionof the model, using this larger hypothesis space, is shown in Figure 3.5. It is strikingly similarto the human predictive distribution, shown in Figure 3.1, even though it was not \ufb01t to humandata (modulo the choice of hypothesis space).\n3.3 The beta-binomial model\nThe number game involved inferring a distribution over a discrete variable drawn from a \ufb01nitehypothesis space, h\u2208H, given a series of discrete observations. This made the computations\nparticularly simple: we just needed to sum, multiply and divide. However, in many applications,the unknown parameters are continuous, so the hypothesis space is (some subset) of R\nK,w h e r e", "103": "3.3. The beta-binomial model 73\n481216202428323640444852566064687276808488929610000.5116Examples\n481216202428323640444852566064687276808488929610000.5160\n481216202428323640444852566064687276808488929610000.5116   8   2  64\n481216202428323640444852566064687276808488929610000.5116  23  19  20\nFigure 3.5 Predictive distributions for the model using the full hypothesis space. Compare to Figure 3.1.\nThe predictions of the Bayesian model are only plotted for those values of \u02dcxfor which human data is\navailable; this is why the top line looks sparser than Figure 3.4. Source: Figure 5.6 of (Tenenbaum 1999).\nUsed with kind permission of Josh Tenenbaum.\nKis the number of parameters. This complicates the mathematics, since we have to replace\nsums with integrals. However, the basic ideas are the same.\nWe will illustrate this by considering the problem of inferring the probability that a coin shows\nup heads, given a series of observed coin tosses. Although this might seem trivial, it turns out\nthat this model forms the basis of many of the methods we will consider later in this book,including naive Bayes classi\ufb01ers, Markov models, etc. It is historically important, since it was theexample which was analyzed in Bayes\u2019 original paper of 1763. (Bayes\u2019 analysis was subsequentlygeneralized by Pierre-Simon Laplace, creating what we now call \u201cBayes rule\u201d \u2014 see (Stigler 1986)for further historical details.)\nWe will follow our now-familiar recipe of specifying the likelihood and prior, and deriving the\nposterior and posterior predictive.\n3.3.1 Likelihood\nSuppose Xi\u223cBer(\u03b8),w h e r eXi=1represents \u201cheads\u201d, Xi=0represents \u201ctails\u201d, and\n\u03b8\u2208[0,1]is the rate parameter (probability of heads). If the data are iid, the likelihood has the\nform\np(D|\u03b8)=\u03b8N1(1\u2212\u03b8)N0(3.11)", "104": "74 Chapter 3. Generative models for discrete data\nw h e r ew eh a v eN 1=\u23a8summationtextN\ni=1I(xi=1 )heads and N0=\u23a8summationtextNi=1I(xi=0 )tails. These two counts\nare called the sufficient statistics of the data, since this is all we need to know about Dto\ninfer\u03b8. (An alternative set of sufficient statistics are N1andN=N0+N1.)\nMore formally, we say s(D)is a sufficient statistic for data Difp(\u03b8|D)=p(\u03b8|s(data)).I f\nwe use a uniform prior, this is equivalent to saying p(D|\u03b8\u221dp(s(D)|\u03b8). Consequently, if we\nhave two datasets with the same sufficient statistics, we will infer the same value for \u03b8.\nNow suppose the data consists of the count of the number of heads N1observed in a \ufb01xed\nnumberN=N1+N0of trials. In this case, we have N1\u223cBin(N,\u03b8),w h e r eBinrepresents\nthe binomial distribution, which has the following pmf:\nBin(k|n,\u03b8)/defines\u23a8parenleftbiggn\nk\u23a8parenrightbigg\n\u03b8k(1\u2212\u03b8)n\u2212k(3.12)\nSince\u23a8parenleftbiggn\nk\u23a8parenrightbigg\nis a constant independent of \u03b8, the likelihood for the binomial sampling model is the\nsame as the likelihood for the Bernoulli model. So any inferences we make about \u03b8will be the\nsame whether we observe the counts, D=(N1,N), or a sequence of trials, D={x1,...,x N}.\n3.3.2 Prior\nWe need a prior which has support over the interval [0,1]. To make the math easier, it would\nconvenient if the prior had the same form as the likelihood, i.e., if the prior looked like\np(\u03b8)\u221d\u03b8\u03b31(1\u2212\u03b8)\u03b32(3.13)\nfor some prior parameters \u03b31and\u03b32. If this were the case, then we could easily evaluate the\nposterior by simply adding up the exponents:\np(\u03b8)\u221dp(D|\u03b8)p(\u03b8)=\u03b8N1(1\u2212\u03b8)N0\u03b8\u03b31(1\u2212\u03b8)\u03b32=\u03b8N1+\u03b31(1\u2212\u03b8)N0+\u03b32(3.14)\nWhen the prior and the posterior have the same form, we say that the prior is a conjugate\npriorfor the corresponding likelihood. Conjugate priors are widely used because they simplify\ncomputation, and are easy to interpret, as we see below.\nIn the case of the Bernoulli, the conjugate prior is the beta distribution, which we encountered\nin Section 2.4.5:\nBeta(\u03b8|a,b)\u221d\u03b8a\u22121(1\u2212\u03b8)b\u22121(3.15)\nThe parameters of the prior are called hyper-parameters. We can set them in order to encode\nour prior beliefs. For example, to encode our beliefs that \u03b8has mean 0.7 and standard deviation\n0.2, we set a=2.975andb=1.275(Exercise 3.15). Or to encode our beliefs that \u03b8has mean\n0.15 and that we think it lives in the interval (0.05,0.30)with probability, then we \ufb01nd a=4.5\nandb=2 5.5(Exercise 3.16).\nIf we know \u201cnothing\u201d about \u03b8, except that it lies in the interval [0,1], we can use a uni-\nform prior, which is a kind of uninformative prior (see Section 5.4.2 for details). The uniform\ndistribution can be represented by a beta distribution with a=b=1.", "105": "3.3. The beta-binomial model 75\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10123456\n  \nprior Be(2.0, 2.0)\nlik Be(4.0, 18.0)\npost Be(5.0, 19.0)\n(a)0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.511.522.533.544.5\n  \nprior Be(5.0, 2.0)\nlik Be(12.0, 14.0)\npost Be(16.0, 15.0)\n(b)\nFigure 3.6 (a) Updating a Beta(2, 2) prior with a Binomial likelihood with sufficient statistics N1=\n3,N0=1 7to yield a Beta(5,19) posterior. (b) Updating a Beta(5, 2) prior with a Binomial likeli-\nhood with sufficient statistics N1=1 1,N0=1 3to yield a Beta(16, 15) posterior. Figure generated by\nbinomialBetaPosteriorDemo .\n3.3.3 Posterior\nIf we multiply the likelihood by the beta prior we get the following posterior (following Equa-\ntion 3.14):\np(\u03b8|D)\u221dBin(N 1|\u03b8,N0+N1)Beta(\u03b8|a,b)Beta(\u03b8|N1+a,N0+b) (3.16)\nIn particular, the posterior is obtained by adding the prior hyper-parameters to the empiricalcounts. For this reason, the hyper-parameters are known as pseudo counts. The strength of the\nprior, also known as the effective sample size of the prior, is the sum of the pseudo counts,\na+b; this plays a role analogous to the data set size, N\n1+N0=N.\nFigure 3.6(a) gives an example where we update a weak Beta(2,2) prior with a peaked likelihood\nfunction, corresponding to a large sample size; we see that the posterior is essentially identicalto the likelihood: since the data has overwhelmed the prior. Figure 3.6(b) gives an examplewhere we update a strong Beta(5,2) prior with a peaked likelihood function; now we see that theposterior is a \u201ccompromise\u201d between the prior and likelihood.\nNote that updating the posterior sequentially is equivalent to updating in a single batch.\nTo see this, suppose we have two data sets D\naandDbwith sufficient statistics Na\n1,Na\n0and\nNb\n1,Nb\n0.L e tN1=Na\n1+Nb\n1andN0=Na\n0+Nb\n0be the sufficient statistics of the combined\ndatasets. In batch mode we have\np(\u03b8|Da,Db)\u221dBin(N 1|\u03b8,N1+N0)Beta(\u03b8|a,b)\u221dBeta(\u03b8|N1+a,N0+b)(3.17)\nIn sequential mode, we have\np(\u03b8|Da,Db)\u221dp(Db|\u03b8)p(\u03b8|Da) (3.18)\n\u221dBin(Nb\n1|\u03b8,Nb\n1+Nb\n0)Beta(\u03b8|Na\n1+a,Na\n0+b) (3.19)\n\u221dBeta(\u03b8|Na\n1+Nb\n1+a,Na\n0+Nb\n0+b) (3.20)\nThis makes Bayesian inference particularly well-suited to online learning, as we will see later.", "106": "76 Chapter 3. Generative models for discrete data\n3.3.3.1 Posterior mean and mode\nFrom Equation 2.62, the MAP estimate is given by\n\u02c6\u03b8MAP=a+N1\u22121\na+b+N\u22122(3.21)\nIf we use a uniform prior, then the MAP estimate reduces to the MLE, which is just the empirical\nfraction of heads:\n\u02c6\u03b8MLE=N1\nN(3.22)\nThis makes intuitive sense, but it can also be derived by applying elementary calculus tomaximize the likelihood function in Equation 3.11. (Exercise 3.1).\nBy contrast, the posterior mean is given by,\n\u03b8=a+N1\na+b+N(3.23)\nThis difference between the mode and the mean will prove important later.\nWe will now show that the posterior mean is convex combination of the prior mean and the\nMLE, which captures the notion that the posterior is a compromise between what we previouslybelieved and what the data is telling us.\nLet\u03b1\n0=a+bbe theequivalent sample size of the prior, which controls its strength, and\nlet the prior mean be m1=a/\u03b10. Then the posterior mean is given by\nE[\u03b8|D]=\u03b10m1+N1\nN+\u03b10=\u03b10\nN+\u03b10m1+N\nN+\u03b10N1\nN=\u03bbm1+(1\u2212\u03bb)\u02c6\u03b8MLE (3.24)\nwhere\u03bb=\u03b10\nN+\u03b1 0is the ratio of the prior to posterior equivalent sample size. So the weaker the\nprior, the smaller is \u03bb, and hence the closer the posterior mean is to the MLE. One can show\nsimilarly that the posterior mode is a convex combination of the prior mode and the MLE, andthat it too converges to the MLE.\n3.3.3.2 Posterior variance\nThe mean and mode are point estimates, but it is useful to know how much we can trust them.The variance of the posterior is one way to measure this. The variance of the Beta posterior isgiven by\nvar[\u03b8|D]=(a+N\n1)(b+N0)\n(a+N1+b+N0)2(a+N1+b+N0+1)(3.25)\nWe can simplify this formidable expression in the case that N/greatermucha,b,t og e t\nvar[\u03b8|D]\u2248N1N0\nNNN=\u02c6\u03b8(1\u2212\u02c6\u03b8)\nN(3.26)\nwhere\u02c6\u03b8is the MLE. Hence the \u201cerror bar\u201d in our estimate (i.e., the posterior standard deviation),\nis given by\n\u03c3=\u23a8radicalbig\nvar[\u03b8|D]\u2248\u23a8radicalBigg\n\u02c6\u03b8(1\u2212\u02c6\u03b8)\nN(3.27)", "107": "3.3. The beta-binomial model 77\nWe see that the uncertainty goes down at a rate of 1/\u221a\nN. Note, however, that the uncertainty\n(variance) is maximized when \u02c6\u03b8=0.5, and is minimized when \u02c6\u03b8is close to 0 or 1. This means\nit is easier to be sure that a coin is biased than to be sure that it is fair.\n3.3.4 Posterior predictive distribution\nSo far, we have been focusing on inference of the unknown parameter(s). Let us now turn our\nattention to prediction of future observable data.\nConsider predicting the probability of heads in a single future trial under a Beta(a,b)poste-\nrior. We have\np(\u02dcx=1|D)=\u23a8integraldisplay1\n0p(x=1|\u03b8)p(\u03b8|D)d\u03b8 (3.28)\n=\u23a8integraldisplay1\n0\u03b8Beta(\u03b8|a,b)d\u03b8=E[\u03b8|D]=a\na+b(3.29)\nThus we see that the mean of the posterior predictive distribution is equivalent (in this case) toplugging in the posterior mean parameters: p(\u02dcx|D)=B e r (\u02dcx|E[\u03b8|D]).\n3.3.4.1 Over\ufb01tting and the black swan paradox\nSuppose instead that we plug-in the MLE, i.e., we use p(\u02dcx|D)\u2248Ber(\u02dcx|\u02c6\u03b8\nMLE). Unfortunately,\nthis approximation can perform quite poorly when the sample size is small. For example,suppose we have seen N=3tails in a row. The MLE is \u02c6\u03b8=0/3=0, since this makes the\nobserved data as probable as possible. However, using this estimate, we predict that heads areimpossible. This is called the zero count problem or thesparse data problem, and frequently\noccurs when estimating counts from small amounts of data. One might think that in the eraof \u201cbig data\u201d, such concerns are irrelevant, but note that once we partition the data based oncertain criteria \u2014 such as the number of times a speci\ufb01c person has engaged in a speci\ufb01c activity\n\u2014 the sample sizes can become much smaller. This problem arises, for example, when tryingto perform personalized recommendation of web pages. Thus Bayesian methods are still useful,even in the big data regime (Jordan 2011).\nThe zero-count problem is analogous to a problem in philosophy called the black swan\nparadox. This is based on the ancient Western conception that all swans were white. Inthat context, a black swan was a metaphor for something that could not exist. (Black swanswere discovered in Australia by European explorers in the 17th Century.) The term \u201cblack swanparadox\u201d was \ufb01rst coined by the famous philosopher of science Karl Popper; the term has alsobeen used as the title of a recent popular book (Taleb 2007). This paradox was used to illustratethe problem of induction, which is the problem of how to draw general conclusions about the\nfuture from speci\ufb01c observations from the past.\nLet us now derive a simple Bayesian solution to the problem. We will use a uniform prior, so\na=b=1. In this case, plugging in the posterior mean gives Laplace\u2019s rule of succession\np(\u02dcx=1|D)=N\n1+1\nN1+N0+2(3.30)\nThis justi\ufb01es the common practice of adding 1 to the empirical counts, normalizing and thenplugging them in, a technique known as add-one smoothing. (Note that plugging in the MAP", "108": "78 Chapter 3. Generative models for discrete data\nparameters would not have this smoothing effect, since the mode has the form \u02c6\u03b8=N1+a\u22121\nN+a+b\u22122,\nwhich becomes the MLE if a=b=1.)\n3.3.4.2 Predicting the outcome of multiple future trials\nSuppose now we were interested in predicting the number of heads, x,i nMfuture trials. This\nis given by\np(x|D,M)=\u23a8integraldisplay1\n0Bin(x|\u03b8,M)Beta(\u03b8|a,b)d\u03b8 (3.31)\n=\u23a8parenleftbiggM\nx\u23a8parenrightbigg1\nB(a,b)\u23a8integraldisplay1\n0\u03b8x(1\u2212\u03b8)M\u2212x\u03b8a\u22121(1\u2212\u03b8)b\u22121d\u03b8 (3.32)\nWe recognize the integral as the normalization constant for a Beta(a+x,M\u2212x+b)distribution.\nHence\n\u23a8integraldisplay1\n0\u03b8x(1\u2212\u03b8)M\u2212x\u03b8a\u22121(1\u2212\u03b8)b\u22121d\u03b8=B(x+a,M\u2212x+b) (3.33)\nThus we \ufb01nd that the posterior predictive is given by the following, known as the (compound)\nbeta-binomial distribution:\nBb(x|a,b,M)/defines\u23a8parenleftbiggM\nx\u23a8parenrightbiggB(x+a,M\u2212x+b)\nB(a,b)(3.34)\nThis distribution has the following mean and variance\nE[x]=Ma\na+b,var[x]=Mab\n(a+b)2(a+b+M)\na+b+1(3.35)\nIfM=1, and hence x\u2208{0,1}, we see that the mean becomes E[x|D]=p(x=1|D)=a\na+b,\nwhich is consistent with Equation 3.29.\nThis process is illustrated in Figure 3.7(a). We start with a Beta(2,2) prior, and plot the\nposterior predictive density after seeing N1=3heads and N0=1 7tails. Figure 3.7(b) plots\na plug-in approximation using a MAP estimate. We see that the Bayesian prediction has longertails, spreading its probablity mass more widely, and is therefore less prone to over\ufb01tting andblackswan type paradoxes.\n3.4 The Dirichlet-multinomial model\nIn the previous section, we discussed how to infer the probability that a coin comes up heads.In this section, we generalize these results to infer the probability that a dice with Ksides\ncomes up as face k. This might seem like another toy exercise, but the methods we will study\nare widely used to analyse text data, biosequence data, etc., as we will see later.", "109": "3.4. The Dirichlet-multinomial model 79\n0 1 2 3 4 5 6 7 8 9 1000.050.10.150.20.250.30.35posterior predictive\n(a)0 1 2 3 4 5 6 7 8 9 1000.050.10.150.20.250.30.35plugin predictive\n(b)\nFigure 3.7 (a) Posterior predictive distributions after seeing N1=3,N0=1 7. (b) Plugin approximation.\nFigure generated by betaBinomPostPredDemo .\n3.4.1 Likelihood\nSuppose we observe Ndice rolls, D={x1,...,x N},w h e r e xi\u2208{1,...,K}. If we assume\nthe data is iid, the likelihood has the form\np(D|\u03b8)=K\u23a8productdisplay\nk=1\u03b8Nk\nk(3.36)\nwhereNk=\u23a8summationtextN\ni=1I(yi=k)is the number of times event koccured (these are the sufficient\nstatistics for this model). The likelihood for the multinomial model has the same form, up to an\nirrelevant constant factor.\n3.4.2 Prior\nSince the parameter vector lives in the K-dimensional probability simplex, we need a prior that\nhas support over this simplex. Ideally it would also be conjugate. Fortunately, the Dirichletdistribution (Section 2.5.4) satis\ufb01es both criteria. So we will use the following prior:\nDir(\u03b8|\u03b1)=1\nB(\u03b1)K\u23a8productdisplay\nk=1\u03b8\u03b1k\u22121\nkI(x\u2208SK) (3.37)\n3.4.3 Posterior\nMultiplying the likelihood by the prior, we \ufb01nd that the posterior is also Dirichlet:\np(\u03b8|D)\u221dp(D|\u03b8)p(\u03b8) (3.38)\n\u221dK\u23a8productdisplay\nk=1\u03b8Nk\nk\u03b8\u03b1k\u22121\nk=K\u23a8productdisplay\nk=1\u03b8\u03b1k+Nk\u22121\nk(3.39)\n= Dir(\u03b8 |\u03b11+N1,...,\u03b1 K+NK) (3.40)", "110": "80 Chapter 3. Generative models for discrete data\nWe see that the posterior is obtained by adding the prior hyper-parameters (pseudo-counts) \u03b1k\nto the empirical counts Nk.\nWe can derive the mode of this posterior (i.e., the MAP estimate) by using calculus. However,\nwe must enforce the constraint that\u23a8summationtext\nk\u03b8k=1.2. We can do this by using a Lagrange\nmultiplier. The constrained objective function, or Lagrangian, is given by the log likelihood\nplus log prior plus the constraint:\n/lscript(\u03b8,\u03bb)=\u23a8summationdisplay\nkNklog\u03b8k+\u23a8summationdisplay\nk(\u03b1k\u22121)log\u03b8k+\u03bb\u23a8parenleftBigg\n1\u2212\u23a8summationdisplay\nk\u03b8k\u23a8parenrightBigg\n(3.41)\nTo simplify notation, we de\ufb01ne N/prime\nk/definesNk+\u03b1k\u22121. Taking derivatives with respect to \u03bbyields\nthe original constraint:\n\u2202/lscript\n\u2202\u03bb=\u23a8parenleftBigg\n1\u2212\u23a8summationdisplay\nk\u03b8k\u23a8parenrightBigg\n=0 (3.42)\nTaking derivatives with respect to \u03b8kyields\n\u2202/lscript\n\u2202\u03b8k=N/prime\nk\n\u03b8k\u2212\u03bb=0 (3.43)\nN/prime\nk=\u03bb\u03b8k (3.44)\nWe can solve for \u03bbusing the sum-to-one constraint:\n\u23a8summationdisplay\nkN/prime\nk=\u03bb\u23a8summationdisplay\nk\u03b8k (3.45)\nN+\u03b10\u2212K=\u03bb (3.46)\nwhere\u03b10/defines\u23a8summationtextK\nk=1\u03b1kis the equivalent sample size of the prior. Thus the MAP estimate is\ngiven by\n\u02c6\u03b8k=Nk+\u03b1k\u22121\nN+\u03b10\u2212K(3.47)\nwhich is consistent with Equation 2.77. If we use a uniform prior, \u03b1k=1, we recover the MLE:\n\u02c6\u03b8k=Nk/N (3.48)\nThis is just the empirical fraction of times face kshows up.\n2. We do not need to explicitly enforce the constraint that \u03b8k\u22650since the gradient of the objective has the form\nNk/\u03b8k\u2212\u03bb; so negative values would reduce the objective, rather than maximize it. (Of course, this does not preclude\nsetting\u03b8k=0, and indeed this is the optimal solution if Nk=0and\u03b1k=1.)", "111": "3.4. The Dirichlet-multinomial model 81\n3.4.4 Posterior predictive\nThe posterior predictive distribution for a single multinoulli trial is given by the following\nexpression:\np(X=j|D)=\u23a8integraldisplay\np(X=j|\u03b8)p(\u03b8|D)d\u03b8 (3.49)\n=\u23a8integraldisplay\np(X=j|\u03b8j)\u23a8bracketleftbigg\u23a8integraldisplay\np(\u03b8\u2212j,\u03b8j|D)d\u03b8\u2212j\u23a8bracketrightbigg\nd\u03b8j (3.50)\n=\u23a8integraldisplay\n\u03b8jp(\u03b8j|D)d\u03b8j=E[\u03b8j|D]=\u03b1j+Nj\u23a8summationtext\nk(\u03b1k+Nk)=\u03b1j+Nj\n\u03b10+N(3.51)\nwhere\u03b8\u2212jare all the components of \u03b8except\u03b8j. See also Exercise 3.13.\nThe above expression avoids the zero-count problem, just as we saw in Section 3.3.4.1. In\nfact, this form of Bayesian smoothing is even more important in the multinomial case than thebinary case, since the likelihood of data sparsity increases once we start partitioning the datainto many categories.\n3.4.4.1 Worked example: language models using bag of words\nOne application of Bayesian smoothing using the Dirichlet-multinomial model is to language\nmodeling, which means predicting which words might occur next in a sequence. Here wewill take a very simple-minded approach, and assume that the i\u2019th word, X\ni\u2208{1,...,K},i s\nsampled independently from all the other words using a Cat(\u03b8)distribution. This is called the\nbag of words model. Given a past sequence of words, how can we predict which one is likely\nto come next?\nFor example, suppose we observe the following sequence (part of a children\u2019s nursery rhyme):\nMary had a little lamb, little lamb, little lamb,Mary had a little lamb, its fleece as white as snow\nFurthermore, suppose our vocabulary consists of the following words:mary lamb little big fleece white black snow rain unk\n1 2 34 56 7 8 9 1 0\nHereunkstands for unknown, and represents all other words that do not appear elsewhere on\nthe list. To encode each line of the nursery rhyme, we \ufb01rst strip off punctuation, and remove\nanystop words such as \u201ca\u201d, \u201cas\u201d, \u201cthe\u201d, etc. We can also perform stemming, which means\nreducing words to their base form, such as stripping off the \ufb01nal sin plural words, or the ing\nfrom verbs (e.g., runningbecomes run). In this example, no words need stemming. Finally, we\nreplace each word by its index into the vocabulary to get:\n11 0323232\n11 0321 051 068\nWe now ignore the word order, and count how often each word occurred, resulting in a\nhistogram of word counts:", "112": "82 Chapter 3. Generative models for discrete data\nToken123 4 56789 1 0\nWordmary lamb little big \ufb02eece white black snow rain unk\nCount244 0 1 1010 4\nDenote the above counts by Nj. If we use a Dir(\u03b1)prior for \u03b8, the posterior predictive is\njust\np(\u02dcX=j|D)=E[\u03b8j|D]=\u03b1j+Nj\u23a8summationtext\nj/prime\u03b1j/prime+Nj/prime=1+Nj\n10+17(3.52)\nIf we set\u03b1j=1,w eg e t\np(\u02dcX=j|D)=( 3/27,5/27,5/27,1/27,2/27,2/27,1/27,2/27,1/27,5/27) (3.53)\nThe modes of the predictive distribution are X=2(\u201clamb\u201d) and X=1 0(\u201cunk\u201d). Note that the\nwords \u201cbig\u201d, \u201cblack\u201d and \u201crain\u201d are predicted to occur with non-zero probability in the future,\neven though they have never been seen before. Later on we will see more sophisticated languagemodels.\n3.5 Naive Bayes classi\ufb01ers\nIn this section, we discuss how to classify vectors of discrete-valued features, x\u2208{1,...,K}D,\nwhereKis the number of values for each feature, and Dis the number of features. We will use\na generative approach. This requires us to specify the class conditional distribution, p(x|y=c).\nThe simplest approach is to assume the features are conditionally independent given the class\nlabel. This allows us to write the class conditional density as a product of one dimensionaldensities:\np(x|y=c,\u03b8)=D\u23a8productdisplay\nj=1p(xj|y=c,\u03b8jc) (3.54)\nThe resulting model is called a naive Bayes classi\ufb01er (NBC).\nThe model is called \u201cnaive\u201d since we do not expect the features to be independent, even\nconditional on the class label. However, even if the naive Bayes assumption is not true, it oftenresults in classi\ufb01ers that work well (Domingos and Pazzani 1997). One reason for this is that themodel is quite simple (it only has O(CD)parameters, for Cclasses and Dfeatures), and hence\nit is relatively immune to over\ufb01tting.\nThe form of the class-conditional density depends on the type of each feature. We give some\npossibilities below:\n\u2022 In the case of real-valued features, we can use the Gaussian distribution: p(x|y=c,\u03b8)=\u23a8producttext\nD\nj=1N(xj|\u03bcjc,\u03c32\njc),w h e r e\u03bcjcis the mean of feature jin objects of class c, and\u03c32\njcis its\nvariance.\n\u2022 In the case of binary features, xj\u2208{0,1}, we can use the Bernoulli distribution: p(x|y=\nc,\u03b8)=\u23a8producttextDj=1Ber(xj|\u03bcjc),w h e r e\u03bcjcis the probability that feature joccurs in class c.\nThis is sometimes called the multivariate Bernoulli naive Bayes model. We will see an\napplication of this below.", "113": "3.5. Naive Bayes classi\ufb01ers 83\n\u2022 In the case of categorical features, xj\u2208{1,...,K}, we can model use the multinoulli\ndistribution: p(x|y=c,\u03b8)=\u23a8producttextD\nj=1Cat(xj|\u03bcjc),w h e r e\u03bcjcis a histogram over the K\npossible values for xjin classc.\nObviously we can handle other kinds of features, or use different distributional assumptions.\nAlso, it is easy to mix and match features of different types.\n3.5.1 Model \ufb01tting\nWe now discuss how to \u201ctrain\u201d a naive Bayes classi\ufb01er. This usually means computing the MLEor the MAP estimate for the parameters. However, we will also discuss how to compute the fullposterior, p(\u03b8|D).\n3.5.1.1 MLE for NBC\nThe probability for a single data case is given by\np(x\ni,yi|\u03b8)=p(yi|\u03c0)\u23a8productdisplay\njp(xij|\u03b8j)=\u23a8productdisplay\nc\u03c0I(yi=c)\nc\u23a8productdisplay\nj\u23a8productdisplay\ncp(xij|\u03b8jc)I(yi=c)(3.55)\nHence the log-likelihood is given by\nlogp(D|\u03b8)=C\u23a8summationdisplay\nc=1Nclog\u03c0c+D\u23a8summationdisplay\nj=1C\u23a8summationdisplay\nc=1\u23a8summationdisplay\ni:yi=clogp(xij|\u03b8jc) (3.56)\nWe see that this expression decomposes into a series of terms, one concerning \u03c0, andDC\nterms containing the \u03b8jc\u2019s. Hence we can optimize all these parameters separately.\nFrom Equation 3.48, the MLE for the class prior is given by\n\u02c6\u03c0c=Nc\nN(3.57)\nwhereNc/defines\u23a8summationtext\niI(yi=c)is the number of examples in class c.\nThe MLE for the likelihood depends on the type of distribution we choose to use for each\nfeature. For simplicity, let us suppose all features are binary, so xj|y=c\u223cBer(\u03b8jc). In this\ncase, the MLE becomes\n\u02c6\u03b8jc=Njc\nNc(3.58)\nIt is extremely simple to implement this model \ufb01tting procedure: See Algorithm 8 for some\npseudo-code (and naiveBayesFit for some Matlab code). This algorithm obviously takes\nO(ND)time. The method is easily generalized to handle features of mixed type. This simplicity\nis one reason the method is so widely used.\nFigure 3.8 gives an example where we have 2 classes and 600 binary features, representing the\npresence or absence of words in a bag-of-words model. The plot visualizes the \u03b8cvectors for the\ntwo classes. The big spike at index 107 corresponds to the word \u201csubject\u201d, which occurs in bothclasses with probability 1. (In Section 3.5.4, we discuss how to \u201c\ufb01lter out\u201d such uninformativefeatures.)", "114": "84 Chapter 3. Generative models for discrete data\nAlgorithm 3.1: Fitting a naive Bayes classi\ufb01er to binary features\n1Nc=0,Njc=0;\n2fori=1:Ndo\n3c=yi// Class label of i\u2019th example;\n4Nc:=Nc+1;\n5forj=1:Ddo\n6 ifxij=1then\n7 Njc:=Njc+1\n8\u02c6\u03c0c=Nc\nN,\u02c6\u03b8jc=Njc\nN\n0 100 200 300 400 500 600 70000.10.20.30.40.50.60.70.80.91p(xj=1|y=1)\n(a)0 100 200 300 400 500 600 70000.10.20.30.40.50.60.70.80.91p(xj=1|y=2)\n(b)\nFigure 3.8 Class conditional densities p(xj=1|y=c)for two document classes, corresponding to \u201cX\nwindows\u201d and \u201cMS windows\u201d. Figure generated by naiveBayesBowDemo .\n3.5.1.2 Bayesian naive Bayes\nThe trouble with maximum likelihood is that it can over\ufb01t. For example, consider the example\nin Figure 3.8: the feature corresponding to the word \u201csubject\u201d (call it feature j) always occurs\nin both classes, so we estimate \u02c6\u03b8jc=1. What will happen if we encounter a new email which\ndoes not have this word in it? Our algorithm will crash and burn, since we will \ufb01nd thatp(y=c|x,\u02c6\u03b8)=0for both classes! This is another manifestation of the black swan paradox\ndiscussed in Section 3.3.4.1.\nA simple solution to over\ufb01tting is to be Bayesian. For simplicity, we will use a factored prior:\np(\u03b8)=p( \u03c0)D\u23a8productdisplay\nj=1C\u23a8productdisplay\nc=1p(\u03b8jc) (3.59)\nWe will use a Dir(\u03b1)prior for \u03c0and aBeta(\u03b20,\u03b21)prior for each \u03b8jc. Often we just take\n\u03b1=1and\u03b2=1, corresponding to add-one or Laplace smoothing.", "115": "3.5. Naive Bayes classi\ufb01ers 85\nCombining the factored likelihood in Equation 3.56 with the factored prior above gives the\nfollowing factored posterior:\np(\u03b8|D)=p( \u03c0|D)D\u23a8productdisplay\nj=1C\u23a8productdisplay\nc=p(\u03b8jc|D) (3.60)\np(\u03c0|D)=D i r ( N1+\u03b11...,NC+\u03b1C) (3.61)\np(\u03b8jc|D) = Beta((N c\u2212Njc)+\u03b20,Njc+\u03b21) (3.62)\nIn other words, to compute the posterior, we just update the prior counts with the empirical\ncounts from the likelihood. It is straightforward to modify algorithm 8 to handle this version ofmodel \u201c\ufb01tting\u201d.\n3.5.2 Using the model for prediction\nAt test time, the goal is to compute\np(y=c|x,D)\u221dp(y=c|D)D\u23a8productdisplay\nj=1p(xj|y=c,D) (3.63)\nThe correct Bayesian procedure is to integrate out the unknown parameters:\np(y=c|x,D)\u221d\u23a8bracketleftbigg\u23a8integraldisplay\nCat(y=c|\u03c0)p(\u03c0|D)d\u03c0\u23a8bracketrightbigg\n(3.64)\nD\u23a8productdisplay\nj=1\u23a8bracketleftbigg\u23a8integraldisplay\nBer(xj|y=c,\u03b8jc)p(\u03b8jc|D)\u23a8bracketrightbigg\n(3.65)\nFortunately, this is easy to do, at least if the posterior is Dirichlet. In particular, from Equa-tion 3.51, we know the posterior predictive density can be obtained by simply plugging in theposterior mean parameters\n\u03b8. Hence\np(y=c|x,D)\u221d\u03c0cD\u23a8productdisplay\nj=1(\u03b8jc)I(xj=1)(1\u2212\u03b8jc)I(xj=0)(3.66)\n\u03b8jk=Njc+\u03b21\nNc+\u03b20+\u03b21(3.67)\n\u03c0c=Nc+\u03b1c\nN+\u03b10(3.68)\nwhere\u03b10=\u23a8summationtext\nc\u03b1c.\nIf we have approximated the posterior by a single point, p(\u03b8|D)\u2248\u03b4\u02c6\u03b8(\u03b8),w h e r e\u02c6\u03b8may be\nthe ML or MAP estimate, then the posterior predictive density is obtained by simply plugging inthe parameters, to yield a virtually identical rule:\np(y=c|x,D)\u221d\u02c6\u03c0\ncD\u23a8productdisplay\nj=1(\u02c6\u03b8jc)I(xj=1)(1\u2212\u02c6\u03b8jc)I(xj=0)(3.69)", "116": "86 Chapter 3. Generative models for discrete data\nThe only difference is we replaced the posterior mean \u03b8with the posterior mode or MLE \u02c6\u03b8.\nHowever, this small difference can be important in practice, since the posterior mean will result\nin less over\ufb01tting (see Section 3.4.4.1).\n3.5.3 The log-sum-exp trick\nWe now discuss one important practical detail that arises when using generative classi\ufb01ers of anykind. We can compute the posterior over class labels using Equation 2.13, using the appropriateclass-conditional density (and a plug-in approximation). Unfortunately a naive implementationof Equation 2.13 can fail due to numerical under\ufb02ow. The problem is that p(x|y=c)is often\na very small number, especially if xis a high-dimensional vector. This is because we require\nthat\u23a8summationtext\nxp(x|y)=1, so the probability of observing any particular high-dimensional vector is\nsmall. The obvious solution is to take logs when applying Bayes rule, as follows:\nlogp(y=c|x)=b c\u2212log\u23a8bracketleftBiggC\u23a8summationdisplay\nc/prime=1ebc/prime\u23a8bracketrightBigg\n(3.70)\nbc/defineslogp(x|y=c)+logp(y=c) (3.71)\nHowever, this requires evaluating the following expression\nlog[\u23a8summationdisplay\nc/primeebc/prime]=l o g\u23a8summationdisplay\nc/primep(y=c/prime,x)=l o gp(x) (3.72)\nand we can\u2019t add up in the log domain. Fortunately, we can factor out the largest term, and justrepresent the remaining numbers relative to that. For example,\nlog(e\n\u2212120+e\u2212121)=l o g\u23a8parenleftbig\ne\u2212120(e0+e\u22121)\u23a8parenrightbig\n= log(e0+e\u22121)\u2212120 (3.73)\nIn general, we have\nlog\u23a8summationdisplay\ncebc=l o g\u23a8bracketleftBigg\n(\u23a8summationdisplay\ncebc\u2212B)eB\u23a8bracketrightBigg\n=\u23a8bracketleftBigg\nlog(\u23a8summationdisplay\ncebc\u2212B)\u23a8bracketrightBigg\n+B (3.74)\nwhereB=m a x cbc. This is called the log-sum-exp trick, and is widely used. (See the function\nlogsumexp for an implementation.)\nThis trick is used in Algorithm 1 which gives pseudo-code for using an NBC to compute\np(yi|xi,\u02c6\u03b8). See naiveBayesPredict for the Matlab code. Note that we do not need the\nlog-sum-exp trick if we only want to compute \u02c6yi, since we can just maximize the unnormalized\nquantitylogp(yi=c)+logp(xi|y=c).\n3.5.4 Feature selection using mutual information\nSince an NBC is \ufb01tting a joint distribution over potentially many features, it can suffer fromover\ufb01tting. In addition, the run-time cost is O(D), which may be too high for some applications.\nOne common approach to tackling both of these problems is to perform feature selection,t o\nremove \u201cirrelevant\u201d features that do not help much with the classi\ufb01cation problem. The simplestapproach to feature selection is to evaluate the relevance of each feature separately, and then", "117": "3.5. Naive Bayes classi\ufb01ers 87\nAlgorithm 3.2: Predicting with a naive bayes classi\ufb01er for binary features\n1fori=1:Ndo\n2forc=1:Cdo\n3 Lic= log\u02c6\u03c0c;\n4 forj=1:Ddo\n5 ifxij=1thenLic:=Lic+log\u02c6\u03b8jcelseLic:=Lic+log(1\u2212 \u02c6\u03b8jc)\n6pic=e x p (Lic\u2212logsumexp( Li,:));\n7\u02c6yi=a r g m a xcpic;\ntake the top K,w h e r eKis chosen based on some tradeoff between accuracy and complexity.\nThis approach is known as variable ranking, \ufb01ltering,o rscreening.\nOne way to measure relevance is to use mutual information (Section 2.8.3) between feature\nXjand the class label Y:\nI(X,Y)=\u23a8summationdisplay\nxj\u23a8summationdisplay\nyp(xj,y)logp(xj,y)\np(xj)p(y)(3.75)\nThe mutual information can be thought of as the reduction in entropy on the label distribution\nonce we observe the value of feature j. If the features are binary, it is easy to show (Exercise 3.21)\nthat the MI can be computed as follows\nIj=\u23a8summationdisplay\nc\u23a8bracketleftbigg\n\u03b8jc\u03c0clog\u03b8jc\n\u03b8j+(1\u2212\u03b8jc)\u03c0clog1\u2212\u03b8jc\n1\u2212\u03b8j\u23a8bracketrightbigg\n(3.76)\nwhere\u03c0c=p(y=c),\u03b8jc=p(xj=1|y=c), and\u03b8j=p(xj=1 )=\u23a8summationtext\nc\u03c0c\u03b8jc. (All of these\nquantities can be computed as a by-product of \ufb01tting a naive Bayes classi\ufb01er.)\nFigure 3.1 illustrates what happens if we apply this to the binary bag of words dataset used in\nFigure 3.8. We see that the words with highest mutual information are much more discriminativethan the words which are most probable. For example, the most probable word in both classesis \u201csubject\u201d, which always occurs because this is newsgroup data, which always has a subjectline. But obviously this is not very discriminative. The words with highest MI with the classlabel are (in decreasing order) \u201cwindows\u201d, \u201cmicrosoft\u201d, \u201cDOS\u201d and \u201cmotif\u201d, which makes sense,since the classes correspond to Microsoft Windows and X Windows.\n3.5.5 Classifying documents using bag of words\nDocument classi\ufb01cation is the problem of classifying text documents into different categories.\nOne simple approach is to represent each document as a binary vector, which records whethereach word is present or not, so x\nij=1iff wordjoccurs in document i, otherwise xij=0.\nWe can then use the following class conditional density:\np(xi|yi=c,\u03b8)=D\u23a8productdisplay\nj=1Ber(xij|\u03b8jc)=D\u23a8productdisplay\nj=1\u03b8I(xij)\njc(1\u2212\u03b8jc)I(1\u2212xij)(3.77)", "118": "88 Chapter 3. Generative models for discrete data\nclass 1 prob class 2 prob highest MI MI\nsubject 0.998 subject 0.998 windows 0.215\nthis 0.628 windows 0.639 microsoft 0.095\nwith 0.535 this 0.540 dos 0.092\nbut 0.471 with 0.538 motif 0.078\nyou 0.431 but 0.518 window 0.067\nTable 3.1 We list the 5 most likely words for class 1 (X windows) and class 2 (MS windows). We also show\nthe 5 words with highest mutual information with class label. Produced by naiveBayesBowDemo\nThis is called the Bernoulli product model,o rt h ebinary independence model.\nHowever, ignoring the number of times each word occurs in a document loses some in-\nformation (McCallum and Nigam 1998). A more accurate representation counts the number\nof occurrences of each word. Speci\ufb01cally, let xibe a vector of counts for document i,s o\nxij\u2208{0,1,...,N i},w h e r eNiis the number of terms in document i(so\u23a8summationtextD\nj=1xij=Ni). For\nthe class conditional densities, we can use a multinomial distribution:\np(xi|yi=c,\u03b8)=M u (xi|Ni,\u03b8c)=Ni!\u23a8producttextDj=1xij!D\u23a8productdisplay\nj=1\u03b8xij\njc (3.78)\nwhere we have implicitly assumed that the document length Niis independent of the class.\nHere\u03b8jcis the probability of generating word jin documents of class c; these parameters satisfy\nthe constraint that\u23a8summationtextDj=1\u03b8jc=1for each class c.3\nAlthough the multinomial classi\ufb01er is easy to train and easy to use at test time, it does not\nwork particularly well for document classi\ufb01cation. One reason for this is that it does not take\ninto account the burstiness of word usage. This refers to the phenomenon that most words\nnever appear in any given document, but if they do appear once, they are likely to appear morethan once, i.e., words occur in bursts.\nThe multinomial model cannot capture the burstiness phenomenon. To see why, note that\nEquation 3.78 has the form \u03b8Nij\njc, and since \u03b8jc/lessmuch1for rare words, it becomes increasingly\nunlikely to generate many of them. For more frequent words, the decay rate is not as fast. Tosee why intuitively, note that the most frequent words are function words which are not speci\ufb01cto the class, such as \u201cand\u201d, \u201cthe\u201d, and \u201cbut\u201d; the chance of the word \u201cand\u201d occuring is prettymuch the same no matter how many time it has previously occurred (modulo document length),so the independence assumption is more reasonable for common words. However, since rarewords are the ones that matter most for classi\ufb01cation purposes, these are the ones we want tomodel the most carefully.\nVarious ad hoc heuristics have been proposed to improve the performance of the multinomial\ndocument classi\ufb01er (Rennie et al. 2003). We now present an alternative class conditional densitythat performs as well as these ad hoc methods, yet is probabilistically sound (Madsen et al.2005).\n3. Since Equation 3.78 models each word independently, this model is often called a naive Bayes classi\ufb01er, although\ntechnically the features xijare not independent, because of the constraint/summationtext\njxij=Ni.", "119": "3.5. Naive Bayes classi\ufb01ers 89\nSuppose we simply replace the multinomial class conditional density with the Dirichlet\nCompound Multinomial orDCMdensity, de\ufb01ned as follows:\np(xi|yi=c,\u03b1)=\u23a8integraldisplay\nMu(xi|Ni,\u03b8c)Dir(\u03b8 c|\u03b1c)d\u03b8c=Ni!\u23a8producttextD\nj=1xij!B(xi+\u03b1c)\nB(\u03b1c)(3.79)\n(This equation is derived in Equation 5.24.) Surprisingly this simple change is all that is needed\nto capture the burstiness phenomenon. The intuitive reason for this is as follows: After seeingone occurence of a word, say word j, the posterior counts on \u03b8\njgets updated, making another\noccurence of word jmore likely. By contrast, if \u03b8jis \ufb01xed, then the occurences of each word are\nindependent. The multinomial model corresponds to drawing a ball from an urn with Kcolors\nof ball, recording its color, and then replacing it. By contrast, the DCM model corresponds todrawing a ball, recording its color, and then replacing it with one additional copy; this is calledthePolya urn.\nUsing the DCM as the class conditional density gives much better results than using the\nmultinomial, and has performance comparable to state of the art methods, as described in(Madsen et al. 2005). The only disadvantage is that \ufb01tting the DCM model is more complex; see(Minka 2000e; Elkan 2006) for the details.\nExercises\nExercise 3.1 MLE for the Bernoulli/ binomial model\nDerive Equation 3.22 by optimizing the log of the likelihood in Equation 3.11.\nExercise 3.2 Marginal likelihood for the Beta-Bernoulli model\nIn Equation 5.23, we showed that the marginal likelihood is the ratio of the normalizing constants:\np(D)=Z(\u03b11+N1,\u03b10+N0)\nZ(\u03b11,\u03b10)=\u0393(\u03b11+N1)\u0393(\u03b10+N0)\n\u0393(\u03b11+\u03b10+N)\u0393(\u03b11+\u03b10)\n\u0393(\u03b11)\u0393(\u03b10)(3.80)\nWe will now derive an alternative derivation of this fact. By the chain rule of probability,\np(x1:N)=p(x1)p(x2|x1)p(x3|x1:2)... (3.81)\nIn Section 3.3.4, we showed that the posterior predictive distribution is\np(X=k|D1:N)=Nk+\u03b1k/summationtext\niNi+\u03b1i/definesNk+\u03b1k\nN+\u03b1(3.82)\nwherek\u2208{0,1}andD1:Nis the data seen so far. Now suppose D=H,T,T,H,H orD=1,0,0,1,1.\nThen\np(D)=\u03b11\n\u03b1\u00b7\u03b10\n\u03b1+1\u00b7\u03b10+1\n\u03b1+2\u00b7\u03b11+1\n\u03b1+3\u00b7\u03b11+2\n\u03b1+4(3.83)\n=[\u03b11(\u03b11+1)(\u03b11+2)][\u03b10(\u03b10+1)]\n\u03b1(\u03b1+1)\u00b7\u00b7\u00b7(\u03b1+4)(3.84)\n=[(\u03b11)\u00b7\u00b7\u00b7(\u03b11+N1\u22121)][(\u03b1 0)\u00b7\u00b7\u00b7(\u03b10+N0\u22121)]\n(\u03b1)\u00b7\u00b7\u00b7(\u03b1+N\u22121)(3.85)\nShow how this reduces to Equation 3.80 by using the fact that, for integers, (\u03b1\u22121)! = \u0393(\u03b1 ).", "120": "90 Chapter 3. Generative models for discrete data\nExercise 3.3 Posterior predictive for Beta-Binomial model\nRecall from Equation 3.32 that the posterior predictive for the Beta-Binomial is given by\np(x|n,D)=Bb (x|\u03b1/prime\n0,\u03b1/prime\n1,n) (3.86)\n=B(x+\u03b1/prime\n1,n\u2212x+\u03b1/prime\n0)\nB(\u03b1/prime\n1,\u03b1/prime\n0)/parenleftbiggn\nx/parenrightbigg\n(3.87)\nProve that this reduces to\np(\u02dcx=1|D)=\u03b1/prime\n1\n\u03b1/prime\n0+\u03b1/prime\n1(3.88)\nwhenn=1(and hence x\u2208{0,1}). i.e., show that\nBb(1|\u03b1/prime\n1,\u03b1/prime\n0,1) =\u03b1/prime\n1\n\u03b1/prime\n1+\u03b1/prime\n0(3.89)\nHint: use the fact that\n\u0393(\u03b10+\u03b11+1)=(\u03b10+\u03b11+1)\u0393(\u03b10+\u03b11) (3.90)\nExercise 3.4 Beta updating from censored likelihood\n(Source: Gelman.) Suppose we toss a coin n=5times. Let Xbe the number of heads. We observe that\nthere are fewer than 3 heads, but we don\u2019t know exactly how many. Let the prior probability of heads be\np(\u03b8)=B e t a ( \u03b8|1,1). Compute the posterior p(\u03b8|X<3)up to normalization constants, i.e., derive an\nexpression proportional to p(\u03b8,X <3). Hint: the answer is a mixture distribution.\nExercise 3.5 Uninformative prior for log-odds ratio\nLet\n\u03c6= logit(\u03b8 )=l o g\u03b8\n1\u2212\u03b8(3.91)\nShow that if p(\u03c6)\u221d1, thenp(\u03b8)\u221dBeta(\u03b8|0,0). Hint: use the change of variables formula.\nExercise 3.6 MLE for the Poisson distribution\nThe Poisson pmf is de\ufb01ned as Poi(x|\u03bb)=e\u2212\u03bb\u03bbx\nx!,f o rx\u2208{0,1,2,...}where\u03bb>0is the rate\nparameter. Derive the MLE.\nExercise 3.7 Bayesian analysis of the Poisson distribution\nIn Exercise 3.6, we de\ufb01ned the Poisson distribution with rate \u03bband derived its MLE. Here we perform a\nconjugate Bayesian analysis.\na. Derive the posterior p(\u03bb|D)assuming a conjugate prior p(\u03bb)=G a (\u03bb|a,b)\u221d\u03bba\u22121e\u2212\u03bbb. Hint: the\nposterior is also a Gamma distribution.\nb. What does the posterior mean tend to as a\u21920andb\u21920? (Recall that the mean of a Ga(a,b)\ndistribution is a/b.)\nExercise 3.8 MLE for the uniform distribution\n(Source: Kaelbling.) Consider a uniform distribution centered on 0 with width 2a. The density function is\ngiven by\np(x)=1\n2aI(x\u2208[\u2212a,a]) (3.92)", "121": "3.5. Naive Bayes classi\ufb01ers 91\na. Given a data set x1,...,x n, what is the maximum likelihood estimate of a(call it\u02c6a)?\nb. What probability would the model assign to a new data point xn+1using\u02c6a?\nc. Do you see any problem with the above approach? Brie\ufb02y suggest (in words) a better approach.\nExercise 3.9 Bayesian analysis of the uniform distribution\nConsider the uniform distribution Unif(0,\u03b8). The maximum likelihood estimate is \u02c6\u03b8=m a x (D),a sw e\nsaw in Exercise 3.8, but this is unsuitable for predicting future data since it puts zero probability mass\noutside the training data. In this exercise, we will perform a Bayesian analysis of the uniform distribution(following (Minka 2001a)). The conjugate prior is the Pareto distribution, p(\u03b8)=Pareto(\u03b8|b,K), de\ufb01ned in\nSection 2.4.6. Given a Pareto prior, the joint distribution of \u03b8andD=(x\n1,...,x N)is\np(D,\u03b8)=KbK\n\u03b8N+K+1I(\u03b8\u2265max(D)) (3.93)\nLetm=m a x (D). The evidence (the probability that all Nsamples came from the same uniform\ndistribution) is\np(D)=/integraldisplay\u221e\nmKbK\n\u03b8N+K+1d\u03b8 (3.94)\n=/braceleftBiggK\n(N+K)bNifm\u2264b\nKbK\n(N+K)mN+Kifm>b(3.95)\nDerive the posterior p(\u03b8|D), and show that if can be expressed as a Pareto distribution.\nExercise 3.10 Taxicab (tramcar) problem\nSuppose you arrive in a new city and see a taxi numbered 100. How many taxis are there in this city? Letus assume taxis are numbered sequentially as integers starting from 0, up to some unknown upper bound\u03b8. (We number taxis from 0 for simplicity; we can also count from 1 without changing the analysis.) Hence\nthe likelihood function is p(x)=U(0,\u03b8), the uniform distribution. The goal is to estimate \u03b8. We will use\nthe Bayesian analysis from Exercise 3.9.\na. Suppose we see one taxi numbered 100, so D={100},m= 100,N=1. Using an (improper)\nnon-informative prior on \u03b8of the form p(\u03b8)=Pa(\u03b8|0,0)\u221d1/\u03b8, what is the posterior p(\u03b8|D)?\nb. Compute the posterior mean, mode and median number of taxis in the city, if such quantities exist.\nc. Rather than trying to compute a point estimate of the number of taxis, we can compute the predictive\ndensity over the next taxicab number using\np(D\n/prime|D,\u03b1)=/integraldisplay\np(D/prime|\u03b8)p(\u03b8|D,\u03b1)d\u03b8=p(D/prime|\u03b2) (3.96)\nwhere\u03b1=(b,K)are the hyper-parameters, \u03b2=(c,N+K)are the updated hyper-parameters. Now\nconsider the case D={m}, andD/prime={x}. Using Equation 3.95, write down an expression for\np(x|D,\u03b1) (3.97)\nAs above, use a non-informative prior b=K=0.\nd. Use the predictive density formula to compute the probability that the next taxi you will see (say,\nthe next day) has number 100, 50 or 150, i.e., compute p(x= 100|D,\u03b1 ),p(x=5 0|D,\u03b1),p(x=\n150|D,\u03b1 ).\ne. Brie\ufb02y describe (1-2 sentences) some ways we might make the model more accurate at prediction.", "122": "92 Chapter 3. Generative models for discrete data\nExercise 3.11 Bayesian analysis of the exponential distribution\nA lifetime Xof a machine is modeled by an exponential distribution with unknown parameter \u03b8. The\nlikelihood is p(x|\u03b8)=\u03b8e\u2212\u03b8xforx\u22650,\u03b8>0.\na. Show that the MLE is \u02c6\u03b8=1/x,w h e r ex=1\nN/summationtextN\ni=1xi.\nb. Suppose we observe X1=5,X2=6,X3=4(the lifetimes (in years) of 3 different iid machines).\nWhat is the MLE given this data?\nc. Assume that an expert believes \u03b8should have a prior distribution that is also exponential\np(\u03b8) = Expon( \u03b8|\u03bb) (3.98)\nChoose the prior parameter, call it \u02c6\u03bb, such that E[\u03b8]=1/3. Hint: recall that the Gamma distribution\nhas the form\nGa(\u03b8|a,b)\u221d\u03b8a\u22121e\u2212\u03b8b(3.99)\nand its mean is a/b.\nd. What is the posterior, p(\u03b8|D,\u02c6\u03bb)?\ne. Is the exponential prior conjugate to the exponential likelihood?\nf. What is the posterior mean, E/bracketleftBig\n\u03b8|D,\u02c6\u03bb/bracketrightBig\n?\ng. Explain why the MLE and posterior mean differ. Which is more reasonable in this example?\nExercise 3.12 MAP estimation for the Bernoulli with non-conjugate priors\n(Source: Jaakkola.) In the book, we discussed Bayesian inference of a Bernoulli rate parameter with the\npriorp(\u03b8)=B e t a ( \u03b8|\u03b1,\u03b2). We know that, with this prior, the MAP estimate is given by\n\u02c6\u03b8=N1+\u03b1\u22121\nN+\u03b1+\u03b2\u22122(3.100)\nwhereN1is the number of heads, N0is the number of tails, and N=N0+N1is the total number of\ntrials.\na. Now consider the following prior, that believes the coin is fair, or is slightly biased towards tails:\np(\u03b8)=\u23a7\n\u23a8\n\u23a90.5if\u03b8=0.5\n0.5if\u03b8=0.4\n0otherwise(3.101)\nDerive the MAP estimate under this prior as a function of N1andN.\nb. Suppose the true parameter is \u03b8=0.41. Which prior leads to a better estimate when Nis small?\nWhich prior leads to a better estimate when Nis large?\nExercise 3.13 Posterior predictive distribution for a batch of data with the dirichlet-multinomial model\nIn Equation 3.51, we gave the the posterior predictive distribution for a single multinomial trial using a\ndirichlet prior. Now consider predicting a batchof new data, \u02dcD=(X1,...,X m), consisting of msingle\nmultinomial trials (think of predicting the next mwords in a sentence, assuming they are drawn iid).\nDerive an expression for\np(\u02dcD|D,\u03b1) (3.102)", "123": "3.5. Naive Bayes classi\ufb01ers 93\nYour answer should be a function of \u03b1, and the old and new counts (sufficient statistics), de\ufb01ned as\nNold\nk=/summationdisplay\ni\u2208DI(xi=k) (3.103)\nNnew\nk=/summationdisplay\ni\u2208\u02dcDI(xi=k) (3.104)\nHint: recall that, for a vector of counts, N1:K, the marginal likelihood (evidence) is given by\np(D|\u03b1)=\u0393(\u03b1)\n\u0393(N+\u03b1)/productdisplay\nk\u0393(Nk+\u03b1k)\n\u0393(\u03b1k)(3.105)\nwhere\u03b1=/summationtext\nk\u03b1kandN=/summationtext\nkNk.\nExercise 3.14 Posterior predictive for Dirichlet-multinomial\n(Source: Koller.).\na. Suppose we compute the empirical distribution over letters of the Roman alphabet plus the space\ncharacter (a distribution over 27 values) from 2000 samples. Suppose we see the letter \u201ce\u201d 260 times.\nWhat isp(x2001=e|D), if we assume \u03b8\u223cDir(\u03b11,...,\u03b1 27),w h e r e\u03b1 k=1 0for allk?\nb. Suppose, in the 2000 samples, we saw \u201ce\u201d 260 times, \u201ca\u201d 100 times, and \u201cp\u201d 87 times. What is\np(x2001=p,x2002=a|D), if we assume \u03b8\u223cDir(\u03b11,...,\u03b1 27),w h e r e\u03b1k=1 0for allk? Show\nyour work.\nExercise 3.15 Setting the beta hyper-parameters\nSuppose\u03b8\u223c\u03b2(\u03b11,\u03b12)and we believe that E[\u03b8]=mandvar[\u03b8]=v. Using Equation 2.62, solve for\n\u03b11and\u03b12in terms of mandv. What values do you get if m=0.7andv=0.22?\nExercise 3.16 Setting the beta hyper-parameters II\n(Source: Draper.) Suppose \u03b8\u223c\u03b2(\u03b11,\u03b12)and we believe that E[\u03b8]=mandp(/lscript<\u03b8<u )=0.95.\nWrite a program that can solve for \u03b11and\u03b12in terms of m,/lscriptandu. Hint: write \u03b12as a function of \u03b11\nandm, so the pdf only has one unknown; then write down the probability mass contained in the interval\nas an integral, and minimize its squared discrepancy from 0.95. What values do you get if m=0.15,\n/lscript=0.05andu=0.3? What is the equivalent sample size of this prior?\nExercise 3.17 Marginal likelihood for beta-binomial under uniform prior\nSuppose we toss a coin Ntimes and observe N1heads. Let N1\u223cBin(N,\u03b8)and\u03b8\u223cBeta(1,1). Show\nthat the marginal likelihood is p(N1|N)=1/(N+1). Hint:\u0393(x+1)=x!ifxis an integer.\nExercise 3.18 Bayes factor for coin tossing\nSuppose we toss a coin N=1 0times and observe N1=9heads. Let the null hypothesis be that the\ncoin is fair, and the alternative be that the coin can have any bias, so p(\u03b8) = Unif(0, 1). Derive the\nBayes factor BF1,0in favor of the biased coin hypothesis. What if N= 100andN1=9 0? Hint: see\nExercise 3.17.\nExercise 3.19 Irrelevant features with naive Bayes\n(Source: Jaakkola.) Let xiw=1if wordwoccurs in document iandxiw=0otherwise. Let \u03b8cwbe the\nestimated probability that word woccurs in documents of class c. Then the log-likelihood that document", "124": "94 Chapter 3. Generative models for discrete data\nxbelongs to class cis\nlogp(xi|c,\u03b8)=l o gW/productdisplay\nw=1\u03b8xiwcw(1\u2212\u03b8cw)1\u2212xiw(3.106)\n=W/summationdisplay\nw=1xiwlog\u03b8cw+(1\u2212xiw)log(1\u2212\u03b8cw) (3.107)\n=W/summationdisplay\nw=1xiwlog\u03b8cw\n1\u2212\u03b8cw+/summationdisplay\nwlog(1\u2212\u03b8cw) (3.108)\nwhereWis the number of words in the vocabulary. We can write this more succintly as\nlogp(xi|c,\u03b8)=\u03c6 (xi)T\u03b2c (3.109)\nwherexi=(xi1,...,x iW)is a bit vector, \u03c6(xi)=(xi,1), and\n\u03b2c=( l o g\u03b8c1\n1\u2212\u03b8c1,...,log\u03b8cW\n1\u2212\u03b8cW,/summationdisplay\nwlog(1\u2212\u03b8cw))T(3.110)\nWe see that this is a linear classi\ufb01er, since the class-conditional density is a linear function (an inner\nproduct) of the parameters \u03b2c.\na. Assuming p(C=1 )=p (C=2 )=0 .5, write down an expression for the log posterior odds ratio,\nlog2p(c=1| xi)\np(c=2| xi), in terms of the features \u03c6(xi)and the parameters \u03b21and\u03b22.\nb. Intuitively, words that occur in both classes are not very \u201cdiscriminative\u201d, and therefore should not\naffect our beliefs about the class label. Consider a particular word w. State the conditions on \u03b81,wand\n\u03b82,w(or equivalently the conditions on \u03b21,w,\u03b22,w) under which the presence or absence of win a\ntest document will have no effect on the class posterior (such a word will be ignored by the classi\ufb01er).Hint: using your previous result, \ufb01gure out when the posterior odds ratio is 0.5/0.5.\nc. The posterior mean estimate of \u03b8, using a Beta(1,1) prior, is given by\n\u02c6\u03b8\ncw=1+/summationtext\ni\u2208cxiw\n2+nc(3.111)\nwhere the sum is over the ncdocuments in class c. Consider a particular word w, and suppose it\nalways occurs in every document (regardless of class). Let there be n1documents of class 1 and n2be\nthe number of documents in class 2, where n1/negationslash=n2(since e.g., we get much more non-spam than\nspam; this is an example of class imbalance). If we use the above estimate for \u03b8cw, will word wbe\nignored by our classi\ufb01er? Explain why or why not.\nd. What other ways can you think of which encourage \u201cirrelevant\u201d words to be ignored?\nExercise 3.20 Class conditional densities for binary data\nConsider a generative classi\ufb01er for Cclasses with class conditional density p(x|y)and uniform class prior\np(y). Suppose all the Dfeatures are binary, xj\u2208{0,1}. If we assume all the features are conditionally\nindependent (the naive Bayes assumption), we can write\np(x|y=c)=D/productdisplay\nj=1Ber(xj|\u03b8jc) (3.112)\nThis requires DCparameters.", "125": "3.5. Naive Bayes classi\ufb01ers 95\na. Now consider a different model, which we will call the \u201cfull\u201d model, in which all the features are fully\ndependent (i.e., we make no factorization assumptions). How might we represent p(x|y=c)in this\ncase? How many parameters are needed to represent p(x|y=c)?\nb. Assume the number of features Dis \ufb01xed. Let there be Ntraining cases. If the sample size Nis very\nsmall, which model (naive Bayes or full) is likely to give lower test set error, and why?\nc. If the sample size Nis very large, which model (naive Bayes or full) is likely to give lower test set error,\nand why?\nd. What is the computational complexity of \ufb01tting the full and naive Bayes models as a function of N\nandD? Use big-Oh notation. (Fitting the model here means computing the MLE or MAP parameter\nestimates. You may assume you can convert a D-bit vector to an array index in O(D)time.)\ne. What is the computational complexity of applying the full and naive Bayes models at test time to a\nsingle test case?\nf. Suppose the test case has missing data. Let xvbe the visible features of size v, andxhbe the hidden\n(missing) features of size h,w h e r e v+h=D. What is the computational complexity of computing\np(y|xv,\u02c6\u03b8)for the full and naive Bayes models, as a function of vandh?\nExercise 3.21 Mutual information for naive Bayes classi\ufb01ers with binary features\nDerive Equation 3.76.\nExercise 3.22 Fitting a naive bayes spam \ufb01lter by hand\n(Source: Daphne Koller.). Consider a Naive Bayes model (multivariate Bernoulli version) for spam classi\ufb01ca-\ntion with the vocabulary V=\"secret\", \"offer\", \"low\", \"price\", \"valued\", \"customer\", \"today\", \"dollar\", \"million\",\"sports\", \"is\", \"for\", \"play\", \"healthy\", \"pizza\". We have the following example spam messages \"million dollaroffer\", \"secret offer today\", \"secret is secret\" and normal messages, \"low price for valued customer\", \"playsecret sports today\", \"sports is healthy\", \"low price pizza\". Give the MLEs for the following parameters:\u03b8spam,\u03b8secret|spam,\u03b8secret|non-spam,\u03b8sports|non-spam,\u03b8dollar|spam.", "126": "", "127": "4 Gaussian models\n4.1 Introduction\nIn this chapter, we discuss the multivariate Gaussian ormultivariate normal (MVN), which\nis the most widely used joint probability density function for continuous variables. It will form\nthe basis for many of the models we will encounter in later chapters.\nUnfortunately, the level of mathematics in this chapter is higher than in many other chapters.\nIn particular, we rely heavily on linear algebra and matrix calculus. This is the price one mustpay in order to deal with high-dimensional data. Beginners may choose to skip sections markedwith a *. In addition, since there are so many equations in this chapter, we have put a boxaround those that are particularly important.\n4.1.1 Notation\nLet us brie\ufb02y say a few words about notation. We denote vectors by boldface lower case letters,such asx. We denote matrices by boldface upper case letters, such as X. We denote entries in\na matrix by non-bold upper case letters, such as X\nij.\nAll vectors are assumed to be column vectors unless noted otherwise. We use [x1,...,x D]to\ndenote a column vector created by stacking Dscalars. Similarly, if we write x=[x1,...,xD],\nwhere the left hand side is a tall column vector, we mean to stack the xialong the rows; this is\nusually written as x=(xT\n1,...,xT\nD)T, but that is rather ugly. If we write X=[x1,...,xD],\nwhere the left hand side is a matrix, we mean to stack the xialong the columns, creating a\nmatrix.\n4.1.2 Basics\nRecall from Section 2.5.2 that the pdf for an MVN in Ddimensions is de\ufb01ned by the following:\nN(x|\u03bc,\u03a3)/defines1\n(2\u03c0)D/2|\u03a3|1/2exp\u23a8bracketleftbigg\n\u22121\n2(x\u2212\u03bc)T\u03a3\u22121(x\u2212\u03bc)\u23a8bracketrightbigg\n(4.1)", "128": "98 Chapter 4. Gaussian models\n\u03bcu1u2\n\u03bb11/2\n\u03bb21/2\nx1x2\nFigure 4.1 Visualization of a 2 dimensional Gaussian density. The major and minor axes of the ellipse\nare de\ufb01ned by the \ufb01rst two eigenvectors of the covariance matrix, namely u1andu2. Based on Figure 2.7\nof (Bishop 2006a).\nThe expression inside the exponent is the Mahalanobis distance between a data vector x\nand the mean vector \u03bc, We can gain a better understanding of this quantity by performing an\neigendecomposition of\u03a3. That is, we write \u03a3=U\u039bUT,w h e r eU is an orthonormal matrix\nof eigenvectors satsifying UTU=I, and\u039bis a diagonal matrix of eigenvalues.\nUsing the eigendecomposition, we have that\n\u03a3\u22121=U\u2212T\u039b\u22121U\u22121=U\u039b\u22121UT=D\u23a8summationdisplay\ni=11\n\u03bbiuiuT\ni (4.2)\nwhereuiis thei\u2019th column of U, containing the i\u2019th eigenvector. Hence we can rewrite the\nMahalanobis distance as follows:\n(x\u2212\u03bc)T\u03a3\u22121(x\u2212\u03bc)=(x\u2212\u03bc)T\u23a8parenleftBiggD\u23a8summationdisplay\ni=11\n\u03bbiuiuTi\u23a8parenrightBigg\n(x\u2212\u03bc) (4.3)\n=D\u23a8summationdisplay\ni=11\n\u03bbi(x\u2212\u03bc)TuiuTi(x\u2212\u03bc)=D\u23a8summationdisplay\ni=1y2\ni\n\u03bbi(4.4)\nwhereyi/definesuT\ni(x\u2212\u03bc). Recall that the equation for an ellipse in 2d is\ny2\n1\n\u03bb1+y2\n2\n\u03bb2=1 (4.5)\nHence we see that the contours of equal probability density of a Gaussian lie along ellipses.\nThis is illustrated in Figure 4.1. The eigenvectors determine the orientation of the ellipse, andthe eigenvalues determine how elogonated it is.\nIn general, we see that the Mahalanobis distance corresponds to Euclidean distance in a\ntransformed coordinate system, where we shift by \u03bcand rotate by U.", "129": "4.1. Introduction 99\n4.1.3 MLE for an MVN\nWe now describe one way to estimate the parameters of an MVN, using MLE. In later sections,\nwe will discuss Bayesian inference for the parameters, which can mitigate over\ufb01tting, and canprovide a measure of con\ufb01dence in our estimates.\nTheorem 4.1.1 (MLE for a Gaussian). If we have Niid samples x\ni\u223cN(\u03bc,\u03a3), then the MLE for\nthe parameters is given by\n\u02c6\u03bcmle=1\nNN\u23a8summationdisplay\ni=1xi/definesx (4.6)\n\u02c6\u03a3mle=1\nNN\u23a8summationdisplay\ni=1(xi\u2212x)(xi\u2212x)T=1\nN(N\u23a8summationdisplay\ni=1xixT\ni)\u2212xxT(4.7)\nThat is, the MLE is just the empirical mean and empirical covariance. In the univariate case, we\nget the following familiar results:\n\u02c6\u03bc=1\nN\u23a8summationdisplay\nixi=x (4.8)\n\u02c6\u03c32=1\nN\u23a8summationdisplay\ni(xi\u2212x)2=\u23a8parenleftBigg\n1\nN\u23a8summationdisplay\nix2\ni\u23a8parenrightBigg\n\u2212(x)2(4.9)\n4.1.3.1 Proof *\nTo prove this result, we will need several results from matrix algebra, which we summarize\nbelow. In the equations, aandbare vectors, and AandBare matrices. Also, the notation\ntr(A)refers to the traceof a matrix, which is the sum of its diagonals: tr(A)=\u23a8summationtext\niAii.\n\u2202(bTa)\n\u2202a=b\n\u2202(aTAa)\n\u2202a=(A+AT)a\n\u2202\n\u2202Atr(BA)=BT\n\u2202\n\u2202Alog|A|=A\u2212T/defines(A\u22121)T\ntr(ABC)=t r ( CAB)=t r (BCA)(4.10)\nThe last equation is called the cyclic permutation property of the trace operator. Using this,\nwe can derive the widely used trace trick, which reorders the scalar inner product xTAxas\nfollows\nxTAx=t r (xTAx)=t r ( xxTA)=t r ( AxxT) (4.11)", "130": "100 Chapter 4. Gaussian models\nProof.We can now begin with the proof. The log-likelihood is\n/lscript(\u03bc,\u03a3)=l o g p(D|\u03bc,\u03a3)=N\n2log|\u039b|\u22121\n2N\u23a8summationdisplay\ni=1(xi\u2212\u03bc)T\u039b(xi\u2212\u03bc) (4.12)\nwhere\u039b=\u03a3\u22121is the precision matrix.\nUsing the substitution yi=xi\u2212\u03bcand the chain rule of calculus, we have\n\u2202\n\u2202\u03bc(xi\u2212\u03bc)T\u03a3\u22121(xi\u2212\u03bc)=\u2202\n\u2202yiyT\ni\u03a3\u22121yi\u2202yi\n\u2202\u03bc(4.13)\n=\u22121(\u03a3\u22121+\u03a3\u2212T)yi (4.14)\nHence\n\u2202\n\u2202\u03bc/lscript(\u03bc,\u03a3)=\u22121\n2N\u23a8summationdisplay\ni=1\u22122\u03a3\u22121(xi\u2212\u03bc)=\u03a3\u22121N\u23a8summationdisplay\ni=1(xi\u2212\u03bc)=0 (4.15)\n\u02c6\u03bc=1\nNN\u23a8summationdisplay\ni=1xi=x (4.16)\nSo the MLE of \u03bcis just the empirical mean.\nNow we can use the trace-trick to rewrite the log-likelihood for \u039bas follows:\n/lscript(\u039b)=N\n2log|\u039b|\u22121\n2\u23a8summationdisplay\nitr[(xi\u2212\u03bc)(xi\u2212\u03bc)T\u039b] (4.17)\n=N\n2log|\u039b|\u22121\n2tr[S\u03bc\u039b] (4.18)\n(4.19)\nwhere\nS\u03bc/definesN\u23a8summationdisplay\ni=1(xi\u2212\u03bc)(xi\u2212\u03bc)T(4.20)\nis the scatter matrix centered on \u03bc. Taking derivatives of this expression with respect to \u039b\nyields\n\u2202/lscript(\u039b)\n\u2202\u039b=N\n2\u039b\u2212T\u22121\n2ST\n\u03bc=0 (4.21)\n\u039b\u2212T=\u039b\u22121=\u03a3=1\nNS\u03bc (4.22)\nso\n\u02c6\u03a3=1\nNN\u23a8summationdisplay\ni=1(xi\u2212\u03bc)(xi\u2212\u03bc)T(4.23)\nwhich is just the empirical covariance matrix centered on \u03bc. If we plug-in the MLE \u03bc=x\n(since both parameters must be simultaneously optimized), we get the standard equation for the\nMLE of a covariance matrix.", "131": "4.2. Gaussian discriminant analysis 101\n4.1.4 Maximum entropy derivation of the Gaussian *\nIn this section, we show that the multivariate Gaussian is the distribution with maximum entropy\nsubject to having a speci\ufb01ed mean and covariance (see also Section 9.2.6). This is one reason theGaussian is so widely used: the \ufb01rst two moments are usually all that we can reliably estimatefrom data, so we want a distribution that captures these properties, but otherwise makes as fewaddtional assumptions as possible.\nTo simplify notation, we will assume the mean is zero. The pdf has the form\np(x)=1\nZexp(\u22121\n2xT\u03a3\u22121x) (4.24)\nIf we de\ufb01ne fij(x)=xixjand\u03bbij=1\n2(\u03a3\u22121)ij,f o ri,j\u2208{1,...,D}, we see that this is in\nthe same form as Equation 9.74. The (differential) entropy of this distribution (using log base e)\nis given by\nh(N(\u03bc,\u03a3)) =1\n2ln\u23a8bracketleftbig\n(2\u03c0e)D|\u03a3|\u23a8bracketrightbig\n(4.25)\nWe now show the MVN has maximum entropy amongst all distributions with a speci\ufb01ed co-variance\u03a3.\nTheorem 4.1.2. Letq(x)be any density satisfying\u23a8integraltext\nq(x)x\nixj=\u03a3ij. Letp=N(0,\u03a3). Then\nh(q)\u2264h(p).\nProof.(From (Cover and Thomas 1991, p234).) We have\n0\u2264KL(q||p)=\u23a8integraldisplay\nq(x)logq(x)\np(x)dx (4.26)\n=\u2212h(q)\u2212\u23a8integraldisplay\nq(x)logp(x)dx (4.27)\n=\u2217\u2212h(q)\u2212\u23a8integraldisplay\np(x)logp(x)dx (4.28)\n=\u2212h(q)+h(p) (4.29)\nwhere the key step in Equation 4.28 (marked with a *) follows since qandpyield the same\nmoments for the quadratic form encoded by logp(x).\n4.2 Gaussian discriminant analysis\nOne important application of MVNs is to de\ufb01ne the the class conditional densities in a generativeclassi\ufb01er, i.e.,\np(x|y=c,\u03b8)=N(x|\u03bc\nc,\u03a3c) (4.30)\nThe resulting technique is called (Gaussian) discriminant analysis or GDA (even though it is a\ngenerative, not discriminative, classi\ufb01er \u2014 see Section 8.6 for more on this distinction). If \u03a3cis\ndiagonal, this is equivalent to naive Bayes.", "132": "102 Chapter 4. Gaussian models\n55 60 65 70 75 8080100120140160180200220240260280\nheightweightred = female, blue=male\n(a)55 60 65 70 75 8080100120140160180200220240260280\nheightweightred = female, blue=male\n(b)\nFigure 4.2 (a) Height/weight data. (b) Visualization of 2d Gaussians \ufb01t to each class. 95% of the probability\nmass is inside the ellipse. Figure generated by gaussHeightWeight .\nWe can classify a feature vector using the following decision rule, derived from Equation 2.13:\n\u02c6y(x) = argmax\nc[logp(y=c|\u03c0)+logp(x|\u03b8c)] (4.31)\nWhen we compute the probability of xunder each class conditional density, we are measuring\nthe distance from xto the center of each class, \u03bcc, using Mahalanobis distance. This can be\nthought of as a nearest centroids classi\ufb01er.\nAs an example, Figure 4.2 shows two Gaussian class-conditional densities in 2d, representing\nthe height and weight of men and women. We can see that the features are correlated, as is\nto be expected (tall people tend to weigh more). The ellipses for each class contain 95% of theprobability mass. If we have a uniform prior over classes, we can classify a new test vector asfollows:\n\u02c6y(x) = argmin\nc(x\u2212\u03bcc)T\u03a3\u22121\nc(x\u2212\u03bcc) (4.32)\n4.2.1 Quadratic discriminant analysis (QDA)\nThe posterior over class labels is given by Equation 2.13. We can gain further insight into thismodel by plugging in the de\ufb01nition of the Gaussian density, as follows:\np(y=c|x,\u03b8)=\u03c0\nc|2\u03c0\u03a3c|\u22121\n2exp\u23a8bracketleftbig\n\u22121\n2(x\u2212\u03bcc)T\u03a3\u22121\nc(x\u2212\u03bcc)\u23a8bracketrightbig\n\u23a8summationtext\nc/prime\u03c0c/prime|2\u03c0\u03a3c/prime|\u22121\n2exp\u23a8bracketleftbig\n\u22121\n2(x\u2212\u03bcc/prime)T\u03a3\u22121\nc/prime(x\u2212\u03bcc/prime)\u23a8bracketrightbig (4.33)\nThresholding this results in a quadratic function of x. The result is known as quadratic\ndiscriminant analysis (QDA). Figure 4.3 gives some examples of what the decision boundaries\nlook like in 2D.", "133": "4.2. Gaussian discriminant analysis 103\n\u22122 0 2\u2212202Parabolic Boundary\n(a)\u22122 0 2 4 6\u2212202468Some Linear, Some Quadratic\n(b)\nFigure 4.3 Quadratic decision boundaries in 2D for the 2 and 3 class case. Figure generated by\ndiscrimAnalysisDboundariesDemo .\n1 2 300.10.20.30.4T=100\n1 2 300.51T=1\n1 2 300.51T=0.1\n1 2 300.51T=0.01\nFigure 4.4 Softmax distribution S(\u03b7/T),w h e r e\u03b7=( 3,0,1), at different temperatures T. When the\ntemperature is high (left), the distribution is uniform, whereas when the temperature is low (right), the\ndistribution is \u201cspiky\u201d, with all its mass on the largest element. Figure generated by softmaxDemo2 .\n4.2.2 Linear discriminant analysis (LDA)\nWe now consider a special case in which the covariance matrices are tiedorsharedacross\nclasses,\u03a3c=\u03a3. In this case, we can simplify Equation 4.33 as follows:\np(y=c|x,\u03b8)\u221d\u03c0cexp\u23a8bracketleftbigg\n\u03bcT\nc\u03a3\u22121x\u22121\n2xT\u03a3\u22121x\u22121\n2\u03bcTc\u03a3\u22121\u03bcc\u23a8bracketrightbigg\n(4.34)\n= exp\u23a8bracketleftbigg\n\u03bcTc\u03a3\u22121x\u22121\n2\u03bcTc\u03a3\u22121\u03bcc+log\u03c0c\u23a8bracketrightbigg\nexp[\u22121\n2xT\u03a3\u22121x](4.35)\nSince the quadratic term xT\u03a3\u22121xis independent of c, it will cancel out in the numerator and\ndenominator. If we de\ufb01ne\n\u03b3c=\u22121\n2\u03bcTc\u03a3\u22121\u03bcc+log\u03c0c (4.36)\n\u03b2c=\u03a3\u22121\u03bcc (4.37)", "134": "104 Chapter 4. Gaussian models\nthen we can write\np(y=c|x,\u03b8)=e\u03b2T\ncx+\u03b3c\n\u23a8summationtext\nc/primee\u03b2T\nc/primex+\u03b3c/prime=S(\u03b7)c (4.38)\nwhere\u03b7=[\u03b2T\n1x+\u03b31,...,\u03b2TCx+\u03b3C], andSis thesoftmax function, de\ufb01ned as follows:\nS(\u03b7)c=e\u03b7c\n\u23a8summationtextC\nc/prime=1e\u03b7c/prime(4.39)\nThe softmax function is so-called since it acts a bit like the max function. To see this, let us\ndivide each \u03b7cby a constant Tcalled the temperature. Then as T\u21920,w e\ufb01 n d\nS(\u03b7/T)c=\u23a8braceleftbigg1.0ifc=a r g m a xc/prime\u03b7c/prime\n0.0otherwise(4.40)\nIn other words, at low temperatures, the distribution spends essentially all of its time in themost probable state, whereas at high temperatures, it visits all states uniformly. See Figure 4.4for an illustration. Note that this terminology comes from the area of statistical physics, whereit is common to use the Boltzmann distribution, which has the same form as the softmax\nfunction.\nAn interesting property of Equation 4.38 is that, if we take logs, we end up with a linear\nfunction of x. (The reason it is linear is because the x\nT\u03a3\u22121xcancels from the numerator\nand denominator.) Thus the decision boundary between any two classes, say candc/prime, will be\na straight line. Hence this technique is called linear discriminant analysis orLDA.1We can\nderive the form of this line as follows:\np(y=c|x,\u03b8)=p( y=c/prime|x,\u03b8) (4.41)\n\u03b2T\ncx+\u03b3c=\u03b2Tc/primex+\u03b3c/prime (4.42)\nxT(\u03b2c/prime\u2212\u03b2)=\u03b3 c/prime\u2212\u03b3c (4.43)\nSee Figure 4.5 for some examples.\nAn alternative to \ufb01tting an LDA model and then deriving the class posterior is to directly\n\ufb01tp(y|x,W)=C a t ( y|Wx)for some C\u00d7Dweight matrix W. This is called multi-class\nlogistic regression ,o rmultinomial logistic regression.2We will discuss this model in detail\nin Section 8.2. The difference between the two approaches is explained in Section 8.6.\n4.2.3 Two-class LDA\nTo gain further insight into the meaning of these equations, let us consider the binary case. In\nthis case, the posterior is given by\np(y=1|x,\u03b8)=e\u03b2T\n1x+\u03b31\ne\u03b2T\n1x+\u03b31+e\u03b2T0x+\u03b30(4.44)\n=1\n1+e(\u03b20\u2212\u03b21)Tx+(\u03b3 0\u2212\u03b31)=s i g m\u23a8parenleftbig\n(\u03b21\u2212\u03b20)Tx+(\u03b31\u2212\u03b30)\u23a8parenrightbig\n(4.45)\n1. The abbreviation \u201cLDA\u201d, could either stand for \u201clinear discriminant analysis\u201d or \u201clatent Dirichlet allocation\u201d (Sec-\ntion 27.3). We hope the meaning is clear from text.\n2. In the language modeling community, this model is called a maximum entropy model, for reasons explained in\nSection 9.2.6.", "135": "4.2. Gaussian discriminant analysis 105\n\u22122 0 2\u2212202Linear Boundary\n(a)\u22122 0 2 4 6\u221220246All Linear Boundaries \n(b)\nFigure 4.5 Linear decision boundaries in 2D for the 2 and 3 class case. Figure generated by\ndiscrimAnalysisDboundariesDemo .\nFigure 4.6 Geometry of LDA in the 2 class case where \u03a31=\u03a32=I.\nwheresigm(\u03b7)refers to the sigmoid function (Equation 1.10).\nNow\n\u03b31\u2212\u03b30=\u22121\n2\u03bcT\n1\u03a3\u22121\u03bc1+1\n2\u03bcT\n0\u03a3\u22121\u03bc0+log(\u03c01/\u03c00) (4.46)\n=\u22121\n2(\u03bc1\u2212\u03bc0)T\u03a3\u22121(\u03bc1+\u03bc0)+log(\u03c01/\u03c00) (4.47)\nSo if we de\ufb01ne\nw=\u03b21\u2212\u03b20=\u03a3\u22121(\u03bc1\u2212\u03bc0) (4.48)\nx0=1\n2(\u03bc1+\u03bc0)\u2212(\u03bc1\u2212\u03bc0)log(\u03c01/\u03c00)\n(\u03bc1\u2212\u03bc0)T\u03a3\u22121(\u03bc1\u2212\u03bc0)(4.49)", "136": "106 Chapter 4. Gaussian models\nthen we have wTx0=\u2212(\u03b31\u2212\u03b30), and hence\np(y=1|x,\u03b8) = sigm(wT(x\u2212x0)) (4.50)\n(This is closely related to logistic regression, which we will discuss in Section 8.2.) So the \ufb01nal\ndecision rule is as follows: shift xbyx0, project onto the line w, and see if the result is positive\nor negative.\nIf\u03a3=\u03c32I, thenwis in the direction of \u03bc1\u2212\u03bc0. So we classify the point based on whether\nits projection is closer to \u03bc0or\u03bc1. This is illustrated in Figure 4.6. Furthemore, if \u03c01=\u03c00, then\nx0=1\n2(\u03bc1+\u03bc0), which is half way between the means. If we make \u03c01>\u03c00, thenx0gets\ncloser to\u03bc0, so more of the line belongs to class 1 a priori. Conversely if \u03c01<\u03c00, the boundary\nshifts right. Thus we see that the class prior, \u03c0c, just changes the decision threshold, and not\nthe overall geometry, as we claimed above. (A similar argument applies in the multi-class case.)\nThe magnitude of wdetermines the steepness of the logistic function, and depends on\nhow well-separated the means are, relative to the variance. In psychology and signal detectiontheory, it is common to de\ufb01ne the discriminability of a signal from the background noise using\na quantity called d-prime:\nd\n/prime/defines\u03bc1\u2212\u03bc0\n\u03c3(4.51)\nwhere\u03bc1is the mean of the signal and \u03bc0is the mean of the noise, and \u03c3is the standard\ndeviation of the noise. If d/primeis large, the signal will be easier to discriminate from the noise.\n4.2.4 MLE for discriminant analysis\nWe now discuss how to \ufb01t a discriminant analysis model. The simplest way is to use maximumlikelihood. The log-likelihood function is as follows:\nlogp(D|\u03b8)=\u23a8bracketleftBigg\nN\u23a8summationdisplay\ni=1C\u23a8summationdisplay\nc=1I(yi=c)log\u03c0c\u23a8bracketrightBigg\n+C\u23a8summationdisplay\nc=1\u23a1\n\u23a3\u23a8summationdisplay\ni:yi=clogN(x|\u03bcc,\u03a3c)\u23a4\u23a6 (4.52)\nWe see that this factorizes into a term for \u03c0, andCterms for each \u03bc\ncand\u03a3c. Hence we\ncan estimate these parameters separately. For the class prior, we have \u02c6\u03c0c=Nc\nN, as with naive\nBayes. For the class-conditional densities, we just partition the data based on its class label, and\ncompute the MLE for each Gaussian:\n\u02c6\u03bcc=1\nNc\u23a8summationdisplay\ni:yi=cxi,\u02c6\u03a3c=1\nNc\u23a8summationdisplay\ni:yi=c(xi\u2212\u02c6\u03bcc)(xi\u2212\u02c6\u03bcc)T(4.53)\nSeediscrimAnalysisFit for a Matlab implementation. Once the model has been \ufb01t, you can\nmake predictions using discrimAnalysisPredict , which uses a plug-in approximation.\n4.2.5 Strategies for preventing over\ufb01tting\nThe speed and simplicity of the MLE method is one of its greatest appeals. However, the MLEcan badly over\ufb01t in high dimensions. In particular, the MLE for a full covariance matrix issingular if N\nc<D. And even when Nc>D, the MLE can be ill-conditioned, meaning it is\nclose to singular. There are several possible solutions to this problem:", "137": "4.2. Gaussian discriminant analysis 107\n\u2022 Use a diagonal covariance matrix for each class, which assumes the features are conditionally\nindependent; this is equivalent to using a naive Bayes classi\ufb01er (Section 3.5).\n\u2022 Use a full covariance matrix, but force it to be the same for all classes, \u03a3c=\u03a3. This is an\nexample of parameter tying orparameter sharing, and is equivalent to LDA (Section 4.2.2).\n\u2022 Useadiagonalcovariancematrix andforcedittobeshared. Thisiscalleddiagonalcovariance\nLDA, and is discussed in Section 4.2.7.\n\u2022 Use a full covariance matrix, but impose a prior and then integrate it out. If we use a\nconjugate prior, this can be done in closed form, using the results from Section 4.6.3; this\nis analogous to the \u201cBayesian naive Bayes\u201d method in Section 3.5.1.2. See (Minka 2000f) fordetails.\n\u2022 Fit a full or diagonal covariance matrix by MAP estimation. We discuss two different kinds\nof prior below.\n\u2022 Project the data into a low dimensional subspace and \ufb01t the Gaussians there. See Sec-\ntion 8.6.3.3 for a way to \ufb01nd the best (most discriminative) linear projection.\nWe discuss some of these options below.\n4.2.6 Regularized LDA *\nSuppose we tie the covariance matrices, so \u03a3c=\u03a3, as in LDA, and furthermore we perform\nMAP estimation of \u03a3using an inverse Wishart prior of the form IW(diag( \u02c6\u03a3mle),\u03bd0)(see\nSection 4.5.1). Then we have\n\u02c6\u03a3=\u03bbdiag(\u02c6\u03a3mle)+(1\u2212\u03bb)\u02c6\u03a3mle (4.54)\nwhere\u03bbcontrols the amount of regularization, which is related to the strength of the prior, \u03bd0\n(see Section 4.6.2.1 for details). This technique is known as regularized discriminant analysis\nor RDA (Hastie et al. 2009, p656).\nWhen we evaluate the class conditional densities, we need to compute \u02c6\u03a3\u22121, and hence \u02c6\u03a3\u22121\nmle,\nwhich is impossible to compute if D>N. However, we can use the SVD of X(Section 12.2.3)\nto get around this, as we show below. (Note that this trick cannot be applied to QDA, which isa nonlinear function of x.)\nLetX=UDV\nTbe the SVD of the design matrix, where VisD\u00d7N,Uis anN\u00d7N\northogonal matrix, and Dis a diagonal matrix of size N. Furthermore, de\ufb01ne the N\u00d7N\nmatrixZ=UD; this is like a design matrix in a lower dimensional space (since we assume\nN<D). Also, de\ufb01ne \u03bcz=VT\u03bcas the mean of the data in this reduced space; we can recover\nthe original mean using \u03bc=V\u03bcz, sinceVTV=VVT=I. With these de\ufb01nitions, we can", "138": "108 Chapter 4. Gaussian models\nrewrite the MLE as follows:\n\u02c6\u03a3mle=1\nNXTX\u2212\u03bc\u03bcT(4.55)\n=1\nN(ZVT)T(ZVT)\u2212(V\u03bcz)(V\u03bcz)T(4.56)\n=1\nNVZTZVT\u2212V\u03bcz\u03bcT\nzVT(4.57)\n=V(1\nNZTZ\u2212\u03bcz\u03bcTz)VT(4.58)\n=V\u02c6\u03a3zVT(4.59)\nwhere\u02c6\u03a3zis the empirical covariance of Z. Hence we can rewrite the MAP estimate as\n\u02c6\u03a3map=V\u02dc\u03a3zVT(4.60)\n\u02dc\u03a3z=\u03bbdiag(\u02c6\u03a3z)+(1\u2212\u03bb)\u02c6\u03a3z (4.61)\nNote, however, that we never need to actually compute the D\u00d7Dmatrix\u02c6\u03a3map. This is because\nEquation 4.38 tells us that to classify using LDA, all we need to compute is p(y=c|x,\u03b8)\u221d\nexp(\u03b4c),w h e r e\n\u03b4c=\u2212xT\u03b2c+\u03b3c,\u03b2c=\u02c6\u03a3\u22121\u03bcc,\u03b3c\u22121\n2\u03bcTc\u03b2c+log\u03c0c (4.62)\nWe can compute the crucial \u03b2cterm for RDA without inverting the D\u00d7Dmatrix as follows:\n\u03b2c=\u02c6\u03a3\u22121\nmap\u03bcc=(V\u02dc\u03a3zVT)\u22121\u03bcc=V\u02dc\u03a3\u22121\nzVT\u03bcc=V\u02dc\u03a3\u22121\nz\u03bcz,c (4.63)\nwhere\u03bcz,c=VT\u03bccis the mean of the Zmatrix for data belonging to class c. See rdaFitfor\nthe code.\n4.2.7 Diagonal LDA\nA simple alternative to RDA is to tie the covariance matrices, so \u03a3c=\u03a3as in LDA, and then to\nuse a diagonal covariance matrix for each class. This is called the diagonal LDA model, and is\nequivalent to RDA with \u03bb=1. The corresponding discriminant function is as follows (compare\nto Equation 4.33):\n\u03b4c(x)=l o gp(x,y=c|\u03b8)=\u2212D\u23a8summationdisplay\nj=1(xj\u2212\u03bccj)2\n2\u03c32\nj+log\u03c0c (4.64)\nTypically we set \u02c6\u03bccj=xcjand\u02c6\u03c32\nj=s2\nj, which is the pooled empirical variance of feature j\n(pooled across classes) de\ufb01ned by\ns2\nj=\u23a8summationtextC\nc=1\u23a8summationtext\ni:yi=c(xij\u2212xcj)2\nN\u2212C(4.65)\nIn high dimensional settings, this model can work much better than LDA and RDA (Bickel and\nLevina 2004).", "139": "4.2. Gaussian discriminant analysis 109\nNumber of Genes\n2308 1355 352 106 36 12 5 0\n1 2 3 4 5 6 7 800.10.20.30.40.50.60.70.80.91\n\u03bbMisclassification Error\n  \nTest\nTrain\nCV\nFigure 4.7 Error versus amount of shrinkage for nearest shrunken centroid classi\ufb01er applied to the\nSRBCT gene expression data. Based on Figure 18.4 of (Hastie et al. 2009). Figure generated by\nshrunkenCentroidsSRBCTdemo .\n4.2.8 Nearest shrunken centroids classi\ufb01er *\nOne drawback of diagonal LDA is that it depends on all of the features. In high dimensional\nproblems, we might prefer a method that only depends on a subset of the features, for reasonsof accuracy and interpretability. One approach is to use a screening method, perhaps basedon mutual information, as in Section 3.5.4. We now discuss another approach to this problemknown as the nearest shrunken centroids classi\ufb01er (Hastie et al. 2009, p652).\nThe basic idea is to perform MAP estimation for diagonal LDA with a sparsity-promoting\n(Laplace) prior (see Section 13.3). More precisely, de\ufb01ne the class-speci\ufb01c feature mean, \u03bc\ncj,i n\nterms of the class-independent feature mean, mj, and a class-speci\ufb01c offset, \u0394cj. Thus we have\n\u03bccj=mj+\u0394cj (4.66)\nWe will then put a prior on the \u0394cjterms to encourage them to be strictly zero and compute\na MAP estimate. If, for feature j, we \ufb01nd that \u0394cj=0for allc, then feature jwill play no role\nin the classi\ufb01cation decision (since \u03bccjwill be independent of c). Thus features that are not\ndiscriminative are automatically ignored. The details can be found in (Hastie et al. 2009, p652)and (Greenshtein and Park 2009). See shrunkenCentroidsFit for some code.\nLet us give an example of the method in action, based on (Hastie et al. 2009, p652). Consider\nthe problem of classifying a gene expression dataset, which 2308 genes, 4 classes, 63 trainingsamples and 20 test samples. Using a diagonal LDA classi\ufb01er produces 5 errors on the test set.Using the nearest shrunken centroids classi\ufb01er produced 0 errors on the test set, for a range of\u03bbvalues: see Figure 4.7. More importantly, the model is sparse and hence more interpretable:\nFigure 4.8 plots an unpenalized estimate of the difference, d\ncj, in gray, as well as the shrunken\nestimates \u0394cjin blue. (These estimates are computed using the value of \u03bbestimated by CV.)\nWe see that only 39 genes are used, out of the original 2308.\nNow consider an even harder problem, with 16,603 genes, a training set of 144 patients, a\ntest set of 54 patients, and 14 different types of cancer (Ramaswamy et al. 2001). Hastie et al.(Hastie et al. 2009, p656) report that nearest shrunken centroids produced 17 errors on the test", "140": "110 Chapter 4. Gaussian models\n0 500 1000 1500 2000 2500\u22126\u22124\u221220246Class 1\n(a)0 500 1000 1500 2000 2500\u22128\u22126\u22124\u2212202468Class 2\n(b)\n0 500 1000 1500 2000 2500\u22126\u22124\u221220246Class 3\n(c)0 500 1000 1500 2000 2500\u22124\u2212202468Class 4\n(d)\nFigure 4.8 Pro\ufb01le of the shrunken centroids corresponding to \u03bb=4.4(CV optimal in Fig-\nure 4.7). This selects 39 genes. Based on Figure 18.4 of (Hastie et al. 2009). Figure generated by\nshrunkenCentroidsSRBCTdemo .\nset, using 6,520 genes, and that RDA (Section 4.2.6) produced 12 errors on the test set, using\nall 16,603 genes. The PMTK function cancerHighDimClassifDemo can be used to reproduce\nthese numbers.\n4.3 Inference in jointly Gaussian distributions\nGiven a joint distribution, p(x1,x2), it is useful to be able to compute marginals p(x1)and\nconditionals p(x1|x2). We discuss how to do this below, and then give some applications. These\noperations take O(D3)time in the worst case. See Section 20.4.3 for faster methods.", "141": "4.3. Inference in jointly Gaussian distributions 111\n4.3.1 Statement of the result\nTheorem 4.3.1 (Marginals and conditionals of an MVN). Supposex=(x1,x2)is jointly Gaussian\nwith parameters\n\u03bc=\u23a8parenleftbigg\n\u03bc1\n\u03bc2\u23a8parenrightbigg\n,\u03a3=\u23a8parenleftbigg\n\u03a311\u03a312\n\u03a321\u03a322\u23a8parenrightbigg\n,\u039b=\u03a3\u22121=\u23a8parenleftbigg\n\u039b11\u039b12\n\u039b21\u039b22\u23a8parenrightbigg\n(4.67)\nThen the marginals are given by\np(x1)=N (x1|\u03bc1,\u03a311)\np(x2)=N (x2|\u03bc2,\u03a322) (4.68)\nand the posterior conditional is given by\np(x1|x2)=N(x1|\u03bc1|2,\u03a31|2)\n\u03bc1|2=\u03bc1+\u03a312\u03a3\u22121\n22(x2\u2212\u03bc2)\n=\u03bc1\u2212\u039b\u22121\n11\u039b12(x2\u2212\u03bc2)\n=\u03a31|2(\u039b11\u03bc1\u2212\u039b12(x2\u2212\u03bc2))\n\u03a31|2=\u03a311\u2212\u03a312\u03a3\u22121\n22\u03a321=\u039b\u22121\n11(4.69)\nEquation 4.69 is of such crucial importance in this book that we have put a box around it, so\nyou can easily \ufb01nd it. For the proof, see Section 4.3.4.\nWe see that both the marginal and conditional distributions are themselves Gaussian. For the\nmarginals, we just extract the rows and columns corresponding to x1orx2. For the conditional,\nwe have to do a bit more work. However, it is not that complicated: the conditional mean is\njust a linear function of x2, and the conditional covariance is just a constant matrix that is\nindependent of x2. We give three different (but equivalent) expressions for the posterior mean,\nand two different (but equivalent) expressions for the posterior covariance; each one is useful indifferent circumstances.\n4.3.2 Examples\nBelow we give some examples of these equations in action, which will make them seem moreintuitive.\n4.3.2.1 Marginals and conditionals of a 2d Gaussian\nLet us consider a 2d example. The covariance matrix is\n\u03a3=\u23a8parenleftbigg\u03c3\n2\n1\u03c1\u03c31\u03c32\n\u03c1\u03c31\u03c32\u03c32\n2\u23a8parenrightbigg\n(4.70)\nThe marginal p(x1)is a 1D Gaussian, obtained by projecting the joint distribution onto the x1\nline:\np(x1)=N(x1|\u03bc1,\u03c32\n1) (4.71)", "142": "112 Chapter 4. Gaussian models\n\u22125 \u22124 \u22123 \u22122 \u22121 0 1 2 3 4 5\u22123\u22122\u221210123\nx1x2p(x1,x2)\n(a)\u22125 \u22124 \u22123 \u22122 \u22121 0 1 2 3 4 500.010.020.030.040.050.060.070.08\nx1x2p(x1)\n(b)\u22125 \u22124 \u22123 \u22122 \u22121 0 1 2 3 4 501234567\nx1x2p(x1|x2=1)\n(c)\nFigure 4.9 (a) A joint Gaussian distribution p(x1,x2)with a correlation coefficient of 0.8. We plot the\n95% contour and the principal axes. (b) The unconditional marginal p(x1). (c) The conditional p(x1|x2)=\nN(x1|0.8,0.36), obtained by slicing (a) at height x2=1. Figure generated by gaussCondition2Ddemo2 .\nSuppose we observe X2=x2; the conditional p(x1|x2)is obtained by \u201cslicing\u201d the joint\ndistribution through the X2=x2line (see Figure 4.9):\np(x1|x2)=N\u23a8parenleftbigg\nx1|\u03bc1+\u03c1\u03c31\u03c32\n\u03c32\n2(x2\u2212\u03bc2),\u03c32\n1\u2212(\u03c1\u03c31\u03c32)2\n\u03c32\n2\u23a8parenrightbigg\n(4.72)\nIf\u03c31=\u03c32=\u03c3,w eg e t\np(x1|x2)=N\u23a8parenleftbig\nx1|\u03bc1+\u03c1(x2\u2212\u03bc2),\u03c32(1\u2212\u03c12)\u23a8parenrightbig\n(4.73)\nIn Figure 4.9 we show an example where \u03c1=0.8,\u03c31=\u03c32=1,\u03bc=0andx2=1.W e\nsee that E[x1|x2=1 ]=0 .8, which makes sense, since \u03c1=0.8means that we believe that if\nx2increases by 1 (beyond its mean), then x1increases by 0.8. We also see var[x1|x2=1 ]=\n1\u22120.82=0.36. This also makes sense: our uncertainty about x1has gone down, since we\nhave learned something about x1(indirectly) by observing x2.I f\u03c1=0,w eg e tp(x1|x2)=\nN\u23a8parenleftbig\nx1|\u03bc1,\u03c32\n1\u23a8parenrightbig\n, sincex2conveys no information about x1if they are uncorrelated (and hence\nindependent).\n4.3.2.2 Interpolating noise-free data\nSuppose we want to estimate a 1d function, de\ufb01ned on the interval [0,T], such that yi=f(ti)\nforNobserved points ti. We assume for now that the data is noise-free, so we want to\ninterpolate it, that is, \ufb01t a function that goes exactly through the data. (See Section 4.4.2.3 for\nthe noisy data case.) The question is: how does the function behave in between the observed\ndata points? It is often reasonable to assume that the unknown function is smooth. In Chapter 15,we shall see how to encode priors over functions, and how to update such a prior with observed\nvalues to get a posterior over functions. But in this section, we take a simpler approach, whichis adequate for MAP estimation of functions de\ufb01ned on 1d inputs. We follow the presentationof (Calvetti and Somersalo 2007, p135).\nWe start by discretizing the problem. First we divide the support of the function into Dequal\nsubintervals. We then de\ufb01ne\nx\nj=f(sj),sj=jh, h=T\nD,1\u2264j\u2264D (4.74)", "143": "4.3. Inference in jointly Gaussian distributions 113\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\u22125\u22124\u22123\u22122\u22121012345\u03bb=30\n(a)0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\u22125\u22124\u22123\u22122\u22121012345\u03bb=0p1\n(b)\nFigure 4.10 Interpolating noise-free data using a Gaussian with prior precision \u03bb. (a)\u03bb=3 0. (b)\n\u03bb=0.01. See also Figure 4.15. Based on Figure 7.1 of (Calvetti and Somersalo 2007). Figure generated by\ngaussInterpDemo .\nWe can encode our smoothness prior by assuming that xjis an average of its neighbors, xj\u22121\nandxj+1, plus some Gaussian noise:\nxj=1\n2(xj\u22121+xj+1)+/epsilon1j,2\u2264j\u2264D\u22122 (4.75)\nwhere/epsilon1\u223cN(0,(1/\u03bb)I). The precision term \u03bbcontrols how much we think the function will\nvary: a large \u03bbcorresponds to a belief that the function is very smooth, a small \u03bbcorresponds\nto a belief that the function is quite \u201cwiggly\u201d. In vector form, the above equation can be written\nas follows:\nLx=/epsilon1 (4.76)\nwhereLis the(D\u22122)\u00d7Dsecond order \ufb01nite difference matrix\nL=1\n2\u239b\n\u239c\u239c\u239c\u239d\u221212\u2212 1\n\u221212\u2212 1\n...\n\u221212\u2212 1\u239e\n\u239f\u239f\u239f\u23a0(4.77)\nThe corresponding prior has the form\np(x)=N(x|0,(\u03bb\n2LTL)\u22121)\u221dexp\u23a8parenleftbigg\n\u2212\u03bb2\n2||Lx||2\n2\u23a8parenrightbigg\n(4.78)\nWe will henceforth assume we have scaled Lby\u03bbso we can ignore the \u03bbterm, and just write\n\u039b=LTLfor the precision matrix.\nNote that although xisD-dimensional, the precision matrix \u039bonly has rank D\u22122. Thus\nthis is an improper prior, known as an intrinsic Gaussian random \ufb01eld (see Section 19.4.4 for", "144": "114 Chapter 4. Gaussian models\nmore information). However, providing we observe N\u22652data points, the posterior will be\nproper.\nNow letx2be theNnoise-free observations of the function, and x1be theD\u2212Nunknown\nfunction values. Without loss of generality, assume that the unknown variables are ordered \ufb01rst,\nthen the known variables. Then we can partition the Lmatrix as follows:\nL=[L1,L2],L1\u2208R(D\u22122)\u00d7(D\u2212N),L2\u2208R(D\u22122)\u00d7(N)(4.79)\nWe can also partition the precision matrix of the joint distribution:\n\u039b=LTL=\u23a8parenleftbigg\u039b11\u039b12\n\u039b21\u039b22\u23a8parenrightbigg\n=\u23a8parenleftbiggLT\n1L1LT1L2\nLT2L1LT2L2\u23a8parenrightbigg\n(4.80)\nUsing Equation 4.69, we can write the conditional distribution as follows:\np(x1|x2)=N (\u03bc1|2,\u03a31|2) (4.81)\n\u03bc1|2=\u2212\u039b\u22121\n11\u039b12x2=\u2212LT\n1L2x2 (4.82)\n\u03a31|2=\u039b\u22121\n11 (4.83)\nNote that we can compute the mean by solving the following system of linear equations:\nL1\u03bc1|2=\u2212L2x2 (4.84)\nThis is efficient since L1is tridiagonal. Figure 4.10 gives an illustration of these equations. We\nsee that the posterior mean \u03bc1|2equals the observed data at the speci\ufb01ed points, and smoothly\ninterpolates in between, as desired.\nIt is also interesting to plot the 95% pointwise marginal credibility intervals, \u03bcj\u00b1\n2\u23a8radicalbig\u03a31|2,jj, shown in grey. We see that the variance goes up as we move away from the\ndata. We also see that the variance goes up as we decrease the precision of the prior, \u03bb. In-\nterestingly, \u03bbhas no effect on the posterior mean, since it cancels out when multiplying \u039b11\nand\u039b12. By contrast, when we consider noisy data in Section 4.4.2.3, we will see that the prior\nprecision affects the smoothness of posterior mean estimate.\nThe marginal credibility intervals do not capture the fact that neighboring locations are\ncorrelated. We can represent that by drawing complete functions (i.e., vectors x)f r o mt h e\nposterior, and plotting them. These are shown by the thin lines in Figure 4.10. These are not\nquite as smooth as the posterior mean itself. This is because the prior only penalizes \ufb01rst-orderdifferences. See Section 4.4.2.3 for further discussion of this point.\n4.3.2.3 Data imputation\nSuppose we are missing some entries in a design matrix. If the columns are correlated, we canuse the observed entries to predict the missing entries. Figure 4.11 shows a simple example. Wesampled some data from a 20 dimensional Gaussian, and then deliberately \u201chid\u201d 50% of the datain each row. We then inferred the missing entries given the observed entries, using the true(generating) model. More precisely, for each row i, we compute p(x\nhi|xvi,\u03b8),w h e r ehiandvi\nare the indices of the hidden and visible entries in case i. From this, we compute the marginal\ndistribution of each missing variable, p(xhij|xvi,\u03b8). We then plot the mean of this distribution,\n\u02c6xij=E[xj|xvi,\u03b8]; this represents our \u201cbest guess\u201d about the true value of that entry, in the", "145": "4.3. Inference in jointly Gaussian distributions 115\n0 5 10 15 20\u221210\u221250510observed\n0 5 10 15 20\u221210\u221250510imputed\n0 5 10 15 20\u221210\u221250510truth\nFigure 4.11 Illustration of data imputation. Left column: visualization of three rows of the data matrix\nwith missing entries. Middle column: mean of the posterior predictive, based on partially observed\ndata in that row, but the true model parameters. Right column: true values. Figure generated bygaussImputationDemo .\nsense that it minimizes our expected squared error (see Section 5.7 for details). Figure 4.11 shows\nthat the estimates are quite close to the truth. (Of course, if j\u2208vi, the expected value is equal\nto the observed value, \u02c6xij=xij.)\nWe can use var\u23a8bracketleftbig\nxhij|xvi,\u03b8\u23a8bracketrightbig\nas a measure of con\ufb01dence in this guess, although this is not\nshown. Alternatively, we could draw multiple samples from p(xhi|xvi,\u03b8); this is called multiple\nimputation.\nIn addition to imputing the missing entries, we may be interested in computing the like-\nlihood of each partially observed row in the table, p(xvi|\u03b8), which can be computed using\nEquation 4.68. This is useful for detecting outliers (atypical observations).\n4.3.3 Information form\nSupposex\u223cN(\u03bc,\u03a3). One can show that E[x]=\u03bcis the mean vector, and cov[x]=\u03a3is\nthe covariance matrix. These are called the moment parameters of the distribution. However,\nit is sometimes useful to use the canonical parameters ornatural parameters, de\ufb01ned as\n\u039b/defines\u03a3\u22121,\u03be/defines\u03a3\u22121\u03bc (4.85)\nWe can convert back to the moment parameters using\n\u03bc=\u039b\u22121\u03be,\u03a3=\u039b\u22121(4.86)\nUsing the canonical parameters, we can write the MVN in information form (i.e., in exponential\nfamily form, de\ufb01ned in Section 9.2):\nNc(x|\u03be,\u039b)=( 2 \u03c0)\u2212D/2|\u039b|1\n2exp\u23a8bracketleftbigg\n\u22121\n2(xT\u039bx+\u03beT\u039b\u22121\u03be\u22122xT\u03be)\u23a8bracketrightbigg\n(4.87)\nwhere we use the notation Nc()to distinguish from the moment parameterization N().\nIt is also possible to derive the marginalization and conditioning formulas in information\nform. We \ufb01nd\np(x2)=N c(x2|\u03be2\u2212\u039b21\u039b\u22121\n11\u03be1,\u039b22\u2212\u039b21\u039b\u22121\n11\u039b12) (4.88)\np(x1|x2)=N c(x1|\u03be1\u2212\u039b12x2,\u039b11) (4.89)", "146": "116 Chapter 4. Gaussian models\nThus we see that marginalization is easier in moment form, and conditioning is easier in\ninformation form.\nAnother operation that is signi\ufb01cantly easier in information form is multiplying two Gaussians.\nOne can show that\nNc(\u03bef,\u03bbf)Nc(\u03beg,\u03bbg)=N c(\u03bef+\u03beg,\u03bbf+\u03bbg) (4.90)\nHowever, in moment form, things are much messier:\nN(\u03bcf,\u03c32\nf)N(\u03bcg,\u03c32\ng)=N\u23a8parenleftBigg\n\u03bcf\u03c32\ng+\u03bcg\u03c32\nf\n\u03c32g+\u03c32g,\u03c32\nf\u03c32\ng\n\u03c32g+\u03c32g\u23a8parenrightBigg\n(4.91)\n4.3.4 Proof of the result *\nWe now prove Theorem 4.3.1. Readers who are intimidated by heavy matrix algebra can safelyskip this section. We \ufb01rst derive some results that we will need here and elsewhere in the book.We will return to the proof at the end.\n4.3.4.1 Inverse of a partitioned matrix using Schur complements\nThe key tool we need is a way to invert a partitioned matrix. This can be done using thefollowing result.\nTheorem 4.3.2 (Inverse of a partitioned matrix). Consider a general partitioned matrix\nM=\u23a8parenleftbiggEF\nGH\u23a8parenrightbigg\n(4.92)\nwhere we assume EandHare invertible. We have\nM\n\u22121=\u23a8parenleftbigg(M/H)\u22121\u2212(M/H)\u22121FH\u22121\n\u2212H\u22121G(M/H)\u22121H\u22121+H\u22121G(M/H)\u22121FH\u22121\u23a8parenrightbigg\n(4.93)\n=\u23a8parenleftbiggE\u22121+E\u22121F(M/E)\u22121GE\u22121\u2212E\u22121F(M/E)\u22121\n\u2212(M/E)\u22121GE\u22121(M/E)\u22121\u23a8parenrightbigg\n(4.94)\nwhere\nM/H/definesE\u2212FH\u22121G (4.95)\nM/E/definesH\u2212GE\u22121F (4.96)\nWe say that M/His theSchur complement ofMwrtH. Equation 4.93 is called the partitioned\ninverse formula.Proof.If we could block diagonalize M, it would be easier to invert. To zero out the top right\nblock ofMwe can pre-multiply as follows\n\u23a8parenleftbiggI\u2212FH\n\u22121\n0I\u23a8parenrightbigg\u23a8parenleftbiggEF\nGH\u23a8parenrightbigg\n=\u23a8parenleftbiggE\u2212FH\u22121G0\nGH\u23a8parenrightbigg\n(4.97)", "147": "4.3. Inference in jointly Gaussian distributions 117\nSimilarly, to zero out the bottom left we can post-multiply as follows\n\u23a8parenleftbigg\nE\u2212FH\u22121G0\nGH\u23a8parenrightbigg\u23a8parenleftbigg\nI0\n\u2212H\u22121GI\u23a8parenrightbigg\n=\u23a8parenleftbigg\nE\u2212FH\u22121G0\n0H\u23a8parenrightbigg\n(4.98)\nPutting it all together we get\n\u23a8parenleftbigg\nI\u2212FH\u22121\n0I\u23a8parenrightbigg\n\u23a8bracehtipupleft\u23a8bracehtipdownright\u23a8bracehtipdownleft\u23a8bracehtipupright\nX\u23a8parenleftbigg\nEF\nGH\u23a8parenrightbigg\n\u23a8bracehtipupleft\u23a8bracehtipdownright\u23a8bracehtipdownleft\u23a8bracehtipupright\nM\u23a8parenleftbigg\nI0\n\u2212H\u22121GI\u23a8parenrightbigg\n\u23a8bracehtipupleft\u23a8bracehtipdownright\u23a8bracehtipdownleft\u23a8bracehtipupright\nZ=\u23a8parenleftbigg\nE\u2212FH\u22121G0\n0H\u23a8parenrightbigg\n\u23a8bracehtipupleft\u23a8bracehtipdownright\u23a8bracehtipdownleft\u23a8bracehtipupright\nW(4.99)\nTaking the inverse of both sides yields\nZ\u22121M\u22121X\u22121=W\u22121(4.100)\nand hence\nM\u22121=ZW\u22121X (4.101)\nSubstituting in the de\ufb01nitions we get\n\u23a8parenleftbiggEF\nGH\u23a8parenrightbigg\u22121\n=\u23a8parenleftbiggI0\n\u2212H\u22121GI\u23a8parenrightbigg\u23a8parenleftbigg(M/H)\u221210\n0H\u22121\u23a8parenrightbigg\u23a8parenleftbiggI\u2212FH\u22121\n0I\u23a8parenrightbigg\n(4.102)\n=\u23a8parenleftbigg(M/H)\u221210\n\u2212H\u22121G(M/H)\u22121H\u22121\u23a8parenrightbigg\u23a8parenleftbiggI\u2212FH\u22121\n0I\u23a8parenrightbigg\n(4.103)\n=\u23a8parenleftbigg(M/H)\u22121\u2212(M/H)\u22121FH\u22121\n\u2212H\u22121G(M/H)\u22121H\u22121+H\u22121G(M/H)\u22121FH\u22121\u23a8parenrightbigg\n(4.104)\nAlternatively, we could have decomposed the matrix Min terms of EandM/E=(H\u2212\nGE\u22121F), yielding\n\u23a8parenleftbiggEF\nGH\u23a8parenrightbigg\u22121\n=\u23a8parenleftbiggE\u22121+E\u22121F(M/E)\u22121GE\u22121\u2212E\u22121F(M/E)\u22121\n\u2212(M/E)\u22121GE\u22121(M/E)\u22121\u23a8parenrightbigg\n(4.105)\n4.3.4.2 The matrix inversion lemma\nWe now derive some useful corollaries of the above result.\nCorollary 4.3.1 (Matrix inversion lemma). Consider a general partitioned matrix M=\u23a8parenleftbiggEF\nGH\u23a8parenrightbigg\n,\nwhere we assume EandHare invertible. We have\n(E\u2212FH\u22121G)\u22121=E\u22121+E\u22121F(H\u2212GE\u22121F)\u22121GE\u22121(4.106)\n(E\u2212FH\u22121G)\u22121FH\u22121=E\u22121F(H\u2212GE\u22121F)\u22121(4.107)\n|E\u2212FH\u22121G|=|H\u2212GE\u22121F||H\u22121||E| (4.108)", "148": "118 Chapter 4. Gaussian models\nThe \ufb01rst two equations are s known as the matrix inversion lemma or theSherman-\nMorrison-Woodbury formula. The third equation is known as the matrix determinant\nlemma. A typical application in machine learning/ statistics is the following. Let E=\u03a3\nbe aN\u00d7Ndiagonal matrix, let F=GT=Xof sizeN\u00d7D,w h e r eN/greatermuchD, and let\nH\u22121=\u2212I. Then we have\n(\u03a3+XXT)\u22121=\u03a3\u22121\u2212\u03a3\u22121X(I+XT\u03a3\u22121X)\u22121XT\u03a3\u22121(4.109)\nThe LHS takes O(N3)time to compute, the RHS takes time O(D3)to compute.\nAnother application concerns computing a rank one update of an inverse matrix. Let\nH=\u22121(a scalar), F=u(a column vector), and G=vT(a row vector). Then we have\n(E+uvT)\u22121=E\u22121+E\u22121u(\u22121\u2212vTE\u22121u)\u22121vTE\u22121(4.110)\n=E\u22121\u2212E\u22121uvTE\u22121\n1+vTE\u22121u(4.111)\nThis is useful when we incrementally add a data vector to a design matrix, and want to update\nour sufficient statistics. (One can derive an analogous formula for removing a data vector.)\nProof.To prove Equation 4.106, we simply equate the top left block of Equation 4.93 and Equa-\ntion 4.94. To prove Equation 4.107, we simple equate the top right blocks of Equations 4.93 and\n4.94. The proof of Equation 4.108 is left as an exercise.\n4.3.4.3 Proof of Gaussian conditioning formulas\nWe can now return to our original goal, which is to derive Equation 4.69. Let us factor the jointp(x\n1,x2)asp(x2)p(x1|x2)as follows:\nE=e x p\u23a8braceleftBigg\n\u22121\n2\u23a8parenleftbiggx1\u2212\u03bc1\nx2\u2212\u03bc2\u23a8parenrightbiggT\u23a8parenleftbigg\u03a311\u03a312\n\u03a321\u03a322\u23a8parenrightbigg\u22121\u23a8parenleftbiggx1\u2212\u03bc1\nx2\u2212\u03bc2\u23a8parenrightbigg\u23a8bracerightBigg\n(4.112)\nUsing Equation 4.102 the above exponent becomes\nE=e x p\u23a8braceleftBigg\n\u22121\n2\u23a8parenleftbiggx1\u2212\u03bc1\nx2\u2212\u03bc2\u23a8parenrightbiggT\u23a8parenleftbiggI0\n\u2212\u03a3\u22121\n22\u03a321I\u23a8parenrightbigg\u23a8parenleftbigg(\u03a3/\u03a322)\u221210\n0\u03a3\u22121\n22\u23a8parenrightbigg\n(4.113)\n\u00d7\u23a8parenleftbigg\nI\u2212\u03a312\u03a3\u22121\n22\n0I\u23a8parenrightbigg\u23a8parenleftbiggx1\u2212\u03bc1\nx2\u2212\u03bc2\u23a8parenrightbigg\u23a8bracerightbigg\n(4.114)\n=e x p\u23a8braceleftbigg\n\u22121\n2(x1\u2212\u03bc1\u2212\u03a312\u03a3\u22121\n22(x2\u2212\u03bc2))T(\u03a3/\u03a322)\u22121(4.115)\n(x1\u2212\u03bc1\u2212\u03a312\u03a3\u22121\n22(x2\u2212\u03bc2))\u23a8bracerightbig\n\u00d7exp\u23a8braceleftbigg\n\u22121\n2(x2\u2212\u03bc2)T\u03a3\u22121\n22(x2\u2212\u03bc2)\u23a8bracerightbigg\n(4.116)\nThis is of the form\nexp(quadratic form in x1,x2)\u00d7exp(quadratic form in x2) (4.117)", "149": "4.4. Linear Gaussian systems 119\nHence we have successfully factorized the joint as\np(x1,x2)=p( x1|x2)p(x2) (4.118)\n=N(x1|\u03bc1|2,\u03a31|2)N(x2|\u03bc2,\u03a322) (4.119)\nwhere the parameters of the conditional distribution can be read off from the above equations\nusing\n\u03bc1|2=\u03bc1+\u03a312\u03a3\u22121\n22(x2\u2212\u03bc2) (4.120)\n\u03a31|2=\u03a3/\u03a322=\u03a311\u2212\u03a312\u03a3\u22121\n22\u03a321 (4.121)\nWe can also use the fact that |M|=|M/H||H|to check the normalization constants are\ncorrect:\n(2\u03c0)(d1+d2)/2|\u03a3|1\n2=( 2\u03c0)(d1+d2)/2(|\u03a3/\u03a322||\u03a322|)1\n2 (4.122)\n=( 2\u03c0)d1/2|\u03a3/\u03a322|1\n2(2\u03c0)d2/2|\u03a322|1\n2 (4.123)\nwhered1=d i m (x1)andd2=d i m (x2).\nWe leave the proof of the other forms of the result in Equation 4.69 as an exercise.\n4.4 Linear Gaussian systems\nSuppose we have two variables, xandy.L e tx\u2208RDxbe a hidden variable, and y\u2208RDybe\na noisy observation of x. Let us assume we have the following prior and likelihood:\np(x)=N(x|\u03bcx,\u03a3x)\np(y|x)=N(y|Ax+b,\u03a3y)(4.124)\nwhereAis a matrix of size Dy\u00d7Dx. This is an example of a linear Gaussian system.W e\ncan represent this schematically as x\u2192y, meaning xgenerates y. In this section, we show\nhow to \u201cinvert the arrow\u201d, that is, how to infer xfromy. We state the result below, then give\nseveral examples, and \ufb01nally we derive the result. We will see many more applications of theseresults in later chapters.\n4.4.1 Statement of the result\nTheorem 4.4.1 (Bayes rule for linear Gaussian systems). Given a linear Gaussian system, as in\nEquation 4.124, the posterior p(x|y)is given by the following:\np(x|y)=N(x|\u03bcx|y,\u03a3x|y)\n\u03a3\u22121\nx|y=\u03a3\u22121\nx+AT\u03a3\u22121\nyA\n\u03bcx|y=\u03a3x|y[AT\u03a3\u22121\ny(y\u2212b)+\u03a3\u22121\nx\u03bcx](4.125)", "150": "120 Chapter 4. Gaussian models\nIn addition, the normalization constant p(y)is given by\np(y)=N(y|A\u03bcx+b,\u03a3y+A\u03a3xAT) (4.126)\nFor the proof, see Section 4.4.3.\n4.4.2 Examples\nIn this section, we give some example applications of the above result.\n4.4.2.1 Inferring an unknown scalar from noisy measurements\nSuppose we make Nnoisy measurements yiof some underlying quantity x; let us assume the\nmeasurement noise has \ufb01xed precision \u03bby=1/\u03c32, so the likelihood is\np(yi|x)=N (yi|x,\u03bb\u22121\ny) (4.127)\nNow let us use a Gaussian prior for the value of the unknown source:\np(x)=N (x|\u03bc0,\u03bb\u22121\n0) (4.128)\nWe want to compute p(x|y1,...,y N,\u03c32). We can convert this to a form that lets us apply\nBayes rule for Gaussians by de\ufb01ning y=(y1,...,y N),A=1T\nN(an1\u00d7Nrow vector of 1\u2019s),\nand\u03a3\u22121\ny=d i a g (\u03bbyI). Then we get\np(x|y)=N (x|\u03bcN,\u03bb\u22121\nN) (4.129)\n\u03bbN=\u03bb0+N\u03bby (4.130)\n\u03bcN=N\u03bbyy+\u03bb0\u03bc0\n\u03bbN=N\u03bby\nN\u03bby+\u03bb0y+\u03bb0\nN\u03bby+\u03bb0\u03bc0 (4.131)\nThese equations are quite intuitive: the posterior precision \u03bbNis the prior precision \u03bb0plusN\nunits of measurement precision \u03bby. Also, the posterior mean \u03bcNis a convex combination of\nthe MLEyand the prior mean \u03bc0. This makes it clear that the posterior mean is a compromise\nbetween the MLE and the prior. If the prior is weak relative to the signal strength (\u03bb 0is\nsmall relative to \u03bby), we put more weight on the MLE. If the prior is strong relative to the\nsignal strength (\u03bb 0is large relative to \u03bby), we put more weight on the prior. This is illustrated\nin Figure 4.12, which is very similar to the analogous results for the beta-binomial model in\nFigure 3.6.\nNote that the posterior mean is written in terms of N\u03bbyy, so having Nmeasurements each\nof precision \u03bbyis like having one measurement with value yand precision N\u03bby.\nWe can rewrite the results in terms of the posterior variance, rather than posterior precision,", "151": "4.4. Linear Gaussian systems 121\n\u22125 0 500.10.20.30.40.50.6prior variance = 1.00\n  \nprior\nlik\npost\n\u22125 0 500.10.20.30.40.50.6prior variance = 5.00\n  \nprior\nlik\npost\nFigure 4.12 Inference about xgiven a noisy observation y=3. (a) Strong prior N(0,1). The posterior\nmean is \u201cshrunk\u201d towards the prior mean, which is 0. (a) Weak prior N(0,5). The posterior mean is\nsimilar to the MLE. Figure generated by gaussInferParamsMean1d .\nas follows:\np(x|D,\u03c32)=N (x|\u03bcN,\u03c42\nN) (4.132)\n\u03c42\nN=1\nN\n\u03c32+1\n\u03c42\n0=\u03c32\u03c42\n0\nN\u03c42\n0+\u03c32(4.133)\n\u03bcN=\u03c42\nN\u23a8parenleftbigg\u03bc0\n\u03c42\n0+Ny\n\u03c32\u23a8parenrightbigg\n=\u03c32\nN\u03c42\n0+\u03c32\u03bc0+N\u03c42\n0\nN\u03c42\n0+\u03c32y (4.134)\nwhere\u03c42\n0=1/\u03bb0is the prior variance and \u03c42\nN=1/\u03bbNis the posterior variance.\nWe can also compute the posterior sequentially, by updating after each observation. If\nN=1, we can rewrite the posterior after seeing a single observation as follows (where we\nde\ufb01ne\u03a3y=\u03c32,\u03a30=\u03c42\n0and\u03a31=\u03c42\n1to be the variances of the likelihood, prior and\nposterior):\np(x|y)=N (x|\u03bc1,\u03a31) (4.135)\n\u03a31=\u23a8parenleftbigg1\n\u03a30+1\n\u03a3y\u23a8parenrightbigg\u22121\n=\u03a3y\u03a30\n\u03a30+\u03a3y(4.136)\n\u03bc1=\u03a31\u23a8parenleftbigg\u03bc0\n\u03a30+y\n\u03a3y\u23a8parenrightbigg\n(4.137)\nWe can rewrite the posterior mean in 3 different ways:\n\u03bc1=\u03a3y\n\u03a3y+\u03a30\u03bc0+\u03a30\n\u03a3y+\u03a30y (4.138)\n=\u03bc0+(y\u2212\u03bc0)\u03a30\n\u03a3y+\u03a30(4.139)\n=y\u2212(y\u2212\u03bc0)\u03a3y\n\u03a3y+\u03a30(4.140)", "152": "122 Chapter 4. Gaussian models\nThe \ufb01rst equation is a convex combination of the prior and the data. The second equation is the\nprior mean adjusted towards the data. The third equation is the data adjusted towards the priormean; this is called shrinkage. These are all equivalent ways of expressing the tradeoff between\nlikelihood and prior. If \u03a3\n0is small relative to \u03a3Y, corresponding to a strong prior, the amount\nof shrinkage is large (see Figure 4.12(a)), whereas if \u03a30is large relative to \u03a3y, corresponding to\na weak prior, the amount of shrinkage is small (see Figure 4.12(b)).\nAnother way to quantify the amount of shrinkage is in terms of the signal-to-noise ratio,\nwhich is de\ufb01ned as follows:\nSNR/definesE\u23a8bracketleftbig\nX2\u23a8bracketrightbig\nE[/epsilon12]=\u03a30+\u03bc2\n0\n\u03a3y(4.141)\nwherex\u223cN(\u03bc0,\u03a30)is the true signal, y=x+/epsilon1is the observed signal, and /epsilon1\u223cN(0,\u03a3y)\nis the noise term.\n4.4.2.2 Inferring an unknown vector from noisy measurements\nNow consider Nvector-valued observations, yi\u223cN(x,\u03a3y), and a Gaussian prior, x\u223c\nN(\u03bc0,\u03a30). Setting A=I,b=0, and using yfor the effective observation with precision\nN\u03a3\u22121\ny,w eh a v e\np(x|y1,...,yN)=N (x|\u03bcN,\u03a3N) (4.142)\n\u03a3\u22121\nN=\u03a3\u22121\n0+N\u03a3\u22121\ny (4.143)\n\u03bcN=\u03a3N(\u03a3\u22121\ny(Ny)+\u03a3\u22121\n0\u03bc0) (4.144)\nSee Figure 4.13 for a 2d example. We can think of xas representing the true, but unknown,\nlocation of an object in 2d space, such as a missile or airplane, and the yias being noisy\nobservations, such as radar \u201cblips\u201d. As we receive more blips, we are better able to localize the\nsource. In Section 18.3.1, we will see how to extend this example to track moving objects usingthe famous Kalman \ufb01lter algorithm.\nNow suppose we have multiple measuring devices, and we want to combine them together;\nthis is known as sensor fusion. If we have multiple observations with different covariances (cor-\nresponding to sensors with different reliabilities), the posterior will be an appropriate weightedaverage of the data. Consider the example in Figure 4.14. We use an uninformative prior on x,\nnamelyp(x)=N(\u03bc\n0,\u03a30)=N(0,1010I2). We get 2 noisy observations, y1\u223cN(x,\u03a3y,1)\nandy2\u223cN(x,\u03a3y,2). We then compute p(x|y1,y2).\nIn Figure 4.14(a), we set \u03a3y,1=\u03a3y,2=0.01I2, so both sensors are equally reliable. In this\ncase, the posterior mean is half way between the two observations, y1andy2. In Figure 4.14(b),\nwe set\u03a3y,1=0.05I2and\u03a3y,2=0.01I2, so sensor 2 is more reliable than sensor 1. In this\ncase, the posterior mean is closer to y2. In Figure 4.14(c), we set\n\u03a3y,1=0.01\u23a8parenleftbigg10 1\n11\u23a8parenrightbigg\n,\u03a3y,2=0.01\u23a8parenleftbigg1111 0\u23a8parenrightbigg\n(4.145)\nso sensor 1 is more reliable in the y\n2component (vertical direction), and sensor 2 is more\nreliable in the y1component (horizontal direction). In this case, the posterior mean uses y1\u2019s\nvertical component and y2\u2019s horizontal component.", "153": "4.4. Linear Gaussian systems 123\n\u22121 0 1\u22121\u22120.500.51data prior\n\u22121 0 1\u22121\u22120.500.51post after 10 obs\n\u22121 0 1\u22121\u22120.500.51\nFigure 4.13 Illustration of Bayesian inference for the mean of a 2d Gaussian. (a) The data is generated\nfromyi\u223cN(x,\u03a3y),w h e r ex=[ 0.5,0.5]Tand\u03a3y=0.1[2,1;1,1]). We assume the sensor noise\ncovariance \u03a3yis known but xis unknown. The black cross represents x. (b) The prior is p(x)=\nN(x|0,0.1I2). (c) We show the posterior after 10 data points have been observed. Figure generated by\ngaussInferParamsMean2d .\n\u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4\u22121.4\u22121.2\u22121\u22120.8\u22120.6\u22120.4\u22120.200.20.4\n(a)\u22120.6\u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4\u22121.6\u22121.4\u22121.2\u22121\u22120.8\u22120.6\u22120.4\u22120.200.20.4\n(b)\u22121 \u22120.5 0 0.5 1 1.5\u22121.5\u22121\u22120.500.51\n(c)\nFigure 4.14 We observe y1=( 0,\u22121)(red cross) and y2=( 1,0)(green cross) and infer E(\u03bc|y1,y2,\u03b8)\n(black cross). (a) Equally reliable sensors, so the posterior mean estimate is in between the two circles.\n(b) Sensor 2 is more reliable, so the estimate shifts more towards the green circle. (c) Sensor 1 is morereliable in the vertical direction, Sensor 2 is more reliable in the horizontal direction. The estimate is anappropriate combination of the two measurements. Figure generated by sensorFusion2d .\nNote that this technique crucially relies on modeling our uncertainty of each sensor; comput-\ning an unweighted average would give the wrong result. However, we have assumed the sensor\nprecisions are known. When they are not, we should model out uncertainty about \u03a31and\u03a32\nas well. See Section 4.6.4 for details.\n4.4.2.3 Interpolating noisy data\nWe now revisit the example of Section 4.3.2.2. This time we no longer assume noise-freeobservations. Instead, let us assume that we obtain Nnoisy observations y\ni; without loss\nof generality, assume these correspond to x1,...,x N. We can model this setup as a linear", "154": "124 Chapter 4. Gaussian models\nGaussian system:\ny=Ax+/epsilon1 (4.146)\nwhere/epsilon1\u223cN(0,\u03a3y),\u03a3y=\u03c32I,\u03c32is the observation noise, and Ais aN\u00d7Dprojection\nmatrix that selects out the observed elements. For example, if N=2andD=4we have\nA=\u23a8parenleftbigg\n1000\n0100\u23a8parenrightbigg\n(4.147)\nUsing the same improper prior as before, \u03a3x=(LTL)\u22121, we can easily compute the posterior\nmean and variance. In Figure 4.15, we plot the posterior mean, posterior variance, and someposterior samples. Now we see that the prior precision \u03bbeffects the posterior mean as well as\nthe posterior variance. In particular, for a strong prior (large \u03bb), the estimate is very smooth, and\nthe uncertainty is low. but for a weak prior (small \u03bb), the estimate is wiggly, and the uncertainty\n(away from the data) is high.\nThe posterior mean can also be computed by solving the following optimization problem:\nmin\nx1\n2\u03c32N\u23a8summationdisplay\ni=1(xi\u2212yi)2+\u03bb\n2D\u23a8summationdisplay\nj=1\u23a8bracketleftBig\n(xj\u2212xj\u22121)2+(xj\u2212xj+1)2\u23a8bracketrightBig\n(4.148)\nwhere we have de\ufb01ned x0=x1andxD+1=xDfor notational simplicity. We recognize this\nas a discrete approximation to the following problem:\nmin\nf1\n2\u03c32\u23a8integraldisplay\n(f(t)\u2212y(t))2dt+\u03bb\n2\u23a8integraldisplay\n[f/prime(t)]2dt (4.149)\nwheref/prime(t)is the \ufb01rst derivative of f. The \ufb01rst term measures \ufb01t to the data, and the second\nterm penalizes functions that are \u201ctoo wiggly\u201d. This is an example of Tikhonov regularization,\nwhich is a popular approach to functional data analysis. See Chapter 15 for more sophisticated\napproaches, which enforce higher order smoothness (so the resulting samples look less \u201cjagged\u201d).\n4.4.3 Proof of the result *\nWe now derive Equation 4.125. The basic idea is to derive the joint distribution, p(x,y)=\np(x)p(y|x), and then to use the results from Section 4.3.1 for computing p(x|y).\nIn more detail, we proceed as follows. The log of the joint distribution is as follows (dropping\nirrelevant constants):\nlogp(x,y)=\u22121\n2(x\u2212\u03bcx)T\u03a3\u22121\nx(x\u2212\u03bcx)\u22121\n2(y\u2212Ax\u2212b)T\u03a3\u22121\ny(y\u2212Ax\u2212b)(4.150)\nThis is clearly a joint Gaussian distribution, since it is the exponential of a quadratic form.\nExpanding out the quadratic terms involving xandy, and ignoring linear and constant terms,\nwe have\nQ=\u22121\n2xT\u03a3\u22121\nxx\u22121\n2yT\u03a3\u22121\nyy\u22121\n2(Ax)T\u03a3\u22121\ny(Ax)+yT\u03a3\u22121\nyAx (4.151)\n=\u22121\n2\u23a8parenleftbiggx\ny\u23a8parenrightbiggT\u23a8parenleftbigg\u03a3\u22121\nx+AT\u03a3\u22121\nyA\u2212AT\u03a3\u22121\ny\n\u2212\u03a3\u22121\nyA\u03a3\u22121\ny\u23a8parenrightbigg\u23a8parenleftbiggx\ny\u23a8parenrightbigg\n(4.152)\n=\u22121\n2\u23a8parenleftbigg\nx\ny\u23a8parenrightbiggT\n\u03a3\u22121\u23a8parenleftbigg\nx\ny\u23a8parenrightbigg\n(4.153)", "155": "4.5. Digression: The Wishart distribution * 125\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\u22125\u22124\u22123\u22122\u22121012345\u03bb=30\n(a)0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\u22125\u22124\u22123\u22122\u22121012345\u03bb=0p1\n(b)\nFigure 4.15 Interpolating noisy data (noise variance \u03c32=1) using a Gaussian with prior precision \u03bb. (a)\n\u03bb=3 0. (b)\u03bb=0.01. See also Figure 4.10. Based on Figure 7.1 of (Calvetti and Somersalo 2007). Figure\ngenerated by gaussInterpNoisyDemo . See also splineBasisDemo .\nwhere the precision matrix of the joint is de\ufb01ned as\n\u03a3\u22121=\u23a8parenleftbigg\u03a3\u22121\nx+AT\u03a3\u22121\nyA\u2212AT\u03a3\u22121\ny\n\u2212\u03a3\u22121\nyA\u03a3\u22121\ny\u23a8parenrightbigg\n/defines\u039b=\u23a8parenleftbigg\u039bxx\u039bxy\n\u039byx\u039byy\u23a8parenrightbigg\n(4.154)\nFrom Equation 4.69, and using the fact that \u03bcy=A\u03bcx+b,w eh a v e\np(x|y)=N (\u03bcx|y,\u03a3x|y) (4.155)\n\u03a3x|y=\u039b\u22121\nxx=(\u03a3\u22121\nx+AT\u03a3\u22121\nyA)\u22121(4.156)\n\u03bcx|y=\u03a3x|y\u23a8parenleftbig\n\u039bxx\u03bcx\u2212\u039bxy(y\u2212\u03bcy)\u23a8parenrightbig\n(4.157)\n=\u03a3x|y\u23a8parenleftbig\n\u03a3\u22121\nx\u03bc+AT\u03a3\u22121\ny(y\u2212b)\u23a8parenrightbig\n(4.158)\n4.5 Digression: The Wishart distribution *\nTheWishartdistribution is the generalization of the Gamma distribution to positive de\ufb01nite\nmatrices. Press (Press 2005, p107) has said \u201cThe Wishart distribution ranks next to the (multi-\nvariate) normal distribution in order of importance and usefuleness in multivariate statistics\u201d.We will mostly use it to model our uncertainty in covariance matrices, \u03a3, or their inverses,\n\u039b=\u03a3\n\u22121.\nThe pdf of the Wishart is de\ufb01ned as follows:\nWi(\u039b|S,\u03bd)=1\nZWi|\u039b|(\u03bd\u2212D\u22121)/2exp\u23a8parenleftbigg\n\u22121\n2tr(\u039bS\u22121)\u23a8parenrightbigg\n(4.159)\nHere\u03bdis called the \u201cdegrees of freedom\u201d and Sis the \u201cscale matrix\u201d. (We shall get more\nintuition for these parameters shortly.) The normalization constant for this distribution (which", "156": "126 Chapter 4. Gaussian models\nrequires integrating over all symmetric pd matrices) is the following formidable expression\nZWi=2\u03bdD/2\u0393D(\u03bd/2)|S|\u03bd/2(4.160)\nwhere\u0393D(a)is themultivariate gamma function:\n\u0393D(x)=\u03c0D(D\u22121)/4D\u23a8productdisplay\ni=1\u0393(x+(1\u2212i)/2) (4.161)\nHence\u03931(a)=\u0393 (a)and\n\u0393D(\u03bd0/2) =D\u23a8productdisplay\ni=1\u0393(\u03bd0+1\u2212i\n2) (4.162)\nThe normalization constant only exists (and hence the pdf is only well de\ufb01ned) if \u03bd>D\u22121.\nThere is a connection between the Wishart distribution and the Gaussian. In particular,\nletxi\u223cN(0,\u03a3). Then the scatter matrix S=\u23a8summationtextN\ni=1xixT\nihas a Wishart distribution:\nS\u223cWi(\u03a3,1). Hence E[S]=N\u03a3. More generally, one can show that the mean and mode of\nWi(S,\u03bd)are given by\nmean=\u03bdS,mode=(\u03bd\u2212D\u22121)S (4.163)\nwhere the mode only exists if \u03bd>D+1.\nIfD=1, the Wishart reduces to the Gamma distribution:\nWi(\u03bb|s\u22121,\u03bd)=G a (\u03bb|\u03bd\n2,s\n2) (4.164)\n4.5.1 Inverse Wishart distribution\nRecall that we showed (Exercise 2.10) that if \u03bb\u223cGa(a,b), then that1\n\u03bb\u223cIG(a,b). Similarly,\nif\u03a3\u22121\u223cWi(S,\u03bd)then\u03a3\u223cIW(S\u22121,\u03bd+D+1 ),w h e r eIWis theinverse Wishart, the\nmultidimensional generalization of the inverse Gamma. It is de\ufb01ned as follows, for \u03bd>D\u22121\nandS/follows0:\nIW(\u03a3|S,\u03bd)=1\nZIW|\u03a3|\u2212(\u03bd+D+1)/2exp\u23a8parenleftbigg\n\u22121\n2tr(S\u22121\u03a3\u22121)\u23a8parenrightbigg\n(4.165)\nZIW=|S|\u2212\u03bd/22\u03bdD/2\u0393D(\u03bd/2) (4.166)\nOne can show that the distribution has these properties\nmean=S\u22121\n\u03bd\u2212D\u22121,mode=S\u22121\n\u03bd+D+1(4.167)\nIfD=1, this reduces to the inverse Gamma:\nIW(\u03c32|S\u22121,\u03bd)=I G (\u03c32|\u03bd/2,S/2) (4.168)", "157": "4.6. Inferring the parameters of an MVN 127\n\u22124\u22122 024\u2212202\n\u22125 0 5\u2212505\n\u221210 0 10\u2212505\n\u22125 0 5\u22124\u22122024\n\u22125 0 5\u2212505\n\u22124\u22122 024\u2212202\n\u22122 02\u2212202\n\u22124\u22122 024\u2212202Wi(dof=3.0, S), E=[9.5, \u22120.1; \u22120.1, 1.9], \u03c1=\u22120.0\n\u221210 0 10\u2212505\n(a)0 5 10 15 2000.020.040.060.08\u03c32\n1\n0 1 2 3 400.10.20.30.4\u03c322\u22122 \u22121 0 1 200.20.40.60.8\u03c1(1,2)\n(b)\nFigure 4.16 Visualization of the Wishart distribution. Left: Some samples from the Wishart distribution,\n\u03a3\u223cWi(S,\u03bd),w h e r eS=[ 3.1653,\u22120.0262;\u22120.0262,0.6477]and\u03bd=3. Right: Plots of the marginals\n(which are Gamma), and the approximate (sample-based) marginal on the correlation coefficient. If \u03bd=3\nthere is a lot of uncertainty about the value of the correlation coefficient \u03c1(see the almost uniform\ndistribution on [\u22121,1]). The sampled matrices are highly variable, and some are nearly singular. As \u03bd\nincreases, the sampled matrices are more concentrated on the prior S. Figure generated by wiPlotDemo .\n4.5.2 Visualizing the Wishart distribution *\nSince the Wishart is a distribution over matrices, it is hard to plot as a density function. However,\nwe can easily sample from it, and in the 2d case, we can use the eigenvectors of the resultingmatrix to de\ufb01ne an ellipse, as explained in Section 4.1.2. See Figure 4.16 for some examples.\nFor higher dimensional matrices, we can plot marginals of the distribution. The diagonals of\na Wishart distributed matrix have Gamma distributions, so are easy to plot. It is hard in generalto work out the distribution of the off-diagonal elements, but we can sample matrices fromthe distribution, and then compute the distribution empirically. In particular, we can converteach sampled matrix to a correlation matrix, and thus compute a Monte Carlo approximation(Section 2.7) to the expected correlation coefficients:\nE[R\nij]\u22481\nSS\u23a8summationdisplay\ns=1R(\u03a3(s))ij (4.169)\nwhere\u03a3(s)\u223cWi(\u03a3,\u03bd)andR(\u03a3)converts matrix \u03a3into a correlation matrix:\nRij=\u03a3ij\u23a8radicalbig\n\u03a3ii\u03a3jj(4.170)\nWe can then use kernel density estimation (Section 14.7.2) to produce a smooth approximationto the univariate density E[R\nij]for plotting purposes. See Figure 4.16 for some examples.\n4.6 Inferring the parameters of an MVN\nSo far, we have discussed inference in a Gaussian assuming the parameters \u03b8=(\u03bc,\u03a3)are\nknown. We now discuss how to infer the parameters themselves. We will assume the data has", "158": "128 Chapter 4. Gaussian models\nthe form xi\u223cN(\u03bc,\u03a3)fori=1:N and is fully observed, so we have no missing data (see\nSection 11.6.1 for how to estimate parameters of an MVN in the presence of missing values). To\nsimplify the presentation, we derive the posterior in three parts: \ufb01rst we compute p(\u03bc|D,\u03a3);\nthen we compute p(\u03a3|D,\u03bc); \ufb01nally we compute the joint p(\u03bc,\u03a3|D).\n4.6.1 Posterior distribution of \u03bc\nWe have discussed how to compute the MLE for \u03bc; we now discuss how to compute its posterior,\nwhich is useful for modeling our uncertainty about its value.\nThe likelihood has the form\np(D|\u03bc)=N(x|\u03bc,1\nN\u03a3) (4.171)\nFor simplicity, we will use a conjugate prior, which in this case is a Gaussian. In particular, ifp(\u03bc)=N(\u03bc|m\n0,V0)then we can derive a Gaussian posterior for \u03bcbased on the results in\nSection 4.4.2.2. We get\np(\u03bc|D,\u03a3)=N (\u03bc|mN,VN) (4.172)\nV\u22121\nN=V\u22121\n0+N\u03a3\u22121(4.173)\nmN=VN(\u03a3\u22121(Nx)+V\u22121\n0m0) (4.174)\nThis is exactly the same process as inferring the location of an object based on noisy radar\u201cblips\u201d, except now we are inferring the mean of a distribution based on noisy samples. (To aBayesian, there is no difference between uncertainty about parameters and uncertainty aboutanything else.)\nWe can model an uninformative prior by setting V\n0=\u221eI. In this case we have p(\u03bc|D,\u03a3)=\nN(x,1\nN\u03a3), so the posterior mean is equal to the MLE. We also see that the posterior variance\ngoes down as 1/N, which is a standard result from frequentist statistics.\n4.6.2 Posterior distribution of \u03a3*\nWe now discuss how to compute p(\u03a3|D,\u03bc). The likelihood has the form\np(D|\u03bc,\u03a3)\u221d|\u03a3|\u2212N\n2exp\u23a8parenleftbigg\n\u22121\n2tr(S\u03bc\u03a3\u22121)\u23a8parenrightbigg\n(4.175)\nThe corresponding conjugate prior is known as the inverse Wishart distribution (Section 4.5.1).Recall that this has the following pdf:\nIW(\u03a3|S\n\u22121\n0,\u03bd0)\u221d|\u03a3|\u2212(\u03bd0+D+1)/2exp\u23a8parenleftbigg\n\u22121\n2tr(S0\u03a3\u22121)\u23a8parenrightbigg\n(4.176)\nHere\u03bd0>D\u22121is the degrees of freedom (dof), and S0is a symmetric pd matrix. We see\nthatS\u22121\n0plays the role of the prior scatter matrix, and N0/defines\u03bd0+D+1controls the strength\nof the prior, and hence plays a role analogous to the sample size N.", "159": "4.6. Inferring the parameters of an MVN 129\n0 5 10 15 20 2500.511.5eigenvalueN=100, D=50\n  \ntrue, k=10.00\nMLE, k=  71\nMAP, k=8.62\n0 5 10 15 20 2500.511.5eigenvalueN=50, D=50\n  \ntrue, k=10.00\nMLE, k=1.7e+17\nMAP, k=8.85\n0 5 10 15 20 2500.511.5eigenvalueN=25, D=50\n  \ntrue, k=10.00\nMLE, k=2.2e+18\nMAP, k=21.09\nFigure 4.17 Estimating a covariance matrix in D=5 0dimensions using N\u2208{100,50,25}samples.\nWe plot the eigenvalues in descending order for the true covariance matrix (solid black), the MLE (dotted\nblue) and the MAP estimate (dashed red), using Equation 4.184 with \u03bb=0.9. We also list the condition\nnumber of each matrix in the legend. Based on Figure 1 of (Schaefer and Strimmer 2005). Figure generatedbyshrinkcovDemo .\nMultiplying the likelihood and prior we \ufb01nd that the posterior is also inverse Wishart:\np(\u03a3|D,\u03bc)\u221d|\u03a3|\u2212N\n2exp\u23a8parenleftbigg\n\u22121\n2tr(\u03a3\u22121S\u03bc)\u23a8parenrightbigg\n|\u03a3|\u2212(\u03bd0+D+1)/2\nexp\u23a8parenleftbigg\n\u22121\n2tr(\u03a3\u22121S0)\u23a8parenrightbigg\n(4.177)\n=|\u03a3|\u2212N+(\u03bd0+D+1)\n2exp\u23a8parenleftbigg\n\u22121\n2tr\u23a8bracketleftbig\n\u03a3\u22121(S\u03bc+S0)\u23a8bracketrightbig\u23a8parenrightbigg\n(4.178)\n=I W (\u03a3|SN,\u03bdN) (4.179)\n\u03bdN=\u03bd0+N (4.180)\nS\u22121\nN=S0+S\u03bc (4.181)\nIn words, this says that the posterior strength \u03bdNis the prior strength \u03bd0plus the number of\nobservations N, and the posterior scatter matrix SNis the prior scatter matrix S0plus the data\nscatter matrix S\u03bc.\n4.6.2.1 MAP estimation\nWe see from Equation 4.7 that \u02c6\u03a3mleis a rank min(N,D )matrix. If N<D, this is not\nfull rank, and hence will be uninvertible. And even if N>D, it may be the case that \u02c6\u03a3is\nill-conditioned (meaning it is nearly singular).\nTo solve these problems, we can use the posterior mode (or mean). One can show (using\ntechniques analogous to the derivation of the MLE) that the MAP estimate is given by\n\u02c6\u03a3map=SN\n\u03bdN+D+1=S0+S\u03bc\nN0+N(4.182)\nIf we use an improper uniform prior, corresponding to N0=0andS0=0, we recover the\nMLE.", "160": "130 Chapter 4. Gaussian models\nLet us now consider the use of a proper informative prior, which is necessary whenever D/N\nis large (say bigger than 0.1). Let \u03bc=x,s oS\u03bc=Sx. Then we can rewrite the MAP estimate\nas a convex combination of the prior mode and the MLE. To see this, let \u03a30/definesS0\nN0be the prior\nmode. Then the posterior mode can be rewritten as\n\u02c6\u03a3map=S0+Sx\nN0+N=N0\nN0+NS0\nN0+N\nN0+NS\nN=\u03bb\u03a30+(1\u2212\u03bb)\u02c6\u03a3mle (4.183)\nwhere\u03bb=N0\nN0+N, controls the amount of shrinkage towards the prior.\nThis begs the question: where do the parameters of the prior come from? It is common to\nset\u03bbby cross validation. Alternatively, we can use the closed-form formula provided in (Ledoit\nand Wolf 2004b,a; Schaefer and Strimmer 2005), which is the optimal frequentist estimate if we\nuse squared loss. This is arguably not the most natural loss function for covariance matrices(because it ignores the postive de\ufb01nite constraint), but it results in a simple estimator, whichis implemented in the PMTK function shrinkcov . We discuss Bayesian ways of estimating \u03bb\nlater.\nAs for the prior covariance matrix, S\n0, it is common to use the following (data dependent)\nprior:S0=d i a g (\u02c6\u03a3mle). In this case, the MAP estimate is given by\n\u02c6\u03a3map(i,j)=\u23a8braceleftbigg\u02c6\u03a3mle(i,j) ifi=j\n(1\u2212\u03bb)\u02c6\u03a3mle(i,j)otherwise(4.184)\nThus we see that the diagonal entries are equal to their ML estimates, and the off diago-nal elements are \u201cshrunk\u201d somewhat towards 0. This technique is therefore called shrinkage\nestimation,o rregularized estimation.\nThe bene\ufb01ts of MAP estimation are illustrated in Figure 4.17. We consider \ufb01tting a 50 dimen-\nsional Gaussian to N= 100,N=5 0andN=2 5data points. We see that the MAP estimate\nis always well-conditioned, unlike the MLE. In particular, we see that the eigenvalue spectrum\nof the MAP estimate is much closer to that of the true matrix than the MLE\u2019s. The eigenvectors,however, are unaffected.\nThe importance of regularizing the estimate of \u03a3will become apparent in later chapters,\nwhen we consider \ufb01tting covariance matrices to high dimensional data.\n4.6.2.2 Univariate posterior\nIn the 1d case, the likelihood has the form\np(D|\u03c3\n2)\u221d(\u03c32)\u2212N/2exp\u23a8parenleftBigg\n\u22121\n2\u03c32N\u23a8summationdisplay\ni=1(xi\u2212\u03bc)2\u23a8parenrightBigg\n(4.185)\nThe standard conjugate prior is the inverse Gamma distribution, which is just the scalar versionof the inverse Wishart:\nIG(\u03c3\n2|a0,b0)\u221d(\u03c32)\u2212(a0+1)exp(\u2212b0\n\u03c32) (4.186)", "161": "4.6. Inferring the parameters of an MVN 131\n0 5 10 1500.050.10.150.20.250.30.35\n\u03c32prior = IW( \u03bd=0.001, S=0.001), true \u03c32=10.000\n  N=2\nN=5\nN=50\nN=100\nFigure 4.18 Sequential updating of the posterior for \u03c32starting from an uninformative prior. The data\nwas generated from a Gaussian with known mean \u03bc=5and unknown variance \u03c32=1 0. Figure generated\nbygaussSeqUpdateSigma1D .\nMultiplying the likelihood and the prior, we see that the posterior is also IG:\np(\u03c32|D)=I G ( \u03c32|aN,bN) (4.187)\naN=a0+N/2 (4.188)\nbN=b0+1\n2N\u23a8summationdisplay\ni=1(xi\u2212\u03bc)2(4.189)\nSee Figure 4.18 for an illustration.\nThe form of the posterior is not quite as pretty as the multivariate case, because of the\nfactors of1\n2. This arises because IW(\u03c32|s0,\u03bd0)=I G (\u03c32|s0\n2,\u03bd0\n2). Another problem with using\ntheIG(a0,b0)distribution is that the strength of the prior is encoded in both a0andb0.\nTo avoid both of these problems, it is common (in the statistics literature) to use an alternative\nparameterization of the IG distribution, known as the (scaled) inverse chi-squared distribution.\nThis is de\ufb01ned as follows:\n\u03c7\u22122(\u03c32|\u03bd0,\u03c32\n0)=I G (\u03c32|\u03bd0\n2,\u03bd0\u03c32\n0\n2)\u221d(\u03c32)\u2212\u03bd0/2\u22121exp(\u2212\u03bd0\u03c32\n0\n2\u03c32) (4.190)\nHere\u03bd0controls the strength of the prior, and \u03c32\n0encodes the value of the prior. With this\nprior, the posterior becomes\np(\u03c32|D,\u03bc)=\u03c7\u22122(\u03c32|\u03bdN,\u03c32\nN) (4.191)\n\u03bdN=\u03bd0+N (4.192)\n\u03c32\nN=\u03bd0\u03c32\n0+\u23a8summationtextN\ni=1(xi\u2212\u03bc)2\n\u03bdN(4.193)\nWe see that the posterior dof \u03bdNis the prior dof \u03bd0plusN, and the posterior sum of squares\n\u03bdN\u03c32\nNis the prior sum of squares \u03bd0\u03c32\n0plus the data sum of squares.\nWe can emulate an uninformative prior, p(\u03c32)\u221d\u03c3\u22122, by setting \u03bd0=0, which makes\nintuitive sense (since it corresponds to a zero virtual sample size).", "162": "132 Chapter 4. Gaussian models\n4.6.3 Posterior distribution of \u03bcand\u03a3*\nWe now discuss how to compute p(\u03bc,\u03a3|D). These results are a bit complex, but will prove\nuseful later on in this book. Feel free to skip this section on a \ufb01rst reading.\n4.6.3.1 Likelihood\nThe likelihood is given by\np(D|\u03bc,\u03a3)=( 2 \u03c0)\u2212ND/2|\u03a3|\u2212N\n2exp\u23a8parenleftBigg\n\u22121\n2N\u23a8summationdisplay\ni=1(xi\u2212\u03bc)T\u03a3\u22121(xi\u2212\u03bc)\u23a8parenrightBigg\n(4.194)\nNow one can show that\nN\u23a8summationdisplay\ni=1(xi\u2212\u03bc)T\u03a3\u22121(xi\u2212\u03bc)=t r ( \u03a3\u22121Sx)+N(x\u2212\u03bc)T\u03a3\u22121(x\u2212\u03bc) (4.195)\nHence we can rewrite the likelihood as follows:\np(D|\u03bc,\u03a3)=( 2 \u03c0)\u2212ND/2|\u03a3|\u2212N\n2exp\u23a8parenleftbigg\n\u2212N\n2(\u03bc\u2212x)T\u03a3\u22121(\u03bc\u2212x)\u23a8parenrightbigg\n(4.196)\nexp\u23a8parenleftbigg\n\u2212N\n2tr(\u03a3\u22121Sx)\u23a8parenrightbigg\n(4.197)\nWe will use this form below.\n4.6.3.2 Prior\nThe obvious prior to use is the following\np(\u03bc,\u03a3)=N(\u03bc|m0,V0)IW(\u03a3|S0,\u03bd0) (4.198)\nUnfortunately, this is not conjugate to the likelihood. To see why, note that \u03bcand\u03a3appear\ntogether in a non-factorized way in the likelihood; hence they will also be coupled together in\nthe posterior.\nThe above prior is sometimes called semi-conjugate orconditionally conjugate, since both\nconditionals, p(\u03bc|\u03a3)andp(\u03a3|\u03bc), are individually conjugate. To create a full conjugate prior,\nwe need to use a prior where \u03bcand\u03a3are dependent on each other. We will use a joint\ndistribution of the form\np(\u03bc,\u03a3)=p(\u03a3)p(\u03bc|\u03a3) (4.199)\nLooking at the form of the likelihood equation, Equation 4.197, we see that a natural conjugate", "163": "4.6. Inferring the parameters of an MVN 133\nprior has the form of a Normal-inverse-wishart orNIWdistribution, de\ufb01ned as follows:\nNIW(\u03bc,\u03a3|m0,\u03ba0,\u03bd0,S0)/defines (4.200)\nN(\u03bc|m0,1\n\u03ba0\u03a3)\u00d7IW(\u03a3|S0,\u03bd0) (4.201)\n=1\nZNIW|\u03a3|\u22121\n2exp\u23a8parenleftBig\n\u2212\u03ba0\n2(\u03bc\u2212m0)T\u03a3\u22121(\u03bc\u2212m0)\u23a8parenrightBig\n(4.202)\n\u00d7|\u03a3|\u2212\u03bd0+D+1\n2exp\u23a8parenleftbigg\n\u22121\n2tr(\u03a3\u22121S0)\u23a8parenrightbigg\n(4.203)\n=1\nZNIW|\u03a3|\u2212\u03bd0+D+2\n2 (4.204)\n\u00d7exp\u23a8parenleftbigg\n\u2212\u03ba0\n2(\u03bc\u2212m0)T\u03a3\u22121(\u03bc\u2212m0)\u22121\n2tr(\u03a3\u22121S0)\u23a8parenrightbigg\n(4.205)\nZNIW=2v0D/2\u0393D(\u03bd0/2)(2\u03c0/\u03ba0)D/2|S0|\u2212\u03bd0/2(4.206)\nwhere\u0393D(a)is the multivariate Gamma function.\nThe parameters of the NIW can be interpreted as follows: m0is our prior mean for \u03bc, and\n\u03ba0is how strongly we believe this prior; and S0is (proportional to) our prior mean for \u03a3, and\n\u03bd0is how strongly we believe this prior.3\nOne can show (Minka 2000f) that the (improper) uninformative prior has the form\nlim\nk\u21920N(\u03bc|m0,\u03a3/k)IW(\u03a3|S0,k)\u221d|2\u03c0\u03a3|\u22121\n2|\u03a3|\u2212(D+1)/2(4.207)\n\u221d|\u03a3|\u2212(D\n2+1)\u221dNIW(\u03bc,\u03a3|0,0,0,0I) (4.208)\nIn practice, it is often better to use a weakly informative data-dependent prior. A common\nchoice (see e.g., (Chipman et al. 2001, p81), (Fraley and Raftery 2007, p6)) is to use S0=\ndiag(S x)/N, and\u03bd0=D+2, to ensure E[\u03a3]=S0, and to set \u03bc0=xand\u03ba0to some small\nnumber, such as 0.01.\n3. Although this prior has four parameters, there are really only three free parameters, since our uncertainty in the\nmean is proportional to the variance. In particular, if we believe that the variance is large, then our uncertainty in \u03bc\nmust be large too. This makes sense intuitively, since if the data has large spread, it may be hard to pin down its mean.\nSee also Exercise 9.1, where we will see the three free parameters more explicitly. If we want separate \u201ccontrol\u201d over our\ncon\ufb01dence in \u03bcand\u03a3, we must use a semi-conjugate prior.", "164": "134 Chapter 4. Gaussian models\n4.6.3.3 Posterior\nThe posterior can be shown (Exercise 4.11) to be NIW with updated parameters:\np(\u03bc,\u03a3|D)=N I W ( \u03bc,\u03a3|mN,\u03baN,\u03bdN,SN) (4.209)\nmN=\u03ba0m0+Nx\n\u03baN=\u03ba0\n\u03ba0+Nm0+N\n\u03ba0+Nx (4.210)\n\u03baN=\u03ba0+N (4.211)\n\u03bdN=\u03bd0+N (4.212)\nSN=S0+Sx+\u03ba0N\n\u03ba0+N(x\u2212m0)(x\u2212m0)T(4.213)\n=S0+S+\u03ba0m0mT\n0\u2212\u03baNmNmTN(4.214)\nwhere we have de\ufb01ned S/defines\u23a8summationtextN\ni=1xixT\nias the uncentered sum-of-squares matrix (this is easier\nto update incrementally than the centered version).\nThis result is actually quite intuitive: the posterior mean is a convex combination of the prior\nmean and the MLE, with \u201cstrength\u201d \u03ba0+N; and the posterior scatter matrix SNis the prior\nscatter matrix S0plus the empirical scatter matrix Sxplus an extra term due to the uncertainty\nin the mean (which creates its own virtual scatter matrix).\n4.6.3.4 Posterior mode\nThe mode of the joint distribution has the following form:\nargmaxp(\u03bc,\u03a3|D)=(mN,SN\n\u03bdN+D+2) (4.215)\nIf we set\u03ba0=0, this reduces to\nargmaxp(\u03bc,\u03a3|D)=(x,S0+Sx\n\u03bd0+N+D+2) (4.216)\nThe corresponding estimate \u02c6\u03a3is almost the same as Equation 4.183, but differs by 1 in the\ndenominator, because this is the mode of the joint, not the mode of the marginal.\n4.6.3.5 Posterior marginals\nThe posterior marginal for \u03a3is simply\np(\u03a3|D)=\u23a8integraldisplay\np(\u03bc,\u03a3|D)d\u03bc=I W (\u03a3|SN,\u03bdN) (4.217)\nThe mode and mean of this marginal are given by\n\u02c6\u03a3map=SN\n\u03bdN+D+1,E[\u03a3]=SN\n\u03bdN\u2212D\u22121(4.218)\nOne can show that the posterior marginal for \u03bchas a multivariate Student T distribution:\np(\u03bc|D)=\u23a8integraldisplay\np(\u03bc,\u03a3|D)d\u03a3=T(\u03bc|mN,1\n\u03baN(\u03bdN\u2212D+1)SN,\u03bdN\u2212D+1)(4.219)\nThis follows from the fact that the Student distribution can be represented as a scaled mixture\nof Gaussians (see Equation 11.61).", "165": "4.6. Inferring the parameters of an MVN 135\nFigure 4.19 TheNI\u03c72(m0,\u03ba0,\u03bd0,\u03c32\n0)distribution. m0is the prior mean and \u03ba0is how strongly we\nbelieve this; \u03c32\n0is the prior variance and \u03bd0is how strongly we believe this. (a) m0=0,\u03ba0=1,\u03bd0=\n1,\u03c32\n0=1. Notice that the contour plot (underneath the surface) is shaped like a \u201csquashed egg\u201d. (b) We\nincrease the strength of our belief in the mean, so it gets narrower: m0=0,\u03ba0=5,\u03bd0=1,\u03c32\n0=1. (c)\nWe increase the strength of our belief in the variance, so it gets narrower: m0=0,\u03ba0=1,\u03bd0=5,\u03c32\n0=\n1. Figure generated by NIXdemo2 .\n4.6.3.6 Posterior predictive\nThe posterior predictive is given by\np(x|D)=p(x,D)\np(D)(4.220)\nso it can be easily evaluated in terms of a ratio of marginal likelihoods.\nIt turns out that this ratio has the form of a multivariate Student-T distribution:\np(x|D)=\u23a8integraldisplay\u23a8integraldisplay\nN(x|\u03bc,\u03a3)NIW(\u03bc,\u03a3|mN,\u03baN,\u03bdN,SN)d\u03bcd\u03a3 (4.221)\n=T(x|mN,\u03baN+1\n\u03baN(\u03bdN\u2212D+1)SN,\u03bdN\u2212D+1) (4.222)\nThe Student-T has wider tails than a Gaussian, which takes into account the fact that \u03a3is\nunknown. However, this rapidly becomes Gaussian-like.\n4.6.3.7 Posterior for scalar data\nWe now specialise the above results to the case where xiis 1d. These results are widely used\nin the statistics literature. As in Section 4.6.2.2, it is conventional not to use the normal inverse", "166": "136 Chapter 4. Gaussian models\nWishart, but to use the normal inverse chi-squared orNIXdistribution, de\ufb01ned by\nNI\u03c72(\u03bc,\u03c32|m0,\u03ba0,\u03bd0,\u03c32\n0)/definesN(\u03bc|m0,\u03c32/\u03ba0)\u03c7\u22122(\u03c32|\u03bd0,\u03c32\n0) (4.223)\n\u221d(1\n\u03c32)(\u03bd0+3)/2exp\u23a8parenleftbigg\n\u2212\u03bd0\u03c32\n0+\u03ba0(\u03bc\u2212m0)2\n2\u03c32\u23a8parenrightbigg\n(4.224)\nSee Figure 4.19 for some plots. Along the \u03bcaxis, the distribution is shaped like a Gaussian, and\nalong the \u03c32axis, the distribution is shaped like a \u03c7\u22122; the contours of the joint density have\na \u201csquashed egg\u201d appearance. Interestingly, we see that the contours for \u03bcare more peaked\nfor small values of \u03c32, which makes sense, since if the data is low variance, we will be able to\nestimate its mean more reliably.\nOne can show that the posterior is given by\np(\u03bc,\u03c32|D)=NI\u03c72(\u03bc,\u03c32|mN,\u03baN,\u03bdN,\u03c32\nN) (4.225)\nmN=\u03ba0m0+Nx\n\u03baN(4.226)\n\u03baN=\u03ba0+N (4.227)\n\u03bdN=\u03bd0+N (4.228)\n\u03bdN\u03c32\nN=\u03bd0\u03c32\n0+N\u23a8summationdisplay\ni=1(xi\u2212x)2+N\u03ba0\n\u03ba0+N(m0\u2212x)2(4.229)\nThe posterior marginal for \u03c32is just\np(\u03c32|D)=\u23a8integraldisplay\np(\u03bc,\u03c32|D)d\u03bc=\u03c7\u22122(\u03c32|\u03bdN,\u03c32\nN) (4.230)\nwith the posterior mean given by E\u23a8bracketleftbig\n\u03c32|D\u23a8bracketrightbig\n=\u03bdN\n\u03bdN\u22122\u03c32\nN.\nThe posterior marginal for \u03bchas a Student T distribution, which follows from the scale\nmixture representation of the student:\np(\u03bc|D)=\u23a8integraldisplay\np(\u03bc,\u03c32|D)d\u03c32=T(\u03bc|mN,\u03c32\nN/\u03baN,\u03bdN) (4.231)\nwith the posterior mean given by E[\u03bc|D]=mN.\nLet us see how these results look if we use the following uninformative prior:\np(\u03bc,\u03c32)\u221dp(\u03bc)p(\u03c32)\u221d\u03c3\u22122\u221dNI\u03c72(\u03bc,\u03c32|\u03bc0=0,\u03ba0=0,\u03bd0=\u22121,\u03c32\n0=0 ) (4.232)\nWith this prior, the posterior has the form\np(\u03bc,\u03c32|D)=NI\u03c72(\u03bc,\u03c32|mN=x,\u03baN=N,\u03bdN=N\u22121,\u03c32\nN=s2) (4.233)\nwhere\ns2/defines1\nN\u22121N\u23a8summationdisplay\ni=1(xi\u2212x)2=N\nN\u22121\u02c6\u03c32\nmle (4.234)\nis the the sample standard deviation. (In Section 6.4.2, we show that this is an unbiased\nestimate of the variance.) Hence the marginal posterior for the mean is given by\np(\u03bc|D)=T(\u03bc|x,s2\nN,N\u22121) (4.235)", "167": "4.6. Inferring the parameters of an MVN 137\nand the posterior variance of \u03bcis\nvar[\u03bc|D]=\u03bdN\n\u03bdN\u22122\u03c32\nN=N\u22121\nN\u22123s2\nN\u2192s2\nN(4.236)\nThe square root of this is called the standard error of the mean:\n\u23a8radicalbig\nvar[\u03bc|D]\u2248s\u221a\nN(4.237)\nThus an approximate 95% posterior credible interval for the mean is\nI.95(\u03bc|D)=x\u00b12s\u221a\nN(4.238)\n(Bayesian credible intervals are discussed in more detail in Section 5.2.2; they are contrasted\nwith frequentist con\ufb01dence intervals in Section 6.6.1.)\n4.6.3.8 Bayesian t-test\nSuppose we want to test the hypothesis that \u03bc/negationslash=\u03bc0for some known value \u03bc0(often 0), given\nvaluesxi\u223cN(\u03bc,\u03c32). This is called a two-sided, one-sample t-test. A simple way to perform\nsuch a test is just to check if \u03bc0\u2208I0.95(\u03bc|D). If it is not, then we can be 95% sure that\n\u03bc/negationslash=\u03bc0.4A more common scenario is when we want to test if two paired samples have\nthe same mean. More precisely, suppose yi\u223cN(\u03bc1,\u03c32)andzi\u223cN(\u03bc2,\u03c32). We want to\ndetermine if \u03bc=\u03bc1\u2212\u03bc2>0, usingxi=yi\u2212zias our data. We can evaluate this quantity\nas follows:\np(\u03bc>\u03bc0|D)=\u23a8integraldisplay\u221e\n\u03bc0p(\u03bc|D)d\u03bc (4.239)\nThis is called a one-sided, paired t-test. (For a similar approach to unpaired tests, comparing\nthe difference in binomial proportions, see Section 5.2.3.)\nTo calculate the posterior, we must specify a prior. Suppose we use an uninformative prior.\nAs we showed above, we \ufb01nd that the posterior marginal on \u03bchas the form\np(\u03bc|D)=T(\u03bc|x,s2\nN,N\u22121) (4.240)\nNow let us de\ufb01ne the following t statistic:\nt/definesx\u2212\u03bc0\ns/\u221a\nN(4.241)\nwhere the denominator is the standard error of the mean. We see that\np(\u03bc|D)=1\u2212FN\u22121(t) (4.242)\nwhereF\u03bd(t)is the cdf of the standard Student t distribution T(0,1,\u03bd).\n4. A more complex approach is to perform Bayesian model comparison. That is, we compute the Bayes factor (described\nin Section 5.3.3) p(D|H0)/p(D|H1),w h e r eH0is the point null hypothesis that \u03bc=\u03bc0, andH1is the alternative\nhypothesis that \u03bc/negationslash=\u03bc0. See (Gonen et al. 2005; Rouder et al. 2009) for details.", "168": "138 Chapter 4. Gaussian models\n4.6.3.9 Connection with frequentist statistics *\nIf we use an uninformative prior, it turns out that the above Bayesian analysis gives the same\nresult as derived using frequentist methods. (We discuss frequentist statistics in Chapter 6.)Speci\ufb01cally, from the above results, we see that\n\u03bc\u2212\nx\u23a8radicalbig\ns/N|D \u223ctN\u22121 (4.243)\nThis has the same form as the sampling distribution of the MLE:\n\u03bc\u2212X\u23a8radicalbig\ns/N|\u03bc\u223ctN\u22121 (4.244)\nThereasonisthattheStudentdistributionissymmetricinits\ufb01rsttwoarguments, so T(x|\u03bc,\u03c32,\u03bd)=\nT(\u03bc|x,\u03c32,\u03bd); hence statements about the posterior for \u03bchave the same form as statements\nabout the sampling distribution of x. Consequently, the (one-sided) p-value (de\ufb01ned in Sec-\ntion 6.6.2) returned by a frequentist test is the same as p(\u03bc>\u03bc0|D)returned by the Bayesian\nmethod. See bayesTtestDemo for an example.\nDespite the super\ufb01cial similarity, these two results have a different interpretation: in the\nBayesian approach, \u03bcis unknown and xis \ufb01xed, whereas in the frequentist approach, X\nis unknown and \u03bcis \ufb01xed. More equivalences between frequentist and Bayesian inference\nin simple models using uninformative priors can be found in (Box and Tiao 1973). See alsoSection 7.6.3.3.\n4.6.4 Sensor fusion with unknown precisions *\nIn this section, we apply the results in Section 4.6.3 to the problem of sensor fusion in thecase where the precision of each measurement device is unknown. This generalizes the resultsof Section 4.4.2.2, where the measurement model was assumed to be Gaussian with knownprecision. The unknown precision case turns out to give qualitatively different results, yieldinga potentially multi-modal posterior as we will see. Our presentation is based on (Minka 2001e).\nSuppose we want to pool data from multiple sources to estimate some quantity \u03bc\u2208R, but the\nreliability of the sources is unknown. Speci\ufb01cally, suppose we have two different measurementdevices,xandy, with different precisions: x\ni|\u03bc\u223cN(\u03bc,\u03bb\u22121\nx)andyi|\u03bc\u223cN(\u03bc,\u03bb\u22121\ny).W e\nmake two independent measurements with each device, which turn out to be\nx1=1.1,x2=1.9,y1=2.9,y2=4.1 (4.245)\nWe will use a non-informative prior for \u03bc,p(\u03bc)\u221d1, which we can emulate using an in\ufb01nitely\nbroad Gaussian, p(\u03bc)=N(\u03bc|m0=0,\u03bb\u22121\n0=\u221e). If the\u03bbxand\u03bbyterms were known, then\nthe posterior would be Gaussian:\np(\u03bc|D,\u03bbx,\u03bby)=N (\u03bc|mN,\u03bb\u22121\nN) (4.246)\n\u03bbN=\u03bb0+Nx\u03bbx+Ny\u03bby (4.247)\nmN=\u03bbxNxx+\u03bbyNyy\nNx\u03bbx+Ny\u03bby(4.248)", "169": "4.6. Inferring the parameters of an MVN 139\nwhereNx=2is the number of xmeasurements, Ny=2is the number of y measurements,\nx=1\nNx\u23a8summationtextNx\ni=1xi=1.5andy=1\nNy\u23a8summationtextNy\ni=1yi=3.5. This result follows because the posterior\nprecision is the sum of the measurement precisions, and the posterior mean is a weighted sum\nof the prior mean (which is 0) and the data means.\nHowever, the measurement precisions are not known. Initially we will estimate them by\nmaximum likelihood. The log-likelihood is given by\n/lscript(\u03bc,\u03bbx,\u03bby)=l o g \u03bbx\u2212\u03bbx\n2\u23a8summationdisplay\ni(xi\u2212\u03bc)2+log\u03bby\u2212\u03bby\n2\u23a8summationdisplay\ni(yi\u2212\u03bc)2(4.249)\nThe MLE is obtained by solving the following simultaneous equations:\n\u2202/lscript\n\u2202\u03bc=\u03bbxNx(x\u2212\u03bc)+\u03bbyNy(y\u2212\u03bc)=0 (4.250)\n\u2202/lscript\n\u2202\u03bbx=1\n\u03bbx\u22121\nNxNx\u23a8summationdisplay\ni=1(xi\u2212\u03bc)2=0 (4.251)\n\u2202/lscript\n\u2202\u03bby=1\n\u03bby\u22121\nNyNy\u23a8summationdisplay\ni=1(yi\u2212\u03bc)2=0 (4.252)\nThis gives\n\u02c6\u03bc=Nx\u02c6\u03bbxx+Ny\u02c6\u03bbyy\nNx\u02c6\u03bbx+Ny\u02c6\u03bby(4.253)\n1/\u02c6\u03bbx=1\nNx\u23a8summationdisplay\ni(xi\u2212\u02c6\u03bc)2(4.254)\n1/\u02c6\u03bby=1\nNy\u23a8summationdisplay\ni(yi\u2212\u02c6\u03bc)2(4.255)\nWe notice that the MLE for \u03bchas the same form as the posterior mean, mN.\nWecansolvetheseequationsby\ufb01xedpointiteration. Letusinitializebyestimating \u03bbx=1/s2\nx\nand\u03bby=1/s2y,w h e r es2x=1\nNx\u23a8summationtextNx\ni=1(xi\u2212x)2=0.16ands2y=1\nNy\u23a8summationtextNy\ni=1(yi\u2212y)2=0.36.\nUsing this, we get \u02c6\u03bc=2.1154,s o p(\u03bc|D,\u02c6\u03bbx,\u02c6\u03bby)=N(\u03bc|2.1154,0.0554).I fw en o wi t e r a t e ,\nwe converge to \u02c6\u03bbx=1/0.1662,\u02c6\u03bby=1/4.0509,p(\u03bc|D,\u02c6\u03bbx,\u02c6\u03bby)=N(\u03bc|1.5788,0.0798).\nThe plug-in approximation to the posterior is plotted in Figure 4.20(a). This weights each\nsensor according to its estimated precision. Since sensor ywas estimated to be much less\nreliable than sensor x,w eh a v eE\u23a8bracketleftBig\n\u03bc|D,\u02c6\u03bbx,\u02c6\u03bby\u23a8bracketrightBig\n\u2248x, so we effectively ignore the ysensor.\nNow we will adopt a Bayesian approach and integrate out the unknown precisions, rather\nthan trying to estimate them. That is, we compute\np(\u03bc|D)\u221dp(\u03bc)\u23a8bracketleftbigg\u23a8integraldisplay\np(Dx|\u03bc,\u03bbx)p(\u03bbx|\u03bc)d\u03bbx\u23a8bracketrightbigg\u23a8bracketleftbigg\u23a8integraldisplay\np(Dy|\u03bc,\u03bby)p(\u03bby|\u03bc)d\u03bby\u23a8bracketrightbigg\n(4.256)\nWe will use uninformative Jeffrey\u2019s priors, p(\u03bc)\u221d1,p(\u03bbx|\u03bc)\u221d1/\u03bbxandp(\u03bby|\u03bc)\u221d1/\u03bby.", "170": "140 Chapter 4. Gaussian models\nSince the xandyterms are symmetric, we will just focus on one of them. The key integral is\nI=\u23a8integraldisplay\np(Dx|\u03bc,\u03bbx)p(\u03bbx|\u03bc)d\u03bbx\u221d\u23a8integraldisplay\n\u03bb\u22121\nx(Nx\u03bbx)Nx/2(4.257)\nexp\u23a8parenleftbigg\n\u2212Nx\n2\u03bbx(x\u2212\u03bc)2\u2212Nx\n2s2\nx\u03bbx\u23a8parenrightbigg\nd\u03bbx(4.258)\nExploiting the fact that Nx=2this simpli\ufb01es to\nI=\u23a8integraldisplay\n\u03bb\u22121\nx\u03bb1xexp(\u2212\u03bbx[(x\u2212\u03bc)2+s2x])d\u03bbx (4.259)\nWe recognize this as proportional to the integral of an unnormalized Gamma density\nGa(\u03bb|a,b)\u221d\u03bba\u22121e\u2212\u03bbb(4.260)\nwherea=1andb=(x\u2212\u03bc)2+s2\nx. Hence the integral is proportional to the normalizing\nconstant of the Gamma distribution, \u0393(a)b\u2212a,s ow eg e t\nI\u221d\u23a8integraldisplay\np(Dx|\u03bc,\u03bbx)p(\u03bbx|\u03bc)d\u03bbx\u221d\u23a8parenleftbig\nx\u2212\u03bc)2+s2\nx\u23a8parenrightbig\u22121(4.261)\nand the posterior becomes\np(\u03bc|D)\u221d1\n(x\u2212\u03bc)2+s2x1\n(y\u2212\u03bc)2+s2y(4.262)\nThe exact posterior is plotted in Figure 4.20(b). We see that it has two modes, one near\nx=1.5and one near y=3.5. These correspond to the beliefs that the xsensor is more\nreliable than the yone, and vice versa. The weight of the \ufb01rst mode is larger, since the data\nfrom the xsensor agree more with each other, so it seems slightly more likely that the xsensor\nis the reliable one. (They obviously cannot both be reliable, since they disagree on the values\nthat they are reporting.) However, the Bayesian solution keeps open the possibility that the y\nsensor is the more reliable one; from two measurements, we cannot tell, and choosing just thexsensor, as the plug-in approximation does, results in over con\ufb01dence (a posterior that is too\nnarrow).\nExercises\nExercise 4.1 Uncorrelated does not imply independent\nLetX\u223cU(\u22121,1)andY=X2. Clearly Yis dependent on X(in fact,Yis uniquely determined\nbyX). However, show that \u03c1(X,Y)=0. Hint: if X\u223cU(a,b)thenE[X]=(a+b)/2and\nvar[X]=(b\u2212a)2/12.\nExercise 4.2 Uncorrelated and Gaussian does not imply independent unless jointlyGaussian\nLetX\u223cN(0,1)andY=WX,w h e r ep(W=\u22121) =p(W=1 )=0 .5. It is clear that XandYare\nnot independent, since Yis a function of X.\na. ShowY\u223cN(0,1).", "171": "4.6. Inferring the parameters of an MVN 141\n\u22122 \u22121 0 1 2 3 4 5 600.10.20.30.40.50.60.70.8\n(a)\u22122 \u22121 0 1 2 3 4 5 600.511.5\n(b)\nFigure 4.20 Posterior for \u03bc. (a) Plug-in approximation. (b) Exact posterior. Figure generated by\nsensorFusionUnknownPrec .\nb. Showcov[X,Y ]=0. ThusXandYare uncorrelated but dependent, even though they are Gaussian.\nHint: use the de\ufb01nition of covariance\ncov[X,Y ]=E[XY]\u2212E[X]E[Y] (4.263)\nand therule of iterated expectation\nE[XY]=E[E[XY|W]] (4.264)\nExercise 4.3 Correlation coefficient is between -1 and +1\nProve that \u22121\u2264\u03c1(X,Y)\u22641\nExercise 4.4 Correlation coefficient for linearly related variables is \u00b11\nShow that, if Y=aX+bfor some parameters a>0andb, then\u03c1(X,Y)=1. Similarly show that if\na<0, then\u03c1(X,Y)=\u22121.\nExercise 4.5 Normalization constant for a multidimensional Gaussian\nProve that the normalization constant for a d-dimensional Gaussian is given by\n(2\u03c0)d/2|\u03a3|1\n2=/integraldisplay\nexp(\u22121\n2(x\u2212\u03bc)T\u03a3\u22121(x\u2212\u03bc))dx (4.265)\nHint: diagonalize \u03a3and use the fact that |\u03a3|=/producttext\ni\u03bbito write the joint pdf as a product of done-\ndimensional Gaussians in a transformed coordinate system. (You will need the change of variables formula.)\nFinally, use the normalization constant for univariate Gaussians.\nExercise 4.6 Bivariate Gaussian\nLetx\u223cN(\u03bc,\u03a3)wherex\u2208R2and\n\u03a3=/parenleftbigg\u03c32\n1\u03c1\u03c31\u03c32\n\u03c1\u03c31\u03c32\u03c32\n2/parenrightbigg\n(4.266)\nwhere\u03c1is the correlation coefficient. Show that the pdf is given by\np(x1,x2)=1\n2\u03c0\u03c31\u03c32/radicalbig\n1\u2212\u03c12(4.267)\nexp/parenleftbigg\n\u22121\n2(1\u2212\u03c12)/parenleftbigg(x1\u2212\u03bc1)2\n\u03c32\n1+(x2\u2212\u03bc2)2\n\u03c32\n2\u22122\u03c1(x1\u2212\u03bc1)\n\u03c31(x2\u2212\u03bc2)\n\u03c32/parenrightbigg/parenrightbigg\n(4.268)", "172": "142 Chapter 4. Gaussian models\n60 65 70 75 8080100120140160180200220240260280\n12\n34\n56\n789\n1011\n1213\n141516\n17\n181920\n2122\n23\n2425\n26\n27\n28\n293031\n32333435\n36\n37\n383940\n41424344\n45\n46474849\n5051\n52\n53\n5455\n5657\n58\n5960\n61 62636465\n6667\n68\n69 70\n717273raw\n\u22123 \u22122 \u22121 0 1 2 3\u22123\u22122\u2212101234\n12\n34\n56\n789\n1011\n1213\n141516\n17\n181920\n2122\n23\n2425\n26\n27\n28\n293031\n32333435\n3637\n383940\n41424344\n45\n46474849\n5051\n52\n535455\n5657\n58\n596061 62636465\n6667\n68\n69 70717273standarized\n\u22123 \u22122 \u22121 0 1 2 3\u22123\u22122\u2212101234\n12\n34\n56\n789\n1011\n1213\n141516\n17\n181920\n2122\n23\n2425\n2627\n28\n293031\n32333435\n3637\n383940\n41424344\n45\n464748 49\n5051\n52\n535455\n5657\n58\n596061 6263 6465\n666768\n69 70717273whitened\nFigure 4.21 (a) Height/weight data for the men. (b) Standardized. (c) Whitened.\nExercise 4.7 Conditioning a bivariate Gaussian\nConsider a bivariate Gaussian distribution p(x1,x2)=N(x|\u03bc,\u03a3)where\n\u03a3=/parenleftbigg\u03c32\n1\u03c312\n\u03c321\u03c32\n2/parenrightbigg\n=\u03c31\u03c32/parenleftbigg\u03c31\n\u03c32\u03c1\n\u03c1\u03c32\n\u03c31/parenrightbigg\n(4.269)\nwhere the correlation coefficient is given by\n\u03c1/defines\u03c312\n\u03c31\u03c32(4.270)\na. What is P(X2|x1)? Simplify your answer by expressing it in terms of \u03c1,\u03c32,\u03c31,\u03bc1,\u03bc2andx1.\nb. Assume \u03c31=\u03c32=1. What is P(X2|x1)now?\nExercise 4.8 Whitening vs standardizing\na. Load the height/weight data using rawdata = dlmread(\u2019heightWeightData.txt\u2019) . The \ufb01rst col-\numn is the class label (1=male, 2=female), the second column is height, the third weight. Extract the\nheight/weight data corresponding to the males. Fit a 2d Gaussian to the male data, using the empiricalmean and covariance. Plot your Gaussian as an ellipse (use gaussPlot2d ), superimposing on your\nscatter plot. It should look like Figure 4.21(a), where have labeled each datapoint by its index. Turn inyour \ufb01gure and code.\nb.Standardizing the data means ensuring the empirical variance along each dimension is 1. This can be\ndone by computing\nxij\u2212xj\n\u03c3j,w h e r e\u03c3jis the empirical std of dimension j. Standardize the data and\nreplot. It should look like Figure 4.21(b). (Use axis(\u2019equal\u2019) .) Turn in your \ufb01gure and code.\nc.Whitening orsphereing the data means ensuring its empirical covariance matrix is proportional to\nI, so the data is uncorrelated and of equal variance along each dimension. This can be done by\ncomputing \u039b\u22121\n2UTxfor each data vector x,w h e r e Uare the eigenvectors and \u039bthe eigenvalues of\nX. Whiten the data and replot. It should look like Figure 4.21(c). Note that whitening rotates the data,\nso people move to counter-intuitive locations in the new coordinate system (see e.g., person 2, who\nmoves from the right hand side to the left).\nExercise 4.9 Sensor fusion with known variances in 1d\nSuppose we have two sensors with known (and different) variances v1andv2, but unknown (and the same)\nmean\u03bc. Suppose we observe n1observations y(1)\ni\u223cN(\u03bc,v1)from the \ufb01rst sensor and n2observations", "173": "4.6. Inferring the parameters of an MVN 143\ny(2)\ni\u223cN(\u03bc,v2)from the second sensor. (For example, suppose \u03bcis the true temperature outside,\nand sensor 1 is a precise (low variance) digital thermosensing device, and sensor 2 is an imprecise (high\nvariance) mercury thermometer.) Let Drepresent all the data from both sensors. What is the posterior\np(\u03bc|D), assuming a non-informative prior for \u03bc(which we can simulate using a Gaussian with a precision\nof 0)? Give an explicit expression for the posterior mean and variance.\nExercise 4.10 Derivation of information form formulae for marginalizing and conditioning\nDerive the information form results of Section 4.3.1.Exercise 4.11 Derivation of the NIW posterior\nDerive Equation 4.209. Hint: one can show that\nN(\nx\u2212\u03bc)(x\u2212\u03bc)T+\u03ba0(\u03bc\u2212m0)(\u03bc\u2212m0)T(4.271)\n=\u03baN(\u03bc\u2212mN)(\u03bc\u2212mN)T+\u03ba0N\n\u03baN(x\u2212m0)(x\u2212m0)T(4.272)\nThis is a matrix generalization of an operation called completing the square.5\nDerive the corresponding result for the normal-Wishart model.Exercise 4.12 BIC for Gaussians\n(Source: Jaakkola.)\nThe Bayesian information criterion (BIC) is a penalized log-likelihood function that can be used for model\nselection (see Section 5.3.2.4). It is de\ufb01ned as\nBIC=l o gp(D|\u02c6\u03b8\nML)\u2212d\n2log(N) (4.273)\nwheredis the number of free parameters in the model and Nis the number of samples. In this question,\nwe will see how to use this to choose between a full covariance Gaussian and a Gaussian with a diagonalcovariance. Obviously a full covariance Gaussian has higher likelihood, but it may not be \u201cworth\u201d the extraparameters if the improvement over a diagonal covariance matrix is too small. So we use the BIC score tochoose the model.\nFollowing Section 4.1.3, we can write\nlogp(D|\u02c6\u03a3,\u02c6\u03bc)=\u2212N\n2tr/parenleftBig\n\u02c6\u03a3\u22121\u02c6S/parenrightBig\n\u2212N\n2log(\u02c6|\u03a3|) (4.274)\n\u02c6S=1\nNN/summationdisplay\ni=1(xi\u2212x)(xi\u2212x)T(4.275)\nwhere\u02c6Sis the scatter matrix (empirical covariance), the trace of a matrix is the sum of its diagonals, and\nwe have used the trace trick.\na. Derive the BIC score for a Gaussian in Ddimensions with full covariance matrix. Simplify your answer\nas much as possible, exploiting the form of the MLE. Be sure to specify the number of free parameters\nd.\nb. Derive the BIC score for a Gaussian in Ddimensions with a diagonal covariance matrix. Be sure to\nspecify the number of free parameters d. Hint: for the digaonal case, the ML estimate of \u03a3is the same\nas\u02c6\u03a3MLexcept the off-diagonal terms are zero:\n\u02c6\u03a3diag=d i a g (\u02c6\u03a3ML(1,1),...,\u02c6\u03a3ML(D,D)) (4.276)\n5. In the scalar case, completing the square means rewriting c2x2+c1x+c0as\u2212a(x\u2212b)2+wwherea=\u2212c2,\nb=c1\n2c2andw=c2\n1\n4c2+c0.", "174": "144 Chapter 4. Gaussian models\nExercise 4.13 Gaussian posterior credible interval\n(Source: DeGroot.)\nLetX\u223cN(\u03bc,\u03c32=4 )where\u03bcis unknown but has prior \u03bc\u223cN(\u03bc0,\u03c32\n0=9 ). The posterior after\nseeingnsamples is \u03bc\u223cN(\u03bcn,\u03c32\nn). (This is called a credible interval, and is the Bayesian analog of a\ncon\ufb01dence interval.) How big does nhave to be to ensure\np(/lscript\u2264\u03bcn\u2264u|D)\u22650.95 (4.277)\nwhere(/lscript,u)is an interval (centered on \u03bcn) of width 1 and Dis the data. Hint: recall that 95% of the\nprobability mass of a Gaussian is within \u00b11.96\u03c3of the mean.\nExercise 4.14 MAP estimation for 1D Gaussians\n(Source: Jaakkola.)Consider samples x\n1,...,x nfrom a Gaussian random variable with known variance \u03c32and unknown\nmean\u03bc. We further assume a prior distribution (also Gaussian) over the mean, \u03bc\u223cN(m,s2), with \ufb01xed\nmeanmand \ufb01xed variance s2. Thus the only unknown is \u03bc.\na. Calculate the MAP estimate \u02c6\u03bcMAP. You can state the result without proof. Alternatively, with a lot\nmore work, you can compute derivatives of the log posterior, set to zero and solve.\nb. Show that as the number of samples nincrease, the MAP estimate converges to the maximum likelihood\nestimate.\nc. Suppose nis small and \ufb01xed. What does the MAP estimator converge to if we increase the prior\nvariances2?\nd. Suppose nis small and \ufb01xed. What does the MAP estimator converge to if we decrease the prior\nvariances2?\nExercise 4.15 Sequential (recursive) updating of \u02c6\u03a3\n(Source: (Duda et al. 2001, Q3.35,3.36).)The unbiased estimates for the covariance of a d-dimensional Gaussian based on nsamples is given by\n\u02c6\u03a3=C\nn=1\nn\u22121n/summationdisplay\ni=1(xi\u2212mn)(xi\u2212mn)T(4.278)\nIt is clear that it takes O(nd2)time to compute Cn. If the data points arrive one at a time, it is more\nefficient to incrementally update these estimates than to recompute from scratch.\na. Show that the covariance can be sequentially udpated as follows\nCn+1=n\u22121\nnCn+1\nn+1(xn+1\u2212mn)(xn+1\u2212mn)T(4.279)\nb. How much time does it take per sequential update? (Use big-O notation.)\nc. Show that we can sequentially update the precision matrix using\nC\u22121\nn+1=n\nn\u22121/bracketleftBigg\nC\u22121\nn\u2212C\u22121\nn(xn+1\u2212mn)(xn+1\u2212mn)TC\u22121\nn\nn2\u22121\nn+(xn+1\u2212mn)TC\u22121\nn(xn+1\u2212mn)/bracketrightBigg\n(4.280)\nHint: notice that the update to Cn+1consists of adding a rank-one matrix, namely uuT,w h e r e\nu=xn+1\u2212mn. Use the matrix inversion lemma for rank-one updates (Equation 4.111), which we\nrepeat here for convenience:\n(E+uvT)\u22121=E\u22121\u2212E\u22121uvTE\u22121\n1+vTE\u22121u(4.281)", "175": "4.6. Inferring the parameters of an MVN 145\nd. What is the time complexity per update?\nExercise 4.16 Likelihood ratio for Gaussians\nSource: Source: Alpaydin p103 ex 4. Consider a binary classi\ufb01er where the Kclass conditional densities\nare MVN p(x|y=j)=N(x|\u03bcj,\u03a3j). By Bayes rule, we have\nlogp(y=1|x)\np(y=0|x)=l o gp(x|y=1 )\np(x|y=0 )+logp(y=1 )\np(y=0 )(4.282)\nIn other words, the log posterior ratio is the log likelihood ratio plus the log prior ratio. For each of the 4\ncases in the table below, derive an expression for the log likelihood ratio logp(x|y=1)\np(x|y=0), simplifying as much\nas possible.\nForm of\u03a3j Cov Num parameters\nArbitrary \u03a3j Kd(d+1)/2\nShared \u03a3j=\u03a3 d(d+1)/2\nShared, axis-aligned \u03a3j=\u03a3with\u03a3ij=0fori/negationslash=jd\nShared, spherical \u03a3j=\u03c32I 1\nExercise 4.17 LDA/QDA on height/weight data\nThe function discrimAnalysisHeightWeightDemo \ufb01ts an LDA and QDA model to the height/weight\ndata. Compute the misclassi\ufb01cation rate of both of these models on the training set. Turn in your numbers\nand code.\nExercise 4.18 Naive Bayes with mixed features\nConsider a 3 class naive Bayes classi\ufb01er with one binary feature and one Gaussian feature:\ny\u223cMu(y|\u03c0,1),x1|y=c\u223cBer(x1|\u03b8c),x2|y=c\u223cN(x2|\u03bcc,\u03c32\nc) (4.283)\nLet the parameter vectors be as follows:\n\u03c0=( 0.5,0.25,0.25),\u03b8=( 0.5,0.5,0.5),\u03bc=(\u22121,0,1),\u03c32=( 1,1,1) (4.284)\na. Compute p(y|x1=0,x2=0 )(the result should be a vector of 3 numbers that sums to 1).\nb. Compute p(y|x1=0 ).\nc. Compute p(y|x2=0 ).\nd. Explain any interesting patterns you see in your results. Hint: look at the parameter vector \u03b8.\nExercise 4.19 Decision boundary for LDA with semi tied covariances\nConsider a generative classi\ufb01er with class conditional densities of the form N(x|\u03bcc,\u03a3c). I nL D A ,w e\nassume\u03a3c=\u03a3, and in QDA, each \u03a3cis arbitrary. Here we consider the 2 class case in which\n\u03a31=k\u03a30,f o rk>1. That is, the Gaussian ellipsoids have the same \u201cshape\u201d, but the one for class 1\nis \u201cwider\u201d. Derive an expression for p(y=1|x,\u03b8), simplifying as much as possible. Give a geometric\ninterpretation of your result, if possible.Exercise 4.20 Logistic regression vs LDA/QDA\n(Source: Jaakkola.) Suppose we train the following binary classi\ufb01ers via maximum likelihood.\na. GaussI: A generative classi\ufb01er, where the class conditional densities are Gaussian, with both covariance\nmatrices set to I(identity matrix), i.e., p(x|y=c)=N(x|\u03bc\nc,I). We assume p(y)is uniform.\nb. GaussX: as for GaussI, but the covariance matrices are unconstrained, i.e., p(x|y=c)=N(x|\u03bcc,\u03a3c).", "176": "146 Chapter 4. Gaussian models\nc. LinLog: A logistic regression model with linear features.\nd. QuadLog: A logistic regression model, using linear and quadratic features (i.e., polynomial basis function\nexpansion of degree 2).\nAfter training we compute the performance of each model Mon the training set as follows:\nL(M)=1\nnn/summationdisplay\ni=1logp(yi|xi,\u02c6\u03b8,M) (4.285)\n(Note that this is the conditional log-likelihood p(y|x,\u02c6\u03b8)and not the joint log-likelihood p(y,x|\u02c6\u03b8).) We\nnow want to compare the performance of each model. We will write L(M)\u2264L(M/prime)if modelMmust\nhave lower (or equal) log likelihood (on the training set) than M/prime, for any training set (in other words, Mis\nworse than M/prime, at least as far as training set logprob is concerned). For each of the following model pairs,\nstate whether L(M)\u2264L(M/prime),L(M)\u2265L(M/prime), or whether no such statement can be made (i.e., M\nmight sometimes be better than M/primeand sometimes worse); also, for each question, brie\ufb02y (1-2 sentences)\nexplain why.\na. GaussI, LinLog.\nb. GaussX, QuadLog.\nc. LinLog, QuadLog.\nd. GaussI, QuadLog.\ne. Now suppose we measure performance in terms of the average misclassi\ufb01cation rate on the training\nset:\nR(M)=1\nnn/summationdisplay\ni=1I(yi/negationslash=\u02c6y(xi)) (4.286)\nIs it true in general that L(M)>L(M/prime)implies that R(M)<R(M/prime)? Explain why or why not.\nExercise 4.21 Gaussian decision boundaries\n(Source: (Duda et al. 2001, Q3.7).) Let p(x|y=j)=N(x|\u03bcj,\u03c3j)wherej=1,2and\u03bc1=0,\u03c32\n1=\n1,\u03bc2=1,\u03c32\n2=1 06. Let the class priors be equal, p(y=1 )=p (y=2 )=0 .5.\na. Find the decision region\nR1={x:p(x|\u03bc1,\u03c31)\u2265p(x|\u03bc2,\u03c32)} (4.287)\nSketch the result. Hint: draw the curves and \ufb01nd where they intersect. Find bothsolutions of the\nequation\np(x|\u03bc1,\u03c31)=p(x|\u03bc2,\u03c32) (4.288)\nHint: recall that to solve a quadratic equation ax2+bx+c=0,w eu s e\nx=\u2212b\u00b1\u221a\nb2\u22124ac\n2a(4.289)\nb. Now suppose \u03c32=1(and all other parameters remain the same). What is R1in this case?", "177": "4.6. Inferring the parameters of an MVN 147\nExercise 4.22 QDA with 3 classes\nConsider a three category classi\ufb01cation problem. Let the prior probabilites:\nP(Y=1 )=P (Y=2 )=P (Y=3 )=1 /3 (4.290)\nThe class-conditional densities are multivariate normal densities with parameters:\n\u03bc1=[ 0,0]T,\u03bc2=[ 1,1]T,\u03bc3=[\u22121,1]T(4.291)\n\u03a31=/bracketleftbigg0.70\n00.7/bracketrightbigg\n,\u03a32=/bracketleftbigg0.80.2\n0.20.8/bracketrightbigg\n,\u03a33=/bracketleftbigg0.80.2\n0.20.8/bracketrightbigg\n(4.292)\nClassify the following points:\na.x=[\u22120.5,0.5]\nb.x=[ 0.5,0.5]\nExercise 4.23 Scalar QDA\n[Note: you can solve this exercise by hand or using a computer (matlab, R, whatever). In either case, show\nyour work.] Consider the following training set of heights x(in inches) and gender y(male/female) of some\nUS college students: x=( 6 7,79,71,68,67,60),y=(m,m,m,f,f,f ).\na. Fit a Bayes classi\ufb01er to this data, using maximum likelihood estimation, i.e., estimate the parameters of\nthe class conditional likelihoods\np(x|y=c)=N(x;\u03bcc,\u03c3c) (4.293)\nand the class prior\np(y=c)=\u03c0c (4.294)\nWhat are your values of \u03bcc,\u03c3c,\u03c0cforc=m,f? Show your work (so you can get partial credit if you\nmake an arithmetic error).\nb. Compute p(y=m|x,\u02c6\u03b8),w h e r ex=7 2, and\u02c6\u03b8are the MLE parameters. (This is called a plug-in\nprediction.)\nc. What would be a simple way to extend this technique if you had multiple attributes per person, such\nas height and weight? Write down your proposed model as an equation.", "178": "", "179": "5 Bayesian statistics\n5.1 Introduction\nWe have now seen a variety of different probability models, and we have discussed how to\n\ufb01t them to data, i.e., we have discussed how to compute MAP parameter estimates \u02c6\u03b8=\nargmaxp(\u03b8|D), using a variety of different priors. We have also discussed how to compute\nthe full posterior p(\u03b8|D), as well as the posterior predictive density, p(x|D), for certain special\ncases (and in later chapters, we will discuss algorithms for the general case).\nUsing the posterior distribution to summarize everything we know about a set of unknown\nvariables is at the core of Bayesian statistics. In this chapter, we discuss this approach to\nstatistics in more detail. In Chapter 6, we discuss an alternative approach to statistics known asfrequentist or classical statistics.\n5.2 Summarizing posterior distributions\nThe posterior p(\u03b8|D)summarizes everything we know about the unknown quantities \u03b8. In this\nsection, we discuss some simple quantities that can be derived from a probability distribution,such as a posterior. These summary statistics are often easier to understand and visualize thanthe full joint.\n5.2.1 MAP estimation\nWe can easily compute a point estimate of an unknown quantity by computing the posterior\nmean, median or mode. In Section 5.7, we discuss how to use decision theory to choose betweenthese methods. Typically the posterior mean or median is the most appropriate choice for a real-valued quantity, and the vector of posterior marginals is the best choice for a discrete quantity.However, the posterior mode, aka the MAP estimate, is the most popular choice because itreduces to an optimization problem, for which efficient algorithms often exist. Futhermore, MAPestimation can be interpreted in non-Bayesian terms, by thinking of the log prior as a regularizer(see Section 6.5 for more details).\nAlthough this approach is computationally appealing, it is important to point out that there\nare various drawbacks to MAP estimation, which we brie\ufb02y discuss below. This will providemotivation for the more thoroughly Bayesian approach which we will study later in this chapter(and elsewhere in this book).", "180": "150 Chapter 5. Bayesian statistics\n\u22122 \u22121 0 1 2 3 400.511.522.533.544.5\n(a)1 2 3 4 5 6 70.10.20.30.40.50.60.70.80.9\n(b)\nFigure 5.1 (a) A bimodal distribution in which the mode is very untypical of the distribution. The thin\nblue vertical line is the mean, which is arguably a better summary of the distribution, since it is near the\nmajority of the probability mass. Figure generated by bimodalDemo . (b) A skewed distribution in which\nthe mode is quite different from the mean. Figure generated by gammaPlotDemo .\n5.2.1.1 No measure of uncertainty\nThe most obvious drawback of MAP estimation, and indeed of any other point estimate such\nas the posterior mean or median, is that it does not provide any measure of uncertainty. In\nmany applications, it is important to know how much one can trust a given estimate. We canderive such con\ufb01dence measures from the posterior, as we discuss in Section 5.2.2.\n5.2.1.2 Plugging in the MAP estimate can result in over\ufb01tting\nIn machine learning, we often care more about predictive accuracy than in interpreting theparameters of our models. However, if we don\u2019t model the uncertainty in our parameters, thenour predictive distribution will be overcon\ufb01dent. We saw several examples of this in Chapter 3,and we will see more examples later. Overcon\ufb01dence in predictions is particularly problematicin situations where we may be risk averse; see Section 5.7 for details.\n5.2.1.3 The mode is an untypical point\nChoosing the mode as a summary of a posterior distribution is often a very poor choice, sincethe mode is usually quite untypical of the distribution, unlike the mean or median. This isillustrated in Figure 5.1(a) for a 1d continuous space. The basic problem is that the mode is apoint of measure zero, whereas the mean and median take the volume of the space into account.Another example is shown in Figure 5.1(b): here the mode is 0, but the mean is non-zero. Suchskewed distributions often arise when inferring variance parameters, especially in hierarchicalmodels. In such cases the MAP estimate (and hence the MLE) is obviously a very bad estimate.\nHow should we summarize a posterior if the mode is not a good choice? The answer is to\nuse decision theory, which we discuss in Section 5.7. The basic idea is to specify a loss function,whereL(\u03b8,\u02c6\u03b8)is the loss you incur if the truth is \u03b8and your estimate is \u02c6\u03b8. If we use 0-1 loss,\nL(\u03b8,\u02c6\u03b8)=I(\u03b8/negationslash=\u02c6\u03b8), then the optimal estimate is the posterior mode. 0-1 loss means you only\nget \u201cpoints\u201d if you make no errors, otherwise you get nothing: there is no \u201cpartial credit\u201d under", "181": "5.2. Summarizing posterior distributions 151\n0 2 4 6 8 10 1200.10.20.30.40.50.60.70.80.91\npXpYg\nFigure 5.2 Example of the transformation of a density under a nonlinear transform. Note how the mode\nof the transformed distribution is not the transform of the original mode. Based on Exercise 1.4 of (Bishop\n2006b). Figure generated by bayesChangeOfVar .\nthis loss function! For continuous-valued quantities, we often prefer to use squared error loss,\nL(\u03b8,\u02c6\u03b8)=(\u03b8\u2212\u02c6\u03b8)2; the corresponding optimal estimator is then the posterior mean, as we show\nin Section 5.7. Or we can use a more robust loss function, L(\u03b8,\u02c6\u03b8)=|\u03b8\u2212\u02c6\u03b8|, which gives rise to\nthe posterior median.\n5.2.1.4 MAP estimation is not invariant to reparameterization *\nA more subtle problem with MAP estimation is that the result we get depends on how we pa-rameterize the probability distribution. Changing from one representation to another equivalentrepresentation changes the result, which is not very desirable, since the units of measurementare arbitrary (e.g., when measuring distance, we can use centimetres or inches).\nTo understand the problem, suppose we compute the posterior for x. If we de\ufb01ne y=f(x),\nthe distribution for yis given by Equation 2.87, which we repeat here for convenience:\np\ny(y)=px(x)\u23a8vextendsingle\u23a8vextendsingledx\ndy\u23a8vextendsingle\u23a8vextendsingle (5.1)\nThe|dx\ndy|term is called the Jacobian, and it measures the change in size of a unit volume passed\nthroughf.L e t\u02c6x= argmaxxpx(x)be the MAP estimate for x. In general it is not the case\nthat\u02c6y= argmaxypy(y)is given by f(\u02c6x). For example, let x\u223cN(6,1)andy=f(x),w h e r e\nf(x)=1\n1+exp(\u2212x+5)(5.2)\nWe can derive the distribution of yusing Monte Carlo simulation (see Section 2.7.1). The result\nis shown in Figure 5.2. We see that the original Gaussian has become \u201csquashed\u201d by the sigmoid\nnonlinearity. In particular, we see that the mode of the transformed distribution is not equal tothe transform of the original mode.", "182": "152 Chapter 5. Bayesian statistics\nTo see how this problem arises in the context of MAP estimation, consider the following\nexample, due to Michael Jordan. The Bernoulli distribution is typically parameterized by its\nmean\u03bc,s op(y=1|\u03bc)=\u03bc,w h e r ey\u2208{0,1}. Suppose we have a uniform prior on the\nunit interval: p\u03bc(\u03bc)=1I(0\u2264\u03bc\u22641). If there is no data, the MAP estimate is just the\nmode of the prior, which can be anywhere between 0 and 1. We will now show that differentparameterizations can pick different points in this interval arbitrarily.\nFirst let\u03b8=\u221a\n\u03bcso\u03bc=\u03b82. The new prior is\np\u03b8(\u03b8)=p\u03bc(\u03bc)\u23a8vextendsingle\u23a8vextendsingled\u03bc\nd\u03b8\u23a8vextendsingle\u23a8vextendsingle=2\u03b8 (5.3)\nfor\u03b8\u2208[0,1]so the new mode is\n\u02c6\u03b8\nMAP=a r gm a x\n\u03b8\u2208[0,1]2\u03b8=1 (5.4)\nNow let\u03c6=1\u2212\u221a1\u2212\u03bc. The new prior is\np\u03c6(\u03c6)=p\u03bc(\u03bc)\u23a8vextendsingle\u23a8vextendsingled\u03bc\nd\u03c6\u23a8vextendsingle\u23a8vextendsingle=2 ( 1\u2212\u03c6) (5.5)\nfor\u03c6\u2208[0,1], so the new mode is\n\u02c6\u03c6\nMAP=a r gm a x\n\u03c6\u2208[0,1]2\u22122\u03c6=0 (5.6)\nThus the MAP estimate depends on the parameterization. The MLE does not suffer from this\nsince the likelihood is a function, not a probability density. Bayesian inference does not sufferfrom this problem either, since the change of measure is taken into account when integratingover the parameter space.\nOne solution to the problem is to optimize the following objective function:\n\u02c6\u03b8=a r g m a x\n\u03b8p(D|\u03b8)p(\u03b8)|I(\u03b8)|\u22121\n2 (5.7)\nHereI(\u03b8)is the Fisher information matrix associated with p(x|\u03b8)(see Section 6.2.2). This\nestimate is parameterization independent, for reasons explained in (Jermyn 2005; Druilhet andMarin 2007). Unfortunately, optimizing Equation 5.7 is often difficult, which minimizes theappeal of the whole approach.\n5.2.2 Credible intervals\nIn addition to point estimates, we often want a measure of con\ufb01dence. A standard measure ofcon\ufb01dence in some (scalar) quantity \u03b8is the \u201cwidth\u201d of its posterior distribution. This can be\nmeasured using a 100(1\u2212\u03b1)%credible interval, which is a (contiguous) region C=(/lscript,u)\n(standing for lower and upper) which contains 1\u2212\u03b1of the posterior probability mass, i.e.,\nC\n\u03b1(D)=(/lscript,u):P(/lscript\u2264\u03b8\u2264u|D)=1\u2212\u03b1 (5.8)\nThere may be many such intervals, so we choose one such that there is (1\u2212\u03b1)/2mass in each\ntail; this is called a central interval.", "183": "5.2. Summarizing posterior distributions 153\n0 0.2 0.4 0.6 0.8 100.511.522.533.5\n(a)0 0.2 0.4 0.6 0.8 100.511.522.533.5\n(b)\nFigure 5.3 (a) Central interval and (b) HPD region for a Beta(3,9) posterior. The CI is (0.06, 0.52) and the\nHPD is (0.04, 0.48). Based on Figure 3.6 of (Hoff 2009). Figure generated by betaHPD.\nIf the posterior has a known functional form, we can compute the posterior central interval\nusing/lscript=F\u22121(\u03b1/2)andu=F\u22121(1\u2212\u03b1/2),w h e r eFis the cdf of the posterior. For example, if\nthe posterior is Gaussian, p(\u03b8|D)=N(0,1), and\u03b1=0.05, then we have /lscript=\u03a6 (\u03b1/2) =\u22121.96,\nandu=\u03a6 ( 1\u2212\u03b1/2) = 1.96,w h e r e \u03a6denotes the cdf of the Gaussian. This is illustrated in\nFigure 2.3(c). This justi\ufb01es the common practice of quoting a credible interval in the form of\n\u03bc\u00b12\u03c3,w h e r e\u03bcrepresents the posterior mean, \u03c3represents the posterior standard deviation,\nand 2 is a good approximation to 1.96.\nOf course, the posterior is not always Gaussian. For example, in our coin example, if we\nuse a uniform prior and we observe N1=4 7heads out of N= 100trials, then the posterior\nis a beta distribution, p(\u03b8|D) = Beta(48, 54). We \ufb01nd the 95% posterior credible interval is\n(0.3749,0.5673)(seebetaCredibleInt for the one line of Matlab code we used to compute\nthis).\nIf we don\u2019t know the functional form, but we can draw samples from the posterior, then we\ncan use a Monte Carlo approximation to the posterior quantiles: we simply sort the Ssamples,\nand \ufb01nd the one that occurs at location \u03b1/Salong the sorted list. As S\u2192\u221e, this converges\nto the true quantile. See mcQuantileDemo for a demo.\nPeople often confuse Bayesian credible intervals with frequentist con\ufb01dence intervals. How-\never, they are not the same thing, as we discuss in Section 6.6.1. In general, credible intervals areusually what people want to compute, but con\ufb01dence intervals are usually what they actuallycompute, because most people are taught frequentist statistics but not Bayesian statistics. Fortu-nately, the mechanics of computing a credible interval is just as easy as computing a con\ufb01denceinterval (see e.g., betaCredibleInt for how to do it in Matlab).\n5.2.2.1 Highest posterior density regions *\nA problem with central intervals is that there might be points outside the CI which have higherprobability density. This is illustrated in Figure 5.3(a), where we see that points outside theleft-most CI boundary have higher density than those just inside the right-most CI boundary.\nThis motivates an alternative quantity known as the highest posterior density orHPDregion.\nThis is de\ufb01ned as the (set of) most probable points that in total constitute 100(1\u2212\u03b1)% of the", "184": "154 Chapter 5. Bayesian statistics\n\u03b1/2 \u03b1/2\n(a)pMIN\n(b)\nFigure 5.4 (a) Central interval and (b) HPD region for a hypothetical multimodal posterior. Based on\nFigure 2.2 of (Gelman et al. 2004). Figure generated by postDensityIntervals .\nprobability mass. More formally, we \ufb01nd the threshold p\u2217on the pdf such that\n1\u2212\u03b1=\u23a8integraldisplay\n\u03b8:p(\u03b8|D)>p\u2217p(\u03b8|D)d\u03b8 (5.9)\nand then de\ufb01ne the HPD as\nC\u03b1(D)={\u03b8:p(\u03b8|D)\u2265p\u2217} (5.10)\nIn 1d, the HPD region is sometimes called a highest density interval orHDI. For example,\nFigure 5.3(b) shows the 95% HDI of a Beta(3,9)distribution, which is (0.04,0.48). We see that\nthis is narrower than the CI, even though it still contains 95% of the mass; furthermore, every\npoint inside of it has higher density than every point outside of it.\nFor a unimodal distribution, the HDI will be the narrowest interval around the mode contain-\ning 95% of the mass. To see this, imagine \u201cwater \ufb01lling\u201d in reverse, where we lower the leveluntil 95% of the mass is revealed, and only 5% is submerged. This gives a simple algorithm forcomputing HDIs in the 1d case: simply search over points such that the interval contains 95%of the mass and has minimal width. This can be done by 1d numerical optimization if we knowthe inverse CDF of the distribution, or by search over the sorted data points if we have a bag ofsamples (see betaHPD for a demo).\nIf the posterior is multimodal, the HDI may not even be a connected region: see Figure 5.4(b)\nfor an example. However, summarizing multimodal posteriors is always difficult.\n5.2.3 Inference for a difference in proportions\nSometimes we have multiple parameters, and we are interested in computing the posteriordistribution of some function of these parameters. For example, suppose you are about to buysomething from Amazon.com, and there are two sellers offering it for the same price. Seller 1has 90 positive reviews and 10 negative reviews. Seller 2 has 2 positive reviews and 0 negativereviews. Who should you buy from?\n1\n1. This example is from www.johndcook .com/blog/2011/09/27/bayesian-amazon . See also lingpipe-blog .c\nom/2009/10/13/bayesian-counterpart-to-fisher-exact-test-on-contingency-tables .", "185": "5.3. Bayesian model selection 155\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 102468101214\n  \np(\u03b81|data)\np(\u03b82|data)\n(a)\u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8 100.511.522.5\n\u03b4pdf\n(b)\nFigure 5.5 (a) Exact posteriors p(\u03b8i|Di). (b) Monte Carlo approximation to p(\u03b4|D). We use kernel density\nestimation to get a smooth plot. The vertical lines enclose the 95% central interval. Figure generated by\namazonSellerDemo ,\nOn the face of it, you should pick seller 2, but we cannot be very con\ufb01dent that seller 2 is\nbetter since it has had so few reviews. In this section, we sketch a Bayesian analysis of this\nproblem. Similar methodology can be used to compare rates or proportions across groups for avariety of other settings.\nLet\u03b8\n1and\u03b82be the unknown reliabilities of the two sellers. Since we don\u2019t know much\nabout them, we\u2019ll endow them both with uniform priors, \u03b8i\u223cBeta(1,1). The posteriors are\np(\u03b81|D1) = Beta(91, 11)andp(\u03b82|D2) = Beta(3 ,1).\nWe want to compute p(\u03b81>\u03b82|D). For convenience, let us de\ufb01ne \u03b4=\u03b81\u2212\u03b82as the\ndifference in the rates. (Alternatively we might want to work in terms of the log-odds ratio.) Wecan compute the desired quantity using numerical integration:\np(\u03b4>0|D)=\u23a8integraldisplay\n1\n0\u23a8integraldisplay1\n0I(\u03b81>\u03b82)Beta(\u03b81|y1+1,N1\u2212y1+1)\nBeta(\u03b82|y2+1,N2\u2212y2+1)d\u03b81d\u03b82 (5.11)\nWe \ufb01nd p(\u03b4>0|D)=0.710, which means you are better off buying from seller 1! See\namazonSellerDemo for the code. (It is also possible to solve the integral analytically (Cook\n2005).)\nA simpler way to solve the problem is to approximate the posterior p(\u03b4|D)by Monte Carlo\nsampling. This is easy, since \u03b81and\u03b82are independent in the posterior, and both have beta\ndistributions, which can be sampled from using standard methods. The distributions p(\u03b8i|Di)\nare shown in Figure 5.5(a), and a MC approximation to p(\u03b4|D), together with a 95% HPD, is\nshown Figure 5.5(b). An MC approximation to p(\u03b4>0|D)is obtained by counting the fraction\nof samples where \u03b81>\u03b82; this turns out to be 0.718, which is very close to the exact value. (See\namazonSellerDemo for the code.)\n5.3 Bayesian model selection\nIn Figure 1.18, we saw that using too high a degree polynomial results in over\ufb01tting, and usingtoo low a degree results in under\ufb01tting. Similarly, in Figure 7.8(a), we saw that using too small", "186": "156 Chapter 5. Bayesian statistics\na regularization parameter results in over\ufb01tting, and too large a value results in under\ufb01tting. In\ngeneral, when faced with a set of models (i.e., families of parametric distributions) of differentcomplexity, how should we choose the best one? This is called the model selection problem.\nOne approach is to use cross-validation to estimate the generalization error of all the candiate\nmodels, and then to pick the model that seems the best. However, this requires \ufb01tting eachmodelKtimes, where Kis the number of CV folds. A more efficient approach is to compute\nthe posterior over models,\np(m|D)=p(D|m)p(m)\n\u23a8summationtext\nm\u2208Mp(m,D)(5.12)\nFrom this, we can easily compute the MAP model, \u02c6m=a r g m a x p(m|D). This is called\nBayesian model selection.\nIf we use a uniform prior over models, p(m)\u221d1, this amounts to picking the model which\nmaximizes\np(D|m)=\u23a8integraldisplay\np(D|\u03b8)p(\u03b8|m)d\u03b8 (5.13)\nThis quantity is called the marginal likelihood, the integrated likelihood, or the evidence for\nmodelm. The details on how to perform this integral will be discussed in Section 5.3.2. But\n\ufb01rst we give an intuitive interpretation of what this quantity means.\n5.3.1 Bayesian Occam\u2019s razor\nOne might think that using p(D|m)to select models would always favor the model with the\nmost parameters. This is true if we use p(D|\u02c6\u03b8m)to select models, where \u02c6\u03b8mis the MLE or\nMAP estimate of the parameters for model m, because models with more parameters will \ufb01t the\ndata better, and hence achieve higher likelihood. However, if we integrate out the parameters,rather than maximizing them, we are automatically protected from over\ufb01tting: models withmore parameters do not necessarily have higher marginal likelihood. This is called the Bayesian\nOccam\u2019s razor effect (MacKay 1995b; Murray and Ghahramani 2005), named after the principle\nknown as Occam\u2019s razor, which says one should pick the simplest model that adequately\nexplains the data.\nOne way to understand the Bayesian Occam\u2019s razor is to notice that the marginal likelihood\ncan be rewritten as follows, based on the chain rule of probability (Equation 2.5):\np(D)=p(y\n1)p(y2|y1)p(y3|y1:2)...p(yN|y1:N\u22121) (5.14)\nwhere we have dropped the conditioning on xfor brevity. This is similar to a leave-one-out\ncross-validation estimate (Section 1.4.8) of the likelihood, since we predict each future point givenall the previous ones. (Of course, the order of the data does not matter in the above expression.)If a model is too complex, it will over\ufb01t the \u201cearly\u201d examples and will then predict the remainingones poorly.\nAnother way to understand the Bayesian Occam\u2019s razor effect is to note that probabilities must\nsum to one. Hence\u23a8summationtext\nD/primep(D/prime|m)=1, where the sum is over all possible data sets. Complex\nmodels, which can predict many things, must spread their probability mass thinly, and hencewill not obtain as large a probability for any given data set as simpler models. This is sometimes", "187": "5.3. Bayesian model selection 157\nFigure 5.6 A schematic illustration of the Bayesian Occam\u2019s razor. The broad (green) curve corresponds\nto a complex model, the narrow (blue) curve to a simple model, and the middle (red) curve is just right.\nBased on Figure 3.13 of (Bishop 2006a). See also (Murray and Ghahramani 2005, Figure 2) for a similar plot\nproduced on real data.\ncalled the conservation of probability mass principle, and is illustrated in Figure 5.6. On the\nhorizontal axis we plot all possible data sets in order of increasing complexity (measured in\nsome abstract sense). On the vertical axis we plot the predictions of 3 possible models: a simple\none,M1; a medium one, M2; and a complex one, M3. We also indicate the actually observed\ndataD0by a vertical line. Model 1 is too simple and assigns low probability to D0. Model 3\nalso assigns D0relatively low probability, because it can predict many data sets, and hence it\nspreads its probability quite widely and thinly. Model 2 is \u201cjust right\u201d: it predicts the observed\ndata with a reasonable degree of con\ufb01dence, but does not predict too many other things. Hence\nmodel 2 is the most probable model.\nAs a concrete example of the Bayesian Occam\u2019s razor, consider the data in Figure 5.7. We plot\npolynomials of degrees 1, 2 and 3 \ufb01t to N=5data points. It also shows the posterior over\nmodels, where we use a Gaussian prior (see Section 7.6 for details). There is not enough data\nto justify a complex model, so the MAP model is d=1. Figure 5.8 shows what happens when\nN=3 0. Now it is clear that d=2is the right model (the data was in fact generated from a\nquadratic).\nAs another example, Figure 7.8(c) plots logp(D|\u03bb)vslog(\u03bb), for the polynomial ridge regres-\nsion model, where \u03bbranges over the same set of values used in the CV experiment. We see\nthat the maximum evidence occurs at roughly the same point as the minimum of the test MSE,\nwhich also corresponds to the point chosen by CV.\nWhen using the Bayesian approach, we are not restricted to evaluating the evidence at a\n\ufb01nite grid of values. Instead, we can use numerical optimization to \ufb01nd \u03bb\u2217=a r g m a x\u03bbp(D|\u03bb).\nThis technique is called empirical Bayes ortype II maximum likelihood (see Section 5.6 for\ndetails). An example is shown in Figure 7.8(b): we see that the curve has a similar shape to the\nCV estimate, but it can be computed more efficiently.", "188": "158 Chapter 5. Bayesian statistics\n\u22122 0 2 4 6 8 10 12\u221220\u221210010203040506070d=1, logev=\u221218.593, EB\n(a)\u22122 0 2 4 6 8 10 12\u221280\u221260\u221240\u221220020406080d=2, logev=\u221220.218, EB\n(b)\n\u22122 0 2 4 6 8 10 12\u2212200\u2212150\u2212100\u221250050100150200250300d=3, logev=\u221221.718, EB\n(c)1 2 300.20.40.60.81\nMP(M|D)N=5, method=EB\n(d)\nFigure 5.7 (a-c) We plot polynomials of degrees 1, 2 and 3 \ufb01t to N=5data points using empirical\nBayes. The solid green curve is the true function, the dashed red curve is the prediction (dotted blue lines\nrepresent \u00b1\u03c3around the mean). (d) We plot the posterior over models, p(d|D), assuming a uniform prior\np(d)\u221d1. Based on a \ufb01gure by Zoubin Ghahramani. Figure generated by linregEbModelSelVsN .\n5.3.2 Computing the marginal likelihood (evidence)\nWhen discussing parameter inference for a \ufb01xed model, we often wrote\np(\u03b8|D,m)\u221dp(\u03b8|m)p(D|\u03b8,m) (5.15)\nthus ignoring the normalization constant p(D|m). This is valid since p(D|m)is constant wrt \u03b8.\nHowever, when comparing models, we need to know how to compute the marginal likelihood,\np(D|m). In general, this can be quite hard, since we have to integrate over all possible parameter\nvalues, but when we have a conjugate prior, it is easy to compute, as we now show.\nLetp(\u03b8)=q(\u03b8)/Z0be our prior, where q(\u03b8)is an unnormalized distribution, and Z0is\nthe normalization constant of the prior. Let p(D|\u03b8)=q(D|\u03b8)/Z/lscriptbe the likelihood, where Z/lscript\ncontains any constant factors in the likelihood. Finally let p(\u03b8|D)=q(\u03b8|D)/ZNbe our poste-", "189": "5.3. Bayesian model selection 159\n\u22122 0 2 4 6 8 10 12\u221210010203040506070d=1, logev=\u2212106.110, EB\n(a)\u22122 0 2 4 6 8 10 12\u22121001020304050607080d=2, logev=\u2212103.025, EB\n(b)\n\u22122 0 2 4 6 8 10 12\u221220020406080100d=3, logev=\u2212107.410, EB\n(c)1 2 300.20.40.60.81\nMP(M|D)N=30, method=EB\n(d)\nFigure 5.8 Same as Figure 5.7 except now N=3 0. Figure generated by linregEbModelSelVsN .\nrior, where q(\u03b8|D)=q(D|\u03b8)q(\u03b8)is the unnormalized posterior, and ZNis the normalization\nconstant of the posterior. We have\np(\u03b8|D)=p(D|\u03b8)p(\u03b8)\np(D)(5.16)\nq(\u03b8|D)\nZN=q(D|\u03b8)q(\u03b8)\nZ/lscriptZ0p(D)(5.17)\np(D)=ZN\nZ0Z/lscript(5.18)\nSo assuming the relevant normalization constants are tractable, we have an easy way to compute\nthe marginal likelihood. We give some examples below.", "190": "160 Chapter 5. Bayesian statistics\n5.3.2.1 Beta-binomial model\nLetusapplytheaboveresulttotheBeta-binomialmodel. Sinceweknow p(\u03b8|D)=B e t a ( \u03b8|a/prime,b/prime),\nwherea/prime=a+N1andb/prime=b+N0, we know the normalization constant of the posterior is\nB(a/prime,b/prime). Hence\np(\u03b8|D)=p(D|\u03b8)p(\u03b8)\np(D)(5.19)\n=1\np(D)\u23a8bracketleftbigg1\nB(a,b)\u03b8a\u22121(1\u2212\u03b8)b\u22121\u23a8bracketrightbigg\u23a8bracketleftbigg\u23a8parenleftbiggN\nN1\u23a8parenrightbigg\n\u03b8N1(1\u2212\u03b8)N0\u23a8bracketrightbigg\n(5.20)\n=\u23a8parenleftbiggN\nN1\u23a8parenrightbigg1\np(D)1\nB(a,b)\u23a8bracketleftbig\n\u03b8a+N 1\u22121(1\u2212\u03b8)b+N 0\u22121\u23a8bracketrightbig\n(5.21)\nSo\n1\nB(a+N1,b+N0)=\u23a8parenleftbiggN\nN1\u23a8parenrightbigg1\np(D)1\nB(a,b)(5.22)\np(D)=\u23a8parenleftbiggN\nN1\u23a8parenrightbiggB(a+N1,b+N0)\nB(a,b)(5.23)\nThe marginal likelihood for the Beta-Bernoulli model is the same as above, except it is missing\nthe\u23a8parenleftbigN\nN1\u23a8parenrightbig\nterm.\n5.3.2.2 Dirichlet-multinoulli model\nBy the same reasoning as the Beta-Bernoulli case, one can show that the marginal likelihood forthe Dirichlet-multinoulli model is given by\np(D)=B(N+\u03b1)\nB(\u03b1)(5.24)\nwhere\nB(\u03b1)=\u23a8producttextK\nk=1\u0393(\u03b1k)\n\u0393(\u23a8summationtext\nk\u03b1k)(5.25)\nHence we can rewrite the above result in the following form, which is what is usually presented\nin the literature:\np(D)=\u0393(\u23a8summationtext\nk\u03b1k)\n\u0393(N+\u23a8summationtext\nk\u03b1k)\u23a8productdisplay\nk\u0393(Nk+\u03b1k)\n\u0393(\u03b1k)(5.26)\nWe will see many applications of this equation later.\n5.3.2.3 Gaussian-Gaussian-Wishart model\nConsider the case of an MVN with a conjugate NIW prior. Let Z0be the normalizer for the\nprior,ZNbe normalizer for the posterior, and let Zl=( 2\u03c0)ND/2be the normalizer for the", "191": "5.3. Bayesian model selection 161\nlikelihood. Then it is easy to see that\np(D)=ZN\nZ0Zl(5.27)\n=1\n\u03c0ND/21\n2ND/2\u23a8parenleftBig\n2\u03c0\n\u03baN\u23a8parenrightBigD/2\n|SN|\u2212\u03bdN/22(\u03bd0+N)D/2\u0393D(\u03bdN/2)\n\u23a8parenleftBig\n2\u03c0\n\u03ba0\u23a8parenrightBigD/2\n|S0|\u2212\u03bd0/22\u03bd0D/2\u0393D(\u03bd0/2)(5.28)\n=1\n\u03c0ND/2\u23a8parenleftbigg\u03ba0\n\u03baN\u23a8parenrightbiggD/2|S0|\u03bd0/2\n|SN|\u03bdN/2\u0393D(\u03bdN/2)\n\u0393D(\u03bd0/2)(5.29)\nThis equation will prove useful later.\n5.3.2.4 BIC approximation to log marginal likelihood\nIn general, computing the integral in Equation 5.13 can be quite difficult. One simple but popular\napproximation is known as the Bayesian information criterion orBIC, which has the following\nform (Schwarz 1978):\nBIC/defineslogp(D|\u02c6\u03b8)\u2212dof(\u02c6\u03b8)\n2logN\u2248logp(D) (5.30)\nwheredof(\u02c6\u03b8)is the number of degrees of freedom in the model, and \u02c6\u03b8is the MLE for the\nmodel.2We see that this has the form of a penalized log likelihood, where the penalty term\ndepends on the model\u2019s complexity. See Section 8.4.2 for the derivation of the BIC score.\nAs an example, consider linear regression. As we show in Section 7.3, the MLE is given by \u02c6w=\n(XTX)\u22121XTyand\u02c6\u03c32=R S S/N,w h e r eRSS =\u23a8summationtextN\ni=1(yi\u2212\u02c6wT\nmlexi)2. The corresponding\nlog likelihood is given by\nlogp(D|\u02c6\u03b8)=\u2212N\n2log(2\u03c0\u02c6\u03c32)\u2212N\n2(5.31)\nHence the BIC score is as follows (dropping constant terms)\nBIC=\u2212N\n2log(\u02c6\u03c32)\u2212D\n2log(N) (5.32)\nwhereDis the number of variables in the model. In the statistics literature, it is common to\nuse an alternative de\ufb01nition of BIC, which we call the BIC cost(since we want to minimize it):\nBIC-cost /defines\u22122logp(D|\u02c6\u03b8)+dof(\u02c6\u03b8)logN\u2248\u22122logp(D) (5.33)\nIn the context of linear regression, this becomes\nBIC-cost=Nlog(\u02c6\u03c32)+Dlog(N) (5.34)\n2. Traditionally the BIC score is de\ufb01ned using the ML estimate \u02c6\u03b8, so it is independent of the prior. However, for models\nsuch as mixtures of Gaussians, the ML estimate can be poorly behaved, so it is better to evaluate the BIC score using\nthe MAP estimate, as in (Fraley and Raftery 2007).", "192": "162 Chapter 5. Bayesian statistics\nThe BIC method is very closely related to the minimum description length orMDLprinciple,\nwhich characterizes the score for a model in terms of how well it \ufb01ts the data, minus how\ncomplex the model is to de\ufb01ne. See (Hansen and Yu 2001) for details.\nThere is a very similar expression to BIC/ MDL called the Akaike information criterion or\nAIC, de\ufb01ned as\nAIC(m,D)/defineslogp(D|\u02c6\u03b8MLE)\u2212dof(m) (5.35)\nThis is derived from a frequentist framework, and cannot be interpreted as an approximationto the marginal likelihood. Nevertheless, the form of this expression is very similar to BIC. Wesee that the penalty for AIC is less than for BIC. This causes AIC to pick more complex models.However, this can result in better predictive accuracy. See e.g., (Clarke et al. 2009, sec 10.2) forfurther discussion on such information criteria.\n5.3.2.5 Effect of the prior\nSometimes it is not clear how to set the prior. When we are performing posterior inference, thedetails of the prior may not matter too much, since the likelihood often overwhelms the prioranyway. But when computing the marginal likelihood, the prior plays a much more importantrole, since we are averaging the likelihood over all possible parameter settings, as weighted bythe prior.\nIn Figures 5.7 and 5.8, where we demonstrated model selection for linear regression, we used\na prior of the form p(w)=N(0,\u03b1\n\u22121I).H e r e\u03b1is a tuning parameter that controls how strong\nthe prior is. This parameter can have a large effect, as we discuss in Section 7.5. Intuitively, if\u03b1is large, the weights are \u201cforced\u201d to be small, so we need to use a complex model with many\nsmall parameters (e.g., a high degree polynomial) to \ufb01t the data. Conversely, if \u03b1is small, we\nwill favor simpler models, since each parameter is \u201callowed\u201d to vary in magnitude by a lot.\nIf the prior is unknown, the correct Bayesian procedure is to put a prior on the prior. That is,\nwe should put a prior on the hyper-parameter \u03b1as well as the parametrs w. To compute the\nmarginal likelihood, we should integrate out all unknowns, i.e., we should compute\np(D|m)=\u23a8integraldisplay\u23a8integraldisplay\np(D|w)p(w|\u03b1,m)p(\u03b1|m)dwd\u03b1 (5.36)\nOf course, this requires specifying the hyper-prior. Fortunately, the higher up we go in theBayesian hierarchy, the less sensitive are the results to the prior settings. So we can usuallymake the hyper-prior uninformative.\nA computational shortcut is to optimize \u03b1rather than integrating it out. That is, we use\np(D|m)\u2248\u23a8integraldisplay\np(D|w)p(w|\u02c6\u03b1,m)dw (5.37)\nwhere\n\u02c6\u03b1=a r g m a x\n\u03b1p(D|\u03b1,m) = argmax\n\u03b1\u23a8integraldisplay\np(D|w)p(w|\u03b1,m)dw (5.38)\nThis approach is called empirical Bayes (EB), and is discussed in more detail in Section 5.6. Thisis the method used in Figures 5.7 and 5.8.", "193": "5.3. Bayesian model selection 163\nBayes factor BF(1,0) Interpretation\nBF <1\n100Decisive evidence for M0\nBF <1\n10Strong evidence for M0\n1\n10<BF<1\n3Moderate evidence for M0\n1\n3<BF<1 Weak evidence for M0\n1<BF<3 Weak evidence for M1\n3<BF<10 Moderate evidence for M1\nBF >10 Strong evidence for M1\nBF >100 Decisive evidence for M1\nTable 5.1 Jeffreys\u2019 scale of evidence for interpreting Bayes factors.\n5.3.3 Bayes factors\nSuppose our prior on models is uniform, p(m)\u221d1. Then model selection is equivalent to\npicking the model with the highest marginal likelihood. Now suppose we just have two models\nwe are considering, call them the null hypothesis, M0, and the alternative hypothesis, M1.\nDe\ufb01ne the Bayes factor as the ratio of marginal likelihoods:\nBF1,0/definesp(D|M1)\np(D|M0)=p(M1|D)\np(M0|D)/p(M1)\np(M0)(5.39)\n(This is like a likelihood ratio, except we integrate out the parameters, which allows us to\ncompare models of different complexity.) If BF1,0>1then we prefer model 1, otherwise we\nprefer model 0.\nOf course, it might be that BF1,0is only slightly greater than 1. In that case, we are not\nvery con\ufb01dent that model 1 is better. Jeffreys (1961) proposed a scale of evidence for interpretingthe magnitude of a Bayes factor, which is shown in Table 5.1. This is a Bayesian alternative tothe frequentist concept of a p-value.\n3Alternatively, we can just convert the Bayes factor to a\nposterior over models. If p(M1)=p(M0)=0.5,w eh a v e\np(M0|D)=BF0,1\n1+BF0,1=1\nBF1,0+1(5.40)\n5.3.3.1 Example: Testing if a coin is fair\nSuppose we observe some coin tosses, and want to decide if the data was generated by a faircoin,\u03b8=0.5, or a potentially biased coin, where \u03b8could be any value in [0,1]. Let us denote\nthe \ufb01rst model by M\n0and the second model by M1. The marginal likelihood under M0is\nsimply\np(D|M0)=\u23a8parenleftbigg1\n2\u23a8parenrightbiggN\n(5.41)\n3. Ap-value, is de\ufb01ned as the probability (under the null hypothesis) of observing some test statistic f(D)(such as the\nchi-squared statistic) that is as large or largerthan that actually observed, i.e., pvalue(D)/definesP(f(\u02dcD)\u2265f(D)|\u02dcD\u223c\nH0). Note that has almost nothing to do with what we really want to know, which is p(H0|D).", "194": "164 Chapter 5. Bayesian statistics\n01111122222222223333333333444445\u22121.8\u22121.6\u22121.4\u22121.2\u22121\u22120.8\u22120.6\u22120.4log10 p(D|M1)\n(a)01111122222222223333333333444445\u22122.5\u22122.45\u22122.4\u22122.35\u22122.3\u22122.25\u22122.2\u22122.15\u22122.1\u22122.05\u22122BIC approximation to log10 p(D|M1)\n(b)\nFigure 5.9 (a) Log marginal likelihood for the coins example. (b) BIC approximation. Figure generated by\ncoinsModelSelDemo .\nwhereNis the number of coin tosses. The marginal likelihood under M1, using a Beta prior, is\np(D|M1)=\u23a8integraldisplay\np(D|\u03b8)p(\u03b8)d\u03b8=B(\u03b11+N1,\u03b10+N0)\nB(\u03b11,\u03b10)(5.42)\nWe plotlogp(D|M1)vs the number of heads N1in Figure 5.9(a), assuming N=5and\n\u03b11=\u03b10=1. (The shape of the curve is not very sensitive to \u03b11and\u03b10, as long as \u03b10=\u03b11.)\nIf we observe 2 or 3 heads, the unbiased coin hypothesis M0is more likely than M1, sinceM0\nis a simpler model (it has no free parameters) \u2014 it would be a suspicious coincidence if the\ncoin were biased but happened to produce almost exactly 50/50 heads/tails. However, as thecounts become more extreme, we favor the biased coin hypothesis. Note that, if we plot the logBayes factor, logBF\n1,0, it will have exactly the same shape, since logp(D|M0)is a constant.\nSee also Exercise 3.18.\nIn Figure 5.9(b) shows the BIC approximation to logp(D|M1)for our biased coin example\nfrom Section 5.3.3.1. We see that the curve has approximately the same shape as the exact logmarginal likelihood, which is all that matters for model selection purposes, since the absolutescale is irrelevant. In particular, it favors the simpler model unless the data is overwhelminglyin support of the more complex model.\n5.3.4 Jeffreys-Lindley paradox *\nProblems can arise when we use improper priors (i.e., priors that do not integrate to 1) for modelselection/ hypothesis testing, even though such priors may be acceptable for other purposes. Forexample, consider testing the hypotheses M\n0:\u03b8\u2208\u03980vsM1:\u03b8\u2208\u03981. To de\ufb01ne the marginal\ndensity on \u03b8, we use the following mixture model\np(\u03b8)=p( \u03b8|M0)p(M0)+p(\u03b8|M1)p(M1) (5.43)", "195": "5.4. Priors 165\nThis is only meaningful if p(\u03b8|M0)andp(\u03b8|M1)are proper (normalized) density functions. In\nthis case, the posterior is given by\np(M0|D)=p(M0)p(D|M0)\np(M0)p(D|M0)+p(M1)p(D|M1)(5.44)\n=p(M0)\u23a8integraltext\n\u03980p(D|\u03b8)p(\u03b8|M0)d\u03b8\np(M0)\u23a8integraltext\n\u03980p(D|\u03b8)p(\u03b8|M0)d\u03b8+p(M1)\u23a8integraltext\n\u03981p(D|\u03b8)p(\u03b8|M1)d\u03b8(5.45)\nNow suppose we use improper priors, p(\u03b8|M0)\u221dc0andp(\u03b8|M1)\u221dc1. Then\np(M0|D)=p(M0)c0\u23a8integraltext\n\u03980p(D|\u03b8)d\u03b8\np(M0)c0\u23a8integraltext\n\u03980p(D|\u03b8)d\u03b8+p(M1)c1\u23a8integraltext\n\u03981p(D|\u03b8)d\u03b8(5.46)\n=p(M0)c0/lscript0\np(M0)c0/lscript0+p(M1)c1/lscript1(5.47)\nwhere/lscripti=\u23a8integraltext\n\u0398ip(D|\u03b8)d\u03b8is the integrated or marginal likelihood for model i. Now let p(M0)=\np(M1)=1\n2. Hence\np(M0|D)=c0/lscript0\nc0/lscript0+c1/lscript1=/lscript0\n/lscript0+(c1/c0)/lscript1(5.48)\nThus we can change the posterior arbitrarily by choosing c1andc0as we please. Note that\nusing proper, but very vague, priors can cause similar problems. In particular, the Bayes factor\nwill always favor the simpler model, since the probability of the observed data under a complexmodel with a very diffuse prior will be very small. This is called the Jeffreys-Lindley paradox.\nThus it is important to use proper priors when performing model selection. Note, however,\nthat, ifM\n0andM1share the same prior over a subset of the parameters, this part of the prior\ncan be improper, since the corresponding normalization constant will cancel out.\n5.4 Priors\nThe most controversial aspect of Bayesian statistics is its reliance on priors. Bayesians arguethis is unavoidable, since nobody is a tabula rasa orblank slate: all inference must be done\nconditional on certain assumptions about the world. Nevertheless, one might be interested inminimizing the impact of one\u2019s prior assumptions. We brie\ufb02y discuss some ways to do thisbelow.\n5.4.1 Uninformative priors\nIf we don\u2019t have strong beliefs about what \u03b8should be, it is common to use an uninformative\nornon-informative prior, and to \u201clet the data speak for itself\u201d.\nThe issue of designing uninformative priors is actually somewhat tricky. As an example\nof the difficulty, consider a Bernoulli parameter, \u03b8\u2208[0,1]. One might think that the most\nuninformative prior would be the uniform distribution, Beta(1,1). But the posterior mean in\nthis case is E[\u03b8|D]=N1+1\nN1+N 0+2, whereas the MLE isN1\nN1+N 0. Hence one could argue that the\nprior wasn\u2019t completely uninformative after all.", "196": "166 Chapter 5. Bayesian statistics\nClearly by decreasing the magnitude of the pseudo counts, we can lessen the impact of the\nprior. By the above argument, the most non-informative prior is\nlim\nc\u21920Beta(c,c) = Beta(0 ,0) (5.49)\nwhich is a mixture of two equal point masses at 0 and 1 (see (Zhu and Lu 2004)). This is also\ncalled the Haldane prior. Note that the Haldane prior is an improper prior, meaning it does not\nintegrate to 1. However, as long as we see at least one head and at least one tail, the posteriorwill be proper.\nIn Section 5.4.2.1 we will argue that the \u201cright\u201d uninformative prior is in fact Beta(\n1\n2,1\n2).\nClearly the difference in practice between these three priors is very likely negligible. In general,it is advisable to perform some kind of sensitivity analysis, in which one checks how much\none\u2019s conclusions or predictions change in response to change in the modeling assumptions,which includes the choice of prior, but also the choice of likelihood and any kind of data pre-processing. If the conclusions are relatively insensitive to the modeling assumptions, one canhave more con\ufb01dence in the results.\n5.4.2 Jeffreys priors *\nHarold Jeffreys4designed a general purpose technique for creating non-informative priors. The\nresult is known as the Jeffreys prior. The key observation is that if p(\u03c6)is non-informative,\nthen any re-parameterization of the prior, such as \u03b8=h(\u03c6)for some function h, should also\nbe non-informative. Now, by the change of variables formula,\np\u03b8(\u03b8)=p\u03c6(\u03c6)\u23a8vextendsingle\u23a8vextendsingled\u03c6\nd\u03b8\u23a8vextendsingle\u23a8vextendsingle (5.50)\nso the prior will in general change. However, let us pick\np\n\u03c6(\u03c6)\u221d(I(\u03c6))1\n2 (5.51)\nwhereI(\u03c6)is theFisher information:\nI(\u03c6)/defines\u2212E\u23a8bracketleftbigg\u23a8parenleftbiggdlogp(X|\u03c6)\nd\u03c6\u23a8parenrightbigg\n2\u23a8bracketrightbigg\n(5.52)\nThis is a measure of curvature of the expected negative log likelihood and hence a measure of\nstability of the MLE (see Section 6.2.2). Now\ndlogp(x|\u03b8)\nd\u03b8=dlogp(x|\u03c6)\nd\u03c6d\u03c6\nd\u03b8(5.53)\nSquaring and taking expectations over x,w eh a v e\nI(\u03b8)=\u2212E\u23a8bracketleftBigg\u23a8parenleftbiggdlogp(X|\u03b8)\nd\u03b8\u23a8parenrightbigg2\u23a8bracketrightBigg\n=I(\u03c6)\u23a8parenleftbiggd\u03c6\nd\u03b8\u23a8parenrightbigg2\n(5.54)\nI(\u03b8)1\n2=I(\u03c6)1\n2\u23a8vextendsingle\u23a8vextendsingled\u03c6\nd\u03b8\u23a8vextendsingle\u23a8vextendsingle (5.55)\n4. Harold Jeffreys, 1891 \u2013 1989, was an English mathematician, statistician, geophysicist, and astronomer.", "197": "5.4. Priors 167\nso we \ufb01nd the transformed prior is\np\u03b8(\u03b8)=p \u03c6(\u03c6)\u23a8vextendsingle\u23a8vextendsingled\u03c6\nd\u03b8\u23a8vextendsingle\u23a8vextendsingle\u221d(I(\u03c6))1\n2\u23a8vextendsingle\u23a8vextendsingled\u03c6\nd\u03b8\u23a8vextendsingle\u23a8vextendsingle=I(\u03b8)1\n2 (5.56)\nSop\u03b8(\u03b8)andp\u03c6(\u03c6)are the same.\nSome examples will make this clearer.\n5.4.2.1 Example: Jeffreys prior for the Bernoulli and multinoulli\nSupposeX\u223cBer(\u03b8). The log likelihood for a single sample is\nlogp(X|\u03b8)=Xlog\u03b8+(1\u2212X)log(1\u2212\u03b8) (5.57)\nThescore function is just the gradient of the log-likelihood:\ns(\u03b8)/definesd\nd\u03b8logp(X|\u03b8)=X\n\u03b8\u22121\u2212X\n1\u2212\u03b8(5.58)\nTheobserved information is the second derivative of the log-likelihood:\nJ(\u03b8)=\u2212d2\nd\u03b82logp(X|\u03b8)=\u2212s/prime(\u03b8|X)=X\n\u03b82+1\u2212X\n(1\u2212\u03b8)2(5.59)\nThe Fisher information is the expected information:\nI(\u03b8)=E [J(\u03b8|X)|X\u223c\u03b8]=\u03b8\n\u03b82+1\u2212\u03b8\n(1\u2212\u03b8)2=1\n\u03b8(1\u2212\u03b8)(5.60)\nHence Jeffreys\u2019 prior is\np(\u03b8)\u221d\u03b8\u22121\n2(1\u2212\u03b8)\u22121\n2=1\u23a8radicalbig\n\u03b8(1\u2212\u03b8)\u221dBeta(1\n2,1\n2) (5.61)\nNow consider a multinoulli random variable with Kstates. One can show that the Jeffreys\u2019\nprior is given by\np(\u03b8)\u221dDir(1\n2,...,1\n2) (5.62)\nNote that this is different from the more obvious choices of Dir(1\nK,...,1\nK)orDir(1,...,1).\n5.4.2.2 Example: Jeffreys prior for location and scale parameters\nOne can show that the Jeffreys prior for a location parameter, such as the Gaussian mean, is\np(\u03bc)\u221d1. Thus is an example of a translation invariant prior, which satis\ufb01es the property\nthat the probability mass assigned to any interval, [A,B]is the same as that assigned to any\nother shifted interval of the same width, such as [A\u2212c,B\u2212c]. That is,\n\u23a8integraldisplayB\u2212c\nA\u2212cp(\u03bc)d\u03bc=(A\u2212c)\u2212(B\u2212c)=(A\u2212B)=\u23a8integraldisplayB\nAp(\u03bc)d\u03bc (5.63)", "198": "168 Chapter 5. Bayesian statistics\nThis can be achieved using p(\u03bc)\u221d1, which we can approximate by using a Gaussian with\nin\ufb01nite variance, p(\u03bc)=N(\u03bc|0,\u221e). Note that this is an improper prior, since it does not\nintegrate to 1. Using improper priors is \ufb01ne as long as the posterior is proper, which will be the\ncase provided we have seen N\u22651data points, since we can \u201cnail down\u201d the location as soon\nas we have seen a single data point.\nSimilarly, one can show that the Jeffreys prior for a scale parameter, such as the Gaussian\nvariance, is p(\u03c32)\u221d1/\u03c32. This is an example of a scale invariant prior, which satis\ufb01es the\nproperty that the probability mass assigned to any interval [A,B]is the same as that assigned\nto any other interval [A/c,B/c] which is scaled in size by some constant factor c>0.( F o r\nexample, if we change units from meters to feet we do not want that to affect our inferences.)This can be achieved by using\np(s)\u221d1/s (5.64)\nTo see this, note that\n\u23a8integraldisplay\nB/c\nA/cp(s)ds= [logs]B/c\nA/c= log(B/c )\u2212log(A/c) (5.65)\n= log(B )\u2212log(A)=\u23a8integraldisplayB\nAp(s)ds (5.66)\nWecanapproximatethisusingadegenerateGammadistribution(Section2.4.4), p(s)=G a (s|0,0).\nThe prior p(s)\u221d1/sis also improper, but the posterior is proper as soon as we have seen\nN\u22652data points (since we need at least two data points to estimate a variance).\n5.4.3 Robust priors\nIn many cases, we are not very con\ufb01dent in our prior, so we want to make sure it does not have\nan undue in\ufb02uence on the result. This can be done by using robust priors (Insua and Ruggeri\n2000), which typically have heavy tails, which avoids forcing things to be too close to the priormean.\nLet us consider an example from (Berger 1985, p7). Suppose x\u223cN(\u03b8,1). We observe that\nx=5and we want to estimate \u03b8. The MLE is of course \u02c6\u03b8=5, which seems reasonable. The\nposterior mean under a uniform prior is also\n\u03b8=5. But now suppose we know that the prior\nmedian is 0, and the prior quantiles are at -1 and 1, so p(\u03b8\u2264\u22121) =p(\u22121<\u03b8\u22640) =p(0<\n\u03b8\u22641) =p(1<\u03b8)=0.25. Let us also assume the prior is smooth and unimodal.\nIt is easy to show that a Gaussian prior of the form N(\u03b8|0,2.192)satis\ufb01es these prior\nconstraints. But in this case the posterior mean is given by 3.43, which doesn\u2019t seem verysatisfactory.\nNow suppose we use as a Cauchy prior T(\u03b8|0,1,1). This also satis\ufb01es the prior constraints of\nour example. But this time we \ufb01nd (using numerical method integration: see robustPriorDemo\nfor the code) that the posterior mean is about 4.6, which seems much more reasonable.\n5.4.4 Mixtures of conjugate priors\nRobust priors are useful, but can be computationally expensive to use. Conjugate priors simplifythe computation, but are often not robust, and not \ufb02exible enough to encode our prior knowl-", "199": "5.4. Priors 169\nedge. However, it turns out that a mixture of conjugate priors is also conjugate (Exercise 5.1),\nand can approximate any kind of prior (Dallal and Hall 1983; Diaconis and Ylvisaker 1985). Thus\nsuch priors provide a good compromise between computational convenience and \ufb02exibility.\nFor example, suppose we are modeling coin tosses, and we think the coin is either fair, or\nis biased towards heads. This cannot be represented by a beta distribution. However, we canmodel it using a mixture of two beta distributions. For example, we might use\np(\u03b8)=0.5B e t a (\u03b8|20,20)+0.5B e t a (\u03b8|30,10) (5.67)\nIf\u03b8comes from the \ufb01rst distribution, the coin is fair, but if it comes from the second, it is\nbiased towards heads.\nWe can represent a mixture by introducing a latent indicator variable z,w h e r ez=kmeans\nthat\u03b8comes from mixture component k. The prior has the form\np(\u03b8)=\u23a8summationdisplay\nkp(z=k)p(\u03b8|z=k) (5.68)\nwhere each p(\u03b8|z=k)is conjugate, and p(z=k)are called the (prior) mixing weights. One can\nshow (Exercise 5.1) that the posterior can also be written as a mixture of conjugate distributionsas follows:\np(\u03b8|D)=\u23a8summationdisplay\nkp(z=k|D)p(\u03b8|D,z=k) (5.69)\nwherep(Z=k|D)are the posterior mixing weights given by\np(Z=k|D)=p(Z=k)p(D|Z=k)\u23a8summationtext\nk/primep(Z=k/prime)p(D|Z=k/prime)(5.70)\nHere the quantity p(D|Z=k)is the marginal likelihood for mixture component k(see Sec-\ntion 5.3.2.1).\n5.4.4.1 Example\nSuppose we use the mixture prior\np(\u03b8)=0.5Beta(\u03b8|a1,b1)+0.5Beta(\u03b8|a2,b2) (5.71)\nwherea1=b1=2 0anda2=b2=1 0. and we observe N1heads and N0tails. The posterior\nbecomes\np(\u03b8|D)=p(Z=1|D)Beta(\u03b8|a1+N1,b1+N0)+p(Z=2|D)Beta(\u03b8|a2+N1,b2+N0)(5.72)\nIfN1=2 0heads and N0=1 0tails, then, using Equation 5.23, the posterior becomes\np(\u03b8|D)=0.346 Beta(\u03b8 |40,30)+0.654 Beta(\u03b8 |50,20) (5.73)\nSee Figure 5.10 for an illustration.", "200": "170 Chapter 5. Bayesian statistics\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.511.522.533.544.55mixture of Beta distributions\n  \nprior\nposterior\nFigure 5.10 A mixture of two Beta distributions. Figure generated by mixBetaDemo .\n5.4.4.2 Application: Finding conserved regions in DNA and protein sequences\nWe mentioned that Dirichlet-multinomial models are widely used in biosequence analysis. Let\nus give a simple example to illustrate some of the machinery that has developed. Speci\ufb01cally,consider the sequence logo discussed in Section 2.3.2.1. Now suppose we want to \ufb01nd locationswhich represent coding regions of the genome. Such locations often have the same letter acrossall sequences, because of evolutionary pressure. So we need to \ufb01nd columns which are \u201cpure\u201d,or nearly so, in the sense that they are mostly all As, mostly all Ts, mostly all Cs, or mostly allGs. One approach is to look for low-entropy columns; these will be ones whose distribution isnearly deterministic (pure).\nBut suppose we want to associate a con\ufb01dence measure with our estimates of purity. This\ncan be useful if we believe adjacent locations are conserved together. In this case, we can letZ\n1=1if location tis conserved, and let Zt=0otherwise. We can then add a dependence\nbetween adjacent Ztvariables using a Markov chain; see Chapter 17 for details.\nIn any case, we need to de\ufb01ne a likelihood model, p(Nt|Zt),w h e r eNtis the vector of\n(A,C,G,T) counts for column t. It is natural to make this be a multinomial distribution with\nparameter \u03b8t. Since each column has a different distribution, we will want to integrate out \u03b8t\nand thus compute the marginal likelihood\np(Nt|Zt)=\u23a8integraldisplay\np(Nt|\u03b8t)p(\u03b8t|Zt)d\u03b8t (5.74)\nBut what prior should we use for \u03b8t? WhenZt=0we can use a uniform prior, p(\u03b8|Zt=0 )=\nDir(1,1,1,1), but what should we use if Zt=1? After all, if the column is conserved, it could\nbe a (nearly) pure column of As, Cs, Gs, or Ts. A natural approach is to use a mixture of Dirichletpriors, each one of which is \u201ctilted\u201d towards the appropriate corner of the 4-dimensional simplex,e.g.,\np(\u03b8|Z\nt=1 )=1\n4Dir(\u03b8|(10,1,1,1))+\u00b7\u00b7\u00b7+1\n4Dir(\u03b8|(1,1,1,10)) (5.75)\nSince this is conjugate, we can easily compute p(Nt|Zt). See (Brown et al. 1993) for an", "201": "5.5. Hierarchical Bayes 171\napplication of these ideas to a real bio-sequence problem.\n5.5 Hierarchical Bayes\nA key requirement for computing the posterior p(\u03b8|D)is the speci\ufb01cation of a prior p(\u03b8|\u03b7),\nwhere\u03b7are the hyper-parameters. What if we don\u2019t know how to set \u03b7? In some cases, we can\nuse uninformative priors, we we discussed above. A more Bayesian approach is to put a prior on\nour priors! In terms of graphical models (Chapter 10), we can represent the situation as follows:\n\u03b7\u2192\u03b8\u2192D (5.76)\nThis is an example of a hierarchical Bayesian model, also called a multi-level model, since\nthere are multiple levels of unknown quantities. We give a simple example below, and we willsee many others later in the book.\n5.5.1 Example: modeling related cancer rates\nConsider the problem of predicting cancer rates in various cities (this example is from (Johnsonand Albert 1999, p24)). In particular, suppose we measure the number of people in variouscities,N\ni, and the number of people who died of cancer in these cities, xi. We assume\nxi\u223cBin(N i,\u03b8i), and we want to estimate the cancer rates \u03b8i. One approach is to estimate\nthem all separately, but this will suffer from the sparse data problem (underestimation of therate of cancer due to small N\ni). Another approach is to assume all the \u03b8iare the same; this is\ncalledparameter tying. The resulting pooled MLE is just \u02c6\u03b8=/summationtext\nixi/summationtext\niNi. But the assumption that\nall the cities have the same rate is a rather strong one. A compromise approach is to assumethat the\u03b8\niare similar, but that there may be city-speci\ufb01c variations. This can be modeled by\nassuming the \u03b8iare drawn from some common distribution, say \u03b8i\u223cBeta(a,b). The full joint\ndistribution can be written as\np(D,\u03b8,\u03b7|N)=p( \u03b7)N\u23a8productdisplay\ni=1Bin(xi|Ni,\u03b8i)Beta(\u03b8i|\u03b7) (5.77)\nwhere\u03b7=(a,b).\nNote that it is crucial that we infer \u03b7=(a,b)from the data; if we just clamp it to a constant,\nthe\u03b8iwill be conditionally independent, and there will be no information \ufb02ow between them.\nBy contrast, by treating \u03b7as an unknown (hidden variable), we allow the data-poor cities to\nborrow statistical strength from data-rich ones.\nSuppose we compute the joint posterior p(\u03b7,\u03b8|D). From this we can get the posterior\nmarginals p(\u03b8i|D). In Figure 5.11(a), we plot the posterior means, E[\u03b8i|D], as blue bars, as well\nas the population level mean, E[a/(a+b)|D], shown as a red line (this represents the average\nof the\u03b8i\u2019s). We see that the posterior mean is shrunk towards the pooled estimate more strongly\nfor cities with small sample sizes Ni. For example, city 1 and city 20 both have a 0 observed\ncancer incidence rate, but city 20 has a smaller population, so its rate is shrunk more towardsthe population-level estimate (i.e., it is closer to the horizontal red line) than city 1.\nFigure 5.11(b) shows the 95% posterior credible intervals for \u03b8\ni. We see that city 15, which has\na very large population (53,637 people), has small posterior uncertainty. Consequently this city", "202": "172 Chapter 5. Bayesian statistics\n0 5 10 15 20 2505number of people with cancer (truncated at 5)\n0 5 10 15 20 25010002000pop of city (truncated at 2000)\n0 5 10 15 20 250510MLE*1000 (red line=pooled MLE)\n0 5 10 15 20 25024posterior mean*1000 (red line=pop mean)\n(a)0 1 2 3 4 5 6 7 8\nx 10\u221230246810121416182095% credible interval on theta, *=median\n(b)\nFigure 5.11 (a) Results of \ufb01tting the model using the data from (Johnson and Albert 1999, p24). First\nrow: Number of cancer incidents xiin 20 cities in Missouri. Second row: population size Ni. The largest\ncity (number 15) has a population of N15= 53637 andx15=5 4incidents, but we truncate the vertical\naxes of the \ufb01rst two rows so that the differences between the other cities are visible. Third row: MLE \u02c6\u03b8i.\nThe red line is the pooled MLE. Fourth row: posterior mean E[\u03b8i|D]. The red line is E[a/(a+b)|D],\nthe population-level mean. (b) Posterior 95% credible intervals on the cancer rates. Figure generated by\ncancerRatesEb\nhas the largest impact on the posterior estimate of \u03b7, which in turn will impact the estimate of\nthe cancer rates for other cities. Cities 10 and 19, which have the highest MLE, also have the\nhighest posterior uncertainty, re\ufb02ecting the fact that such a high estimate is in con\ufb02ict with theprior (which is estimated from all the other cities).\nIn the above example, we have one parameter per city, modeling the probability the response\nis on. By making the Bernoulli rate parameter be a function of covariates, \u03b8\ni=s i g m (wT\nix),w e\ncan model multiple correlated logistic regression tasks. This is called multi-task learning, and\nwill be discussed in more detail in Section 9.5.\n5.6 Empirical Bayes\nIn hierarchical Bayesian models, we need to compute the posterior on multiple levels of latentvariables. For example, in a two-level model, we need to compute\np(\u03b7,\u03b8|D)\u221dp(D|\u03b8)p(\u03b8|\u03b7)p(\u03b7) (5.78)\nIn some cases, we can analytically marginalize out \u03b8; this leaves is with the simpler problem of\njust computing p(\u03b7|D).\nAs a computational shortcut, we can approximate the posterior on the hyper-parameters with\na point-estimate, p(\u03b7|D)\u2248\u03b4\n\u02c6\u03b7(\u03b7),w h e r e\u02c6\u03b7=a r g m a x p(\u03b7|D). Since\u03b7is typically much\nsmaller than \u03b8in dimensionality, it is less prone to over\ufb01tting, so we can safely use a uniform\nprior on\u03b7. Then the estimate becomes\n\u02c6\u03b7=a r g m a x p(D|\u03b7) = argmax\u23a8bracketleftbigg\u23a8integraldisplay\np(D|\u03b8)p(\u03b8|\u03b7)d\u03b8\u23a8bracketrightbigg\n(5.79)", "203": "5.6. Empirical Bayes 173\nwhere the quantity inside the brackets is the marginal or integrated likelihood, sometimes called\nthe evidence. This overall approach is called empirical Bayes (EB)o rtype-II maximum\nlikelihood. In machine learning, it is sometimes called the evidence procedure.\nEmpirical Bayes violates the principle that the prior should be chosen independently of the\ndata. However, we can just view it as a computationally cheap approximation to inference in ahierarchical Bayesian model, just as we viewed MAP estimation as an approximation to inferencein the one level model \u03b8\u2192D. In fact, we can construct a hierarchy in which the more integrals\none performs, the \u201cmore Bayesian\u201d one becomes:\nMethod De\ufb01nition\nMaximum likelihood \u02c6\u03b8=a r g m a x\u03b8p(D|\u03b8)\nMAP estimation \u02c6\u03b8=a r g m a x\u03b8p(D|\u03b8)p(\u03b8|\u03b7)\nML-II (Empirical Bayes) \u02c6\u03b7=a r g m a x\u03b7\u23a8integraltext\np(D|\u03b8)p(\u03b8|\u03b7)d\u03b8=a r g m a x\u03b7p(D|\u03b7)\nMAP-II \u02c6\u03b7=a r g m a x\u03b7\u23a8integraltext\np(D|\u03b8)p(\u03b8|\u03b7)p(\u03b7)d\u03b8=a r g m a x\u03b7p(D|\u03b7)p(\u03b7)\nFull Bayes p(\u03b8,\u03b7|D)\u221dp(D|\u03b8)p(\u03b8|\u03b7)p(\u03b7)\nNote that EB can be shown to have good frequentist properties (see e.g., (Carlin and Louis\n1996; Efron 2010)), so it is widely used by non-Bayesians. For example, the popular James-Steinestimator, discussed in Section 6.3.3.2, can be derived using EB.\n5.6.1 Example: beta-binomial model\nLet us return to the cancer rates model. We can analytically integrate out the \u03b8i\u2019s, and write\ndown the marginal likelihood directly, as follows:\np(D|a,b)=\u23a8productdisplay\ni\u23a8integraldisplay\nBin(xi|Ni,\u03b8i)Beta(\u03b8i|a,b)d\u03b8i (5.80)\n=\u23a8productdisplay\niB(a+xi,b+Ni\u2212xi)\nB(a,b)(5.81)\nVarious ways of maximizing this wrt aandbare discussed in (Minka 2000e).\nHaving estimated aandb, we can plug in the hyper-parameters to compute the posterior\np(\u03b8i|\u02c6a,\u02c6b,D)in the usual way, using conjugate analysis. The net result is that the posterior\nmean of each \u03b8iis a weighted average of its local MLE and the prior means, which depends on\n\u03b7=(a,b); but since \u03b7is estimated based on all the data, each \u03b8iis in\ufb02uenced by all the data.\n5.6.2 Example: Gaussian-Gaussian model\nWe now study another example that is analogous to the cancer rates example, except the data isreal-valued. We will use a Gaussian likelihood and a Gaussian prior. This will allow us to writedown the solution analytically.\nIn particular, suppose we have data from multiple related groups. For example, x\nijcould be\nthe test score for student iin school j,f o rj=1:D andi=1:N j. We want to estimate\nthe mean score for each school, \u03b8j. However, since the sample size, Nj, may be small for", "204": "174 Chapter 5. Bayesian statistics\nsome schools, we can regularize the problem by using a hierarchical Bayesian model, where we\nassume\u03b8jcome from a common prior, N(\u03bc,\u03c42).\nThe joint distribution has the following form:\np(\u03b8,D|\u03b7,\u03c32)=D\u23a8productdisplay\nj=1N(\u03b8j|\u03bc,\u03c42)Nj\u23a8productdisplay\ni=1N(xij|\u03b8j,\u03c32) (5.82)\nwhere we assume \u03c32is known for simplicity. (We relax this assumption in Exercise 24.4.) We\nexplain how to estimate \u03b7below. Once we have estimated \u03b7=(\u03bc,\u03c4), we can compute the\nposteriors over the \u03b8j\u2019s. To do that, it simpli\ufb01es matters to rewrite the joint distribution in the\nfollowing form, exploiting the fact that NjGaussian measurements with values xijand variance\n\u03c32are equivalent to one measurement of value xj/defines1\nNj\u23a8summationtextNj\ni=1xijwith variance \u03c32\nj/defines\u03c32/Nj.\nThis yields\np(\u03b8,D|\u02c6\u03b7,\u03c32)=D\u23a8productdisplay\nj=1N(\u03b8j|\u02c6\u03bc,\u02c6\u03c42)N(xj|\u03b8j,\u03c32\nj) (5.83)\nFrom this, it follows from the results of Section 4.4.1 that the posteriors are given by\np(\u03b8j|D,\u02c6\u03bc,\u02c6\u03c42)=N (\u03b8j|\u02c6Bj\u02c6\u03bc+(1\u2212\u02c6Bj)xj,(1\u2212\u02c6Bj)\u03c32\nj) (5.84)\n\u02c6Bj/defines\u03c32\nj\n\u03c32\nj+\u02c6\u03c42(5.85)\nwhere\u02c6\u03bc=xand\u02c6\u03c42will be de\ufb01ned below.\nThe quantity 0\u2264\u02c6Bj\u22641controls the degree of shrinkage towards the overall mean, \u03bc.I f\nthe data is reliable for group j(e.g., because the sample size Njis large), then \u03c32\njwill be small\nrelative to \u03c42; hence\u02c6Bjwill be small, and we will put more weight on xjwhen we estimate \u03b8j.\nHowever, groups with small sample sizes will get regularized (shrunk towards the overall mean\u03bc) more heavily. We will see an example of this below.\nIf\u03c3\nj=\u03c3for all groups j, the posterior mean becomes\n\u02c6\u03b8j=\u02c6Bx+(1\u2212\u02c6B)xj=x+(1\u2212\u02c6B)(xj\u2212x) (5.86)\nThis has exactly the same form as the James Stein estimator discussed in Section 6.3.3.2.\n5.6.2.1 Example: predicting baseball scores\nWe now give an example of shrinkage applied to baseball batting averages, from (Efron andMorris 1975). We observe the number of hits for D=1 8players during the \ufb01rst T=4 5games.\nCall the number of hits b\ni. We assume bj\u223cBin(T,\u03b8 j),w h e r e\u03b8jis the \u201ctrue\u201d batting average\nfor player j. The goal is to estimate the \u03b8j. The MLE is of course \u02c6\u03b8j=xj,w h e r exj=bj/Tis\nthe empirical batting average. However, we can use an EB approach to do better.\nTo apply the Gaussian shrinkage approach described above, we require that the likelihood be\nGaussian, xj\u223cN(\u03b8j,\u03c32)for known \u03c32. (We drop the isubscript since we assume Nj=1,", "205": "5.6. Empirical Bayes 175\n0.2 0.25 0.3 0.35 0.400.20.40.60.81MLE (top) and shrinkage estimates (bottom)\n(a)1 2 3 4 500.050.10.150.20.250.30.350.4\nplayer numberMSEMSE MLE = 0.0042, MSE shrunk = 0.0013\ntrue\nshrunk\nMLE\n(b)\nFigure 5.12 (a) MLE parameters (top) and corresponding shrunken estimates (bottom). (b) We plot the\ntrue parameters (blue), the posterior mean estimate (green), and the MLEs (red) for 5 of the players. Figure\ngenerated by shrinkageDemoBaseball .\nsincexjalready represents the average for player j.) However, in this example we have a\nbinomial likelihood. While this has the right mean, E[xj]=\u03b8j, the variance is not constant:\nvar[xj]=1\nT2var[bj]=T\u03b8j(1\u2212\u03b8j)\nT2(5.87)\nSo let us apply a variance stabilizing transform5toxjto better match the Gaussian assump-\ntion:\nyj=f(yj)=\u221a\nTarcsin(2y j\u22121) (5.88)\nNow we have approximately yj\u223cN(f(\u03b8j),1) =N(\u03bcj,1). We use Gaussian shrinkage to\nestimate the \u03bcjusing Equation 5.86 with \u03c32=1, and we then transform back to get\n\u02c6\u03b8j=0.5(sin(\u02c6\u03bcj/\u221a\nT)+1) (5.89)\nThe results are shown in Figure 5.12(a-b). In (a), we plot the MLE \u02c6\u03b8jand the posterior mean \u03b8j.\nWe see that all the estimates have shrunk towards the global mean, 0.265. In (b), we plot the\ntrue value \u03b8j, the MLE \u02c6\u03b8jand the posterior mean \u03b8j. (The \u201ctrue\u201d values of \u03b8jare estimated\nfrom a large number of independent games.) We see that, on average, the shrunken estimate\nis much closer to the true parameters than the MLE is. Speci\ufb01cally, the mean squared error,\nde\ufb01ned by MSE =1\nN\u23a8summationtextD\nj=1(\u03b8j\u2212\u03b8j)2, is over three times smaller using the shrinkage estimates\n\u03b8jthan using the MLEs \u02c6\u03b8j.\n5.6.2.2 Estimating the hyper-parameters\nIn this section, we give an algorithm for estimating \u03b7. Suppose initially that \u03c32\nj=\u03c32is the\nsame for all groups. In this case, we can derive the EB estimate in closed form, as we now show.\nFrom Equation 4.126, we have\np(xj|\u03bc,\u03c42,\u03c32)=\u23a8integraldisplay\nN(xj|\u03b8j,\u03c32)N(\u03b8j|\u03bc,\u03c42)d\u03b8j=N(xj|\u03bc,\u03c42+\u03c32) (5.90)\n5. Suppose E[X]=\u03bcandvar[X]=\u03c32(\u03bc).L e t Y=f(X). Then a Taylor series expansions gives Y\u2248\nf(\u03bc)+(X\u2212\u03bc)f/prime(\u03bc). Hence var[Y]\u2248f/prime(\u03bc)2var[X\u2212\u03bc]=f/prime(\u03bc)2\u03c32(\u03bc). A variance stabilizing transformation\nis a function fsuch that f/prime(\u03bc)2\u03c32(\u03bc)is independent of \u03bc.", "206": "176 Chapter 5. Bayesian statistics\nHence the marginal likelihood is\np(D|\u03bc,\u03c42,\u03c32)=D\u23a8productdisplay\nj=1N(xj|\u03bc,\u03c42+\u03c32) (5.91)\nThus we can estimate the hyper-parameters using the usual MLEs for a Gaussian. For \u03bc,w e\nhave\n\u02c6\u03bc=1\nDD\u23a8summationdisplay\nj=1xj=x (5.92)\nwhich is the overall mean.\nFor the variance, we can use moment matching (which is equivalent to the MLE for a\nGaussian): we simply equate the model variance to the empirical variance:\n\u02c6\u03c42+\u03c32=1\nDD\u23a8summationdisplay\nj=1(xj\u2212x)2/definess2(5.93)\nso\u02c6\u03c42=s2\u2212\u03c32. Since we know \u03c42must be positive, it is common to use the following revised\nestimate:\n\u02c6\u03c42=m a x{0,s2\u2212\u03c32}=(s2\u2212\u03c32)+ (5.94)\nHence the shrinkage factor is\n\u02c6B=\u03c32\n\u03c32+\u02c6\u03c42=\u03c32\n\u03c32+(s2\u2212\u03c32)+(5.95)\nIn the case where the \u03c32\nj\u2019s are different, we can no longer derive a solution in closed form.\nExercise 11.13 discusses how to use the EM algorithm to derive an EB estimate, and Exercise 24.4\ndiscusses how to perform full Bayesian inference in this hierarchical model.\n5.7 Bayesian decision theory\nWe have seen how probability theory can be used to represent and updates our beliefs aboutthe state of the world. However, ultimately our goal is to convert our beliefs into actions. In thissection, we discuss the optimal way to do this.\nWe can formalize any given statistical decision problem as a game against nature (as opposed\nto a game against other strategic players, which is the topic of game theory, see e.g., (Shohamand Leyton-Brown 2009) for details). In this game, nature picks a state or parameter or label,y\u2208Y, unknown to us, and then generates an observation, x\u2208X, which we get to see. We\nthen have to make a decision, that is, we have to choose an action afrom some action space\nA. Finally we incur some loss,L(y,a), which measures how compatible our action ais with\nnature\u2019s hidden state y. For example, we might use misclassi\ufb01cation loss, L(y,a)=I(y/negationslash=a),\nor squared loss, L(y,a)=(y\u2212a)\n2. We will see some other examples below.", "207": "5.7. Bayesian decision theory 177\nOur goal is to devise a decision procedure orpolicy,\u03b4:X\u2192A, which speci\ufb01es the\noptimal action for each possible input. By optimal, we mean the action that minimizes the\nexpected loss:\n\u03b4(x) = argmin\na\u2208AE[L(y,a)] (5.96)\nIn economics, it is more common to talk of a utility function; this is just negative loss,\nU(y,a)=\u2212L(y,a). Thus the above rule becomes\n\u03b4(x) = argmax\na\u2208AE[U(y,a)] (5.97)\nThis is called the maximum expected utility principle, and is the essence of what we mean\nbyrational behavior.\nNote that there are two different interpretations of what we mean by \u201cexpected\u201d. In the\nBayesian version, which we discuss below, we mean the expected value of ygiven the data we\nhave seen so far. In the frequentist version, which we discuss in Section 6.3, we mean theexpected value of yandxthat we expect to see in the future.\nIn the Bayesian approach to decision theory, the optimal action, having observed x, is de\ufb01ned\nas the action athat minimizes the posterior expected loss:\n\u03c1(a|x)/definesE\np(y|x)[L(y,a)] =\u23a8summationdisplay\nyL(y,a)p(y|x) (5.98)\n(Ifyis continuous (e.g., when we want to estimate a parameter vector), we should replace the\nsum with an integral.) Hence the Bayes estimator, also called the Bayes decision rule, is given\nby\n\u03b4(x)=a r gm i n\na\u2208A\u03c1(a|x) (5.99)\n5.7.1 Bayes estimators for common loss functions\nIn this section we show how to construct Bayes estimators for the loss functions most commonlyarising in machine learning.\n5.7.1.1 MAP estimate minimizes 0-1 loss\nThe0-1 lossis de\ufb01ned by\nL(y,a)=I(y/negationslash=a)=\u23a8braceleftbigg0ifa=y\n1ifa/negationslash=y(5.100)\nThis is commonly used in classi\ufb01cation problems where yis the true class label and a=\u02c6yis\nthe estimate.\nFor example, in the two class case, we can write the loss matrix as follows:\n\u02c6y=1 \u02c6y=0\ny=101\ny=0 10", "208": "178 Chapter 5. Bayesian statistics\n/g2/g3/g4\n/g4/g3/g4/g1/g2/g3/g4/g5/g2/g6/g7/g8\n/g9/g5/g6/g7/g8/g2/g1 /g9/g1/g10/g11 /g5/g6/g7/g1/g8/g1/g12/g1/g9/g1/g10/g11\n/g13/g14/g15/g14/g16/g17 /g1\n/g13/g14/g18/g19/g20/g21\nFigure 5.13 For some regions of input space, where the class posteriors are uncertain, we may prefer not\nto choose class 1 or 2; instead we may prefer the reject option. Based on Figure 1.26 of (Bishop 2006a).\n(In Section 5.7.2, we generalize this loss function so it penalizes the two kinds of errors on\nthe off-diagonal differently.)\nThe posterior expected loss is\n\u03c1(a|x)=p(a/negationslash=y|x)=1\u2212p(y|x) (5.101)\nHence the action that minimizes the expected loss is the posterior mode or MAP estimate\ny\u2217(x) = argmax\ny\u2208Yp(y|x) (5.102)\n5.7.1.2 Reject option\nIn classi\ufb01cation problems where p(y|x)is very uncertain, we may prefer to choose a reject\naction, in which we refuse to classify the example as any of the speci\ufb01ed classes, and instead\nsay \u201cdon\u2019t know\u201d. Such ambiguous cases can be handled by e.g., a human expert. See Figure 5.13for an illustration. This is useful in risk averse domains such as medicine and \ufb01nance.\nWe can formalize the reject option as follows. Let choosing a=C+1correspond to\npicking the reject action, and choosing a\u2208{1,...,C}correspond to picking one of the classes.\nSuppose we de\ufb01ne the loss function as\nL(y=j,a=i)=\u23a7\n\u23a8\n\u23a90ifi=jandi,j\u2208{1,...,C}\n\u03bb\nr ifi=C+1\n\u03bbs otherwise(5.103)\nwhere\u03bbris the cost of the reject action, and \u03bbsis the cost of a substitution error. In Exercise 5.3,\nyou will show that the optimal action is to pick the reject action if the most probable class has\na probability below 1\u2212\u03bbr\n\u03bbs; otherwise you should just pick the most probable class.", "209": "5.7. Bayesian decision theory 179\n\u22122 \u22121 0 1 2012|x|0.2\n(a)\u22122 \u22121 0 1 2012|x|1.0\n(b)\u22122 \u22121 0 1 2012|x|2.0\n(c)\nFigure 5.14 (a-c). Plots of the L(y,a)=|y\u2212a|qvs|y\u2212a|forq=0.2,q=1andq=2. Figure\ngenerated by lossFunctionFig .\n5.7.1.3 Posterior mean minimizes /lscript2(quadratic) loss\nFor continuous parameters, a more appropriate loss function is squared error, /lscript2loss,o r\nquadratic loss, de\ufb01ned as\nL(y,a)=(y\u2212a)2(5.104)\nThe posterior expected loss is given by\n\u03c1(a|x)=E\u23a8bracketleftbig\n(y\u2212a)2|x\u23a8bracketrightbig\n=E\u23a8bracketleftbig\ny2|x\u23a8bracketrightbig\n\u22122aE[y|x]+a2(5.105)\nHence the optimal estimate is the posterior mean:\n\u2202\n\u2202a\u03c1(a|x)=\u22122E[y|x]+2a=0\u21d2\u02c6y=E[y|x]=\u23a8integraldisplay\nyp(y|x)dy (5.106)\nThis is often called the minimum mean squared error estimate or MMSEestimate.\nIn a linear regression problem, we have\np(y|x,\u03b8)=N(y|xTw,\u03c32) (5.107)\nIn this case, the optimal estimate given some training data Dis given by\nE[y|x,D]=xTE[w|D] (5.108)\nThat is, we just plug-in the posterior mean parameter estimate. Note that this is the optimal\nthing to do no matter what prior we use for w.\n5.7.1.4 Posterior median minimizes /lscript1(absolute) loss\nThe/lscript2loss penalizes deviations from the truth quadratically, and thus is sensitive to outliers. A\nmore robust alternative is the absolute or /lscript1loss,L(y,a)=|y\u2212a|(see Figure 5.14). The optimal\nestimate is the posterior median, i.e., a value asuch that P(y<a|x)=P(y\u2265a|x)=0.5.\nSee Exercise 5.9 for a proof.\n5.7.1.5 Supervised learning\nConsider a prediction function \u03b4:X\u2192Y, and suppose we have some cost function /lscript(y,y/prime)\nwhich gives the cost of predicting y/primewhen the truth is y. We can de\ufb01ne the loss incurred by", "210": "180 Chapter 5. Bayesian statistics\ntaking action \u03b4(i.e., using this predictor) when the unknown state of nature is \u03b8(the parameters\nof the data generating mechanism) as follows:\nL(\u03b8,\u03b4)/definesE(x,y)\u223cp(x,y|\u03b8)[/lscript(y,\u03b4(x)] =\u23a8summationdisplay\nx\u23a8summationdisplay\nyL(y,\u03b4(x))p(x,y|\u03b8) (5.109)\nThis is known as the generalization error. Our goal is to minimize the posterior expected loss,\ngiven by\n\u03c1(\u03b4|D)=\u23a8integraldisplay\np(\u03b8|D)L(\u03b8,\u03b4)d\u03b8 (5.110)\nThis should be contrasted with the frequentist risk which is de\ufb01ned in Equation 6.47.\n5.7.2 The false positive vs false negative tradeoff\nIn this section, we focus on binary decision problems, such as hypothesis testing, two-class\nclassi\ufb01cation, object/ event detection, etc. There are two types of error we can make: a false\npositive(akafalse alarm), which arises when we estimate \u02c6y=1but the truth is y=0;o ra\nfalse negative (akamissed detection), which arises when we estimate \u02c6y=0but the truth is\ny=1. The 0-1 loss treats these two kinds of errors equivalently. However, we can consider the\nfollowing more general loss matrix:\n\u02c6y=1 \u02c6y=0\ny=10LFN\ny=0LFP0\nwhereLFNis the cost of a false negative, and LFPis the cost of a false positive. The\nposterior expected loss for the two possible actions is given by\n\u03c1(\u02c6y=0|x)=L FNp(y=1|x) (5.111)\n\u03c1(\u02c6y=1|x)=L FPp(y=0|x) (5.112)\nHence we should pick class \u02c6y=1iff\n\u03c1(\u02c6y=0|x)>\u03c1(\u02c6y=1|x) (5.113)\np(y=1|x)\np(y=0|x)>LFP\nLFN(5.114)\nIfLFN=cLFP, it is easy to show (Exercise 5.10) that we should pick \u02c6y=1iffp(y=\n1|x)/p(y=0|x)>\u03c4,w h e r e\u03c4=c/(1 +c)(see also (Muller et al. 2004)). For example, if a\nfalse negative costs twice as much as false positive, so c=2, then we use a decision threshold\nof2/3before declaring a positive.\nBelow we discuss ROC curves, which provide a way to study the FP-FN tradeoff without having\nto choose a speci\ufb01c threshold.\n5.7.2.1 ROC curves and all that\nSuppose we are solving a binary decision problem, such as classi\ufb01cation, hypothesis testing,object detection, etc. Also, assume we have a labeled data set, D={(x\ni,yi)}.L e t\u03b4(x)=", "211": "5.7. Bayesian decision theory 181\nTruth\n10 \u03a3\nEstimate1 TP FP \u02c6N+=TP+FP\n0 FN TN \u02c6N\u2212=FN+TN\n\u03a3N+=TP+FN N \u2212=FP+TNN=TP+FP+FN+TN\nTable 5.2 Quantities derivable from a confusion matrix. N+is the true number of positives, \u02c6N+is the\n\u201ccalled\u201d number of positives, N\u2212is the true number of negatives, \u02c6N\u2212is the \u201ccalled\u201d number of negatives.\ny=1 y=0\n\u02c6y=1TP/N +=TPR=sensitivity=recall FP/N \u2212=FPR=type I\n\u02c6y=0FN/N +=FNR=miss rate=type II TN/N \u2212=TNR=speci\ufb01ty\nTable 5.3 Estimating p(\u02c6y|y)from a confusion matrix. Abbreviations: FNR = false negative rate, FPR =\nfalse positive rate, TNR = true negative rate, TPR = true positive rate.\nI(f(x)>\u03c4)be our decision rule, where f(x)is a measure of con\ufb01dence that y=1(this\nshould be monotonically related to p(y=1|x), but does not need to be a probability), and \u03c4is\nsome threshold parameter. For each given value of \u03c4, we can apply our decision rule and count\nthe number of true positives, false positives, true negatives, and false negatives that occur, as\nshown in Table 5.2. This table of errors is called a confusion matrix.\nFrom this table, we can compute the true positive rate (TPR), also known as the sensitivity,\nrecallorhit rate, by using TPR=TP/N +\u2248p(\u02c6y=1|y=1 ). We can also compute the\nfalse positive rate (FPR), also called the false alarm rate, or the type I error rate, by using\nFPR=FP/N \u2212\u2248p(\u02c6y=1|y=0 ). These and other de\ufb01nitions are summarized in Tables 5.3\nand 5.4. We can combine these errors in any way we choose to compute a loss function.\nHowever, rather than than computing the TPR and FPR for a \ufb01xed threshold \u03c4, we can run\nour detector for a set of thresholds, and then plot the TPR vs FPR as an implicit function of\u03c4. This is called a receiver operating characteristic orROCcurve. See Figure 5.15(a) for an\nexample. Any system can achieve the point on the bottom left, (FPR=0,TPR=0 ),b y\nsetting\u03c4=1and thus classifying everything as negative; similarly any system can achieve the\npoint on the top right, (FPR=1,TPR=1 ), by setting \u03c4=0and thus classifying everything\nas positive. If a system is performing at chance level, then we can achieve any point on thediagonal line TPR=FPRby choosing an appropriate threshold. A system that perfectly\nseparates the positives from negatives has a threshold that can achieve the top left corner,(FPR=0,TPR=1 ); by varying the threshold such a system will \u201chug\u201d the left axis and\nthen the top axis, as shown in Figure 5.15(a).\nThe quality of a ROC curve is often summarized as a single number using the area under the\ncurveorAUC. Higher AUC scores are better; the maximum is obviously 1. Another summary\nstatistic that is used is the equal error rate orEER, also called the cross over rate, de\ufb01ned\nas the value which satis\ufb01es FPR=FNR. SinceFNR=1\u2212TPR, we can compute the\nEER by drawing a line from the top left to the bottom right and seeing where it intersects theROC curve (see points A and B in Figure 5.15(a)). Lower EER scores are better; the minimum isobviously 0.", "212": "182 Chapter 5. Bayesian statistics\n0 101\nfprtprA\nB\n(a)0 101\nrecallprecisionA B\n(b)\nFigure 5.15 (a) ROC curves for two hypothetical classi\ufb01cation systems. A is better than B. We plot the\ntrue positive rate (TPR) vs the false positive rate (FPR) as we vary the threshold \u03c4. We also indicate the\nequal error rate (EER) with the red and blue dots, and the area under the curve (AUC) for classi\ufb01er B. (b)\nA precision-recall curve for two hypothetical classi\ufb01cation systems. A is better than B. Figure generated byPRhand.\ny=1 y=0\n\u02c6y=1TP/\u02c6N+=precision=PPV FP/\u02c6N+=FDP\n\u02c6y=0 FN/\u02c6N\u2212 TN/\u02c6N\u2212=NPV\nTable 5.4 Estimating p(y|\u02c6y)from a confusion matrix. Abbreviations: FDP = false discovery probability,\nNPV = negative predictive value, PPV = positive predictive value,\n5.7.2.2 Precision recall curves\nWhen trying to detect a rare event (such as retrieving a relevant document or \ufb01nding a face\nin an image), the number of negatives is very large. Hence comparing TPR=TP/N +to\nFPR=FP/N \u2212is not very informative, since the FPR will be very small. Hence all the\n\u201caction\u201d in the ROC curve will occur on the extreme left. In such cases, it is common to plotthe TPR versus the number of false positives, rather than vs the false positive rate.\nHowever, in some cases, the very notion of \u201cnegative\u201d is not well-de\ufb01ned. For example, when\ndetecting objects in images (see Section 1.2.1.3), if the detector works by classifying patches, thenthe number of patches examined \u2014 and hence the number of true negatives \u2014 is a parameterof the algorithm, not part of the problem de\ufb01nition. So we would like to use a measure thatonly talks about positives.\nTheprecision is de\ufb01ned as TP/\u02c6N\n+=p(y=1|\u02c6y=1 )and therecallis de\ufb01ned as\nTP/N +=p(\u02c6y=1|y=1 ). Precision measures what fraction of our detections are actually\npositive, and recall measures what fraction of the positives we actually detected. If \u02c6yi\u2208{0,1}\nis the predicted label, and yi\u2208{0,1}is the true label, we can estimate precision and recall\nusing\nP=\u23a8summationtext\niyi\u02c6yi\u23a8summationtext\ni\u02c6yi,R=\u23a8summationtext\niyi\u02c6yi\u23a8summationtext\niyi(5.115)\nAprecision recall curve is a plot of precision vs recall as we vary the threshold \u03c4. See\nFigure 5.15(b). Hugging the top right is the best one can do.\nThis curve can be summarized as a single number using the mean precision (averaging over", "213": "5.7. Bayesian decision theory 183\nClass 1\ny=1y=0\n\u02c6y=110 10\n\u02c6y=010 970Class 2\ny=1y=0\n\u02c6y=190 10\n\u02c6y=0 10 890Pooled\ny=1y=0\n\u02c6y=1100 20\n\u02c6y=020 1860\nTable 5.5 Illustration of the difference between macro- and micro-averaging. yis the true label, and \u02c6y\nis the called label. In this example, the macro-averaged precision is [10/(10+10)+90/ (10+90)]/ 2=\n(0.5+0.9)/2=0.7. The micro-averaged precision is 100/(100 + 20) \u22480.83. Based on Table 13.7 of\n(Manning et al. 2008).\nrecall values), which approximates the area under the curve. Alternatively, one can quote the\nprecision for a \ufb01xed recall level, such as the precision of the \ufb01rst K=1 0entities recalled.\nThis is called the average precision at K score. This measure is widely used when evaluating\ninformation retrieval systems.\n5.7.2.3 F-scores *\nFor a \ufb01xed threshold, one can compute a single precision and recall value. These are oftencombined into a single statistic called the Fs c o r e,o rF1 score, which is the harmonic mean of\nprecision and recall:\nF\n1/defines2\n1/P+1/R=2PR\nR+P(5.116)\nUsing Equation 5.115, we can write this as\nF1=2\u23a8summationtextN\ni=1yi\u02c6yi\u23a8summationtextNi=1yi+\u23a8summationtextNi=1\u02c6yi(5.117)\nThis is a widely used measure in information retrieval systems.\nTo understand why we use the harmonic mean instead of the arithmetic mean, (P+R)/2,\nconsider the following scenario. Suppose we recall all entries, so R=1. The precision will be\ngiven by the prevalence, p(y=1 ). Suppose the prevalence is low, say p(y=1 )=1 0\u22124. The\narithmetic mean of PandRis given by (P+R)/2=( 1 0\u22124+1)/2\u224850%. By contrast, the\nharmonic mean of this strategy is only2\u00d710\u22124\u00d71\n1+10\u22124\u22480.2%.\nIn the multi-class case (e.g., for document classi\ufb01cation problems), there are two ways to\ngeneralize F1scores. The \ufb01rst is called macro-averaged F1, and is de\ufb01ned as\u23a8summationtextCc=1F1(c)/C,\nwhereF1(c)is theF1score obtained on the task of distinguishing class cfrom all the others.\nThe other is called micro-averaged F1, and is de\ufb01ned as the F1score where we pool all the\ncounts from each class\u2019s contingency table.\nTable 5.5 gives a worked example that illustrates the difference. We see that the precision of\nclass 1 is 0.5, and of class 2 is 0.9. The macro-averaged precision is therefore 0.7, whereas the\nmicro-averaged precision is 0.83. The latter is much closer to the precision of class 2 than tothe precision of class 1, since class 2 is \ufb01ve times larger than class 1. To give equal weight toeach class, use macro-averaging.", "214": "184 Chapter 5. Bayesian statistics\n5.7.2.4 False discovery rates *\nSuppose we are trying to discover a rare phenomenon using some kind of high throughput\nmeasurement device, such as a gene expression micro array, or a radio telescope. We will needto make many binary decisions of the form p(y\ni=1|D)>\u03c4,w h e r eD ={xi}N\ni=1andNmay\nbe large. This is called multiple hypothesis testing. Note that the difference from standard\nbinary classi\ufb01cation is that we are classifying yibased on all the data, not just based on xi.S o\nthis is a simultaneous classi\ufb01cation problem, where we might hope to do better than a series of\nindividual classi\ufb01cation problems.\nHow should we set the threshold \u03c4? A natural approach is to try to minimize the expected\nnumber of false positives. In the Bayesian approach, this can be computed as follows:\nFD(\u03c4,D)/defines\u23a8summationdisplay\ni(1\u2212pi)\u23a8bracehtipupleft\u23a8bracehtipdownright\u23a8bracehtipdownleft\u23a8bracehtipupright\npr. errorI(pi>\u03c4)\u23a8bracehtipupleft\u23a8bracehtipdownright\u23a8bracehtipdownleft\u23a8bracehtipupright\ndiscovery(5.118)\nwherepi/definesp(yi=1|D)is your belief that this object exhibits the phenomenon in question.\nWe then de\ufb01ne the posterior expected false discovery rate as follows:\nFDR(\u03c4,D)/definesFD(\u03c4,D)/N(\u03c4,D) (5.119)\nwhereN(\u03c4,D)=\u23a8summationtext\niI(pi>\u03c4)is the number of discovered items. Given a desired FDR\ntolerance, say \u03b1=0.05, one can then adapt \u03c4to achieve this; this is called the direct posterior\nprobability approach to controlling the FDR (Newton et al. 2004; Muller et al. 2004).\nIn order to control the FDR it is very helpful to estimate the pi\u2019s jointly (e.g., using a hierar-\nchical Bayesian model, as in Section 5.5), rather than independently. This allows the pooling ofstatistical strength, and thus lower FDR. See e.g., (Berry and Hochberg 1999) for more information.\n5.7.3 Other topics *\nIn this section, we brie\ufb02y mention a few other topics related to Bayesian decision theory. We donot have space to go into detail, but we include pointers to the relevant literature.\n5.7.3.1 Contextual bandits\nAone-armed bandit is a colloquial term for a slot machine, found in casinos around the world.\nThe game is this: you insert some money, pull an arm, and wait for the machine to stop; ifyou\u2019re lucky, you win some money. Now imagine there is a bank of Ksuch machines to choose\nfrom. Which one should you use? This is called a multi-armed bandit, and can be modeled\nusing Bayesian decision theory: there are Kpossible actions, and each action has an unknown\nreward (payoff function) r\nk. By maintaining a belief state, p(r1:K|D)=\u23a8producttext\nkp(rk|D), one can\ndevise an optimal policy; this can be compiled into a series of Gittins Indices (Gittins 1989).\nThis optimally solves the exploration-exploitation tradeoff, which speci\ufb01es how many times\none should try each action before deciding to go with the winner.\nNow consider an extension where each arm, and the player, has an associated feature vector;\ncall all these features x. This is called a contextual bandit (see e.g., (Sarkar 1991; Scott 2010;\nLi et al. 2011)). For example, the \u201carms\u201d could represent ads or news articles which we wantto show to the user, and the features could represent properties of these ads or articles, such", "215": "5.7. Bayesian decision theory 185\nas a bag of words, as well as properties of the user, such as demographics. If we assume a\nlinear model for reward, rk=\u03b8T\nkx, we can maintain a distribution over the parameters of each\narm,p(\u03b8k|D),w h e r eDis a series of tuples of the form (a,x,r), which speci\ufb01es which arm\nwas pulled, what its features were, and what the resulting outcome was (e.g., r=1if the user\nclicked on the ad, and r=0otherwise). We discuss ways to compute p(\u03b8k|D)from linear and\nlogistic regression models in later chapters.\nGiven the posterior, we must decide what action to take. One common heuristic, known as\nUCB(which stands for \u201cupper con\ufb01dence bound\u201d) is to take the action which maximizes\nk\u2217=Kargmax\nk=1\u03bck+\u03bb\u03c3k (5.120)\nwhere\u03bck=E[rk|D],\u03c32\nk= var[r k|D]and\u03bbis a tuning parameter that trades off exploration\nand exploitation. The intuition is that we should pick actions about which we believe are good\n(\u03bckis large), and/ or actions about which we are uncertain (\u03c3 kis large).\nAn even simpler method, known as Thompson sampling, is as follows. At each step, we pick\nactionkwith a probability that is equal to its probability of being the optimal action:\npk=\u23a8integraldisplay\nI(E[r|a,x,\u03b8]=m a x\na/primeE[r|a/prime,x,\u03b8])p(\u03b8|D)d\u03b8 (5.121)\nWe can approximate this by drawing a single sample from the posterior, \u03b8t\u223cp(\u03b8|D), and then\nchoosing k\u2217=a r g m a xkE\u23a8bracketleftbig\nr|x,k,\u03b8t\u23a8bracketrightbig\n. Despite its simplicity, this has been shown to work quite\nwell (Chapelle and Li 2011).\n5.7.3.2 Utility theory\nSuppose we are a doctor trying to decide whether to operate on a patient or not. We imaginethere are 3 states of nature: the patient has no cancer, the patient has lung cancer, or thepatient has breast cancer. Since the action and state space is discrete, we can represent the lossfunctionL(\u03b8,a)as aloss matrix, such as the following:\nSurgery No surgery\nNo cancer 20 0\nLung cancer 10 50\nBreast cancer 10 60\nThese numbers re\ufb02ects the fact that not performing surgery when the patient has cancer is\nvery bad (loss of 50 or 60, depending on the type of cancer), since the patient might die; notperforming surgery when the patient does not have cancer incurs no loss (0); performing surgerywhen the patient does not have cancer is wasteful (loss of 20); and performing surgery whenthe patient does have cancer is painful but necessary (10).\nIt is natural to ask where these numbers come from. Ultimately they represent the personal\npreferences or values of a \ufb01ctitious doctor, and are somewhat arbitrary: just as some people\nprefer chocolate ice cream and others prefer vanilla, there is no such thing as the \u201cright\u201d loss/utility function. However, it can be shown (see e.g., (DeGroot 1970)) that any set of consistentpreferences can be converted to a scalar loss/ utility function. Note that utility can be measuredon an arbitrary scale, such as dollars, since it is only relative values that matter.\n6\n6. People are often squeamish about talking about human lives in monetary terms, but all decision making requires", "216": "186 Chapter 5. Bayesian statistics\n5.7.3.3 Sequential decision theory\nSo far, we have concentrated on one-shot decision problems, where we only have to make\none decision and then the game ends. In Setion 10.6, we will generalize this to multi-stage or\nsequential decision problems. Such problems frequently arise in many business and engineeringsettings. This is closely related to the problem of reinforcement learning. However, furtherdiscussion of this point is beyond the scope of this book.\nExercises\nExercise 5.1 Proof that a mixture of conjugate priors is indeed conjugate\nDerive Equation 5.69.\nExercise 5.2 Optimal threshold on classi\ufb01cation probability\nConsider a case where we have learned a conditional probability distribution P(y|x). Suppose there are\nonly two classes, and let p0=P(Y=0|x)andp1=P(Y=1|x). Consider the loss matrix below:\npredicted true label y\nlabel\u02c6y 01\n0 0\u03bb01\n1 \u03bb100\na. Show that the decision \u02c6ythat minimizes the expected loss is equivalent to setting a probability threshold\n\u03b8and predicting \u02c6y=0ifp1<\u03b8and\u02c6y=1ifp1\u2265\u03b8. What is \u03b8as a function of \u03bb01and\u03bb10? (Show\nyour work.)\nb. Show a loss matrix where the threshold is 0.1. (Show your work.)\nExercise 5.3 Reject option in classi\ufb01ers\n(Source: (Duda et al. 2001, Q2.13).)\nIn many classi\ufb01cation problems one has the option either of assigning xto classjor, if you are too\nuncertain, of choosing the reject option. If the cost for rejects is less than the cost of falsely classifying\nthe object, it may be the optimal action. Let \u03b1imean you choose action i,f o ri=1:C+1,w h e r eC\nis the number of classes and C+1is the reject action. Let Y=jbe the true (but unknown) state of\nnature. De\ufb01ne the loss function as follows\n\u03bb(\u03b1i|Y=j)=\u23a7\n\u23a8\n\u23a90ifi=jandi,j\u2208{1,...,C}\n\u03bbr ifi=C+1\n\u03bbs otherwise(5.122)\nIn otherwords, you incur 0 loss if you correctly classify, you incur \u03bbrloss (cost) if you choose the reject\noption, and you incur \u03bbsloss (cost) if you make a substitution error (misclassi\ufb01cation).\ntradeoffs, and one needs to use some kind of \u201ccurrency\u201d to compare different courses of action. Insurance companies\ndo this all the time. Ross Schachter, a decision theorist at Stanford University, likes to tell a story of a school board who\nrejected a study on absestos removal from schools because it performed a cost-bene\ufb01t analysis, which was considered\n\u201cinhumane\u201d because they put a dollar value on children\u2019s health; the result of rejecting the report was that the absestoswas not removed, which is surely more \u201cinhumane\u201d. In medical domains, one often measures utility in terms of QALY,o r\nquality-adjusted life-years, instead of dollars, but it\u2019s the same idea. Of course, even if you do not explicitly specify howmuch you value different people\u2019s lives, your behaviorwill reveal your implicit values/ preferences, and these preferences\ncan then be converted to a real-valued scale, such as dollars or QALY. Inferring a utility function from behavior is calledinverse reinforcement learning.", "217": "5.7. Bayesian decision theory 187\nDecision true label y\n\u02c6y 01\npredict 0 010\npredict 1 100\nreject 33\na. Show that the minimum risk is obtained if we decide Y=jifp(Y=j|x)\u2265p(Y=k|x)for allk\n(i.e.,jis the most probable class) andifp(Y=j|x)\u22651\u2212\u03bbr\n\u03bbs; otherwise we decide to reject.\nb. Describe qualitatively what happens as \u03bbr/\u03bbsis increased from 0 to 1 (i.e., the relative cost of rejection\nincreases).\nExercise 5.4 More reject options\nIn many applications, the classi\ufb01er is allowed to \u201creject\u201d a test example rather than classifying it into one\nof the classes. Consider, for example, a case in which the cost of a misclassi\ufb01cation is $10 but the cost ofhaving a human manually make the decison is only $3. We can formulate this as the following loss matrix:\na. Suppose P(y=1|x)is predicted to be 0.2. Which decision minimizes the expected loss?\nb. Now suppose P(y=1|x)=0.4. Now which decision minimizes the expected loss?\nc. Show that in general, for this loss matrix, but for any posterior distribution, there will be two thresholds\n\u03b8\n0and\u03b81such that the optimal decisionn is to predict 0 if p1<\u03b80, reject if \u03b80\u2264p1\u2264\u03b81, and\npredict 1 if p1>\u03b81(wherep1=p(y=1|x)). What are these thresholds?\nExercise 5.5 Newsvendor problem\nConsider the following classic problem in decision theory/ economics. Suppose you are trying to decidehow much quantity Qof some product (e.g., newspapers) to buy to maximize your pro\ufb01ts. The optimal\namount will depend on how much demand Dyou think there is for your product, as well as its cost\nto youCand its selling price P. Suppose Dis unknown but has pdf f(D)and cdfF(D). We can\nevaluate the expected pro\ufb01t by considering two cases: if D>Q, then we sell all Qitems, and make pro\ufb01t\n\u03c0=(P\u2212C)Q; but ifD<Q, we only sell Ditems, at pro\ufb01t (P\u2212C)D, but have wasted C(Q\u2212D)\non the unsold items. So the expected pro\ufb01t if we buy quantity Qis\nE\u03c0(Q)=/integraldisplay\n\u221e\nQ(P\u2212C)Qf(D)dD+/integraldisplayQ\n0(P\u2212C)Df(D)\u2212/integraldisplayQ\n0C(Q\u2212D)f(D)dD (5.123)\nSimplify this expression, and then take derivatives wrt Qto show that the optimal quantity Q\u2217(which\nmaximizes the expected pro\ufb01t) satis\ufb01es\nF(Q\u2217)=P\u2212C\nP(5.124)\nExercise 5.6 Bayes factors and ROC curves\nLetB=p(D|H1)/p(D|H0)be the bayes factor in favor of model 1. Suppose we plot two ROC curves,\none computed by thresholding B, and the other computed by thresholding p(H1|D). Will they be the\nsame or different? Explain why.\nExercise 5.7 Bayes model averaging helps predictive accuracy\nLet\u0394be a quantity that we want to predict, let Dbe the observed data and Mbe a \ufb01nite set of models.\nSuppose our action is to provide a probabilistic prediction p(), and the loss function is L(\u0394,p()) =", "218": "188 Chapter 5. Bayesian statistics\n\u2212logp(\u0394). We can either perform Bayes model averaging and predict using\npBMA(\u0394) =/summationdisplay\nm\u2208Mp(\u0394|m,D)p(m|D) (5.125)\nor we could predict using any single model (a plugin approximation)\npm(\u0394) =p(\u0394|m,D) (5.126)\nShow that, for all models m\u2208M, the posterior expected loss using BMA is lower, i.e.,\nE/bracketleftBig\nL(\u0394,pBMA)/bracketrightBig\n\u2264E[L(\u0394,pm)] (5.127)\nwhere the expectation over \u0394is with respect to\np(\u0394|D)=/summationdisplay\nm\u2208Mp(\u0394|m,D)p(m|D) (5.128)\nHint: use the non-negativity of the KL divergence.\nExercise 5.8 MLE and model selection for a 2d discrete distribution\n(Source: Jaakkola.)\nLetx\u2208{0,1}denote the result of a coin toss (x =0for tails,x=1for heads). The coin is potentially\nbiased, so that heads occurs with probability \u03b81. Suppose that someone else observes the coin \ufb02ip and\nreports to you the outcome, y. But this person is unreliable and only reports the result correctly with\nprobability \u03b82; i.e.,p(y|x,\u03b82)is given by\ny=0y=1\nx=0\u03b821\u2212\u03b82\nx=11\u2212\u03b82\u03b82\nAssume that \u03b82is independent of xand\u03b81.\na. Write down the joint probability distribution p(x,y|\u03b8)as a2\u00d72table, in terms of \u03b8=(\u03b81,\u03b82).\nb. Suppose have the following dataset: x=( 1,1,0,1,1,0,0),y=( 1,0,0,0,1,0,1). What are the\nMLEs for \u03b81and\u03b82? Justify your answer. Hint: note that the likelihood function factorizes,\np(x,y|\u03b8)=p(y|x,\u03b82)p(x|\u03b81) (5.129)\nWhat isp(D|\u02c6\u03b8,M2)whereM2denotes this 2-parameter model? (You may leave your answer in\nfractional form if you wish.)\nc. Now consider a model with 4 parameters, \u03b8=(\u03b80,0,\u03b80,1,\u03b81,0,\u03b81,1), representing p(x,y|\u03b8)=\u03b8x,y.\n(Only 3 of these parameters are free to vary, since they must sum to one.) What is the MLE of \u03b8? What\nisp(D|\u02c6\u03b8,M4)whereM4denotes this 4-parameter model?\nd. Suppose we are not sure which model is correct. We compute the leave-one-out cross validated log\nlikelihood of the 2-parameter model and the 4-parameter model as follows:\nL(m)=n/summationdisplay\ni=1logp(xi,yi|m,\u02c6\u03b8(D\u2212i)) (5.130)\nand\u02c6\u03b8(D\u2212i))denotes the MLE computed on Dexcluding row i. Which model will CV pick and why?\nHint: notice how the table of counts changes when you omit each training case one at a time.", "219": "5.7. Bayesian decision theory 189\ne. Recall that an alternative to CV is to use the BIC score, de\ufb01ned as\nBIC(M,D)/defineslogp(D|\u02c6\u03b8MLE)\u2212dof(M)\n2logN (5.131)\nwheredof(M)is the number of free parameters in the model, Compute the BIC scores for both models\n(use log base e). Which model does BIC prefer?\nExercise 5.9 Posterior median is optimal estimate under L1 loss\nProve that the posterior median is optimal estimate under L1 loss.\nExercise 5.10 Decision rule for trading off FPs and FNs\nIfLFN=cLFP, show that we should pick \u02c6y=1iffp(y=1|x)/p(y=0|x)>\u03c4,w h e r e\u03c4=c/(1+c)", "220": "", "221": "6 Frequentist statistics\n6.1 Introduction\nThe approach to statistical inference that we described in Chapter 5 is known as Bayesian\nstatistics. Perhaps surprisingly, this is considered controversial by some people, whereas the ap-plication of Bayes rule to non-statistical problems \u2014 such as medical diagnosis (Section 2.2.3.1),spam \ufb01ltering (Section 3.4.4.1), or airplane tracking (Section 18.2.1) \u2014 is not controversial. Thereason for the objection has to do with a misguided distinction between parameters of a statis-tical model and other kinds of unknown quantities.\n1\nAttempts have been made to devise approaches to statistical inference that avoid treating\nparameters like random variables, and which thus avoid the use of priors and Bayes rule. Suchapproaches are known as frequentist statistics, classical statistics ororthodox statistics.\nInstead of being based on the posterior distribution, they are based on the concept of a samplingdistribution. This is the distribution that an estimator has when applied to multiple data setssampled from the true but unknown distribution; see Section 6.2 for details. It is this notionof variation across repeated trials that forms the basis for modeling uncertainty used by thefrequentist approach.\nBy contrast, in the Bayesian approach, we only ever condition on the actually observed data;\nthere is no notion of repeated trials. This allows the Bayesian to compute the probability ofone-off events, as we discussed in Section 2.1. Perhaps more importantly, the Bayesian approachavoids certain paradoxes that plague the frequentist approach (see Section 6.6). Nevertheless, itis important to be familiar with frequentist statistics (especially Section 6.5), since it is widelyused in machine learning.\n6.2 Sampling distribution of an estimator\nIn frequentist statistics, a parameter estimate \u02c6\u03b8is computed by applying an estimator \u03b4to\nsome data D,s o\u02c6\u03b8=\u03b4(D). The parameter is viewed as \ufb01xed and the data as random, which\nis the exact opposite of the Bayesian approach. The uncertainty in the parameter estimate canbe measured by computing the sampling distribution of the estimator. To understand this\n1. Parameters are sometimes considered to represent true (but unknown) physical quantities, which are therefore not\nrandom. However, we have seen that it is perfectly reasonable to use a probability distribution to represent one\u2019s\nuncertainty about an unknown constant.", "222": "192 Chapter 6. Frequentist statistics\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 105001000150020002500300035004000Boot: true = 0.70, n=10, mle = 0.90, se = 0.001\n(a)0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10500100015002000250030003500Boot: true = 0.70, n=100, mle = 0.70, se = 0.000\n(b)\nFigure 6.1 A bootstrap approximation to the sampling distribution of \u02c6\u03b8for a Bernoulli distribution. We\nuseB=1 0,000bootstrap samples. The Ndatacases were generated from Ber(\u03b8=0.7). (a) MLE with\nN=1 0. (b) MLE with N= 100. Figure generated by bootstrapDemoBer .\nconcept, imagine sampling many different data sets D(s)from some true model, p(\u00b7|\u03b8\u2217), i.e., let\nD(s)={x(s)\ni}N\ni=1,w h e r exsi\u223cp(\u00b7|\u03b8\u2217), and\u03b8\u2217is the true parameter. Here s=1:Sindexes\nthe sampled data set, and Nis the size of each such dataset. Now apply the estimator \u02c6\u03b8(\u00b7)\nto eachD(s)to get a set of estimates, {\u02c6\u03b8(D(s))}.A sw el e t S\u2192\u221e, the distribution induced\non\u02c6\u03b8(\u00b7)is the sampling distribution of the estimator. We will discuss various ways to use the\nsampling distribution in later sections. But \ufb01rst we sketch two approaches for computing the\nsampling distribution itself.\n6.2.1 Bootstrap\nThebootstrap is a simple Monte Carlo technique to approximate the sampling distribution. This\nis particularly useful in cases where the estimator is a complex function of the true parameters.\nThe idea is simple. If we knew the true parameters \u03b8\u2217, we could generate many (say S) fake\ndatasets, each of size N, from the true distribution, xs\ni\u223cp(\u00b7|\u03b8\u2217),f o rs=1:S,i=1:N.\nWe could then compute our estimator from each sample, \u02c6\u03b8s=f(xs\n1:N)and use the empirical\ndistribution of the resulting samples as our estimate of the sampling distribution. Since \u03b8is\nunknown, the idea of the parametric bootstrap is to generate the samples using \u02c6\u03b8(D)instead.\nAn alternative, called the non-parametric bootstrap, is to sample the xs\ni(with replacement)\nfrom the original data D, and then compute the induced distribution as before. Some methods\nfor speeding up the bootstrap when applied to massive data sets are discussed in (Kleiner et al.\n2011).\nFigure 6.1 shows an example where we compute the sampling distribution of the MLE for\na Bernoulli using the parametric bootstrap. (Results using the non-parametric bootstrap areessentially the same.) We see that the sampling distribution is asymmetric, and therefore quitefar from Gaussian, when N=1 0; whenN= 100, the distribution looks more Gaussian, as\ntheory suggests (see below).\nA natural question is: what is the connection between the parameter estimates \u02c6\u03b8\ns=\u02c6\u03b8(xs\n1:N)\ncomputed by the bootstrap and parameter values sampled from the posterior, \u03b8s\u223cp(\u00b7|D)?", "223": "6.2. Sampling distribution of an estimator 193\nConceptually they are quite different. But in the common case that that the prior is not very\nstrong, they can be quite similar. For example, Figure 6.1(c-d) shows an example where wecompute the posterior using a uniform Beta(1,1) prior, and then sample from it. We see thatthe posterior and the sampling distribution are quite similar. So one can think of the bootstrapdistribution as a \u201cpoor man\u2019s\u201d posterior; see (Hastie et al. 2001, p235) for details.\nHowever, perhaps surprisingly, bootstrap can be slower than posterior sampling. The reason\nis that the bootstrap has to \ufb01t the model Stimes, whereas in posterior sampling, we usually\nonly \ufb01t the model once (to \ufb01nd a local mode), and then perform local exploration around themode. Such local exploration is usually much faster than \ufb01tting a model from scratch.\n6.2.2 Large sample theory for the MLE *\nIn some cases, the sampling distribution for some estimators can be computed analytically. Inparticular, it can be shown that, under certain conditions, as the sample size tends to in\ufb01nity,the sampling distribution of the MLE becomes Gaussian. Informally, the requirement for thisresult to hold is that each parameter in the model gets to \u201csee\u201d an in\ufb01nite amount of data, andthat the model be identi\ufb01able. Unfortunately this excludes many of the models of interest tomachine learning. Nevertheless, let us assume we are in a simple setting where the theoremholds.\nThe center of the Gaussian will be the MLE \u02c6\u03b8. But what about the variance of this Gaussian?\nIntuitively the variance of the estimator will be (inversely) related to the amount of curvature ofthe likelihood surface at its peak. If the curvature is large, the peak will be \u201csharp\u201d, and thevariance low; in this case, the estimate is \u201cwell determined\u201d. By contrast, if the curvature issmall, the peak will be nearly \u201c\ufb02at\u201d, so the variance is high.\nLet us now formalize this intuition. De\ufb01ne the score function as the gradient of the log\nlikelihood evaluated at some point \u02c6\u03b8:\ns(\u02c6\u03b8)/defines\u2207logp(D|\u03b8)|\n\u02c6\u03b8(6.1)\nDe\ufb01ne the observed information matrix as the gradient of the negative score function, or\nequivalently, the Hessian of the NLL:\nJ(\u02c6\u03b8(D))/defines\u2212\u2207s(\u02c6\u03b8)=\u2212\u22072\n\u03b8logp(D|\u03b8)|\u02c6\u03b8(6.2)\nIn 1D, this becomes\nJ(\u02c6\u03b8(D)) =\u2212d\nd\u03b82logp(D|\u03b8)|\u02c6\u03b8(6.3)\nThis is just a measure of curvature of the log-likelihood function at \u02c6\u03b8.\nSince we are studying the sampling distribution, D=(x1,...,xN)is a set of random\nvariables. The Fisher information matrix is de\ufb01ned to be the expected value of the observed\ninformation matrix:2\nIN(\u02c6\u03b8|\u03b8\u2217)/definesE\u03b8\u2217\u23a8bracketleftBig\nJ(\u02c6\u03b8|D)\u23a8bracketrightBig\n(6.4)\n2. This is not the usual de\ufb01nition, but is equivalent to it under standard assumptions. More precisely, the standard\nde\ufb01nition is as follows (we just give the scalar case to simplify notation): I(\u02c6\u03b8|\u03b8\u2217)/definesvar\u03b8\u2217/bracketleftBig\nd\nd\u03b8logp(X|\u03b8)|\u02c6\u03b8/bracketrightBig\n, that\nis, the variance of the score function. If \u02c6\u03b8is the MLE, it is easy to see that E\u03b8\u2217/bracketleftBig\nd\nd\u03b8logp(X|\u03b8)|\u02c6\u03b8/bracketrightBig\n=0(since", "224": "194 Chapter 6. Frequentist statistics\nwhere E\u03b8\u2217[f(D)]/defines1\nN\u23a8summationtextN\ni=1f(xi)p(xi|\u03b8\u2217)is the expected value of the function fwhen\napplied to data sampled from \u03b8\u2217.O f t e n\u03b8\u2217, representing the \u201ctrue parameter\u201d that generated\nthe data, is assumed known, so we just write IN(\u02c6\u03b8)/definesIN(\u02c6\u03b8|\u03b8\u2217)for short. Furthermore, it is\neasy to see that IN(\u02c6\u03b8)=NI1(\u02c6\u03b8), because the log-likelihood for a sample of size Nis justN\ntimes \u201csteeper\u201d than the log-likelihood for a sample of size 1. So we can drop the 1subscript\nand just write I(\u02c6\u03b8)/definesI1(\u02c6\u03b8). This is the notation that is usually used.\nNow let\u02c6\u03b8/defines\u02c6\u03b8mle(D)be the MLE, where D\u223c\u03b8\u2217. It can be shown that\n\u02c6\u03b8\u2192N(\u03b8\u2217,IN(\u03b8\u2217)\u22121) (6.5)\nasN\u2192\u221e(see e.g., (Rice 1995, p265) for a proof). We say that the sampling distribution of the\nMLE isasymptotically normal.\nWhat about the variance of the MLE, which can be used as some measure of con\ufb01dence\nin the MLE? Unfortunately, \u03b8\u2217is unknown, so we can\u2019t evaluate the variance of the sampling\ndistribution. However, we can approximate the sampling distribution by replacing \u03b8\u2217with\u02c6\u03b8.\nConsequently, the approximate standard errors of\u02c6\u03b8kare given by\n\u02c6sek/definesIN(\u02c6\u03b8)\u22121\n2\nkk(6.6)\nFor example, from Equation 5.60 we know that the Fisher information for a binomial sampling\nmodel is\nI(\u03b8)=1\n\u03b8(1\u2212\u03b8)(6.7)\nSo the approximate standard error of the MLE is\n\u02c6se=1\u23a8radicalBig\nIN(\u02c6\u03b8)=1\u23a8radicalBig\nNI(\u02c6\u03b8)=\u23a8parenleftBigg\u02c6\u03b8(1\u2212\u02c6\u03b8)\nN\u23a8parenrightBigg1\n2\n(6.8)\nwhere\u02c6\u03b8=1\nN\u23a8summationtext\niXi. Compare this to Equation 3.27, which is the posterior standard deviation\nunder a uniform prior.\n6.3 Frequentist decision theory\nIn frequentist or classical decision theory, there is a loss function and a likelihood, but there is\nno prior and hence no posterior or posterior expected loss. Thus there is no automatic way ofderiving an optimal estimator, unlike the Bayesian case. Instead, in the frequentist approach, weare free to choose any estimator or decision procedure \u03b4:X\u2192Awe want.\n3\nthe gradient must be zero at a maximum), so the variance reduces to the expected square of the score function:\nI(\u02c6\u03b8|\u03b8\u2217)=E \u03b8\u2217/bracketleftBig\n(d\nd\u03b8logp(X|\u03b8))2/bracketrightBig\n. It can be shown (e.g., (Rice 1995, p263)) that E\u03b8\u2217/bracketleftBig\n(d\nd\u03b8logp(X|\u03b8))2/bracketrightBig\n=\n\u2212E\u03b8\u2217/bracketleftBig\nd2\nd\u03b82logp(X|\u03b8)/bracketrightBig\n, so now the Fisher information reduces to the expected second derivative of the NLL, which\nis a much more intuitive quantity than the variance of the score.\n3. In practice, the frequentist approach is usually only applied to one-shot statistical decision problems \u2014 such as\nclassi\ufb01cation, regression and parameter estimation \u2014 since its non-constructive nature makes it difficult to apply to\nsequential decision problems, which adapt to data online.", "225": "6.3. Frequentist decision theory 195\nHaving chosen an estimator, we de\ufb01ne its expected loss or riskas follows:\nR(\u03b8\u2217,\u03b4)/definesEp(\u02dcD|\u03b8\u2217)\u23a8bracketleftBig\nL(\u03b8\u2217,\u03b4(\u02dcD))\u23a8bracketrightBig\n=\u23a8integraldisplay\nL(\u03b8\u2217,\u03b4(\u02dcD))p(\u02dcD|\u03b8\u2217)d\u02dcD (6.9)\nwhere\u02dcDis data sampled from \u201cnature\u2019s distribution\u201d, which is represented by parameter \u03b8\u2217.I n\nother words, the expectation is wrt the sampling distribution of the estimator. Compare this to\nthe Bayesian posterior expected loss:\n\u03c1(a|D,\u03c0)/definesEp(\u03b8|D,\u03c0)[L(\u03b8,a)] =\u23a8integraldisplay\n\u0398L(\u03b8,a)p(\u03b8|D,\u03c0)d\u03b8 (6.10)\nWe see that the Bayesian approach averages over \u03b8(which is unknown) and conditions on D\n(which is known), whereas the frequentist approach averages over \u02dcD(thus ignoring the observed\ndata), and conditions on \u03b8\u2217(which is unknown).\nNot only is the frequentist de\ufb01nition unnatural, it cannot even be computed, because \u03b8\u2217is\nunknown. Consequently, we cannot compare different estimators in terms of their frequentistrisk. We discuss various solutions to this below.\n6.3.1 Bayes risk\nHow do we choose amongst estimators? We need some way to convert R(\u03b8\u2217,\u03b4)into a single\nmeasure of quality, R(\u03b4), which does not depend on knowing \u03b8\u2217. One approach is to put a\nprior on\u03b8\u2217, and then to de\ufb01ne Bayes risk orintegrated risk of an estimator as follows:\nRB(\u03b4)/definesEp(\u03b8\u2217)[R(\u03b8\u2217,\u03b4)] =\u23a8integraldisplay\nR(\u03b8\u2217,\u03b4)p(\u03b8\u2217)d\u03b8\u2217(6.11)\nABayes estimator orBayes decision rule is one which minimizes the expected risk:\n\u03b4B/definesargmin\n\u03b4RB(\u03b4) (6.12)\nNote that the integrated risk is also called the preposterior risk, since it is before we have seen\nthe data. Minimizing this can be useful for experiment design.\nWe will now prove a very important theorem, that connects the Bayesian and frequentist\napproaches to decision theory.\nTheorem 6.3.1. A Bayes estimator can be obtained by minimizing the posterior expected loss for\neachx.\nProof.By switching the order of integration, we have\nRB(\u03b4)=\u23a8integraldisplay\u23a8bracketleftBigg\u23a8summationdisplay\nx\u23a8summationdisplay\nyL(y,\u03b4(x))p(x,y|\u03b8\u2217)\u23a8bracketrightBigg\np(\u03b8\u2217)d\u03b8\u2217(6.13)\n=\u23a8summationdisplay\nx\u23a8summationdisplay\ny\u23a8integraldisplay\n\u0398L(y,\u03b4(x))p(x,y,\u03b8\u2217)d\u03b8\u2217(6.14)\n=\u23a8summationdisplay\nx\u23a8bracketleftBigg\u23a8summationdisplay\nyL(y,\u03b4(x))p(y|x)dy\u23a8bracketrightBigg\np(x) (6.15)\n=\u23a8summationdisplay\nx\u03c1(\u03b4(x)|x)p(x) (6.16)", "226": "196 Chapter 6. Frequentist statistics\nR(\u03b8,\u03b42)R(\u03b8,\u03b41)\n\u03b8R\nFigure 6.2 Risk functions for two decision procedures, \u03b41and\u03b42. Since\u03b41has lower worst case risk, it\nis the minimax estimator, even though \u03b42has lower risk for most values of \u03b8. Thus minimax estimators\nare overly conservative.\nTo minimize the overall expectation, we just minimize the term inside for each x, so our decision\nrule is to pick\n\u03b4B(x) = argmin\na\u2208A\u03c1(a|x) (6.17)\nHence we see that the picking the optimal action on a case-by-case basis (as in the Bayesian\napproach) is optimal on average (as in the frequentist approach). In other words, the Bayesian\napproach provides a good way of achieving frequentist goals. In fact, one can go further andprove the following.\nTheorem 6.3.2 (Wald, 1950). Every admissable decision rule is a Bayes decision rule with respect\nto some, possibly improper, prior distribution.\nThis theorem shows that the best way to minimize frequentist risk is to be Bayesian! See\n(Bernardo and Smith 1994, p448) for further discussion of this point.\n6.3.2 Minimax risk\nObviously some frequentists dislike using Bayes risk since it requires the choice of a prior (al-\nthough this is only in the evaluation of the estimator, not necessarily as part of its construction).An alternative approach is as follows. De\ufb01ne the maximum risk of an estimator as\nR\nmax(\u03b4)/definesmax\n\u03b8\u2217R(\u03b8\u2217,\u03b4) (6.18)\nAminimax rule is one which minimizes the maximum risk:\n\u03b4MM/definesargmin\n\u03b4Rmax(\u03b4) (6.19)", "227": "6.3. Frequentist decision theory 197\nFor example, in Figure 6.2, we see that \u03b41has lower worst-case risk than \u03b42, ranging over all\npossible values of \u03b8\u2217, so it is the minimax estimator (see Section 6.3.3.1 for an explanation of\nhow to compute a risk function for an actual model).\nMinimax estimators have a certain appeal. However, computing them can be hard. And\nfurthermore, they are very pessimistic. In fact, one can show that all minimax estimators\nare equivalent to Bayes estimators under a least favorable prior. In most statistical situations\n(excluding game theoretic ones), assuming nature is an adversary is not a reasonable assumption.\n6.3.3 Admissible estimators\nThe basic problem with frequentist decision theory is that it relies on knowing the true distri-butionp(\u00b7|\u03b8\n\u2217)in order to evaluate the risk. However, It might be the case that some estimators\nare worse than others regardless of the value of \u03b8\u2217. In particular, if R(\u03b8,\u03b4 1)\u2264R(\u03b8,\u03b4 2)for all\n\u03b8\u2208\u0398, then we say that \u03b41dominates \u03b42. The domination is said to be strict if the inequality\nis strict for some \u03b8. An estimator is said to be admissible if it is not strictly dominated by any\nother estimator.\n6.3.3.1 Example\nLet us give an example, based on (Bernardo and Smith 1994). Consider the problem of estimatingthe mean of a Gaussian. We assume the data is sampled from x\ni\u223cN(\u03b8\u2217,\u03c32=1 )and use\nquadratic loss, L(\u03b8,\u02c6\u03b8)=(\u03b8\u2212\u02c6\u03b8)2. The corresponding risk function is the MSE. Some possible\ndecision rules or estimators \u02c6\u03b8(x)=\u03b4(x)are as follows:\n\u2022\u03b41(x)=x, the sample mean\n\u2022\u03b42(x)=\u02dcx, the sample median\n\u2022\u03b43(x)=\u03b80, a \ufb01xed value\n\u2022\u03b4\u03ba(x), the posterior mean under a N(\u03b8|\u03b80,\u03c32/\u03ba)prior:\n\u03b4\u03ba(x)=N\nN+\u03bax+\u03ba\nN+\u03ba\u03b80=wx+(1\u2212w)\u03b80 (6.20)\nFor\u03b4\u03ba, we consider a weak prior, \u03ba=1, and a stronger prior, \u03ba=5. The prior mean is \u03b80,\nsome \ufb01xed value. We assume \u03c32is known. (Thus \u03b43(x)is the same as \u03b4\u03ba(x)with an in\ufb01nitely\nstrong prior, \u03ba=\u221e.)\nLet us now derive the risk functions analytically. (We can do this since in this toy example,\nwe know the true parameter \u03b8\u2217.) In Section 6.4.4, we show that the MSE can be decomposed\ninto squared bias plus variance:\nMSE(\u02c6\u03b8(\u00b7)|\u03b8\u2217)=v a r\u23a8bracketleftBig\n\u02c6\u03b8\u23a8bracketrightBig\n+bias2(\u02c6\u03b8) (6.21)\nThe sample mean is unbiased, so its risk is\nMSE(\u03b41|\u03b8\u2217)=v a r[x]=\u03c32\nN(6.22)", "228": "198 Chapter 6. Frequentist statistics\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 200.050.10.150.20.250.30.350.40.450.5\n\u03b8*R(\u03b8*,\u03b4)risk functions for n=5\n  \nmle\nmedian\nfixed\npostmean1\npostmean5\n(a)\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 200.020.040.060.080.10.120.140.160.18\n\u03b8*R(\u03b8*,\u03b4)risk functions for n=20\n  \nmle\nmedian\nfixed\npostmean1\npostmean5\n(b)\nFigure 6.3 Risk functions for estimating the mean of a Gaussian using data sampled N(\u03b8\u2217,\u03c32=1 ).\nThe solid dark blue horizontal line is the MLE, the solid light blue curved line is the posterior mean when\n\u03ba=5. Left:N=5samples. Right: N=2 0samples. Based on Figure B.1 of (Bernardo and Smith 1994).\nFigure generated by riskFnGauss .\nThe sample median is also unbiased. One can show that the variance is approximately \u03c0/(2N),\nso\nMSE(\u03b42|\u03b8\u2217)=\u03c0\n2N(6.23)\nFor\u03b43(x)=\u03b80, the variance is zero, so\nMSE(\u03b43|\u03b8\u2217)=(\u03b8\u2217\u2212\u03b80)2(6.24)\nFinally, for the posterior mean, we have\nMSE(\u03b4\u03ba|\u03b8\u2217)=E\u23a8bracketleftBig\n(wx+(1\u2212w)\u03b80\u2212\u03b8\u2217)2\u23a8bracketrightBig\n(6.25)\n=E\u23a8bracketleftBig\n(w(x\u2212\u03b8\u2217)+(1\u2212w)(\u03b80\u2212\u03b8\u2217))2\u23a8bracketrightBig\n(6.26)\n=w2\u03c32\nN+(1\u2212w)2(\u03b80\u2212\u03b8\u2217)2(6.27)\n=1\n(N+\u03ba)2\u23a8parenleftbig\nN\u03c32+\u03ba2(\u03b80\u2212\u03b8\u2217)2\u23a8parenrightbig\n(6.28)\nThese functions are plotted in Figure 6.3 for N\u2208{5,20}. We see that in general, the best\nestimator depends on the value of \u03b8\u2217, which is unknown. If \u03b8\u2217is very close to \u03b80, then\u03b43\n(which just predicts \u03b80) is best. If \u03b8\u2217is within some reasonable range around \u03b80, then the\nposterior mean, which combines the prior guess of \u03b80with the actual data, is best. If \u03b8\u2217is far\nfrom\u03b80, the MLE is best. None of this should be suprising: a small amount of shrinkage (using\nthe posterior mean with a weak prior) is usually desirable, assuming our prior mean is sensible.\nWhat is more surprising is that the risk of decision rule \u03b42(sample median) is always higher\nthan that of \u03b41(sample mean) for every value of \u03b8\u2217. Consequently the sample median is an", "229": "6.3. Frequentist decision theory 199\ninadmissible estimator for this particular problem (where the data is assumed to come from a\nGaussian).\nIn practice, the sample median is often better than the sample mean, because it is more\nrobust to outliers. One can show (Minka 2000d) that the median is the Bayes estimator (undersquared loss) if we assume the data comes from a Laplace distribution, which has heavier tailsthan a Gaussian. More generally, we can construct robust estimators by using \ufb02exible modelsof our data, such as mixture models or non-parametric density estimators (Section 14.7.2), andthen computing the posterior mean or median.\n6.3.3.2 Stein\u2019s paradox *\nSuppose we have Niid random variables X\ni\u223cN(\u03b8i,1), and we want to estimate the \u03b8i. The\nobvious estimator is the MLE, which in this case sets \u02c6\u03b8i=xi. It turns out that this is an\ninadmissible estimator under quadratic loss, when N\u22654.\nTo show this, it suffices to construct an estimator that is better. The James-Stein estimator is\none such estimator, and is de\ufb01ned as follows:\n\u02c6\u03b8i=\u02c6Bx+(1\u2212\u02c6B)xi=x+(1\u2212\u02c6B)(xi\u2212x) (6.29)\nwherex=1\nN\u23a8summationtextN\ni=1xiand0<B<1 is some tuning constant. This estimate \u201cshrinks\u201d the\n\u03b8itowards the overall mean. (We derive this estimator using an empirical Bayes approach in\nSection 5.6.2.)\nIt can be shown that this shrinkage estimator has lower frequentist risk (MSE) than the MLE\n(sample mean) for N\u22654. This is known as Stein\u2019s paradox. The reason it is called a paradox\nis illustrated by the following example. Suppose \u03b8iis the \u201ctrue\u201d IQ of student iandXiis his\ntest score. Why should my estimate of \u03b8idepend on the global mean x, and hence on some\nother student\u2019s scores? One can create even more paradoxical examples by making the different\ndimensions be qualitatively different, e.g., \u03b81is my IQ, \u03b82is the average rainfall in Vancouver,\netc.\nThe solution to the paradox is the following. If your goal is to estimate just \u03b8i, you cannot do\nbetter than using xi, but if the goal is to estimate the whole vector \u03b8, and you use squared error\nas your loss function, then shrinkage helps. To see this, suppose we want to estimate ||\u03b8||2\n2from\na single sample x\u223cN(\u03b8,I). A simple estimate is ||x||22, but this will overestimate the result,\nsince\nE\u23a8bracketleftbig\n/bardblx/bardbl2\n2\u23a8bracketrightbig\n=E\u23a8bracketleftBigg\u23a8summationdisplay\nix2i\u23a8bracketrightBigg\n=N\u23a8summationdisplay\ni=1\u23a8parenleftbig\n1+\u03b82\ni\u23a8parenrightbig\n=N+||\u03b8||22(6.30)\nConsequently we can reduce our risk by pooling information, even from unrelated sources, and\nshrinking towards the overall mean. In Section 5.6.2, we give a Bayesian explanation for this.See also (Efron and Morris 1975).\n6.3.3.3 Admissibility is not enough\nIt seems clear that we can restrict our search for good estimators to the class of admissibleestimators. But in fact it is easy to construct admissible estimators, as we show in the followingexample.", "230": "200 Chapter 6. Frequentist statistics\nTheorem 6.3.3. LetX\u223cN(\u03b8,1), and consider estimating \u03b8under squared loss. Let \u03b41(x)=\u03b8 0,\na constant independent of the data. This is an admissible estimator.\nProof.Suppose not. Then there is some other estimator \u03b42with smaller risk, so R(\u03b8\u2217,\u03b42)\u2264\nR(\u03b8\u2217,\u03b41), where the inequality must be strict for some \u03b8\u2217. Suppose the true parameter is\n\u03b8\u2217=\u03b80. ThenR(\u03b8\u2217,\u03b41)=0, and\nR(\u03b8\u2217,\u03b42)=\u23a8integraldisplay\n(\u03b42(x)\u2212\u03b80)2p(x|\u03b80)dx (6.31)\nSince0\u2264R(\u03b8\u2217,\u03b42)\u2264R(\u03b8\u2217,\u03b41)for all\u03b8\u2217, andR(\u03b80,\u03b41)=0,w eh a v e R(\u03b80,\u03b42)=0and\nhence\u03b42(x)=\u03b8 0=\u03b41(x). Thus the only way \u03b42can avoid having higher risk than \u03b41at some\nspeci\ufb01c point \u03b80is by being equal to \u03b41. Hence there is no other estimator \u03b42with strictly lower\nrisk, so\u03b42is admissible.\n6.4 Desirable properties of estimators\nSincefrequentistdecisiontheorydoesnotprovideanautomaticwaytochoosethebestestimator,\nwe need to come up with other heuristics for choosing amongst them. In this section, we discusssome properties we would like estimators to have. Unfortunately, we will see that we cannotachieve all of these properties at the same time.\n6.4.1 Consistent estimators\nAn estimator is said to be consistent if it eventually recovers the true parameters that generated\nthe data as the sample size goes to in\ufb01nity, i.e., \u02c6\u03b8(D)\u2192\u03b8\u2217as|D| \u2192 \u221e (where the arrow\ndenotes convergence in probability). Of course, this concept only makes sense if the data actuallycomes from the speci\ufb01ed model with parameters \u03b8\n\u2217, which is not usually the case with real\ndata. Nevertheless, it can be a useful theoretical property.\nIt can be shown that the MLE is a consistent estimator. The intuitive reason is that maxi-\nmizing likelihood is equivalent to minimizing KL\u23a8parenleftBig\np(\u00b7|\u03b8\u2217)||p(\u00b7|\u02c6\u03b8)\u23a8parenrightBig\n,w h e r ep(\u00b7|\u03b8\u2217)is the true\ndistribution and p(\u00b7|\u02c6\u03b8)is our estimate. We can achieve 0 KL divergence iff \u02c6\u03b8=\u03b8\u2217.4\n6.4.2 Unbiased estimators\nThebiasof an estimator is de\ufb01ned as\nbias(\u02c6\u03b8(\u00b7)) = Ep(D|\u03b8\u2217)\u23a8bracketleftBig\n\u02c6\u03b8(D)\u2212\u03b8\u2217\u23a8bracketrightBig\n(6.32)\nwhere\u03b8\u2217is the true parameter value. If the bias is zero, the estimator is called unbiased. This\nmeans the sampling distribution is centered on the true parameter. For example, the MLE for aGaussian mean is unbiased:\nbias(\u02c6\u03bc)=E[\nx]\u2212\u03bc=E\u23a8bracketleftBigg\n1\nNN\u23a8summationdisplay\ni=1xi\u23a8bracketrightBigg\n\u2212\u03bc=N\u03bc\nN\u2212\u03bc=0 (6.33)\n4. If the model is unidenti\ufb01able, the MLE may select a set of parameters that is different from the true parameters\nbut for which the induced distribution, p(\u00b7|\u02c6\u03b8), is the same as the exact distribution. Such parameters are said to be\nlikelihood equivalent.", "231": "6.4. Desirable properties of estimators 201\nHowever, the MLE for a Gaussian variance, \u02c6\u03c32, is not an unbiased estimator of \u03c32. In fact, one\ncan show (Exercise 6.3) that\nE\u23a8bracketleftbig\n\u02c6\u03c32\u23a8bracketrightbig\n=N\u22121\nN\u03c32(6.34)\nHowever, the following estimator\n\u02c6\u03c32\nN\u22121=N\nN\u22121\u02c6\u03c32=1\nN\u22121N\u23a8summationdisplay\ni=1(xi\u2212x)2(6.35)\nisan unbiased estimator, which we can easily prove as follows:\nE\u23a8bracketleftbig\n\u02c6\u03c32\nN\u22121\u23a8bracketrightbig\n=E\u23a8bracketleftbiggN\nN\u22121\u02c6\u03c32\u23a8bracketrightbigg\n=N\nN\u22121N\u22121\nN\u03c32=\u03c32(6.36)\nIn Matlab, var(X)returns\u02c6\u03c32\nN\u22121, whereas var(X,1) returns\u02c6\u03c32(the MLE). For large enough\nN, the difference will be negligible.\nAlthough the MLE may sometimes be a biased estimator, one can show that asymptotically, it\nis always unbiased. (This is necessary for the MLE to be a consistent estimator.)\nAlthough being unbiased sounds like a desirable property, this is not always true. See Sec-\ntion 6.4.4 and (Lindley 1972) for discussion of this point.\n6.4.3 Minimum variance estimators\nIt seems intuitively reasonable that we want our estimator to be unbiased (although we shall\ngive some arguments against this claim below). However, being unbiased is not enough. Forexample, suppose we want to estimate the mean of a Gaussian from D={x\n1,...,x N}. The\nestimator that just looks at the \ufb01rst data point, \u02c6\u03b8(D)=x 1, is an unbiased estimator, but will\ngenerally be further from \u03b8\u2217than the empirical mean x(which is also unbiased). So the variance\nof an estimator is also important.\nA natural question is: how long can the variance go? A famous result, called the Cramer-\nRao lower bound, provides a lower bound on the variance of any unbiased estimator. Moreprecisely,\nTheorem 6.4.1 (Cramer-Rao inequality). LetX\n1,...,X n\u223cp(X|\u03b80)and\u02c6\u03b8=\u02c6\u03b8(x1,...,x n)be\nan unbiased estimator of \u03b80. Then, under various smoothness assumptions on p(X|\u03b80),w eh a v e\nvar\u23a8bracketleftBig\n\u02c6\u03b8\u23a8bracketrightBig\n\u22651\nnI(\u03b80)(6.37)\nwhereI(\u03b80)is the Fisher information matrix (see Section 6.2.2).\nA proof can be found e.g., in (Rice 1995, p275).\nIt can be shown that the MLE achieves the Cramer Rao lower bound, and hence has the\nsmallest asymptotic variance of any unbiased estimator. Thus MLE is said to be asymptotically\noptimal.", "232": "202 Chapter 6. Frequentist statistics\n6.4.4 The bias-variance tradeoff\nAlthough using an unbiased estimator seems like a good idea, this is not always the case. To see\nwhy, suppose we use quadratic loss. As we showed above, the corresponding risk is the MSE.We now derive a very useful decomposition of the MSE. (All expectations and variances are wrtthe true distribution p(D|\u03b8\n\u2217), but we drop the explicit conditioning for notational brevity.) Let\n\u02c6\u03b8=\u02c6\u03b8(D)denote the estimate, and \u03b8=E/bracketleftBig\n\u02c6\u03b8/bracketrightBig\ndenote the expected value of the estimate (as we\nvaryD). Then we have\nE/bracketleftBig\n(\u02c6\u03b8\u2212\u03b8\u2217)2/bracketrightBig\n=E/bracketleftbigg/bracketleftBig\n(\u02c6\u03b8\u2212\u03b8)+(\u03b8\u2212\u03b8\u2217)/bracketrightBig2/bracketrightbigg\n(6.38)\n=E/bracketleftbigg/parenleftBig\n\u02c6\u03b8\u2212\u03b8/parenrightBig2/bracketrightbigg\n+2(\u03b8\u2212\u03b8\u2217)E/bracketleftBig\n\u02c6\u03b8\u2212\u03b8/bracketrightBig\n+(\u03b8\u2212\u03b8\u2217)2(6.39)\n=E/bracketleftbigg/parenleftBig\n\u02c6\u03b8\u2212\u03b8/parenrightBig2/bracketrightbigg\n+(\u03b8\u2212\u03b8\u2217)2(6.40)\n=v a r/bracketleftBig\n\u02c6\u03b8/bracketrightBig\n+bias2(\u02c6\u03b8) (6.41)\nIn words,\nMSE = variance + bias2(6.42)\nThis is called the bias-variance tradeoff (see e.g., (Geman et al. 1992)). What it means is that\nit might be wise to use a biased estimator, so long as it reduces our variance, assuming our goalis to minimize squared error.\n6.4.4.1 Example: estimating a Gaussian mean\nLet us give an example, based on (Hoff 2009, p79). Suppose we want to estimate the mean of aGaussian from x=(x\n1,...,x N). We assume the data is sampled from xi\u223cN(\u03b8\u2217=1,\u03c32).\nAn obvious estimate is the MLE. This has a bias of 0 and a variance of\nvar[x|\u03b8\u2217]=\u03c32\nN(6.43)\nBut we could also use a MAP estimate. In Section 4.6.1, we show that the MAP estimate undera Gaussian prior of the form N(\u03b8\n0,\u03c32/\u03ba0)is given by\n\u02dcx/definesN\nN+\u03ba0x+\u03ba0\nN+\u03ba0\u03b80=wx+(1\u2212w)\u03b80 (6.44)\nwhere0\u2264w\u22641controls how much we trust the MLE compared to our prior. (This is also the\nposterior mean, since the mean and mode of a Gaussian are the same.) The bias and varianceare given by\nE[\u02dcx]\u2212\u03b8\n\u2217=w\u03b80+(1\u2212w)\u03b80\u2212\u03b8\u2217=( 1\u2212w)(\u03b80\u2212\u03b8\u2217) (6.45)\nvar[\u02dcx]=w2\u03c32\nN(6.46)", "233": "6.4. Desirable properties of estimators 203\n\u22121 \u22120.5 0 0.5 1 1.5 2 2.500.511.5sampling distribution, truth = 1.0, prior = 0.0, n = 5\n  \npostMean0\npostMean1\npostMean2\npostMean3\n(a)0 10 20 30 40 500.50.60.70.80.911.11.21.3\nsample sizerelative MSEMSE of postmean / MSE of MLE\n  \npostMean0\npostMean1\npostMean2\npostMean3\n(b)\nFigure 6.4 Left: Sampling distribution of the MAP estimate with different prior strengths \u03ba0. (The MLE\ncorresponds to \u03ba0=0.) Right: MSE relative to that of the MLE versus sample size. Based on Figure 5.6 of\n(Hoff 2009). Figure generated by samplingDistGaussShrinkage .\nSo although the MAP estimate is biased (assuming w<1), it has lower variance.\nLet us assume that our prior is slightly misspeci\ufb01ed, so we use \u03b80=0, whereas the truth is\n\u03b8\u2217=1. In Figure 6.4(a), we see that the sampling distribution of the MAP estimate for \u03ba0>0\nis biased away from the truth, but has lower variance (is narrower) than that of the MLE.\nIn Figure 6.4(b), we plot mse(\u02dcx)/mse(x)vsN. We see that the MAP estimate has lower MSE\nthan the MLE, especially for small sample size, for \u03ba0\u2208{1,2}. The case \u03ba0=0corresponds to\nthe MLE, and the case \u03ba0=3corresponds to a strong prior, which hurts performance because\nthe prior mean is wrong. It is clearly important to \u201ctune\u201d the strength of the prior, a topic we\ndiscuss later.\n6.4.4.2 Example: ridge regression\nAnother important example of the bias variance tradeoff arises in ridge regression, which wediscuss in Section 7.5. In brief, this corresponds to MAP estimation for linear regression undera Gaussian prior, p(w)=N(w|0,\u03bb\n\u22121I)The zero-mean prior encourages the weights to be\nsmall, which reduces over\ufb01tting; the precision term, \u03bb, controls the strength of this prior. Setting\n\u03bb=0results in the MLE; using \u03bb>0results in a biased estimate. To illustrate the effect on\nthe variance, consider a simple example. Figure 6.5 on the left plots each individual \ufb01tted curve,and on the right plots the average \ufb01tted curve. We see that as we increase the strength of theregularizer, the variance decreases, but the bias increases.\n6.4.4.3 Bias-variance tradeoff for classi\ufb01cation\nIf we use 0-1 loss instead of squared error, the above analysis breaks down, since the frequentistrisk is no longer expressible as squared bias plus variance. In fact, one can show (Exercise 7.2of (Hastie et al. 2009)) that the bias and variance combine multiplicatively. If the estimate is on", "234": "204 Chapter 6. Frequentist statistics\n0 0.2 0.4 0.6 0.8 1\u22121.5\u22121\u22120.500.511.5ln(\u03bb) = 5\n0 0.2 0.4 0.6 0.8 1\u22121\u22120.500.51ln(\u03bb) = 5\n0 0.2 0.4 0.6 0.8 1\u22121.5\u22121\u22120.500.511.5ln(\u03bb) = \u22125\n0 0.2 0.4 0.6 0.8 1\u22121\u22120.500.51ln(\u03bb) = \u22125\nFigure 6.5 Illustration of bias-variance tradeoff for ridge regression. We generate 100 data sets from the\ntrue function, shown in solid green. Left: we plot the regularized \ufb01t for 20 different data sets. We use\nlinear regression with a Gaussian RBF expansion, with 25 centers evenly spread over the [0,1]interval.\nRight: we plot the average of the \ufb01ts, averaged over all 100 datasets. Top row: strongly regularized: we seethat the individual \ufb01ts are similar to each other (low variance), but the average is far from the truth (highbias). Bottom row: lightly regularized: we see that the individual \ufb01ts are quite different from each other(high variance), but the average is close to the truth (low bias). Based on (Bishop 2006a) Figure 3.5. Figuregenerated by biasVarModelComplexity3 .\nthe correct side of the decision boundary, then the bias is negative, and decreasing the variance\nwill decrease the misclassi\ufb01cation rate. But if the estimate is on the wrong side of the decisionboundary, then the bias is positive, so it pays to increasethe variance (Friedman 1997a). This\nlittle known fact illustrates that the bias-variance tradeoff is not very useful for classi\ufb01cation.It is better to focus on expected loss (see below), not directly on bias and variance. We canapproximate the expected loss using cross validatinon, as we discuss in Section 6.5.3.\n6.5 Empirical risk minimization\nFrequentist decision theory suffers from the fundamental problem that one cannot actuallycompute the risk function, since it relies on knowing the true data distribution. (By contrast,the Bayesian posterior expected loss can always be computed, since it conditions on the thedata rather than conditioning on \u03b8\n\u2217.) However, there is one setting which avoids this problem,\nand that is where the task is to predict observable quantities, as opposed to estimating hiddenvariables or parameters. That is, instead of looking at loss functions of the form L(\u03b8,\u03b4(D)),\nwhere\u03b8is the true but unknown parameter, and \u03b4(D)is our estimator, let us look at loss", "235": "6.5. Empirical risk minimization 205\nfunctions of the form L(y,\u03b4(x)),w h e r e yis the true but unknown response, and \u03b4(x)is our\nprediction given the input x. In this case, the frequentist risk becomes\nR(p\u2217,\u03b4)/definesE(x,y)\u223cp\u2217[L(y,\u03b4(x)] =/summationdisplay\nx/summationdisplay\nyL(y,\u03b4(x))p\u2217(x,y) (6.47)\nwherep\u2217represents \u201cnature\u2019s distribution\u201d. Of course, this distribution is unknown, but a simple\napproach is to use the empirical distribution, derived from some training data, to approximate\np\u2217, i.e.,\np\u2217(x,y)\u2248pemp(x,y)/defines1\nNN/summationdisplay\ni=1\u03b4xi(x)\u03b4yi(y) (6.48)\nWe then de\ufb01ne the empirical risk as follows:\nRemp(D,D)/definesR(pemp,\u03b4)=1\nNN/summationdisplay\ni=1L(yi,\u03b4(xi)) (6.49)\nIn the case of 0-1 loss, L(y,\u03b4(x)) =I(y/negationslash=\u03b4(x)), this becomes the misclassi\ufb01cation rate.I n\nthe case of squared error loss, L(y,\u03b4(x)) = (y\u2212\u03b4(x))2, this becomes the mean squared error.\nWe de\ufb01ne the task of empirical risk minimization orERMas \ufb01nding a decision procedure\n(typically a classi\ufb01cation rule) to minimize the empirical risk:\n\u03b4ERM(D) = argmin\n\u03b4Remp(D,\u03b4) (6.50)\nIn the unsupervised case, we eliminate all references to y, and replace L(y,\u03b4(x))with\nL(x,\u03b4(x)), where, for example, L(x,\u03b4(x)) =||x\u2212\u03b4(x)||2\n2, which measures the reconstruc-\ntion error. We can de\ufb01ne the decision rule using \u03b4(x)=decode( encode(x)), as in vector\nquantization (Section 11.4.2.6) or PCA (section 12.2). Finally, we de\ufb01ne the empirical risk as\nRemp(D,\u03b4)=1\nNN/summationdisplay\ni=1L(xi,\u03b4(xi)) (6.51)\nOf course, we can always trivially minimize this risk by setting \u03b4(x)=x, so it is critical that\nthe encoder-decoder go via some kind of bottleneck.\n6.5.1 Regularized risk minimization\nNote that the empirical risk is equal to the Bayes risk if our prior about \u201cnature\u2019s distribution\u201d is\nthat it is exactly equal to the empirical distribution (Minka 2001b):\nE[R(p\u2217,\u03b4)|p\u2217=pemp]=Remp(D,\u03b4) (6.52)\nTherefore minimizing the empirical risk will typically result in over\ufb01tting. It is therefore oftennecessary to add a complexity penalty to the objective function:\nR\n/prime(D,\u03b4)=Remp(D,\u03b4)+\u03bbC(\u03b4) (6.53)", "236": "206 Chapter 6. Frequentist statistics\nwhereC(\u03b4)measures the complexity of the prediction function \u03b4(x)and\u03bbcontrols the strength\nof the complexity penalty. This approach is known as regularized risk minimization (RRM).\nNote that if the loss function is negative log likelihood, and the regularizer is a negative log\nprior, this is equivalent to MAP estimation.\nThe two key issues in RRM are: how do we measure complexity, and how do we pick \u03bb.F o r\na linear model, we can de\ufb01ne the complexity of in terms of its degrees of freedom, discussed inSection 7.5.3. For more general models, we can use the VC dimension, discussed in Section 6.5.4.To pick\u03bb, we can use the methods discussed in Section 6.5.2.\n6.5.2 Structural risk minimization\nThe regularized risk minimization principle says that we should \ufb01t the model, for a givencomplexity penalty, by using\n\u02c6\u03b4\n\u03bb=a r g m i n\n\u03b4[Remp(D,\u03b4)+\u03bbC(\u03b4)] (6.54)\nBut how should we pick \u03bb? We cannot using the training set, since this will underestimate the\ntrue risk, a problem known as optimism of the training error. As an alternative, we can use\nthe following rule, known as the structural risk minimization principle: (Vapnik 1998):\n\u02c6\u03bb=a r g m i n\n\u03bb\u02c6R(\u02c6\u03b4\u03bb) (6.55)\nwhere\u02c6R(\u03b4)is an estimate of the risk. There are two widely used estimates: cross validation\nand theoretical upper bounds on the risk. We discuss both of these below.\n6.5.3 Estimating the risk using cross validation\nWe can estimate the risk of some estimator using a validation set. If we don\u2019t have a separatevalidation set, we can use cross validation (CV), as we brie\ufb02y discussed in Section 1.4.8. More\nprecisely, CV is de\ufb01ned as follows. Let there be N=|D|data cases in the training set. Denote\nthe data in the k\u2019th test fold by D\nkand all the other data by D\u2212k. (Instrati\ufb01ed CV, these folds\nare chosen so the class proportions (if discrete labels are present) are roughly equal in eachfold.) Let Fbe a learning algorithm or \ufb01tting function that takes a dataset and a model index\nm(this could a discrete index, such as the degree of a polynomial, or a continuous index, such\nas the strength of a regularizer) and returns a parameter vector:\n\u02c6\u03b8\nm=F(D,m) (6.56)\nFinally, let Pbe a prediction function that takes an input and a parameter vector and returns a\nprediction:\n\u02c6y=P(x,\u02c6\u03b8)=f(x,\u02c6\u03b8) (6.57)\nThus the combined \ufb01t-predict cycle is denoted as\nfm(x,D)=P(x,F(D,m)) (6.58)", "237": "6.5. Empirical risk minimization 207\nTheK-fold CV estimate of the risk of fmis de\ufb01ned by\nR(m,D,K)/defines1\nNK/summationdisplay\nk=1/summationdisplay\ni\u2208DkL(yi,P(xi,F(D\u2212k,m))) (6.59)\nNote that we can call the \ufb01tting algorithm once per fold. Let fk\nm(x)=P(x,F(D\u2212k,m))be\nthe function that was trained on all the data except for the test data in fold k. Then we can\nrewrite the CV estimate as\nR(m,D,K)=1\nNK/summationdisplay\nk=1/summationdisplay\ni\u2208DkL/parenleftbig\nyi,fk\nm(xi)/parenrightbig\n=1\nNN/summationdisplay\ni=1L/parenleftBig\nyi,fk(i)\nm(xi)/parenrightBig\n(6.60)\nwherek(i)is the fold in which iis used as test data. In other words, we predict yiusing a\nmodel that was trained on data that does not contain xi.\nOfK=N, the method is known as leave one out cross validation or LOOCV. In this case,\nn the estimated risk becomes\nR(m,D,N)=1\nNN/summationdisplay\ni=1L/parenleftbig\nyi,f\u2212i\nm(xi)/parenrightbig\n(6.61)\nwherefi\nm(x)=P(x,F(D\u2212i,m)). This requires \ufb01tting the model Ntimes, where for f\u2212i\nmwe\nomit thei\u2019th training case. Fortunately, for some model classes and loss functions (namely linear\nmodels and quadratic loss), we can \ufb01t the model once, and analytically \u201cremove\u201d the effect of\nthei\u2019th training case. This is known as generalized cross validation or GCV.\n6.5.3.1 Example: using CV to pick \u03bbfor ridge regression\nAs a concrete example, consider picking the strength of the /lscript2regularizer in penalized linear\nregression. We use the following rule:\n\u02c6\u03bb=a r g m i n\n\u03bb\u2208[\u03bbmin,\u03bbmax]R(\u03bb,Dtrain,K) (6.62)\nwhere[\u03bbmin,\u03bbmax]is a \ufb01nite range of \u03bbvalues that we search over, and R(\u03bb,Dtrain,K)is the\nK-fold CV estimate of the risk of using \u03bb, given by\nR(\u03bb,Dtrain,K)=1\n|Dtrain|K/summationdisplay\nk=1/summationdisplay\ni\u2208DkL(yi,fk\n\u03bb(xi)) (6.63)\nwherefk\n\u03bb(x)=xT\u02c6w\u03bb(D\u2212k)is the prediction function trained on data excluding fold k, and\n\u02c6w\u03bb(D)=a r gm i n wNLL(w,D)+\u03bb||w||2\n2is the MAP estimate. Figure 6.6(b) gives an example\nof a CV estimate of the risk vs log(\u03bb), where the loss function is squared error.\nWhen performing classi\ufb01cation, we usually use 0-1 loss. In this case, we optimize a convex\nupper bound on the empirical risk to estimate w\u03bbm but we optimize (the CV estimate of) the\nrisk itself to estimate \u03bb. We can handle the non-smooth 0-1 loss function when estimating \u03bb\nbecause we are using brute-force search over the entire (one-dimensional) space.\nWhen we have more than one or two tuning parameters, this approach becomes infeasible.\nIn such cases, one can use empirical Bayes, which allows one to optimize large numbers of\nhyper-parameters using gradient-based optimizers instead of brute-force search. See Section 5.6for details.", "238": "208 Chapter 6. Frequentist statistics\n\u221225 \u221220 \u221215 \u221210 \u22125 0 502468101214\nlog lambdamean squared error\n  \ntrain mse\ntest mse\n(a)\u221215 \u221210 \u22125 0 502468101214161820\nlog lambdamse5\u2212fold cross validation, ntrain = 50\n(b)\nFigure 6.6 (a) Mean squared error for /lscript2penalized degree 14 polynomial regression vs log regularizer.\nSame as in Figures 7.8, except now we have N=5 0training points instead of 21. The stars correspond\nto the values used to plot the functions in Figure 7.7. (b) CV estimate. The vertical scale is truncated for\nclarity. The blue line corresponds to the value chosen by the one standard error rule. Figure generated bylinregPolyVsRegDemo .\n6.5.3.2 The one standard error rule\nThe above procedure estimates the risk, but does not give any measure of uncertainty. A\nstandard frequentist measure of uncertainty of an estimate is the standard error of the mean,de\ufb01ned by\nse=\u02c6\u03c3\n\u221a\nN=/radicalbigg\n\u02c6\u03c32\nN(6.64)\nwhere\u02c6\u03c32is an estimate of the variance of the loss:\n\u02c6\u03c32=1\nNN/summationdisplay\ni=1(Li\u2212L)2,Li=L(yi,fk(i)\nm(xi))L=1\nNN/summationdisplay\ni=1Li (6.65)\nNote that \u03c3measures the intrinsic variability of Liacross samples, whereas semeasures our\nuncertainty about the mean L.\nSuppose we apply CV to a set of models and compute the mean and se of their estimated\nrisks. A common heuristic for picking a model from these noisy estimates is to pick the valuewhich corresponds to the simplest model whose risk is no more than one standard error abovethe risk of the best model; this is called the one-standard error rule (Hastie et al. 2001, p216).\nFor example, in Figure 6.6, we see that this heuristic does not choose the lowest point on thecurve, but one that is slightly to its right, since that corresponds to a more heavily regularizedmodel with essentially the same empirical performance.", "239": "6.5. Empirical risk minimization 209\n6.5.3.3 CV for model selection in non-probabilistic unsupervised learning\nIf we are performing unsupervised learning, we must use a loss function such as L(x,\u03b4(x)) =\n||x\u2212\u03b4(x)||2, which measures reconstruction error. Here \u03b4(x)is some encode-decode scheme.\nHowever, as we discussed in Section 11.5.2, we cannot use CV to determine the complexity of \u03b4,\nsince we will always get lower loss with a more complex model, even if evaluated on the test set.\nThis is because more complex models will compress the data less, and induce less distortion.Consequently, we must either use probabilistic models, or invent other heuristics.\n6.5.4 Upper bounding the risk using statistical learning theory *\nThe principle problem with cross validation is that it is slow, since we have to \ufb01t the modelmultiple times. This motivates the desire to compute analytic approximations or bounds tothe generalization error. This is the studied in the \ufb01eld of statistical learning theory (SLT).\nMore precisely, SLT tries to bound the risk R(p\n\u2217,h)for any data distribution p\u2217and hypothesis\nh\u2208Hin terms of the empirical risk Remp(D,h), the sample size N=|D|, and the size of the\nhypothesis space H.\nLet us initially consider the case where the hypothesis space is \ufb01nite, with size dim(H)=|H|.\nIn other words, we are selecting a model/ hypothesis from a \ufb01nite list, rather than optimizingreal-valued parameters, Then we can prove the following.\nTheorem 6.5.1. For any data distribution p\n\u2217, and any dataset Dof sizeNdrawn from p\u2217, the\nprobability that our estimate of the error rate will be more than /epsilon1wrong, in the worst case, is upper\nbounded as follows:\nP/parenleftbigg\nmax\nh\u2208H|Remp(D,h)\u2212R(p\u2217,h)|>/epsilon1/parenrightbigg\n\u22642dim(H)e\u22122N/epsilon12(6.66)\nProof.To prove this, we need two useful results. First, Hoeffding\u2019s inequality, which states that\nifX1,...,X N\u223cBer(\u03b8), then, for any /epsilon1>0,\nP(|x\u2212\u03b8|>/epsilon1)\u22642e\u22122N/epsilon12(6.67)\nwherex=1\nN/summationtextN\ni=1xi. Second, the union bound, which says that if A1,...,A dare a set of\nevents, then P(\u222ad\ni=1Ai)\u2264/summationtextd\ni=1P(Ai).\nFinally, fornotationalbrevity, let R(h)=R(h,p\u2217)bethetruerisk, and \u02c6RN(h)=Remp(D,h)\nbe the empirical risk.\nUsing these results we have\nP/parenleftbigg\nmax\nh\u2208H|\u02c6RN(h)\u2212R(h)|>/epsilon1/parenrightbigg\n=P/parenleftBigg/uniondisplay\nh\u2208H|\u02c6RN(h)\u2212R(h)|>/epsilon1/parenrightBigg\n(6.68)\n\u2264/summationdisplay\nh\u2208HP/parenleftBig\n|\u02c6RN(h)\u2212R(h)|>/epsilon1/parenrightBig\n(6.69)\n\u2264/summationdisplay\nh\u2208H2e\u22122N/epsilon12=2d i m ( H)e\u22122N/epsilon12(6.70)", "240": "210 Chapter 6. Frequentist statistics\nThs bound tells us that the optimism of the training error increases with dim(H)but de-\ncreases with N=|D|, as is to be expected.\nIf the hypothesis space His in\ufb01nite (e.g., we have real-valued parameters), we cannot use\ndim(H)=|H|. Instead, we can use a quantity called the Vapnik-Chervonenkis orVCdimen-\nsion of the hypothesis class. See (Vapnik 1998) for details.\nStepping back from all the theory, the key intuition behind statistical learning theory is quite\nsimple. Suppose we \ufb01nd a model with low empirical risk. If the hypothesis space His very\nbig, relative to the data size, then it is quite likely that we just got \u201clucky\u201d and were given a\ndata set that is well-modeled by our chosen function by chance. However, this does not meanthat such a function will have low generalization error. But if the hypothesis class is sufficientlyconstrained in size, and/or the training set is sufficiently large, then we are unlikely to get luckyin this way, so a low empirical risk is evidence of a low true risk.\nNote that optimism of the training error does not necessarily increase with model complexity,\nbut it does increase with the number of different models that are being searched over.\nThe advantage of statistical learning theory compared to CV is that the bounds on the risk\nare quicker to compute than using CV. The disadvantage is that it is hard to compute the VCdimension for many interesting models, and the upper bounds are usually very loose (althoughsee (Kaariainen and Langford 2005)).\nOne can extend statistical learning theory by taking computational complexity of the learner\ninto account. This \ufb01eld is called computational learning theory orCOLT. Most of this work\nfocuses on the case where his a binary classi\ufb01er, and the loss function is 0-1 loss. If we observe\na low empirical risk, and the hypothesis space is suitably \u201csmall\u201d, then we can say that ourestimated function is probably approximately correct orPAC. A hypothesis space is said to be\nefficiently PAC-learnable if there is a polynomial time algorithm that can identify a function\nthat is PAC. See (Kearns and Vazirani 1994) for details.\n6.5.5 Surrogate loss functions\nMinimizing the loss in the ERM/ RRM framework is not always easy. For example, we mightwant to optimize the AUC or F1 scores. Or more simply, we might just want to minimize the 0-1loss, as is common in classi\ufb01cation. Unfortunately, the 0-1 risk is a very non-smooth objectiveand hence is hard to optimize. One alternative is to use maximum likelihood estimation instead,since log-likelihood is a smooth convex upper bound on the 0-1 risk, as we show below.\nTo see this, consider binary logistic regression, and let y\ni\u2208{ \u22121,+1}. Suppose our decision\nfunction computes the log-odds ratio,\nf(xi)=l o gp(y=1|xi,w)\np(y=\u22121|xi,w)=wTxi=\u03b7i (6.71)\nThen the corresponding probability distribution on the output label is\np(yi|xi,w) = sigm(y i\u03b7i) (6.72)\nLet us de\ufb01ne the log-lossas as\nLnll(y,\u03b7)=\u2212logp(y|x,w)=l o g ( 1+ e\u2212y\u03b7) (6.73)", "241": "6.6. Pathologies of frequentist statistics * 211\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 200.511.522.53\n\u03b7loss\n  \n0\u22121\nhinge\nlogloss\nFigure 6.7 Illustration of various loss functions for binary classi\ufb01cation. The horizontal axis is the margin\ny\u03b7, the vertical axis is the loss. The log loss uses log base 2. Figure generated by hingeLossPlot .\nIt is clear that minimizing the average log-loss is equivalent to maximizing the likelihood.\nNow consider computing the most probable label, which is equivalent to using \u02c6y=\u22121if\n\u03b7i<0and\u02c6y=+ 1if\u03b7i\u22650. The 0-1 loss of our function becomes\nL01(y,\u03b7)= I(y/negationslash=\u02c6y)=I(y\u03b7 <0) (6.74)\nFigure 6.7 plots these two loss functions. We see that the NLL is indeed an upper bound on the\n0-1 loss.\nLog-loss is an example of a surrogate loss function. Another example is the hinge loss:\nLhinge(y,\u03b7)=m a x ( 0 ,1\u2212y\u03b7) (6.75)\nSee Figure 6.7 for a plot. We see that the function looks like a door hinge, hence its name.This loss function forms the basis of a popular classi\ufb01cation method known as support vectormachines (SVM), which we will discuss in Section 14.5.\nThe surrogate is usually chosen to be a convex upper bound, since convex functions are easy\nto minimize. See e.g., (Bartlett et al. 2006) for more information.\n6.6 Pathologies of frequentist statistics *\nI believe that it would be very difficult to persuade an intelligent person that current[frequentist] statistical practice was sensible, but that there would be much less difficultywith an approach via likelihood and Bayes\u2019 theorem. \u2014 George Box, 1962.\nFrequentist statistics exhibits various forms of weird and undesirable behaviors, known aspathologies. We give a few examples below, in order to caution the reader; these and otherexamples are explained in more detail in (Lindley 1972; Lindley and Phillips 1976; Lindley 1982;Berger 1985; Jaynes 2003; Minka 1999).", "242": "212 Chapter 6. Frequentist statistics\n6.6.1 Counter-intuitive behavior of con\ufb01dence intervals\nAcon\ufb01dence interval is an interval derived from the sampling distribution of an estimator\n(whereas a Bayesian credible interval is derived from the posterior of a parameter, as we dis-\ncussed in Section 5.2.2). More precisely, a frequentist con\ufb01dence interval for some parameter \u03b8\nis de\ufb01ned by the following (rather un-natural) expression:\nC/prime\n\u03b1(\u03b8)=(/lscript,u):P(/lscript(\u02dcD)\u2264\u03b8\u2264u(\u02dcD)|\u02dcD\u223c\u03b8)=1\u2212\u03b1 (6.76)\nThat is, if we sample hypothetical future data \u02dcDfrom\u03b8, then(/lscript(\u02dcD),u(\u02dcD)), is a con\ufb01dence\ninterval if the parameter \u03b8lies inside this interval 1\u2212\u03b1percent of the time.\nLet us step back for a moment and think about what is going on. In Bayesian statistics,\nwe condition on what is known \u2014 namely the observed data, D\u2014 and average over what\nis not known, namely the parameter \u03b8. In frequentist statistics, we do exactly the opposite:\nwe condition on what is unknown \u2014 namely the true parameter value \u03b8\u2014 and average over\nhypothetical future data sets \u02dcD.\nThis counter-intuitive de\ufb01nition of con\ufb01dence intervals can lead to bizarre results. Consider\nthe following example from (Berger 1985, p11). Suppose we draw two integers D=(x1,x2)from\np(x|\u03b8)=\u23a7\n\u23a8\n\u23a90.5ifx=\u03b8\n0.5ifx=\u03b8+1\n0otherwise(6.77)\nIf\u03b8=3 9, we would expect the following outcomes each with probability 0.25:\n(39,39),(39,40),(40,39),(40,40) (6.78)\nLetm=m i n (x1,x2)and de\ufb01ne the following con\ufb01dence interval:\n[/lscript(D),u(D) ]=[m,m] (6.79)\nFor the above samples this yields\n[39,39],[39,39],[39,39],[40,40] (6.80)\nHence Equation 6.79 is clearly a 75% CI, since 39is contained in 3/4 of these intervals. However,\nifD=( 3 9,40)thenp(\u03b8=3 9|D)=1.0, so we know that \u03b8must be 39, yet we only have 75%\n\u201ccon\ufb01dence\u201d in this fact.\nAnother, less contrived example, is as follows. Suppose we want to estimate the parameter \u03b8\nof a Bernoulli distribution. Let x=1\nN/summationtextN\ni=1xibe the sample mean. The MLE is \u02c6\u03b8=x.A n\napproximate 95% con\ufb01dence interval for a Bernoulli parameter is x\u00b11.96/radicalbig\nx(1\u2212x)/N(this is\ncalled aWald interval and is based on a Gaussian approximation to the Binomial distribution;\ncompare to Equation 3.27). Now consider a single trial, where N=1andx1=0. The MLE\nis 0, which over\ufb01ts, as we saw in Section 3.3.4.1. But our 95% con\ufb01dence interval is also (0,0),\nwhich seems even worse. It can be argued that the above \ufb02aw is because we approximated\nthe true sampling distribution with a Gaussian, or because the sample size was to small, or theparameter \u201ctoo extreme\u201d. However, the Wald interval can behave badly even for large N, and\nnon-extreme parameters (Brown et al. 2001).", "243": "6.6. Pathologies of frequentist statistics * 213\n6.6.2 p-values considered harmful\nSuppose we want to decide whether to accept or reject some baseline model, which we will\ncall thenull hypothesis. We need to de\ufb01ne some decision rule. In frequentist statistics, it\nis standard to \ufb01rst compute a quantity called the p-value, which is de\ufb01ned as the probability\n(under the null) of observing some test statistic f(D)(such as the chi-squared statistic) that is\nas large or largerthan that actually observed:5\npvalue(D )/definesP(f(\u02dcD)\u2265f(D)|\u02dcD\u223cH0) (6.81)\nThis quantity relies on computing a tail area probability of the sampling distribution; we give\nan example of how to do this below.\nGiven the p-value, we de\ufb01ne our decision rule as follows: we reject the null hypothesis iff the\np-value is less than some threshold, such as \u03b1=0.05. If we do reject it, we say the difference\nbetween the observed test statistic and the expected test statistic is statistically signi\ufb01cant at\nlevel\u03b1. This approach is known as null hypothesis signi\ufb01cance testing,o rNHST.\nThis procedure guarantees that our expected type I (false positive) error rate is at most \u03b1.\nThis is sometimes interpreted as saying that frequentist hypothesis testing is very conservative,since it is unlikely to accidently reject the null hypothesis. But in fact the opposite is the case:because this method only worries about trying to reject the null, it can never gather evidencein favor of the null, no matter how large the sample size. Because of this, p-values tend tooverstate the evidence against the null, and are thus very \u201ctrigger happy\u201d.\nIn general there can be huge differences between p-values and the quantity that we really\ncare about, which is the posterior probability of the null hypothesis given the data, p(H\n0|D).\nIn particular, Sellke et al. (2001) show that even if the p-value is as slow as 0.05, the posteriorprobability of H\n0is at least 30%, and often much higher. So frequentists often claim to have\n\u201csigni\ufb01cant\u201d evidence of an effect that cannot be explained by the null hypothesis, whereasBayesians are usually more conservative in their claims. For example, p-values have been usedto \u201cprove\u201d that ESP (extra-sensory perception) is real (Wagenmakers et al. 2011), even though ESPis clearly very improbable. For this reason, p-values have been banned from certain medicaljournals (Matthews 1998).\nAnother problem with p-values is that their computation depends on decisions you make\nabout when to stop collecting data, even if these decisions don\u2019t change the data you actuallyobserved. For example, suppose I toss a coin n=1 2times and observe s=9successes (heads)\nandf=3failures (tails), so n=s+f. In this case, nis \ufb01xed and s(and hence f) is random.\nThe relevant sampling model is the binomial\nBin(s|n,\u03b8)=/parenleftbiggn\ns/parenrightbigg\n\u03b8\ns(1\u2212\u03b8)n\u2212s(6.82)\nLet the null hypothesis be that the coin is fair, \u03b8=0.5,w h e r e \u03b8is the probability of success\n(heads). The one-sided p-value, using test statistic t(s)=s,i s\np1=P(S\u22659|H0)=12/summationdisplay\ns=9Bin(s|12,0.5) =12/summationdisplay\ns=9/parenleftbigg12\ns/parenrightbigg\n0.512=0.073 (6.83)\n5. The reason we cannot just compute the probability of the observed value of the test statistic is that this will have\nprobability zero under a pdf. The p-value is de\ufb01ned in terms of the cdf, so is always a number between 0 and 1.", "244": "214 Chapter 6. Frequentist statistics\nThe two-sided p-value is\np2=12/summationdisplay\ns=9Bin(s|12,0.5)+3/summationdisplay\ns=0Bin(s|12,0.5) = 0.073+0.073 = 0.146 (6.84)\nIn either case, the p-value is larger than the magical 5% threshold, so a frequentist would not\nreject the null hypothesis.\nNow suppose I told you that I actually kept tossing the coin until I observed f=3tails. In\nthis case, fis \ufb01xed and n(and hence s=n\u2212f) is random. The probability model becomes\nthenegative binomial distribution, given by\nNegBinom(s |f,\u03b8)=/parenleftbiggs+f\u22121\nf\u22121/parenrightbigg\n\u03b8s(1\u2212\u03b8)f(6.85)\nwheref=n\u2212s.\nNote that the term which depends on \u03b8is the same in Equations 6.82 and 6.85, so the\nposterior over \u03b8would be the same in both cases. However, these two interpretations of the\nsame data give different p-values. In particular, under the negative binomial model we get\np3=P(S\u22659|H0)=\u221e/summationdisplay\ns=9/parenleftbigg3+s\u22121\n2/parenrightbigg\n(1/2)s(1/2)3=0.0327 (6.86)\nSo the p-value is 3%, and suddenly there seems to be signi\ufb01cant evidence of bias in the coin!Obviously this is ridiculous: the data is the same, so our inferences about the coin should bethe same. After all, I could have chosen the experimental protocol at random. It is the outcomeof the experiment that matters, not the details of how I decided which one to run.\nAlthough this might seem like just a mathematical curiosity, this also has signi\ufb01cant practical\nimplications. In particular, the fact that the stopping rule affects the computation of the p-\nvalue means that frequentists often do not terminate experiments early, even when it is obviouswhat the conclusions are, lest it adversely affect their statistical analysis. If the experiments arecostly or harmful to people, this is obviously a bad idea. Perhaps it is not surprising, then, thatthe US Food and Drug Administration (FDA), which regulates clinical trials of new drugs, hasrecently become supportive of Bayesian methods\n6, since Bayesian methods are not affected by\nthe stopping rule.\n6.6.3 The likelihood principle\nThe fundamental reason for many of these pathologies is that frequentist inference violatesthelikelihood principle, which says that inference should be based on the likelihood of the\nobserved data, not based on hypothetical future data that you have not observed. Bayes obviouslysatis\ufb01es the likelihood principle, and consequently does not suffer from these pathologies.\nA compelling argument in favor of the likelihood principle was presented in (Birnbaum 1962),\nwho showed that it followed automatically from two simpler principles. The \ufb01rst of these is thesufficiency principle, which says that a sufficient statistic contains all the relevant information\n6. See http://yamlb .wordpress .com/2006/06/19/the-us-fda-is-becoming-progressively-more-bayes\nian/.", "245": "6.6. Pathologies of frequentist statistics * 215\nabout an unknown parameter (arguably this is true by de\ufb01nition). The second principle is\nknown as weak conditionality, which says that inferences should be based on the events that\nhappened, not which might have happened. To motivate this, consider an example from (Berger1985). Suppose we need to analyse a substance, and can send it either to a laboratory in NewYork or in California. The two labs seem equally good, so a fair coin is used to decide betweenthem. The coin comes up heads, so the California lab is chosen. When the results come back,should it be taken into account that the coin could have come up tails and thus the New Yorklab could have been used? Most people would argue that the New York lab is irrelevant, sincethe tails event didn\u2019t happen. This is an example of weak conditionality. Given this principle,one can show that all inferences should only be based on what was observed, which is incontrast to standard frequentist procedures. See (Berger and Wolpert 1988) for further details onthe likelihood principle.\n6.6.4 Why isn\u2019t everyone a Bayesian?\nGiven these fundamental \ufb02aws of frequentist statistics, and the fact that Bayesian methodsdo not have such \ufb02aws, an obvious question to ask is: \u201cWhy isn\u2019t everyone a Bayesian?\u201d The(frequentist) statistician Bradley Efron wrote a paper with exactly this title (Efron 1986). His shortpaper is well worth reading for anyone interested in this topic. Below we quote his openingsection:\nThe title is a reasonable question to ask on at least two counts. First of all, everone usedto be a Bayesian. Laplace wholeheatedly endorsed Bayes\u2019s formulation of the inferenceproblem, and most 19th-century scientists followed suit. This included Gauss, whosestatistical work is usually presented in frequentist terms.\nA second and more important point is the cogency of the Bayesian argument. Modern\nstatisticians, following the lead of Savage and de Finetti, have advanced powerful theoret-ical arguments for preferring Bayesian inference. A byproduct of this work is a disturbingcatalogue of inconsistencies in the frequentist point of view.\nNevertheless, everyone is not a Bayesian. The current era (1986) is the \ufb01rst century in\nwhich statistics has been widely used for scienti\ufb01c reporting, and in fact, 20th-centurystatistics is mainly non-Bayesian. However, Lindley (1975) predicts a change for the 21stcentury.\nTime will tell whether Lindley was right....\nExercises\nExercise 6.1 Pessimism of LOOCV\n(Source: Witten05, p152.). Suppose we have a completely random labeled dataset (i.e., the features xtell us\nnothing about the class labels y) withN1examples of class 1, and N2examples of class 2, where N1=N2.\nWhat is the best misclassi\ufb01cation rate any method can achieve? What is the estimated misclassi\ufb01cation\nrate of the same method using LOOCV?\nExercise 6.2 James Stein estimator for Gaussian means\nConsider the 2 stage model Yi|\u03b8i\u223cN(\u03b8i,\u03c32)and\u03b8i|\u03bc\u223cN(m0,\u03c42\n0). Suppose \u03c32= 500is known and\nwe observe the following 6 data points, i=1:6:", "246": "216 Chapter 6. Frequentist statistics\n1505, 1528, 1564, 1498, 1600, 1470\na. Find the ML-II estimates of m0and\u03c42\n0.\nb. Find the posterior estimates E[\u03b8i|yi,m0,\u03c40]andvar[\u03b8i|yi,m0,\u03c40]fori=1. (The other terms,\ni=2:6, are computed similarly.)\nc. Give a 95% credible interval for p(\u03b8i|yi,m0,\u03c40)fori=1. Do you trust this interval (assuming the\nGaussian assumption is reasonable)? i.e. is it likely to be too large or too small, or just right?\nd. What do you expect would happen to your estimates if \u03c32were much smaller (say \u03c32=1)? You do\nnot need to compute the numerical answer; just brie\ufb02y explain what would happen qualitatively, and\nwhy.\nExercise 6.3 \u02c6\u03c32\nMLEis biased\nShow that \u02c6\u03c32\nMLE=1\nN/summationtextN\nn=1(xn\u2212\u02c6\u03bc)2is a biased estimator of \u03c32, i.e., show\nEX1,...,X n\u223cN(\u03bc,\u03c3)[\u02c6\u03c32(X1,...,X n)/negationslash=\u03c32\nHint: note that X1,...,X Nare independent, and use the fact that the expectation of a product of\nindependent random variables is the product of the expectations.\nExercise 6.4 Estimation of \u03c32when\u03bcis known\nSuppose we sample x1,...,x N\u223cN(\u03bc,\u03c32)where\u03bcis aknownconstant. Derive an expression for the\nMLE for\u03c32in this case. Is it unbiased?", "247": "7 Linear regression\n7.1 Introduction\nLinear regression is the \u201cwork horse\u201d of statistics and (supervised) machine learning. When\naugmented with kernels or other forms of basis function expansion, it can model also non-linear relationships. And when the Gaussian output is replaced with a Bernoulli or multinoullidistribution, it can be used for classi\ufb01cation, as we will see below. So it pays to study this modelin detail.\n7.2 Model speci\ufb01cation\nAs we discussed in Section 1.4.5, linear regression is a model of the form\np(y|x,\u03b8)=N(y|wTx,\u03c32) (7.1)\nLinear regression can be made to model non-linear relationships by replacing xwith some\nnon-linear function of the inputs, \u03c6(x). That is, we use\np(y|x,\u03b8)=N(y|wT\u03c6(x),\u03c32) (7.2)\nThis is known as basis function expansion. (Note that the model is still linear in the parameters\nw, so it is still called linear regression; the importance of this will become clear below.) A simple\nexample are polynomial basis functions, where the model has the form\n\u03c6(x)=[ 1,x,x2,...,xd] (7.3)\nFigure 1.18 illustrates the effect of changing d: increasing the degree dallows us to create\nincreasingly complex functions.\nWe can also apply linear regression to more than 1 input. For example, consider modeling\ntemperature as a function of location. Figure 7.1(a) plots E[y|x]=w0+w1x1+w2x2, and\nFigure 7.1(b) plots E[y|x]=w0+w1x1+w2x2+w3x2\n1+w4x22.\n7.3 Maximum likelihood estimation (least squares)\nA common way to esitmate the parameters of a statistical model is to compute the MLE, which\nis de\ufb01ned as\n\u02c6\u03b8/definesargmax\n\u03b8logp(D|\u03b8) (7.4)", "248": "218 Chapter 7. Linear regression\n010203040\n05101520253015.51616.51717.518\n(a)010203040\n01020301515.51616.51717.518\n(b)\nFigure 7.1 Linear regression applied to 2d data. Vertical axis is temperature, horizontal axes are location\nwithin a room. Data was collected by some remote sensing motes at Intel\u2019s lab in Berkeley, CA (data\ncourtesy of Romain Thibaux). (a) The \ufb01tted plane has the form \u02c6f(x)=w 0+w1x1+w2x2. (b)\nTemperature data is \ufb01tted with a quadratic of the form \u02c6f(x)=w 0+w1x1+w2x2+w3x2\n1+w4x22.\nProduced by surfaceFitDemo .\nIt is common to assume the training examples are independent and identically distributed,\ncommonly abbreviated to iid. This means we can write the log-likelihood as follows:\n/lscript(\u03b8)/defineslogp(D|\u03b8)=N/summationdisplay\ni=1logp(yi|xi,\u03b8) (7.5)\nInstead of maximizing the log-likelihood, we can equivalently minimize the negative log likeli-\nhoodorNLL:\nNLL(\u03b8)/defines\u2212N/summationdisplay\ni=1logp(yi|xi,\u03b8) (7.6)\nThe NLL formulation is sometimes more convenient, since many optimization software packagesare designed to \ufb01nd the minima of functions, rather than maxima.\nNow let us apply the method of MLE to the linear regression setting. Inserting the de\ufb01nition\nof the Gaussian into the above, we \ufb01nd that the log likelihood is given by\n/lscript(\u03b8)= N/summationdisplay\ni=1log/bracketleftBigg/parenleftbigg1\n2\u03c0\u03c32/parenrightbigg1\n2\nexp/parenleftbigg\n\u22121\n2\u03c32(yi\u2212wTxi)2/parenrightbigg/bracketrightBigg\n(7.7)\n=\u22121\n2\u03c32RSS(w)\u2212N\n2log(2\u03c0\u03c32) (7.8)\nRSS stands for residual sum of squares and is de\ufb01ned by\nRSS(w)/definesN/summationdisplay\ni=1(yi\u2212wTxi)2(7.9)\nThe RSS is also called the sum of squared errors, or SSE, and SSE/N is called the mean\nsquared error orMSE. It can also be written as the square of the /lscript2normof the vector of", "249": "7.3. Maximum likelihood estimation (least squares) 219\n\u22124 \u22123 \u22122 \u22121 0 1 2 3 4\u22123\u22122\u22121012345\nprediction\ntruth\n(a)Sum of squares error contours for linear regression\nw0w1\n\u22121 0 1 2 3\u22121\u22120.500.511.522.53\n(b)\nFigure 7.2 (a) In linear least squares, we try to minimize the sum of squared distances from each training\npoint (denoted by a red circle) to its approximation (denoted by a blue cross), that is, we minimize the\nsum of the lengths of the little vertical blue lines. The red diagonal line represents \u02c6y(x)=w 0+w1x,\nwhich is the least squares regression line. Note that these residual lines are not perpendicular to the leastsquares line, in contrast to Figure 12.5. Figure generated by residualsDemo . (b) Contours of the RSS error\nsurface for the same example. The red cross represents the MLE, w=( 1.45,0.93). Figure generated by\ncontoursSSEdemo .\nresidual errors:\nRSS(w)=||/epsilon1||2\n2=N/summationdisplay\ni=1/epsilon12i(7.10)\nwhere/epsilon1i=(yi\u2212wTxi).\nWe see that the MLE for wis the one that minimizes the RSS, so this method is known\nasleast squares. This method is illustrated in Figure 7.2(a). The training data (xi,yi)are\nshown as red circles, the estimated values (xi,\u02c6yi)are shown as blue crosses, and the residuals\n/epsilon1i=yi\u2212\u02c6yiare shown as vertical blue lines. The goal is to \ufb01nd the setting of the parameters\n(the slope w1and intercept w0) such that the resulting red line minimizes the sum of squared\nresiduals (the lengths of the vertical blue lines).\nIn Figure 7.2(b), we plot the NLL surface for our linear regression example. We see that it is a\nquadratic \u201cbowl\u201d with a unique minimum, which we now derive. (Importantly, this is true even\nif we use basis function expansion, such as polynomials, because the NLL is still linear in the\nparameters w, even if it is not linear in the inputs x.)\n7.3.1 Derivation of the MLE\nFirst, we rewrite the objective in a form that is more amenable to differentiation:\nNLL(w)=1\n2(y\u2212Xw)T(y\u2212Xw)=1\n2wT(XTX)w\u2212wT(XTy) (7.11)", "250": "220 Chapter 7. Linear regression\nwhere\nXTX=N/summationdisplay\ni=1xixT\ni=N/summationdisplay\ni=1\u239b\n\u239c\u239dx2\ni,1\u00b7\u00b7\u00b7xi,1xi,D\n...\nxi,Dxi,1\u00b7\u00b7\u00b7x2\ni,D\u239e\n\u239f\u23a0 (7.12)\nis thesum of squares matrix and\nXTy=N/summationdisplay\ni=1xiyi. (7.13)\nUsing results from Equation 4.10, we see that the gradient of this is given by\ng(w)=[XTXw\u2212XTy]=N/summationdisplay\ni=1xi(wTxi\u2212yi) (7.14)\nEquating to zero we get\nXTXw=XTy (7.15)\nThis is known as the normal equation. The corresponding solution \u02c6wto this linear system of\nequations is called the ordinary least squares orOLSsolution, which is given by\n\u02c6wOLS=(XTX)\u22121XTy (7.16)\n7.3.2 Geometric interpretation\nThis equation has an elegant geometrical intrepretation, as we now explain. We assume N>D,\nso we have more examples than features. The columns of Xde\ufb01ne a linear subspace of\ndimensionality Dwhich is embedded in Ndimensions. Let the j\u2019th column be \u02dc xj, which is\na vector in RN. (This should not be confused with xi\u2208RD, which represents the i\u2019th data\ncase.) Similarly, yis a vector in RN. For example, suppose we have N=3examples in D=2\ndimensions:\nX=\u239b\n\u239d12\n1\u22122\n12\u239e\n\u23a0,y=\u239b\u239d8.8957\n0.6130\n1.7761\u239e\u23a0 (7.17)\nThese vectors are illustrated in Figure 7.3.\nWe seek a vector \u02c6y\u2208R\nNthat lies in this linear subspace and is as close as possible to y,\ni.e., we want to \ufb01nd\nargmin\n\u02c6y\u2208span({\u02dc x1,...,\u02dc xD})/bardbly\u2212\u02c6y/bardbl2. (7.18)\nSince\u02c6y\u2208span(X), there exists some weight vector wsuch that\n\u02c6y=w1\u02dc x1+\u00b7\u00b7\u00b7+wD\u02dc xD=Xw (7.19)", "251": "7.3. Maximum likelihood estimation (least squares) 221\n00.20.40.60.81\n\u22121\u22120.500.5100.20.40.60.81\ny\nx1x2\u02c6y\nx1\n(0,0,0)\nx2x3\nFigure 7.3 Graphical interpretation of least squares for N=3examples and D=2features. \u02dc x1and\n\u02dc x2are vectors in R3; together they de\ufb01ne a 2D plane. yis also a vector in R3but does not lie on this\n2D plane. The orthogonal projection of yonto this plane is denoted \u02c6y. The red line from yto\u02c6yis\nthe residual, whose norm we want to minimize. For visual clarity, all vectors have been converted to unit\nnorm. Figure generated by leastSquaresProjection .\nTo minimize the norm of the residual, y\u2212\u02c6y, we want the residual vector to be orthogonal to\nevery column of X,s o\u02dc xT\nj(y\u2212\u02c6y)=0forj=1:D. Hence\n\u02dc xT\nj(y\u2212\u02c6y)=0\u21d2XT(y\u2212Xw)=0\u21d2w=(XTX)\u22121XTy (7.20)\nHence our projected value of yis given by\n\u02c6y=X\u02c6w=X(XTX)\u22121XTy (7.21)\nThis corresponds to an orthogonal projection ofyonto the column space of X. The projection\nmatrixP/definesX(XTX)\u22121XTis called the hat matrix, since it \u201cputs the hat on y\u201d.\n7.3.3 Convexity\nWhen discussing least squares, we noted that the NLL had a bowl shape with a unique minimum.\nThe technical term for functions like this is convex. Convex functions play a very important\nrole in machine learning.\nLet us de\ufb01ne this concept more precisely. We say a setSisconvexif for any \u03b8,\u03b8/prime\u2208S,w e\nhave\n\u03bb\u03b8+(1\u2212\u03bb)\u03b8/prime\u2208S,\u2200\u03bb\u2208[0,1] (7.22)", "252": "222 Chapter 7. Linear regression\n(a)\n (b)\nFigure 7.4 (a) Illustration of a convex set. (b) Illustration of a nonconvex set.\nx y\u03bb1 \u2212 \u03bb\n(a)A B\n(b)\nFigure 7.5 (a) Illustration of a convex function. We see that the chord joining (x,f(x))to(y,f(y))lies\nabove the function. (b) A function that is neither convex nor concave. Ais a local minimum, Bis a global\nminimum. Figure generated by convexFnHand .\nThat is, if we draw a line from \u03b8to\u03b8/prime, all points on the line lie inside the set. See Figure 7.4(a)\nfor an illustration of a convex set, and Figure 7.4(b) for an illustration of a non-convex set.\nAfunctionf(\u03b8)is called convex if its epigraph (the set of points above the function) de\ufb01nes\na convex set. Equivalently, a function f(\u03b8)is called convex if it is de\ufb01ned on a convex set and\nif, for any \u03b8,\u03b8/prime\u2208S, and for any 0\u2264\u03bb\u22641,w eh a v e\nf(\u03bb\u03b8+(1\u2212\u03bb)\u03b8/prime)\u2264\u03bbf(\u03b8)+(1\u2212\u03bb)f(\u03b8/prime) (7.23)\nSee Figure 7.5 for a 1d example. A function is called strictly convex if the inequality is strict. A\nfunctionf(\u03b8)isconcaveif\u2212f(\u03b8)is convex. Examples of scalar convex functions include \u03b82,\ne\u03b8, and\u03b8log\u03b8(for\u03b8>0). Examples of scalar concave functions include log(\u03b8)and\u221a\n\u03b8.\nIntuitively, a (strictly) convex function has a \u201cbowl shape\u201d, and hence has a unique global\nminimum \u03b8\u2217corresponding to the bottom of the bowl. Hence its second derivative must be\npositive everywhere,d\nd\u03b8f(\u03b8)>0. A twice-continuously differentiable, multivariate function fis\nconvex iff its Hessian is positive de\ufb01nite for all \u03b8.1In the machine learning context, the function\nfoften corresponds to the NLL.\n1. Recall that the Hessian is the matrix of second partial derivatives, de\ufb01ned by Hjk=\u2202f2(\u03b8)\n\u2202\u03b8j\u2202\u03b8k.Also, recall that a\nmatrixHispositive de\ufb01nite iffvTHv>0for any non-zero vector v.", "253": "7.4. Robust linear regression * 223\n0 0.2 0.4 0.6 0.8 1\u22126\u22125\u22124\u22123\u22122\u2212101234Linear data with noise and outliers\n  \nleast squares\nlaplace\n(a)\u22123 \u22122 \u22121 0 1 2 3\u22120.500.511.522.533.544.55\n  \nL2\nL1\nhuber\n(b)\nFigure 7.6 (a) Illustration of robust linear regression. Figure generated by linregRobustDemoCombined .\n(b) Illustration of /lscript2,/lscript1, and Huber loss functions. Figure generated by huberLossDemo .\nModels where the NLL is convex are desirable, since this means we can always \ufb01nd the\nglobally optimal MLE. We will see many examples of this later in the book. However, many\nmodels of interest will not have concave likelihoods. In such cases, we will discuss ways toderive locally optimal parameter estimates.\n7.4 Robust linear regression *\nIt is very common to model the noise in regression models using a Gaussian distributionwith zero mean and constant variance, /epsilon1\ni\u223cN(0,\u03c32),w h e r e/epsilon1i=yi\u2212wTxi. In this case,\nmaximizing likelihood is equivalent to minimizing the sum of squared residuals, as we haveseen. However, if we have outliersin our data, this can result in a poor \ufb01t, as illustrated in\nFigure 7.6(a). (The outliers are the points on the bottom of the \ufb01gure.) This is because squarederror penalizes deviations quadratically, so points far from the line have more affect on the \ufb01tthan points near to the line.\nOne way to achieve robustness to outliers is to replace the Gaussian distribution for the\nresponse variable with a distribution that has heavy tails. Such a distribution will assign higher\nlikelihood to outliers, without having to perturb the straight line to \u201cexplain\u201d them.\nOne possibility is to use the Laplace distribution, introduced in Section 2.4.3. If we use this\nas our observation model for regression, we get the following likelihood:\np(y|x,w,b) = Lap(y |w\nTx,b)\u221dexp(\u22121\nb|y\u2212wTx|) (7.24)\nThe robustness arises from the use of |y\u2212wTx|instead of (y\u2212wTx)2. For simplicity, we will\nassumebis \ufb01xed. Let ri/definesyi\u2212wTxibe thei\u2019th residual. The NLL has the form\n/lscript(w)=/summationdisplay\ni|ri(w)| (7.25)", "254": "224 Chapter 7. Linear regression\nLikelihood Prior Name Section\nGaussian Uniform Least squares 7.3\nGaussian Gaussian Ridge 7.5Gaussian Laplace Lasso 13.3Laplace Uniform Robust regression 7.4Student Uniform Robust regression Exercise 11.12\nTable 7.1 Summary of various likelihoods and priors used for linear regression. The likelihood refers to\nthe distributional form of p(y|x,w,\u03c32), and the prior refers to the distributional form of p(w). MAP\nestimation with a uniform distribution corresponds to MLE.\nUnfortunately, this is a non-linear objective function, which is hard to optimize. Fortunately, wecan convert the NLL to a linear objective, subject to linear constraints, using the following split\nvariable trick. First we de\ufb01ne\nr\ni/definesr+\ni\u2212r\u2212\ni (7.26)\nand then we impose the linear inequality constraints that r+\ni\u22650andr\u2212\ni\u22650. Now the\nconstrained objective becomes\nmin\nw,r+,r\u2212/summationdisplay\ni(r+\ni\u2212r\u2212\ni)s.t.r+\ni\u22650,r\u2212\ni\u22650,wTxi+r+\ni+r\u2212\ni=yi (7.27)\nThis is an example of a linear program withD+2Nunknowns and 3Nconstraints.\nSince this is a convex optimization problem, it has a unique solution. To solve an LP, we must\n\ufb01rst write it in standard form, which as follows:\nmin\n\u03b8fT\u03b8s.t.A\u03b8\u2264b,Aeq\u03b8=beq,l\u2264\u03b8\u2264u (7.28)\nIn our current example, \u03b8=(w,r+,r\u2212),f=[0,1,1],A=[ ],b=[ ],Aeq=[X,I,\u2212I],\nbeq=y,l=[\u2212\u221e1,0,0],u=[ ]. This can be solved by any LP solver (see e.g., (Boyd and\nVandenberghe 2004)). See Figure 7.6(a) for an example of the method in action.\nAn alternative to using NLL under a Laplace likelihood is to minimize the Huber loss function\n(Huber 1964), de\ufb01ned as follows:\nLH(r,\u03b4)=/braceleftbiggr2/2if|r|\u2264\u03b4\n\u03b4|r|\u2212\u03b42/2if|r|>\u03b4(7.29)\nThis is equivalent to /lscript2for errors that are smaller than \u03b4, and is equivalent to /lscript1for larger errors.\nSee Figure 7.6(b). The advantage of this loss function is that it is everywhere differentiable,using the fact that\nd\ndr|r|=sign(r)ifr/negationslash=0. We can also check that the function is C1\ncontinuous, since the gradients of the two parts of the function match at r=\u00b1\u03b4, namely\nd\ndrLH(r,\u03b4)|r=\u03b4=\u03b4. Consequently optimizing the Huber loss is much faster than using the\nLaplace likelihood, since we can use standard smooth optimization methods (such as quasi-Newton) instead of linear programming.\nFigure 7.6(a) gives an illustration of the Huber loss function. The results are qualitatively\nsimiliar to the probabilistic methods. (In fact, it turns out that the Huber method also has aprobabilistic interpretation, although it is rather unnatural (Pontil et al. 1998).)", "255": "7.5. Ridge regression 225\n0 5 10 15 20\u221210\u2212505101520ln lambda \u221220.135\n(a)0 5 10 15 20\u221215\u221210\u2212505101520ln lambda \u22128.571\n(b)\nFigure 7.7 Degree 14 Polynomial \ufb01t to N=2 1data points with increasing amounts of /lscript2regularization.\nData was generated from noise with variance \u03c32=4. The error bars, representing the noise variance \u03c32,\nget wider as the \ufb01t gets smoother, since we are ascribing more of the data variation to the noise. Figure\ngenerated by linregPolyVsRegDemo .\n7.5 Ridge regression\nOne problem with ML estimation is that it can result in over\ufb01tting. In this section, we discuss a\nway to ameliorate this problem by using MAP estimation with a Gaussian prior. For simplicity,we assume a Gaussian likelihood, rather than a robust likelihood.\n7.5.1 Basic idea\nThe reason that the MLE can over\ufb01t is that it is picking the parameter values that are thebest for modeling the training data; but if the data is noisy, such parameters often result incomplex functions. As a simple example, suppose we \ufb01t a degree 14 polynomial to N=2 1data\npoints using least squares. The resulting curve is very \u201cwiggly\u201d, as shown in Figure 7.7(a). Thecorresponding least squares coefficients (excluding w\n0) are as follows:\n6.560, -36.934, -109.255, 543.452, 1022.561, -3046.224, -3768.013,8524.540, 6607.897, -12640.058, -5530.188, 9479.730, 1774.639, -2821.526\nWe see that there are many large positive and negative numbers. These balance out exactly\nto make the curve \u201cwiggle\u201d in just the right way so that it almost perfectly interpolates the data.But this situation is unstable: if we changed the data a little, the coefficients would change a lot.\nWe can encourage the parameters to be small, thus resulting in a smoother curve, by using a\nzero-mean Gaussian prior:\np(w)=/productdisplay\njN(wj|0,\u03c42) (7.30)\nwhere1/\u03c42controls the strength of the prior. The corresponding MAP estimation problem\nbecomes\nargmax\nwN/summationdisplay\ni=1logN(yi|w0+wTxi,\u03c32)+D/summationdisplay\nj=1logN(wj|0,\u03c42) (7.31)", "256": "226 Chapter 7. Linear regression\n\u221225 \u221220 \u221215 \u221210 \u22125 0 50510152025\nlog lambdamean squared error\n  \ntrain mse\ntest mse\n(a)\u221220 \u221215 \u221210 \u22125 0 50.10.20.30.40.50.60.70.80.9\nlog lambda  \nnegative log marg. likelihood\nCV estimate of MSE\n(b)\nFigure 7.8 (a) Training error (dotted blue) and test error (solid red) for a degree 14 polynomial \ufb01t by\nridge regression, plotted vs log(\u03bb). Data was generated from noise with variance \u03c32=4(training set\nhas sizeN=2 1). Note: Models are ordered from complex (small regularizer) on the left to simple (large\nregularizer) on the right. The stars correspond to the values used to plot the functions in Figure 7.7. (b)\nEstimate of performance using training set. Dotted blue: 5-fold cross-validation estimate of future MSE.Solid black: negative log marginal likelihood, \u2212logp(D|\u03bb). Both curves have been vertically rescaled to\n[0,1] to make them comparable. Figure generated by linregPolyVsRegDemo .\nIt is a simple exercise to show that this is equivalent to minimizing the following:\nJ(w)=1\nNN/summationdisplay\ni=1(yi\u2212(w0+wTxi))2+\u03bb||w||2\n2 (7.32)\nwhere\u03bb/defines\u03c32/\u03c42and||w||2\n2=/summationtext\njw2\nj=wTwis the squared two-norm. Here the \ufb01rst term is\nthe MSE/ NLL as usual, and the second term, \u03bb\u22650, is a complexity penalty. The corresponding\nsolution is given by\n\u02c6wridge=(\u03bbID+XTX)\u22121XTy (7.33)\nThis technique is known as ridge regression,o r penalized least squares. In general, adding\na Gaussian prior to the parameters of a model to encourage them to be small is called /lscript2\nregularization orweight decay. Note that the offset term w0is not regularized, since this just\naffects the height of the function, not its complexity. By penalizing the sum of the magnitudes\nof the weights, we ensure the function is simple (since w=0corresponds to a straight line,\nwhich is the simplest possible function, corresponding to a constant.)\nWe illustrate this idea in Figure 7.7, where we see that increasing \u03bbresults in smoother\nfunctions. The resulting coefficients also become smaller. For example, using \u03bb=1 0\u22123,w e\nhave", "257": "7.5. Ridge regression 227\n2.128, 0.807, 16.457, 3.704, -24.948, -10.472, -2.625, 4.360, 13.711,\n10.063, 8.716, 3.966, -9.349, -9.232\nIn Figure 7.8(a), we plot the MSE on the training and test sets vs log(\u03bb). We see that, as we\nincrease\u03bb(so the model becomes more constrained), the error on the training set increases.\nFor the test set, we see the characteristic U-shaped curve, where the model over\ufb01ts and thenunder\ufb01ts. It is common to use cross validation to pick \u03bb, as shown in Figure 7.8(b). In\nSection 1.4.8, we will discuss a more probabilistic approach.\nWe will consider a variety of different priors in this book. Each of these corresponds to a\ndifferent form of regularization. This technique is very widely used to prevent over\ufb01tting.\n7.5.2 Numerically stable computation *\nInterestingly, ridge regression, which works better statistically, is also easier to \ufb01t numerically,since(\u03bbI\nD+XTX)is much better conditioned (and hence more likely to be invertible) than\nXTX, at least for suitable largy \u03bb.\nNevertheless, inverting matrices is still best avoided, for reasons of numerical stability. (Indeed,\nif you write w=inv(X\u2019 * X)*X\u2019*y in Matlab, it will give you a warning.) We now describe\na useful trick for \ufb01tting ridge regression models (and hence by extension, computing vanillaOLS estimates) that is more numerically robust. We assume the prior has the form p(w)=\nN(0,\u039b\n\u22121),w h e r e\u039bis the precision matrix. In the case of ridge regression, \u039b=( 1/\u03c42)I.T o\navoid penalizing the w0term, we should center the data \ufb01rst, as explained in Exercise 7.5.\nFirst let us augment the original data with some \u201cvirtual data\u201d coming from the prior:\n\u02dcX=/parenleftbiggX/\u03c3\u221a\n\u039b/parenrightbigg\n,\u02dcy=/parenleftbiggy/\u03c3\n0D\u00d71/parenrightbigg\n(7.34)\nwhere\u039b=\u221a\n\u039b\u221a\n\u039bTis aCholesky decomposition of\u039b. We see that \u02dcXis(N+D)\u00d7D,\nwhere the extra rows represent pseudo-data from the prior.\nWe now show that the NLL on this expanded data is equivalent to penalized NLL on the\noriginal data:\nf(w)=(\u02dcy\u2212\u02dcXw)T(\u02dcy\u2212\u02dcXw) (7.35)\n=/parenleftbigg/parenleftbiggy/\u03c3\n0/parenrightbigg\n\u2212/parenleftbiggX/\u03c3\u221a\n\u039b/parenrightbigg\nw/parenrightbiggT/parenleftbigg/parenleftbiggy/\u03c3\n0/parenrightbigg\n\u2212/parenleftbiggX/\u03c3\u221a\n\u039b/parenrightbigg\nw/parenrightbigg\n(7.36)\n=/parenleftbigg1\n\u03c3(y\u2212Xw)\n\u2212\u221a\n\u039bw/parenrightbiggT/parenleftbigg1\n\u03c3(y\u2212Xw)\n\u2212\u221a\n\u039bw/parenrightbigg\n(7.37)\n=1\n\u03c32(y\u2212Xw)T(y\u2212Xw)+(\u221a\n\u039bw)T(\u221a\n\u039bw) (7.38)\n=1\n\u03c32(y\u2212Xw)T(y\u2212Xw)+wT\u039bw (7.39)\nHence the MAP estimate is given by\n\u02c6wridge=(\u02dcXT\u02dcX)\u22121\u02dcXT\u02dcy (7.40)\nas we claimed.", "258": "228 Chapter 7. Linear regression\nNow let\n\u02dcX=QR (7.41)\nbe theQR decomposition ofX,w h e r eQis orthonormal (meaning QTQ=QQT=I), and\nRis upper triangular. Then\n(\u02dcXT\u02dcX)\u22121=(RTQTQR)\u22121=(RTR)\u22121=R\u22121R\u2212T(7.42)\nHence\n\u02c6wridge=R\u22121R\u2212TRTQT\u02dcy=R\u22121Q\u02dcy (7.43)\nNote that Ris easy to invert since it is upper triangular. This gives us a way to compute the\nridge estimate while avoiding having to invert (\u039b+XTX).\nWe can use this technique to \ufb01nd the MLE, by simply computing the QR decomposition of\nthe unaugmented matrix X, and using the original y. This is the method of choice for solving\nleast squares problems. (In fact, it is so sommon that it can be implemented in one line of\nMatlab, using the backslash operator: w=X\\y.) Note that computing the QR decomposition of\nanN\u00d7Dmatrix takes O(ND2)time, and is numerically very stable.\nIfD/greatermuchN, we should \ufb01rst perform an SVD decomposition. In particular, let X=USVTbe\nthe SVD of X,w h e r eVTV=IN,UUT=UTU=IN, andSis a diagonal N\u00d7Nmatrix.\nNow letZ=UDbe anN\u00d7Nmatrix. Then we can rewrite the ridge estimate thus:\n\u02c6wridge=V(ZTZ+\u03bbIN)\u22121ZTy (7.44)\nIn other words, we can replace the D-dimensional vectors xiwith theN-dimensional vectors\nziand perform our penalized \ufb01t as before. We then transform the N-dimensional solution\nto theD-dimensional solution by multiplying by V. Geometrically, we are rotating to a new\ncoordinate system in which all but the \ufb01rst Ncoordinates are zero. This does not affect the\nsolution since the spherical Gaussian prior is rotationally invariant. The overall time is nowO(DN\n2)operations.\n7.5.3 Connection with PCA *\nIn this section, we discuss an interesting connection between ridge regression and PCA (Sec-tion 12.2), which gives further insight into why ridge regression works well. Our discussion isbased on (Hastie et al. 2009, p66).\nLetX=USV\nTbe the SVD of X. From Equation 7.44, we have\n\u02c6wridge=V(S2+\u03bbI)\u22121SUTy (7.45)\nHence the ridge predictions on the training set are given by\n\u02c6y=X\u02c6wridge=USVTV(S2+\u03bbI)\u22121SUTy (7.46)\n=U\u02dcSUTy=D/summationdisplay\nj=1uj\u02dcSjjuT\njy (7.47)", "259": "7.5. Ridge regression 229\nprior meanMAP EstimateML Estimateu1u2\nFigure 7.9 Geometry of ridge regression. The likelihood is shown as an ellipse, and the prior is shown\nas a circle centered on the origin. Based on Figure 3.15 of (Bishop 2006b). Figure generated by geomRidge\nwhere\n\u02dcSjj/defines[S(S2+\u03bbI)\u22121S]jj=\u03c32\nj\n\u03c32\nj+\u03bb(7.48)\nand\u03c3jare the singular values of X. Hence\n\u02c6y=X\u02c6wridge=D/summationdisplay\nj=1uj\u03c32\nj\n\u03c32\nj+\u03bbuT\njy (7.49)\nIn contrast, the least squares prediction is\n\u02c6y=X\u02c6wls=(USVT)(VS\u22121UTy)=UUTy=D/summationdisplay\nj=1ujuTjy (7.50)\nIf\u03c32\njis small compared to \u03bb, then direction ujwill not have much effect on the prediction. In\nview of this, we de\ufb01nethe effective number of degrees of freedom of the model as follows:\ndof(\u03bb)=D/summationdisplay\nj=1\u03c32\nj\n\u03c32\nj+\u03bb(7.51)\nWhen\u03bb=0,dof(\u03bb)=D , and as\u03bb\u2192\u221e,dof(\u03bb)\u21920.\nLet us try to understand why this behavior is desirable. In Section 7.6, we show that\ncov[w|D]=\u03c32(XTX)\u22121, if we use a uniform prior for w. Thus the directions in which\nwe are most uncertain about ware determined by the eigenvectors of this matrix with the\nsmallest eigenvalues, as shown in Figure 4.1. Furthermore, in Section 12.2.3, we show that the\nsquared singular values \u03c32\njare equal to the eigenvalues of XTX. Hence small singular values \u03c3j\ncorrespond to directions with high posterior variance. It is these directions which ridge shrinksthe most.", "260": "230 Chapter 7. Linear regression\nThis process is illustrated in Figure 7.9. The horizontal w1parameter is not-well determined\nby the data (has high posterior variance), but the vertical w2parameter is well-determined.\nHencewmap\n2is close to \u02c6wmle\n2, butwmap\n1is shifted strongly towards the prior mean, which is 0.\n(Compare to Figure 4.14(c), which illustrated sensor fusion with sensors of different reliabilities.)\nIn this way, ill-determined parameters are reduced in size towards 0. This is called shrinkage.\nThere is a related, but different, technique called principal components regression. The idea\nis this: \ufb01rst use PCA to reduce the dimensionality to Kdimensions, and then use these low\ndimensional features as input to regression. However, this technique does not work as well asridge in terms of predictive accuracy (Hastie et al. 2001, p70). The reason is that in PC regression,only the \ufb01rst K(derived) dimensions are retained, and the remaining D\u2212Kdimensions are\nentirely ignored. By contrast, ridge regression uses a \u201csoft\u201d weighting of all the dimensions.\n7.5.4 Regularization effects of big data\nRegularization is the most common way to avoid over\ufb01tting. However, another effective approach\u2014 which is not always available \u2014 is to use lots of data. It should be intuitively obvious thatthe more training data we have, the better we will be able to learn.\n2So we expect the test set\nerror to decrease to some plateau as Nincreases.\nThis is illustrated in Figure 7.10, where we plot the mean squared error incurred on the test set\nachieved by polynomial regression models of different degrees vs N(a plot of error vs training\nset size is known as a learning curve). The level of the plateau for the test error consists of\ntwo terms: an irreducible component that all models incur, due to the intrinsic variability ofthe generating process (this is called the noise \ufb02oor); and a component that depends on the\ndiscrepancy between the generating process (the \u201ctruth\u201d) and the model: this is called structural\nerror.\nIn Figure 7.10, the truth is a degree 2 polynomial, and we try \ufb01tting polynomials of degrees 1,\n2 and 25 to this data. Call the 3 models M\n1,M2andM25. We see that the structural error\nfor models M2andM25is zero, since both are able to capture the true generating process.\nHowever, the structural error for M1is substantial, which is evident from the fact that the\nplateau occurs high above the noise \ufb02oor.\nFor any model that is expressive enough to capture the truth (i.e., one with small structural\nerror), the test error will go to the noise \ufb02oor as N\u2192\u221e. However, it will typically go to\nzero faster for simpler models, since there are fewer parameters to estimate. In particular, for\ufb01nite training sets, there will be some discrepancy between the parameters that we estimateand the best parameters that we could estimate given the particular model class. This is calledapproximation error, and goes to zero as N\u2192\u221e, but it goes to zero faster for simpler\nmodels. This is illustrated in Figure 7.10. See also Exercise 7.1.\nIn domains with lots of data, simple methods can work surprisingly well (Halevy et al. 2009).\nHowever, there are still reasons to study more sophisticated learning methods, because therewill always be problems for which we have little data. For example, even in such a data-richdomain as web search, as soon as we want to start personalizing the results, the amount of dataavailable for any given user starts to look small again (relative to the complexity of the problem).\n2. This assumes the training data is randomly sampled, and we don\u2019t just get repetitions of the same examples. Having\ninformatively sampled data can help even more; this is the motivation for an approach known as active learning, where\nyou get to choose your training data.", "261": "7.6. Bayesian linear regression 231\n0 20 40 60 80 100 120 140 160 180 2000246810121416182022\nsize of training setmsetruth=degree 2, model = degree 1\n  \ntrain\ntest\n(a)0 20 40 60 80 100 120 140 160 180 2000246810121416182022\nsize of training setmsetruth=degree 2, model = degree 2\n  \ntrain\ntest\n(b)\n0 20 40 60 80 100 120 140 160 180 2000246810121416182022\nsize of training setmsetruth=degree 2, model = degree 10\n  \ntrain\ntest\n(c)0 20 40 60 80 100 120 140 160 180 2000246810121416182022\nsize of training setmsetruth=degree 2, model = degree 25\n  \ntrain\ntest\n(d)\nFigure 7.10 MSE on training and test sets vs size of training set, for data generated from a degree 2\npolynomial with Gaussian noise of variance \u03c32=4. We \ufb01t polynomial models of varying degree to this\ndata. (a) Degree 1. (b) Degree 2. (c) Degree 10. (d) Degree 25. Note that for small training set sizes, the test\nerror of the degree 25 polynomial is higher than that of the degree 2 polynomial, due to over\ufb01tting, butthis difference vanishes once we have enough data. Note also that the degree 1 polynomial is too simpleand has high test error even given large amounts of training data. Figure generated by linregPolyVsN .\nIn such cases, we may want to learn multiple related models at the same time, which is known\nas multi-task learning. This will allow us to \u201cborrow statistical strength\u201d from tasks with lots ofdata and to share it with tasks with little data. We will discuss ways to do later in the book.\n7.6 Bayesian linear regression\nAlthough ridge regression is a useful way to compute a point estimate, sometimes we want tocompute the full posterior over wand\u03c3\n2. For simplicity, we will initially assume the noise\nvariance\u03c32is known, so we focus on computing p(w|D,\u03c32). Then in Section 7.6.3 we consider", "262": "232 Chapter 7. Linear regression\nthe general case, where we compute p(w,\u03c32|D). We assume throughout a Gaussian likelihood\nmodel. Performing Bayesian inference with a robust likelihood is also possible, but requires more\nadvanced techniques (see Exercise 24.5).\n7.6.1 Computing the posterior\nIn linear regression, the likelihood is given by\np(y|X,w,\u03bc,\u03c32)=N (y|\u03bc+Xw,\u03c32IN) (7.52)\n\u221dexp/parenleftbigg\n\u22121\n2\u03c32(y\u2212\u03bc1N\u2212Xw)T(y\u2212\u03bc1N\u2212Xw)/parenrightbigg\n(7.53)\nwhere\u03bcis an offset term. If the inputs are centered, so/summationtext\nixij=0for eachj, the mean of the\noutput is equally likely to be positive or negative. So let us put an improper prior on \u03bcof the\nformp(\u03bc)\u221d1, and then integrate it out to get\np(y|X,w,\u03c32)\u221dexp/parenleftbigg\n\u22121\n2\u03c32||y\u2212y1N\u2212Xw||2\n2/parenrightbigg\n(7.54)\nwherey=1\nN/summationtextN\ni=1yiis the empirical mean of the output. For notational simplicity, we shall\nassume the output has been centered, and write yfory\u2212y1N.\nThe conjugate prior to the above Gaussian likelihood is also a Gaussian, which we will denote\nbyp(w)=N(w|w0,V0). Using Bayes rule for Gaussians, Equation 4.125, the posterior is given\nby\np(w|X,y,\u03c32)\u221dN(w|w0,V0)N(y|Xw,\u03c32IN)=N(w|wN,VN) (7.55)\nwN=VNV\u22121\n0w0+1\n\u03c32VNXTy (7.56)\nV\u22121\nN=V\u22121\n0+1\n\u03c32XTX (7.57)\nVN=\u03c32(\u03c32V\u22121\n0+XTX)\u22121(7.58)\nIfw0=0andV0=\u03c42I, then the posterior mean reduces to the ridge estimate, if we de\ufb01ne\n\u03bb=\u03c32\n\u03c42. This is because the mean and mode of a Gaussian are the same.\nTo gain insight into the posterior distribution (and not just its mode), let us consider a 1D\nexample:\ny(x,w)=w0+w1x+/epsilon1 (7.59)\nwhere the \u201ctrue\u201d parameters are w0=\u22120.3andw1=0.5. In Figure 7.11 we plot the prior,\nthe likelihood, the posterior, and some samples from the posterior predictive. In particular,\nthe right hand column plots the function y(x,w(s))wherexranges over [\u22121,1], andw(s)\u223c\nN(w|wN,VN)is a sample from the parameter posterior. Initially, when we sample from the\nprior (\ufb01rst row), our predictions are \u201call over the place\u201d, since our prior is uniform. After we seeone data point (second row), our posterior becomes constrained by the corresponding likelihood,and our predictions pass close to the observed data. However, we see that the posterior hasa ridge-like shape, re\ufb02ecting the fact that there are many possible solutions, with different", "263": "7.6. Bayesian linear regression 233\n W0  W1 prior/posterior\n\u22121 0 1\u2212101\n\u22121 0 1\u2212101\n x  y data space\n W0  W1 \n\u22121 0 1\u2212101\n W0  W1 \n\u22121 0 1\u2212101\n\u22121 0 1\u2212101\n x  y \n W0  W1 \n\u22121 0 1\u2212101\n W0  W1 \n\u22121 0 1\u2212101\n\u22121 0 1\u2212101\n x  y \n W0  W1 \n\u22121 0 1\u2212101\n W0  W1 \n\u22121 0 1\u2212101\n\u22121 0 1\u2212101\n x  y likelihood\nFigure 7.11 Sequential Bayesian updating of a linear regression model p(y|x)=N(y|w0x0+w1x1,\u03c32).\nRow 0 represents the prior, row 1 represents the \ufb01rst data point (x1,y1), row 2 represents the second\ndata point (x2,y2), row 3 represents the 20th data point (x20,y20). Left column: likelihood function for\ncurrent data point. Middle column: posterior given data so far, p(w|x1:n,y1:n)(so the \ufb01rst line is the\nprior). Right column: samples from the current prior/posterior predictive distribution. The white cross in\ncolumns 1 and 2 represents the true parameter value; we see that the mode of the posterior rapidly (after20 samples) converges to this point. The blue circles in column 3 are the observed data points. Based onFigure 3.7 of (Bishop 2006a). Figure generated by bayesLinRegDemo2d .\nslopes/intercepts. This makes sense since we cannot uniquely infer two parameters from one\nobservation. After we see two data points (third row), the posterior becomes much narrower,and our predictions all have similar slopes and intercepts. After we observe 20 data points (lastrow), the posterior is essentially a delta function centered on the true value, indicated by a whitecross. (The estimate converges to the truth since the data was generated from this model, andbecause Bayes is a consistent estimator; see Section 6.4.1 for discussion of this point.)\n7.6.2 Computing the posterior predictive\nIt\u2019s tough to make predictions, especially about the future. \u2014 Yogi Berra", "264": "234 Chapter 7. Linear regression\nIn machine learning, we often care more about predictions than about interpreting the parame-\nters. Using Equation 4.126, we can easily show that the posterior predictive distribution at a testpointxis also Gaussian:\np(y|x,D,\u03c3\n2)=/integraldisplay\nN(y|xTw,\u03c32)N(w|wN,VN)dw (7.60)\n=N(y|wT\nNx,\u03c32\nN(x)) (7.61)\n\u03c32\nN(x)=\u03c32+xTVNx (7.62)\nThe variance in this prediction, \u03c32\nN(x), depends on two terms: the variance of the observation\nnoise,\u03c32, and the variance in the parameters, VN. The latter translates into variance about\nobservations in a way which depends on how close xis to the training data D. This is illustrated\nin Figure 7.12, where we see that the error bars get larger as we move away from the trainingpoints, representing increased uncertainty. This is important for applications such as activelearning, where we want to model what we don\u2019t know as well as what we do. By contrast, theplugin approximation has constant sized error bars, since\np(y|x,D,\u03c3\n2)\u2248/integraldisplay\nN(y|xTw,\u03c32)\u03b4\u02c6w(w)dw=p(y|x,\u02c6w,\u03c32) (7.63)\nSee Figure 7.12(a).\n7.6.3 Bayesian inference when \u03c32is unknown *\nIn this section, we apply the results in Section 4.6.3 to the problem of computing p(w,\u03c32|D)\nfor a linear regression model. This generalizes the results from Section 7.6.1 where we assumed\u03c3\n2was known. In the case where we use an uninformative prior, we will see some interesting\nconnections to frequentist statistics.\n7.6.3.1 Conjugate prior\nAs usual, the likelihood has the form\np(y|X,w,\u03c32)=N(y|Xw,\u03c32IN) (7.64)\nBy analogy to Section 4.6.3, one can show that the natural conjugate prior has the followingform:\np(w,\u03c3\n2)=N I G ( w,\u03c32|w0,V0,a0,b0) (7.65)\n/definesN(w|w0,\u03c32V0)IG(\u03c32|a0,b0) (7.66)\n=ba0\n0\n(2\u03c0)D/2|V0|1\n2\u0393(a0)(\u03c32)\u2212(a0+(D/2)+1)(7.67)\n\u00d7exp/bracketleftbigg\n\u2212(w\u2212w0)TV\u22121\n0(w\u2212w0)+2b0\n2\u03c32/bracketrightbigg\n(7.68)", "265": "7.6. Bayesian linear regression 235\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 80102030405060plugin approximation (MLE)\n  \nprediction\ntraining data\n(a)\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22121001020304050607080Posterior predictive (known variance)\n  \nprediction\ntraining data\n(b)\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 805101520253035404550functions sampled from plugin approximation to posterior\n(c)\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u221220020406080100functions sampled from posterior\n(d)\nFigure 7.12 (a) Plug-in approximation to predictive density (we plug in the MLE of the parameters). (b)\nPosterior predictive density, obtained by integrating out the parameters. Black curve is posterior mean,\nerror bars are 2 standard deviations of the posterior predictive density. (c) 10 samples from the pluginapproximation to posterior predictive. (d) 10 samples from the posterior predictive. Figure generated bylinregPostPredDemo .\nWith this prior and likelihood, one can show that the posterior has the following form:\np(w,\u03c32|D) = NIG(w ,\u03c32|wN,VN,aN,bN) (7.69)\nwN=VN(V\u22121\n0w0+XTy) (7.70)\nVN=(V\u22121\n0+XTX)\u22121(7.71)\naN=a0+n/2 (7.72)\nbN=b0+1\n2/parenleftbig\nwT\n0V\u22121\n0w0+yTy\u2212wT\nNV\u22121\nNwN/parenrightbig\n(7.73)\nThe expressions for wNandVNare similar to the case where \u03c32is known. The expression for\naNis also intuitive, since it just updates the counts. The expression for bNcan be interpreted", "266": "236 Chapter 7. Linear regression\nas follows: it is the prior sum of squares, b0, plus the empirical sum of squares, yTy, plus a\nterm due to the error in the prior on w.\nThe posterior marginals are as follows:\np(\u03c32|D)=I G ( aN,bN) (7.74)\np(w|D)=T (wN,bN\naNVN,2aN) (7.75)\nWe give a worked example of using these equations in Section 7.6.3.3.\nBy analogy to Section 4.6.3.6, the posterior predictive distribution is a Student T distribution.\nIn particular, given mnew test inputs \u02dcX,w eh a v e\np(\u02dcy|\u02dcX,D)=T (\u02dcy|\u02dcXwN,bN\naN(Im+\u02dcXVN\u02dcXT),2aN) (7.76)\nThe predictive variance has two components: (bN/aN)Imdue to the measurement noise, and\n(bN/aN)\u02dcXVN\u02dcXTdue to the uncertainty in w. This latter terms varies depending on how\nclose the test inputs are to the training data.\nIt is common to set a0=b0=0, corresponding to an uninformative prior for \u03c32, and to set\nw0=0andV0=g(XTX)\u22121for any positive value g. This is called Zellner\u2019s g-prior(Zellner\n1986). Here gplays a role analogous to 1/\u03bbin ridge regression. However, the prior covariance is\nproportional to (XTX)\u22121rather than I. This ensures that the posterior is invariant to scaling\nof the inputs (Minka 2000b). See also Exercise 7.10.\nWe will see below that if we use an uninformative prior, the posterior precision given N\nmeasurements is V\u22121\nN=XTX. Theunit information prior is de\ufb01ned to contain as much\ninformation as one sample (Kass and Wasserman 1995). To create a unit information prior for\nlinear regression, we need to use V\u22121\n0=1\nNXTX, which is equivalent to the g-prior with\ng=N.\n7.6.3.2 Uninformative prior\nAn uninformative prior can be obtained by considering the uninformative limit of the conjugateg-prior, which corresponds to setting g=\u221e. This is equivalent to an improper NIG prior with\nw\n0=0,V0=\u221eI,a0=0andb0=0, which gives p(w,\u03c32)\u221d\u03c3\u2212(D+2).\nAlternatively, we can start with the semi-conjugate prior p(w,\u03c32)=p(w)p(\u03c32), and take\neach term to its uninformative limit individually, which gives p(w,\u03c32)\u221d\u03c3\u22122. This is equivalent\nto an improper NIG prior with w0=0,V=\u221eI,a0=\u2212D/2andb0=0. The corresponding\nposterior is given by\np(w,\u03c32|D)=N I G ( w,\u03c32|wN,VN,aN,bN) (7.77)\nwN=\u02c6wmle=(XTX)\u22121XTy (7.78)\nVN=(XTX)\u22121(7.79)\naN=N\u2212D\n2(7.80)\nbN=s2\n2(7.81)\ns2/defines(y\u2212X\u02c6wmle)T(y\u2212X\u02c6wmle (7.82)", "267": "7.6. Bayesian linear regression 237\nwjE[wj|D]/radicalbig\nvar[wj|D]95% CI sig\nw0 10.998 3.06027 [4.652, 17.345] *\nw1 -0.004 0.00156 [-0.008, -0.001] *w2 -0.054 0.02190 [-0.099, -0.008] *w3 0.068 0.09947 [-0.138, 0.274]w4 -1.294 0.56381 [-2.463, -0.124] *w5 0.232 0.10438 [0.015, 0.448] *w6 -0.357 1.56646 [-3.605, 2.892]w7 -0.237 1.00601 [-2.324, 1.849]w8 0.181 0.23672 [-0.310, 0.672]w9 -1.285 0.86485 [-3.079, 0.508]w10 -0.433 0.73487 [-1.957, 1.091]\nTable 7.2 Posterior mean, standard deviation and credible intervals for a linear regression model with an\nuninformative prior \ufb01t to the caterpillar data. Produced by linregBayesCaterpillar .\nThe marginal distribution of the weights is given by\np(w|D)=T(w|\u02c6w,s2\nN\u2212DC,N\u2212D) (7.83)\nwhereC=(XTX)\u22121and\u02c6wis the MLE. We discuss the implications of these equations below.\n7.6.3.3 An example where Bayesian and frequentist inference coincide *\nThe use of a (semi-conjugate) uninformative prior is interesting because the resulting posteriorturns out to be equivalent to the results from frequentist statistics (see also Section 4.6.3.9). Inparticular, from Equation 7.83 we have\np(w\nj|D)=T(wj|\u02c6wj,Cjjs2\nN\u2212D,N\u2212D) (7.84)\nThis is equivalent to the sampling distribution of the MLE which is given by the following (seee.g., (Rice 1995, p542), (Casella and Berger 2002, p554)):\nw\nj\u2212\u02c6wj\nsj\u223ctN\u2212D (7.85)\nwhere\nsj=/radicalbigg\ns2Cjj\nN\u2212D(7.86)\nis the standard error of the estimated parameter. (See Section 6.2 for a discussion of samplingdistributions.) Consequently, the frequentist con\ufb01dence interval and the Bayesian marginalcredible interval for the parameters are the same in this case.\nAs a worked example of this, consider the caterpillar dataset from (Marin and Robert 2007).\n(The details of what the data mean don\u2019t matter for our present purposes.) We can compute", "268": "238 Chapter 7. Linear regression\nthe posterior mean and standard deviation, and the 95% credible intervals (CI) for the regression\ncoefficients using Equation 7.84. The results are shown in Table 7.2. It is easy to check that these95% credible intervals are identical to the 95% con\ufb01dence intervals computed using standardfrequentist methods (see linregBayesCaterpillar for the code).\nWe can also use these marginal posteriors to compute if the coefficients are \u201csigni\ufb01cantly\u201d\ndifferent from 0. An informal way to do this (without using decision theory) is to check if its 95%CI excludes 0. From Table 7.2, we see that the CIs for coefficients 0, 1, 2, 4, 5 are all signi\ufb01cantby this measure, so we put a little star by them. It is easy to check that these results are thesame as those produced by standard frequentist software packages which compute p-values atthe 5% level.\nAlthough the correspondence between the Bayesian and frequentist results might seem ap-\npealing to some readers, recall from Section 6.6 that frequentist inference is riddled with patholo-gies. Also, note that the MLE does not even exist when N<D, so standard frequentist inference\ntheory breaks down in this setting. Bayesian inference theory still works, although it requiresthe use of proper priors. (See (Maruyama and George 2008) for one extension of the g-prior tothe case where D>N.)\n7.6.4 EB for linear regression (evidence procedure)\nSo far, we have assumed the prior is known. In this section, we describe an empirical Bayesprocedure for picking the hyper-parameters. More precisely, we choose \u03b7=(\u03b1,\u03bb)to maximize\nthe marignal likelihood, where \u03bb=1/\u03c3\n2be the precision of the observation noise and \u03b1is\nthe precision of the prior, p(w)=N(w|0,\u03b1\u22121I). This is known as the evidence procedure\n(MacKay 1995b).3See Section 13.7.4 for the algorithmic details.\nThe evidence procedure provides an alternative to using cross validation. For example, in\nFigure 7.13(b), we plot the log marginal likelihood for different values of \u03b1, as well as the\nmaximum value found by the optimizer. We see that, in this example, we get the same resultas 5-CV, shown in Figure 7.13(a). (We kept \u03bb=1/\u03c3\n2\ufb01xed in both methods, to make them\ncomparable.)\nThe principle practical advantage of the evidence procedure over CV will become apparent\nin Section 13.7, where we generalize the prior by allowing a different \u03b1jfor every feature. This\ncan be used to perform feature selection, using a technique known as automatic relevancydetermination or ARD. By contrast, it would not be possible to use CV to tune Ddifferent\nhyper-parameters.\nThe evidence procedure is also useful when comparing different kinds of models, since it\nprovides a good approximation to the evidence:\np(D|m)=/integraldisplay/integraldisplay\np(D|w,m)p(w|m,\u03b7)p(\u03b7|m)dwd\u03b7 (7.87)\n\u2248max\n\u03b7/integraldisplay\np(D|w,m)p(w|m,\u03b7)p(\u03b7|m)dw (7.88)\nIt is important to (at least approximately) integrate over \u03b7rather than setting it arbitrarily, for\nreasons discussed in Section 5.3.2.5. Indeed, this is the method we used to evaluate the marginal\n3. Alternatively, we could integrate out \u03bbanalytically, as shown in Section 7.6.3, and just optimize \u03b1(Buntine and\nWeigend 1991). However, it turns out that this is less accurate than optimizing both \u03b1and\u03bb(MacKay 1999).", "269": "7.6. Bayesian linear regression 239\n\u221225 \u221220 \u221215 \u221210 \u22125 0 5100101102103104105106107\nlog lambdamse5\u2212fold cross validation, ntrain = 21\n(a)\u221225 \u221220 \u221215 \u221210 \u22125 0 5\u2212150\u2212140\u2212130\u2212120\u2212110\u2212100\u221290\u221280\u221270\u221260\u221250\nlog alphalog evidence\n(b)\nFigure 7.13 (a) Estimate of test MSE produced by 5-fold cross-validation vs log(\u03bb). The smallest value is\nindicated by the vertical line. Note the vertical scale is in log units. (c) Log marginal likelihood vs log(\u03b1).\nThe largest value is indicated by the vertical line. Figure generated by linregPolyVsRegDemo .\nlikelihood for the polynomial regression models in Figures 5.7 and 5.8. For a \u201cmore Bayesian\u201d\napproach, in which we model our uncertainty about \u03b7rather than computing point estimates,\nsee Section 21.5.2.\nExercises\nExercise 7.1 Behavior of training set error with increasing sample size\nThe error on the test will always decrease as we get more training data, since the model will be better\nestimated. However, as shown in Figure 7.10, for sufficiently complex models, the error on the training setcan increase we we get more training data, until we reach some plateau. Explain why.\nExercise 7.2 Multi-output linear regression\n(Source: Jaakkola.)\nWhen we have multiple independent outputs in linear regression, the model becomes\np(y|x,W)=M/productdisplay\nj=1N(yj|wT\njxi,\u03c32\nj) (7.89)\nSince the likelihood factorizes across dimensions, so does the MLE. Thus\n\u02c6W=[\u02c6w1,...,\u02c6wM] (7.90)\nwhere\u02c6wj=(XTX)\u22121Y:,j.\nIn this exercise we apply this result to a model with 2 dimensional response vector yi\u2208R2. Suppose we\nhave some binary input data, xi\u2208{0,1}. The training data is as follows:", "270": "240 Chapter 7. Linear regression\nxy\n0(\u22121,\u22121)T\n0(\u22121,\u22122)T\n0(\u22122,\u22121)T\n1(1,1)T\n1(1,2)T\n1(2,1)T\nLet us embed each xiinto 2d using the following basis function:\n\u03c6(0) = (1, 0)T,\u03c6(1) = (0, 1)T(7.91)\nThe model becomes\n\u02c6y=WT\u03c6(x) (7.92)\nwhereWis a2\u00d72matrix. Compute the MLE for Wfrom the above data.\nExercise 7.3 Centering and ridge regression\nAssume that x=0, so the input data has been centered. Show that the optimizer of\nJ(w,w0)=( y\u2212Xw\u2212w01)T(y\u2212Xw\u2212w01)+\u03bbwTw (7.93)\nis\n\u02c6w0=y (7.94)\nw=(XTX+\u03bbI)\u22121XTy (7.95)\nExercise 7.4 MLE for\u03c32for linear regression\nShow that the MLE for the error variance in linear regression is given by\n\u02c6\u03c32=1\nNN/summationdisplay\ni=1(yi\u2212xT\ni\u02c6w)2(7.96)\nThis is just the empirical variance of the residual errors when we plug in our estimate of \u02c6w.\nExercise 7.5 MLE for the offset term in linear regression\nLinear regression has the form E[y|x]=w 0+wTx. It is common to include a column of 1\u2019s in the\ndesign matrix, so we can solve for the offset term w0term and the other parameters wat the same time\nusing the normal equations. However, it is also possible to solve for wandw0separately. Show that\n\u02c6w0=1\nN/summationdisplay\niyi\u22121\nN/summationdisplay\nixTiw=y\u2212xTw (7.97)\nSo\u02c6w0models the difference in the average output from the average predicted output. Also, show that\n\u02c6w=(XTcXc)\u22121XTcyc=/bracketleftBiggN/summationdisplay\ni=1(xi\u2212x)(xi\u2212x)T/bracketrightBigg\u22121/bracketleftBiggN/summationdisplay\ni=1(yi\u2212y)(xi\u2212x)/bracketrightBigg\n(7.98)\nwhereXcis the centered input matrix containing xc\ni=xi\u2212xalong its rows, and yc=y\u2212yis\nthe centered output vector. Thus we can \ufb01rst compute \u02c6won centered data, and then estimate w0using\ny\u2212xT\u02c6w.", "271": "7.6. Bayesian linear regression 241\nExercise 7.6 MLE for simple linear regression\nSimple linear regression refers to the case where the input is scalar, so D=1. Show that the MLE in\nthis case is given by the following equations, which may be familiar from basic statistics classes:\nw1=/summationtext\ni(xi\u2212x)(yi\u2212\u00afy)/summationtext\ni(xi\u2212\u00afx)2=/summationtext\nixiyi\u2212Nxy/summationtext\nix2\ni\u2212Nx2\u2248cov[X,Y ]\nvar[X](7.99)\nw0=\u00afy\u2212w1\u00afx\u2248E[Y]\u2212w1E[X] (7.100)\nSeelinregDemo1 for a demo.\nExercise 7.7 Sufficient statistics for online linear regression\n(Source: Jaakkola.) Consider \ufb01tting the model \u02c6y=w0+w1xusing least squares. Unfortunately we did\nnot keep the original data, xi,yi, but we do have the following functions (statistics) of the data:\nx(n)=1\nnn/summationdisplay\ni=1xi,y(n)=1\nnn/summationdisplay\ni=1yi (7.101)\nC(n)\nxx=1\nnn/summationdisplay\ni=1(xi\u2212x)2,C(n)\nxy=1\nnn/summationdisplay\ni=1(xi\u2212x)(yi\u2212y),C(n)\nyy=1\nnn/summationdisplay\ni=1(yi\u2212y)2(7.102)\na. What are the minimal set of statistics that we need to estimate w1? (Hint: see Equation 7.99.)\nb. What are the minimal set of statistics that we need to estimate w0? (Hint: see Equation 7.97.)\nc. Suppose a new data point, xn+1,yn+1arrives, and we want to update our sufficient statistics without\nlooking at the old data, which we have not stored. (This is useful for online learning.) Show that we\ncan this for xas follows.\nx(n+1)/defines1\nn+1n+1/summationdisplay\ni=1xi=1\nn+1/parenleftBig\nnx(n)+xn+1/parenrightBig\n(7.103)\n=x(n)+1\nn+1(xn+1\u2212x(n)) (7.104)\nThis has the form: new estimate is old estimate plus correction. We see that the size of the correctiondiminishes over time (i.e., as we get more samples). Derive a similar expression to update\ny\nd. Show that one can update C(n+1)\nxyrecursively using\nC(n+1)\nxy=1\nn+1/bracketleftBig\nxn+1yn+1+nC(n)\nxy+nx(n)y(n)\u2212(n+1)x(n+1)y(n+1)/bracketrightBig\n(7.105)\nDerive a similar expression to update Cxx.\ne. Implementtheonlinelearningalgorithm, i.e., writeafunctionoftheform [w,ss] = linregUpdateSS(ss,\nx, y), where x and y are scalars and ss is a structure containing the sufficient statistics.\nf. Plot the coefficients over \u201ctime\u201d, using the dataset in linregDemo1 . (Speci\ufb01cally, use [x,y] =\npolyDataMake(\u2019sampling\u2019,\u2019thibaux\u2019). ) Check that they converge to the solution given by the\nbatch (offline) learner (i.e, ordinary least squares). Your result should look like Figure 7.14.\nTurn in your derivation, code and plot.\nExercise 7.8 Bayesian linear regression in 1d with known \u03c32\n(Source: Bolstad.) Consider \ufb01tting a model of the form\np(y|x,\u03b8)=N(y|w0+w1x,\u03c32) (7.106)\nto the data shown below:", "272": "242 Chapter 7. Linear regression\n0 5 10 15 20\u22126\u22125\u22124\u22123\u22122\u221210123\ntimeweightsonline linear regression\n  \nw0\nw1\nw0 batch\nw1 batch\nFigure 7.14 Regression coefficients over time. Produced by Exercise 7.7.\nx = [94,96,94,95,104,106,108,113,115,121,131];\ny = [0.47, 0.75, 0.83, 0.98, 1.18, 1.29, 1.40, 1.60, 1.75, 1.90, 2.23];\na. Compute an unbiased estimate of \u03c32using\n\u02c6\u03c32=1\nN\u22122N/summationdisplay\ni=1(yi\u2212\u02c6yi)2(7.107)\n(The denominator is N\u22122since we have 2 inputs, namely the offset term and x.) Here\u02c6yi=\u02c6w0+\u02c6w1xi,\nand\u02c6w=(\u02c6w0,\u02c6w1)is the MLE.\nb. Now assume the following prior on w:\np(w)=p(w0)p(w1) (7.108)\nUse an (improper) uniform prior on w0and aN(0,1)prior onw1. Show that this can be written as\na Gaussian prior of the form p(w)=N(w|w0,V0). What are w0andV0?\nc. Compute the marginal posterior of the slope, p(w1|D,\u03c32),w h e r eDis the data above, and \u03c32is the\nunbiased estimate computed above. What is E/bracketleftbig\nw1|D,\u03c32/bracketrightbig\nandvar/bracketleftbig\nw1|D,\u03c32/bracketrightbig\nShow your work. (You\ncan use Matlab if you like.) Hint: the posterior variance is a very small number!\nd. What is a 95% credible interval for w1?\nExercise 7.9 Generative model for linear regression\nLinear regression is the problem of estimating E[Y|x]using a linear function of the form w0+wTx.\nTypically we assume that the conditional distribution of YgivenXis Gaussian. We can either estimate this\nconditional Gaussian directly (a discriminative approach), or we can \ufb01t a Gaussian to the joint distributionofX,Yand then derive E[Y|X=x].\nIn Exercise 7.5 we showed that the discriminative approach leads to these equations\nE[Y|x]=w\n0+wTx (7.109)\nw0=y\u2212xTw (7.110)\nw=(XT\ncXc)\u22121XTcyc (7.111)", "273": "7.6. Bayesian linear regression 243\nwhereXc=X\u2212\u00afXis the centered input matrix, and \u00afX=1nxTreplicates xacross the rows. Similarly,\nyc=y\u2212yis the centered output vector, and y=1nyreplicates yacross the rows.\na. By \ufb01nding the maximum likelihood estimates of \u03a3XX,\u03a3XY,\u03bcXand\u03bcY, derive the above equations\nby \ufb01tting a joint Gaussian to X,Yand using the formula for conditioning a Gaussian (see Section 4.3.1).\nShow your work.\nb. What are the advantages and disadvantages of this approach compared to the standard discriminative\napproach?\nExercise 7.10 Bayesian linear regression using the g-prior\nShow that when we use the g-prior, p(w,\u03c32)=N I G ( w,\u03c32|0,g(XTX)\u22121,0,0), the posterior has the\nfollowing form:\np(w,\u03c32|D)=N I G ( w,\u03c32|wN,VN,aN,bN) (7.112)\nVN=g\ng+1(XTX)\u22121(7.113)\nwN=g\ng+1\u02c6wmle (7.114)\naN=N/2 (7.115)\nbN=s2\n2+1\n2(g+1)\u02c6wT\nmleXTX\u02c6wmle (7.116)\n(7.117)", "274": "", "275": "8 Logistic regression\n8.1 Introduction\nOne way to build a probabilistic classi\ufb01er is to create a joint model of the form p(y,x)and\nthen to condition on x, thereby deriving p(y|x). This is called the generative approach. An\nalternative approach is to \ufb01t a model of the form p(y|x)directly. This is called the discrimi-\nnativeapproach, and is the approach we adopt in this chapter. In particular, we will assume\ndiscriminative models which are linear in the parameters. This will turn out to signi\ufb01cantly sim-\nplify model \ufb01tting, as we will see. In Section 8.6, we compare the generative and discriminativeapproaches, and in later chapters, we will consider non-linear and non-parametric discriminativemodels.\n8.2 Model speci\ufb01cation\nAs we discussed in Section 1.4.6, logistic regression corresponds to the following binary classi\ufb01-cation model:\np(y|x,w)=B e r (y|sigm(w\nTx)) (8.1)\nA 1d example is shown in Figure 1.19(b). Logistic regression can easily be extended to higher-\ndimensional inputs. For example, Figure 8.1 shows plots of p(y=1|x,w) = sigm( wTx)for\n2d input and different weight vectors w. If we threshold these probabilities at 0.5, we induce a\nlinear decision boundary, whose normal (perpendicular) is given by w.\n8.3 Model \ufb01tting\nIn this section, we discuss algorithms for estimating the parameters of a logistic regressionmodel.", "276": "246 Chapter 8. Logistic regression\n\u22123 \u22122 \u22121 0 1 2 3 4 5 6\u22123\u22122\u22121012345\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( \u22122 , \u22121 )\nx1\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( \u22122 , 3 )\nx1\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( 0 , 2 )\nx1\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( 1 , 4 )\nx1\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( 1 , 0 )\nx1\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( 2 , 2 )\nx1\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( 2 , \u22122 )\nx1\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( 3 , 0 )\nx1\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( 5 , 4 )\nx1\n\u221210\n0\n10\u22121001000.51\nx2\nW = ( 5 , 1 )\nx1\nw1w2\nFigure 8.1 Plots ofsigm(w1x1+w2x2).H e r ew=(w1,w2)de\ufb01nes the normal to the decision\nboundary. Points to the right of this have sigm(wTx)>0.5, and points to the left have sigm(wTx)<\n0.5. Based on Figure 39.3 of (MacKay 2003). Figure generated by sigmoidplot2D .\n8.3.1 MLE\nThe negative log-likelihood for logistic regression is given by\nNLL(w)=\u2212N/summationdisplay\ni=1log[\u03bcI(yi=1)\ni\u00d7(1\u2212\u03bci)I(yi=0)] (8.2)\n=\u2212N/summationdisplay\ni=1[yilog\u03bci+(1\u2212yi)log(1\u2212\u03bci)] (8.3)\nThis is also called the cross-entropy error function (see Section 2.8.2).\nAnother way of writing this is as follows. Suppose \u02dcyi\u2208{ \u22121,+1}instead of yi\u2208{0,1}.W e\nhavep(y=1 )=1\n1+exp(\u2212wTx)andp(y=1 )=1\n1+exp(+ wTx). Hence\nNLL(w)=N/summationdisplay\ni=1log(1+exp( \u2212\u02dcyiwTxi)) (8.4)\nUnlike linear regression, we can no longer write down the MLE in closed form. Instead, we\nneed to use an optimization algorithm to compute it. For this, we need to derive the gradient\nand Hessian.\nIn the case of logistic regression, one can show (Exercise 8.3) that the gradient and Hessian", "277": "8.3. Model \ufb01tting 247\n0 0.5 1 1.5 2\u22120.500.511.522.53\n(a)0 0.5 1 1.5 2\u22120.500.511.522.53\n(b)\nFigure 8.2 Gradient descent on a simple function, starting from (0,0), for 20 steps, using a \ufb01xed\nlearning rate (step size) \u03b7. The global minimum is at (1,1). (a)\u03b7=0.1. (b)\u03b7=0.6. Figure generated by\nsteepestDescentDemo .\nof this are given by the following\ng=d\ndwf(w)=/summationdisplay\ni(\u03bci\u2212yi)xi=XT(\u03bc\u2212y) (8.5)\nH=d\ndwg(w)T=/summationdisplay\ni(\u2207w\u03bci)xT\ni=/summationdisplay\ni\u03bci(1\u2212\u03bci)xixTi(8.6)\n=XTSX (8.7)\nwhereS/definesdiag(\u03bc i(1\u2212\u03bci)). One can also show (Exercise 8.3) that His positive de\ufb01nite.\nHence the NLL is convex and has a unique global minimum. Below we discuss some methods\nfor \ufb01nding this minimum.\n8.3.2 Steepest descent\nPerhaps the simplest algorithm for unconstrained optimization is gradient descent, also known\nassteepest descent. This can be written as follows:\n\u03b8k+1=\u03b8k\u2212\u03b7kgk (8.8)\nwhere\u03b7kis thestep size orlearning rate. The main issue in gradient descent is: how should\nwe set the step size? This turns out to be quite tricky. If we use a constant learning rate, butmake it too small, convergence will be very slow, but if we make it too large, the method can failto converge at all. This is illustrated in Figure 8.2. where we plot the following (convex) function\nf(\u03b8)=0.5(\u03b8\n2\n1\u2212\u03b82)2+0.5(\u03b81\u22121)2, (8.9)\nWe arbitrarily decide to start from (0,0). In Figure 8.2(a), we use a \ufb01xed step size of \u03b7=0.1;w e\nsee that it moves slowly along the valley. In Figure 8.2(b), we use a \ufb01xed step size of \u03b7=0.6;w e\nsee that the algorithm starts oscillating up and down the sides of the valley and never convergesto the optimum.", "278": "248 Chapter 8. Logistic regression\nexact line searching 1\n0 0.5 1 1.5 2\u22120.500.511.522.53\n(a) (b)\nFigure 8.3 (a) Steepest descent on the same function as Figure 8.2, starting from (0,0), using line search.\nFigure generated by steepestDescentDemo . (b) Illustration of the fact that at the end of a line search\n(top of picture), the local gradient of the function will be perpendicular to the search direction. Based on\nFigure 10.6.1 of (Press et al. 1988).\nLet us develop a more stable method for picking the step size, so that the method is guaran-\nteed to converge to a local optimum no matter where we start. (This property is called global\nconvergence, which should not be confused with convergence to the global optimum!) By\nTaylor\u2019s theorem, we have\nf(\u03b8+\u03b7d)\u2248f(\u03b8)+\u03b7gTd (8.10)\nwheredis our descent direction. So if \u03b7is chosen small enough, then f(\u03b8+\u03b7d)<f(\u03b8), since\nthe gradient will be negative. But we don\u2019t want to choose the step size \u03b7too small, or we will\nmove very slowly and may not reach the minimum. So let us pick \u03b7to minimize\n\u03c6(\u03b7)=f(\u03b8k+\u03b7dk) (8.11)\nThis is called line minimization orline search. There are various methods for solving this 1d\noptimization problem; see (Nocedal and Wright 2006) for details.\nFigure 8.3(a) demonstrates that line search does indeed work for our simple problem. However,\nwe see that the steepest descent path with exact line searches exhibits a characteristic zig-zag\nbehavior. To see why, note that an exact line search satis\ufb01es \u03b7k=a r g m i n \u03b7>0\u03c6(\u03b7).A\nnecessary condition for the optimum is \u03c6/prime(\u03b7)=0. By the chain rule, \u03c6/prime(\u03b7)=dTg,w h e r e\ng=f/prime(\u03b8+\u03b7d)is the gradient at the end of the step. So we either have g=0, which means\nwe have found a stationary point, or g\u22a5d, which means that exact search stops at a point\nwhere the local gradient is perpendicular to the search direction. Hence consecutive directionswill be orthogonal (see Figure 8.3(b)). This explains the zig-zag behavior.\nOne simple heuristic to reduce the effect of zig-zagging is to add a momentum term,(\u03b8\nk\u2212\n\u03b8k\u22121), as follows:\n\u03b8k+1=\u03b8k\u2212\u03b7kgk+\u03bck(\u03b8k\u2212\u03b8k\u22121) (8.12)", "279": "8.3. Model \ufb01tting 249\nwhere0\u2264\u03bck\u22641controls the importance of the momentum term. In the optimization\ncommunity, this is known as the heavy ball method (see e.g., (Bertsekas 1999)).\nAn alternative way to minimize \u201czig-zagging\u201d is to use the method of conjugate gradients\n(see e.g., (Nocedal and Wright 2006, ch 5) or (Golub and van Loan 1996, Sec 10.2)). This is the\nmethod of choice for quadratic objectives of the form f(\u03b8)=\u03b8TA\u03b8, which arise when solving\nlinear systems. However, non-linear CG is less popular.\n8.3.3 Newton\u2019s method\nAlgorithm 8.1: Newton\u2019s method for minimizing a strictly convex function\n1Initialize\u03b80;\n2fork=1,2,... until convergence do\n3Evaluategk=\u2207f(\u03b8k);\n4EvaluateHk=\u22072f(\u03b8k);\n5SolveHkdk=\u2212gkfordk;\n6Use line search to \ufb01nd stepsize \u03b7kalongdk;\n7\u03b8k+1=\u03b8k+\u03b7kdk;\nOne can derive faster optimization methods by taking the curvature of the space (i.e., the\nHessian) into account. These are called second order optimization metods. The primary\nexample is Newton\u2019s algorithm. This is an iterative algorithm which consists of updates of the\nform\n\u03b8k+1=\u03b8k\u2212\u03b7kH\u22121\nkgk (8.13)\nThe full pseudo-code is given in Algorithm 2.\nThis algorithm can be derived as follows. Consider making a second-order Taylor series\napproximation of f(\u03b8)around\u03b8k:\nfquad(\u03b8)=fk+gT\nk(\u03b8\u2212\u03b8k)+1\n2(\u03b8\u2212\u03b8k)THk(\u03b8\u2212\u03b8k) (8.14)\nLet us rewrite this as\nfquad(\u03b8)=\u03b8TA\u03b8+bT\u03b8+c (8.15)\nwhere\nA=1\n2Hk,b=gk\u2212Hk\u03b8k,c=fk\u2212gT\nk\u03b8k+1\n2\u03b8T\nkHk\u03b8k (8.16)\nThe minimum of fquadis at\n\u03b8=\u22121\n2A\u22121b=\u03b8k\u2212H\u22121\nkgk (8.17)\nThus the Newton step dk=\u2212H\u22121\nkgkis what should be added to \u03b8kto minimize the second\norder approximation of faround\u03b8k. See Figure 8.4(a) for an illustration.", "280": "250 Chapter 8. Logistic regression\nf(x)\nfquad(x)\nxkxk+dk\n(a)f(x)\nfquad(x)\nxkxk+dk\n(b)\nFigure 8.4 Illustration of Newton\u2019s method for minimizing a 1d function. (a) The solid curve is the\nfunctionf(x). The dotted line fquad(x)is its second order approximation at xk. The Newton step dk\nis what must be added to xkto get to the minimum of fquad(x). Based on Figure 13.4 of (Vandenberghe\n2006). Figure generated by newtonsMethodMinQuad . (b) Illustration of Newton\u2019s method applied to a\nnonconvex function. We \ufb01t a quadratic around the current point xkand move to its stationary point,\nxk+1=xk+dk. Unfortunately, this is a local maximum, not minimum. This means we need to be careful\nabout the extent of our quadratic approximation. Based on Figure 13.11 of (Vandenberghe 2006). Figure\ngenerated by newtonsMethodNonConvex .\nIn its simplest form (as listed), Newton\u2019s method requires that Hkbe positive de\ufb01nite, which\nwill hold if the function is strictly convex. If not, the objective function is not convex, then\nHkmay not be positive de\ufb01nite, so dk=\u2212H\u22121\nkgkmay not be a descent direction (see\nFigure 8.4(b) for an example). In this case, one simple strategy is to revert to steepest descent,d\nk=\u2212gk. TheLevenberg Marquardt algorithm is an adaptive way to blend between Newton\nsteps and steepest descent steps. This method is widely used when solving nonlinear leastsquares problems. An alternative approach is this: Rather than computing d\nk=\u2212H\u22121\nkgk\ndirectly, we can solve the linear system of equations Hkdk=\u2212gkfordkusing conjugate\ngradient (CG). If Hkis not positive de\ufb01nite, we can simply truncate the CG iterations as soon\nas negative curvature is detected; this is called truncated Newton.\n8.3.4 Iteratively reweighted least squares (IRLS)\nLet us now apply Newton\u2019s algorithm to \ufb01nd the MLE for binary logistic regression. The Newtonupdate at iteration k+1for this model is as follows (using \u03b7\nk=1, since the Hessian is exact):\nwk+1=wk\u2212H\u22121gk (8.18)\n=wk+(XTSkX)\u22121XT(y\u2212\u03bck) (8.19)\n=(XTSkX)\u22121/bracketleftbig\n(XTSkX)wk+XT(y\u2212\u03bck)/bracketrightbig\n(8.20)\n=(XTSkX)\u22121XT[SkXwk+y\u2212\u03bck] (8.21)\n=(XTSkX)\u22121XTSkzk (8.22)\nwhere we have de\ufb01ned the working response as\nzk/definesXwk+S\u22121\nk(y\u2212\u03bck) (8.23)", "281": "8.3. Model \ufb01tting 251\nEquation 8.22 is an example of a weighted least squares problem, which is a minimizer of\nN/summationdisplay\ni=1Ski(zki\u2212wTxi)2(8.24)\nSinceSkis a diagonal matrix, we can rewrite the targets in component form (for each case\ni=1:N)a s\nzki=wT\nkxi+yi\u2212\u03bcki\n\u03bcki(1\u2212\u03bcki)(8.25)\nThis algorithm is known as iteratively reweighted least squares orIRLSfor short, since at\neach iteration, we solve a weighted least squares problem, where the weight matrix Skchanges\nat each iteration. See Algorithm 10 for some pseudocode.\nAlgorithm 8.2: Iteratively reweighted least squares (IRLS)\n1w=0D;\n2w0= log(y/(1\u2212y));\n3repeat\n4\u03b7i=w0+wTxi;\n5\u03bci=s i g m (\u03b7i);\n6si=\u03bci(1\u2212\u03bci);\n7zi=\u03b7i+yi\u2212\u03bci\nsi;\n8S=d i a g (s1:N);\n9w=(XTSX)\u22121XTSz;\n10until converged ;\n8.3.5 Quasi-Newton (variable metric) methods\nThe mother of all second-order optimization algorithm is Newton\u2019s algorithm, which we dis-\ncussed in Section 8.3.3. Unfortunately, it may be too expensive to compute Hexplicitly. Quasi-\nNewtonmethods iteratively build up an approximation to the Hessian using information gleaned\nfrom the gradient vector at each step. The most common method is called BFGS(named after\nits inventors, Broyden, Fletcher, Goldfarb and Shanno), which updates the approximation to theHessianB\nk\u2248Hkas follows:\nBk+1=Bk+ykyT\nk\nyT\nksk\u2212(Bksk)(Bksk)T\nsT\nkBksk(8.26)\nsk=\u03b8k\u2212\u03b8k\u22121 (8.27)\nyk=gk\u2212gk\u22121 (8.28)\nThis is a rank-two update to the matrix, and ensures that the matrix remains positive de\ufb01nite\n(under certain restrictions on the step size). We typically start with a diagonal approximation,B\n0=I. Thus BFGS can be thought of as a \u201cdiagonal plus low-rank\u201d approximation to the\nHessian.", "282": "252 Chapter 8. Logistic regression\nAlternatively, BFGS can iteratively update an approximation to the inverse Hessian, Ck\u2248H\u22121\nk,\nas follows:\nCk+1=/parenleftbigg\nI\u2212skyT\nk\nyT\nksk/parenrightbigg\nCk/parenleftbigg\nI\u2212yksT\nk\nyT\nksk/parenrightbigg\n+sksTk\nyT\nksk(8.29)\nSince storing the Hessian takes O(D2)space, for very large problems, one can use limited\nmemory BFGS,o rL-BFGS,w h e r e HkorH\u22121\nkis approximated by a diagonal plus low rank\nmatrix. In particular, the product H\u22121\nkgkcan be obtained by performing a sequence of inner\nproducts with skandyk, using only the mmost recent (sk,yk)pairs, and ignoring older\ninformation. The storage requirements are therefore O(mD). Typically m\u223c20suffices for\ngood performance. See (Nocedal and Wright 2006, p177) for more information. L-BFGS is\noften the method of choice for most unconstrained smooth optimization problems that arise inmachine learning (although see Section 8.5).\n8.3.6/lscript2regularization\nJust as we prefer ridge regression to linear regression, so we should prefer MAP estimation forlogistic regression to computing the MLE. In fact, regularization is important in the classi\ufb01cationsetting even if we have lots of data. To see why, suppose the data is linearly separable. Inthis case, the MLE is obtained when ||w|| \u2192 \u221e, corresponding to an in\ufb01nitely steep sigmoid\nfunction, I(w\nTx>w0), also known as a linear threshold unit. This assigns the maximal\namount of probability mass to the training data. However, such a solution is very brittle andwill not generalize well.\nTo prevent this, we can use /lscript\n2regularization, just as we did with ridge regression. We note\nthat the new objective, gradient and Hessian have the following forms:\nf/prime(w) = NLL( w)+\u03bbwTw (8.30)\ng/prime(w)=g (w)+\u03bbw (8.31)\nH/prime(w)=H( w)+\u03bbI (8.32)\nIt is a simple matter to pass these modi\ufb01ed equations into any gradient-based optimizer.\n8.3.7 Multi-class logistic regression\nNow we consider multinomial logistic regression , sometimes called a maximum entropy\nclassi\ufb01er. This is a model of the form\np(y=c|x,W)=exp(wT\ncx)/summationtextC\nc/prime=1exp(wT\nc/primex)(8.33)\nA slight variant, known as a conditional logit model, normalizes over a different set of classes\nfor each data case; this can be useful for modeling choices that users make between different\nsets of items that are offered to them.\nLet us now introduce some notation. Let \u03bcic=p(yi=c|xi,W)=S(\u03b7i)c,w h e r e\u03b7i=\nWTxiis aC\u00d71vector. Also, let yic=I(yi=c)be the one-of-C encoding of yi; thusyiis a\nbit vector, in which the c\u2019th bit turns on iff yi=c. Following (Krishnapuram et al. 2005), let us", "283": "8.3. Model \ufb01tting 253\nsetwC=0, to ensure identi\ufb01ability, and de\ufb01ne w= vec(W(:,1:C\u22121))to be aD\u00d7(C\u22121)\ncolumn vector.\nWith this, the log-likelihood can be written as\n/lscript(W)=l o gN/productdisplay\ni=1C/productdisplay\nc=1\u03bcyic\nic=N/summationdisplay\ni=1C/summationdisplay\nc=1yiclog\u03bcic (8.34)\n=N/summationdisplay\ni=1/bracketleftBigg/parenleftBiggC/summationdisplay\nc=1yicwT\ncxi/parenrightBigg\n\u2212log/parenleftBiggC/summationdisplay\nc/prime=1exp(wT\nc/primexi)/parenrightBigg/bracketrightBigg\n(8.35)\nDe\ufb01ne the NLL as\nf(w)=\u2212/lscript(w) (8.36)\nWe now proceed to compute the gradient and Hessian of this expression. Since wis block-\nstructured, the notation gets a bit heavy, but the ideas are simple. It helps to de\ufb01ne A\u2297B\nbe thekronecker product of matrices AandB.I fAis anm\u00d7nmatrix and Bis ap\u00d7q\nmatrix, then A\u00d7Bis themp\u00d7nqblock matrix\nA\u2297B=\u23a1\n\u23a2\u23a3a11B\u00b7\u00b7\u00b7a1nB\n.........\na\nm1B\u00b7\u00b7\u00b7amnB\u23a4\n\u23a5\u23a6 (8.37)\nReturning to the task at hand, one can show (Exercise 8.4) that the gradient is given by\ng(W)=\u2207f(w)=N/summationdisplay\ni=1(\u03bci\u2212yi)\u2297xi (8.38)\nwhereyi=(I(yi=1 ),...,I(yi=C\u22121))and\u03bci(W)=[p(yi=1|xi,W),...,p(yi=\nC\u22121|xi,W)]are column vectors of length C\u22121, For example, if we have D=3feature\ndimensions and C=3classes, this becomes\ng(W)=/summationdisplay\ni\u239b\n\u239c\u239c\u239c\u239c\u239c\u239c\u239d(\u03bc\ni1\u2212yi1)xi1\n(\u03bci1\u2212yi1)xi2\n(\u03bci1\u2212yi1)xi3\n(\u03bci2\u2212yi2)xi1\n(\u03bci2\u2212yi2)xi2\n(\u03bci2\u2212yi2)xi3\u239e\n\u239f\u239f\u239f\u239f\u239f\u239f\u23a0(8.39)\nIn other words, for each class c, the derivative for the weights in the c\u2019th column is\n\u2207\nwcf(W)=/summationdisplay\ni(\u03bcic\u2212yic)xi (8.40)\nThis has the same form as in the binary logistic regression case, namely an error term times xi.\n(This turns out to be a general property of distributions in the exponential family, as we will see\nin Section 9.3.2.)", "284": "254 Chapter 8. Logistic regression\nOne can also show (Exercise 8.4) that the Hessian is the following block structured D(C\u2212\n1)\u00d7D(C\u22121)matrix:\nH(W)=\u22072f(w)=N/summationdisplay\ni=1(diag(\u03bci)\u2212\u03bci\u03bcT\ni)\u2297(xixTi) (8.41)\nFor example, if we have 3 features and 3 classes, this becomes\nH(W)=/summationdisplay\ni/parenleftbigg\u03bci1\u2212\u03bc2\ni1\u2212\u03bci1\u03bci2\n\u2212\u03bci1\u03bci2\u03bci2\u2212\u03bc2i2/parenrightbigg\n\u2297\u239b\n\u239dxi1xi1xi1xi2xi1xi3\nxi2xi1xi2xi2xi2xi3\nxi3xi1xi3xi2xi3xi3\u239e\u23a0 (8.42)\n=/summationdisplay\ni/parenleftbigg(\u03bci1\u2212\u03bc2\ni1)Xi\u2212\u03bci1\u03bci2Xi\n\u2212\u03bci1\u03bci2Xi(\u03bci2\u2212\u03bc2i2)Xi/parenrightbigg\n(8.43)\nwhereXi=xixTi. In other words, the block c,c/primesubmatrix is given by\nHc,c/prime(W)=/summationdisplay\ni\u03bcic(\u03b4c,c/prime\u2212\u03bci,c/prime)xixT\ni (8.44)\nThis is also a positive de\ufb01nite matrix, so there is a unique MLE.\nNow consider minimizing\nf/prime(W)/defines\u2212logp(D|w)\u2212logp(W) (8.45)\nwherep(W)=/producttext\ncN(wc|0,V0). The new objective, its gradient and Hessian are given by\nf/prime(W)=f (W)+1\n2/summationdisplay\ncwcV\u22121\n0wc (8.46)\ng/prime(W)=g (W)+V\u22121\n0(/summationdisplay\ncwc) (8.47)\nH/prime(W)=H( W)+IC\u2297V\u22121\n0 (8.48)\nThis can be passed to any gradient-based optimizer to \ufb01nd the MAP estimate. Note, however,\nthat the Hessian has size O((CD)\u00d7(CD)), which is Ctimes more row and columns than\nin the binary case, so limited memory BFGS is more appropriate than Newton\u2019s method. SeelogregFit for some Matlab code.\n8.4 Bayesian logistic regression\nIt is natural to want to compute the full posterior over the parameters, p(w|D), for logistic\nregression models. This can be useful for any situation where we want to associate con\ufb01denceintervals with our predictions (e.g., this is necessary when solving contextual bandit problems,discussed in Section 5.7.3.1).\nUnfortunately, unlike the linear regression case, this cannot be done exactly, since there is no\nconvenient conjugate prior for logistic regression. We discuss one simple approximation below;some other approaches include MCMC (Section 24.3.3.1), variational inference (Section 21.8.1.1),expectation propagation (Kuss and Rasmussen 2005), etc. For notational simplicity, we stick tobinary logistic regression.", "285": "8.4. Bayesian logistic regression 255\n8.4.1 Laplace approximation\nIn this section, we discuss how to make a Gaussian approximation to a posterior distribution.\nThe approximation works as follows. Suppose \u03b8\u2208RD.L e t\np(\u03b8|D)=1\nZe\u2212E(\u03b8)(8.49)\nwhereE(\u03b8)is called an energy function, and is equal to the negative log of the unnormal-\nized log posterior, E(\u03b8)=\u2212logp(\u03b8,D), withZ=p(D)being the normalization constant.\nPerforming a Taylor series expansion around the mode \u03b8\u2217(i.e., the lowest energy state) we get\nE(\u03b8)\u2248E(\u03b8\u2217)+(\u03b8\u2212\u03b8\u2217)Tg+1\n2(\u03b8\u2212\u03b8\u2217)TH(\u03b8\u2212\u03b8\u2217) (8.50)\nwheregis the gradient and His the Hessian of the energy function evaluated at the mode:\ng/defines\u2207E(\u03b8)/vextendsingle/vextendsingle\n\u03b8\u2217,H/defines\u22022E(\u03b8)\n\u2202\u03b8\u2202\u03b8T|\u03b8\u2217 (8.51)\nSince\u03b8\u2217is the mode, the gradient term is zero. Hence\n\u02c6p(\u03b8|D)\u22481\nZe\u2212E(\u03b8\u2217)exp/bracketleftbigg\n\u22121\n2(\u03b8\u2212\u03b8\u2217)TH(\u03b8\u2212\u03b8\u2217)/bracketrightbigg\n(8.52)\n=N(\u03b8|\u03b8\u2217,H\u22121) (8.53)\nZ=p(D)\u2248/integraldisplay\n\u02c6p(\u03b8|D)d\u03b8=e\u2212E(\u03b8\u2217)(2\u03c0)D/2|H|\u22121\n2 (8.54)\nThe last line follows from normalization constant of the multivariate Gaussian.\nEquation 8.54 is known as the Laplace approximation to the marginal likelihood. Therefore\nEquation 8.52 is sometimes called the the Laplace approximation to the posterior. However,\nin the statistics community, the term \u201cLaplace approximation\u201d refers to a more sophisticated\nmethod (see e.g. (Rue et al. 2009) for details). It may therefore be better to use the term\u201cGaussian approximation\u201d to refer to Equation 8.52. A Gaussian approximation is often areasonable approximation, since posteriors often become more \u201cGaussian-like\u201d as the samplesize increases, for reasons analogous to the central limit theorem. (In physics, there is ananalogous technique known as a saddle point approximation.)\n8.4.2 Derivation of the BIC\nWe can use the Gaussian approximation to write the log marginal likelihood as follows, droppingirrelevant constants:\nlogp(D)\u2248logp(D|\u03b8\n\u2217)+logp(\u03b8\u2217)\u22121\n2log|H| (8.55)\nThe penalization terms which are added to the logp(D|\u03b8\u2217)are sometimes called the Occam\nfactor, and are a measure of model complexity. If we have a uniform prior, p(\u03b8)\u221d1, we can\ndrop the second term, and replace \u03b8\u2217with the MLE, \u02c6\u03b8.", "286": "256 Chapter 8. Logistic regression\nWe now focus on approximating the third term. We have H=/summationtextN\ni=1Hi,w h e r eHi=\n\u2207\u2207logp(Di|\u03b8). Let us approximate each Hib ya\ufb01 x e dm a t r i x \u02c6H. Then we have\nlog|H|=l o g|N\u02c6H|= log(Nd|\u02c6H|)=DlogN+log|\u02c6H| (8.56)\nwhereD=d i m (\u03b8)and we have assumed His full rank. We can drop the log|\u02c6H|term, since\nit is independent of N, and thus will get overwhelmed by the likelihood. Putting all the pieces\ntogether, we recover the BIC score (Section 5.3.2.4):\nlogp(D)\u2248logp(D|\u02c6\u03b8)\u2212D\n2logN (8.57)\n8.4.3 Gaussian approximation for logistic regression\nNow let us apply the Gaussian approximation to logistic regression. We will use a a Gaussian\nprior of the form p(w)=N(w|0,V0), just as we did in MAP estimation. The approximate\nposterior is given by\np(w|D)\u2248N(w|\u02c6w,H\u22121) (8.58)\nwhere\u02c6w=a r gm i n wE(w),E(w)=\u2212(logp(D|w)+logp(w)), andH=\u22072E(w)|\u02c6w.\nAs an example, consider the linearly separable 2D data in Figure 8.5(a). There are many\nparameter settings that correspond to lines that perfectly separate the training data; we show 4examples. The likelihood surface is shown in Figure 8.5(b), where we see that the likelihood isunbounded as we move up and to the right in parameter space, along a ridge where w\n2/w1=\n2.35(this is indicated by the diagonal line). The reasons for this is that we can maximize the\nlikelihood by driving ||w||to in\ufb01nity (subject to being on this line), since large regression weights\nmake the sigmoid function very steep, turning it into a step function. Consequently the MLE isnot well de\ufb01ned when the data is linearly separable.\nToregularizetheproblem, letususeavaguesphericalpriorcenteredattheorigin, N(w|0,100I).\nMultiplying this spherical prior by the likelihood surface results in a highly skewed posterior,shown in Figure 8.5(c). (The posterior is skewed because the likelihood function \u201cchops off\u201dregions of parameter space (in a \u201csoft\u201d fashion) which disagree with the data.) The MAP estimateis shown by the blue dot. Unlike the MLE, this is not at in\ufb01nity.\nThe Gaussian approximation to this posterior is shown in Figure 8.5(d). We see that this is\na symmetric distribution, and therefore not a great approximation. Of course, it gets the modecorrect (by construction), and it at least represents the fact that there is more uncertainty alongthe southwest-northeast direction (which corresponds to uncertainty about the orientation ofseparating lines) than perpendicular to this. Although a crude approximation, this is surelybetter than approximating the posterior by a delta function, which is what MAP estimation does.\n8.4.4 Approximating the posterior predictive\nGiven the posterior, we can compute credible intervals, perform hypothesis tests, etc., just as wedid in Section 7.6.3.3 in the case of linear regression. But in machine learning, interest usuallyfocusses on prediction. The posterior predictive distribution has the form\np(y|x,D)=/integraldisplay\np(y|x,w)p(w|D)dw (8.59)", "287": "8.4. Bayesian logistic regression 257\n\u221210 \u22125 0 5\u22128\u22126\u22124\u2212202468data\n(a)Log\u2212Likelihood\n123 4\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22128\u22126\u22124\u2212202468\n(b)\nLog\u2212Unnormalised Posterior\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22128\u22126\u22124\u2212202468\n(c)Laplace Approximation to Posterior\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22128\u22126\u22124\u2212202468\n(d)\nFigure 8.5 (a) Two-class data in 2d. (b) Log-likelihood for a logistic regression model. The line is drawn\nfrom the origin in the direction of the MLE (which is at in\ufb01nity). The numbers correspond to 4 points\nin parameter space, corresponding to the lines in (a). (c) Unnormalized log posterior (assuming vaguespherical prior). (d) Laplace approximation to posterior. Based on a \ufb01gure by Mark Girolami. Figuregenerated by logregLaplaceGirolamiDemo .\nUnfortunately this integral is intractable.\nThe simplest approximation is the plug-in approximation, which, in the binary case, takes the\nform\np(y=1|x,D)\u2248p(y=1|x,E[w]) (8.60)\nwhere E[w]is the posterior mean. In this context, E[w]is called the Bayes point. Of course,\nsuch a plug-in estimate underestimates the uncertainty. We discuss some better approximations\nbelow.", "288": "258 Chapter 8. Logistic regression\np(y=1|x, wMAP)\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22128\u22126\u22124\u2212202468\n(a)\u221210 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22128\u22126\u22124\u2212202468decision boundary for sampled w\n(b)\nMC approx of p(y=1|x)\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22128\u22126\u22124\u2212202468\n(c)numerical approx of p(y=1|x)\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22128\u22126\u22124\u2212202468\n(d)\nFigure 8.6 Posterior predictive distribution for a logistic regression model in 2d. Top left: contours of\np(y=1|x,\u02c6wmap). Top right: samples from the posterior predictive distribution. Bottom left: Averaging\nover these samples. Bottom right: moderated output (probit approximation). Based on a \ufb01gure by Mark\nGirolami. Figure generated by logregLaplaceGirolamiDemo .\n8.4.4.1 Monte Carlo approximation\nA better approach is to use a Monte Carlo approximation, as follows:\np(y=1|x,D)\u22481\nSS/summationdisplay\ns=1sigm((ws)Tx) (8.61)\nwherews\u223cp(w|D)are samples from the posterior. (This technique can be trivially extended\nto the multi-class case.) If we have approximated the posterior using Monte Carlo, we can reuse\nthese samples for prediction. If we made a Gaussian approximation to the posterior, we candraw independent samples from the Gaussian using standard methods.\nFigure 8.6(b) shows samples from the posteiror predictive for our 2d example. Figure 8.6(c)", "289": "8.4. Bayesian logistic regression 259\n460 480 500 520 540 560 580 600 620 64000.10.20.30.40.50.60.70.80.91\n(a)\u22126 \u22124 \u22122 0 2 4 600.10.20.30.40.50.60.70.80.91\n  \nsigmoid\nprobit\n(b)\nFigure 8.7 (a) Posterior predictive density for SAT data. The red circle denotes the posterior mean, the\nblue cross the posterior median, and the blue lines denote the 5th and 95th percentiles of the predictive\ndistribution. Figure generated by logregSATdemoBayes . (b) The logistic (sigmoid) function sigm(x) in\nsolid red, with the rescaled probit function \u03a6(\u03bbx)in dotted blue superimposed. Here \u03bb=/radicalbig\n\u03c0/8, which\nwas chosen so that the derivatives of the two curves match at x=0. Based on Figure 4.9 of (Bishop\n2006b). Figure generated by probitPlot . Figure generated by probitRegDemo .\nshows the average of these samples. By averaging over multiple predictions, we see that the\nuncertainty in the decision boundary \u201csplays out\u201d as we move further from the training data.So although the decision boundary is linear, the posterior predictive density is not linear. Notealso that the posterior mean decision boundary is roughly equally far from both classes; this isthe Bayesian analog of the large margin principle discussed in Section 14.5.2.2.\nFigure 8.7(a) shows an example in 1d. The red dots denote the mean of the posterior predictive\nevaluated at the training data. The vertical blue lines denote 95% credible intervals for theposterior predictive; the small blue star is the median. We see that, with the Bayesian approach,we are able to model our uncertainty about the probability a student will pass the exam basedon his SAT score, rather than just getting a point estimate.\n8.4.4.2 Probit approximation (moderated output) *\nIf we have a Gaussian approximation to the posterior p(w|D)\u2248N(w|m\nN,VN), we can also\ncompute a deterministic approximation to the posterior predictive distribution, at least in thebinary case. We proceed as follows:\np(y=1|x,D)\u2248/integraldisplay\nsigm(w\nTx)p(w|D)dw=/integraldisplay\nsigm(a)N(a|\u03bca,\u03c32\na)da (8.62)\na/defineswTx (8.63)\n\u03bca/definesE[a]=mT\nNx (8.64)\n\u03c32\na/definesvar[a]=/integraldisplay\np(a|D)[a2\u2212E/bracketleftbig\na2/bracketrightbig\n]da (8.65)\n=/integraldisplay\np(w|D)[(wTx)2\u2212(mTNx)2]dw=xTVNx (8.66)", "290": "260 Chapter 8. Logistic regression\nThus we see that we need to evaluate the expectation of a sigmoid with respect to a Gaussian.\nThis can be approximated by exploiting the fact that the sigmoid function is similar to theprobitfunction, which is given by the cdf of the standard normal:\n\u03a6(a)/defines/integraldisplay\na\n\u2212\u221eN(x|0,1)dx (8.67)\nFigure 8.7(b) plots the sigmoid and probit functions. We have rescaled the axes so that sigm(a)\nhas the same slope as \u03a6(\u03bba)at the origin, where \u03bb2=\u03c0/8.\nThe advantage of using the probit is that one can convolve it with a Gaussian analytically:\n/integraldisplay\n\u03a6(\u03bba)N(a|\u03bc,\u03c32)da=\u03a6/parenleftbigga\n(\u03bb\u22122+\u03c32)1\n2/parenrightbigg\n(8.68)\nWe now plug in the approximation sigm(a)\u2248\u03a6(\u03bba)to both sides of this equation to get\n/integraldisplay\nsigm(a)N(a|\u03bc,\u03c32)da\u2248sigm(\u03ba(\u03c32)\u03bc) (8.69)\n\u03ba(\u03c32)/defines(1+\u03c0\u03c32/8)\u22121\n2 (8.70)\nApplying this to the logistic regression model we get the following expression (\ufb01rst suggested in(Spiegelhalter and Lauritzen 1990)):\np(y=1|x,D)\u2248sigm(\u03ba(\u03c3\n2\na)\u03bca) (8.71)\nFigure 8.6(d) indicates that this gives very similar results to the Monte Carlo approximation.\nUsing Equation 8.71 is sometimes called a moderated output, since it is less extreme than\nthe plug-in estimate. To see this, note that 0\u2264\u03ba(\u03c32)\u22641and hence\nsigm(\u03ba(\u03c32)\u03bc)\u2264sigm(\u03bc)=p(y=1|x,\u02c6w) (8.72)\nwhere the inequality is strict if \u03bc/negationslash=0.I f\u03bc>0,w eh a v e p(y=1|x,\u02c6w)>0.5, but the\nmoderated prediction is always closer to 0.5, so it is less con\ufb01dent. However, the decisionboundary occurs whenever p(y=1|x,D) = sigm(\u03ba( \u03c3\n2)\u03bc)=0.5, which implies \u03bc=\u02c6wTx=\n0. Hence the decision boundary for the moderated approximation is the same as for the plug-inapproximation. So the number of misclassi\ufb01cations will be the same for the two methods,but the log-likelihood will not. (Note that in the multiclass case, taking into account posteriorcovariance gives different answers than the plug-in approach: see Exercise 3.10.3 of (Rasmussenand Williams 2006).)\n8.4.5 Residual analysis (outlier detection) *\nIt is sometimes useful to detect data cases which are \u201coutliers\u201d. This is called residual analysis\norcase analysis. In a regression setting, this can be performed by computing ri=yi\u2212\u02c6yi,w h e r e\n\u02c6yi=\u02c6wTxi. These values should follow a N(0,\u03c32)distribution, if the modelling assumptions\nare correct. This can be assessed by creating a qq-plot, where we plot the Ntheoretical\nquantiles of a Gaussian distribution against the Nempirical quantiles of the ri. Points that\ndeviate from the straightline are potential outliers.", "291": "8.5. Online learning and stochastic optimization 261\nClassical methods, based on residuals, do not work well for binary data, because they rely\non asymptotic normality of the test statistics. However, adopting a Bayesian approach, we\ncan just de\ufb01ne outliers to be points which which p(yi|\u02c6yi)is small, where we typically use\n\u02c6yi=s i g m (\u02c6wTxi). Note that \u02c6wwas estimated from all the data. A better method is to exclude\n(xi,yi)from the estimate of wwhen predicting yi. That is, we de\ufb01ne outliers to be points\nwhich have low probability under the cross-validated posterior predictive distribution, de\ufb01nedby\np(y\ni|xi,x\u2212i,y\u2212i)=/integraldisplay\np(yi|xi,w)/productdisplay\ni/prime/negationslash=ip(yi/prime|xi/prime,w)p(w)dw (8.73)\nThis can be efficiently approximated by sampling methods (Gelfand 1996). For further discussionof residual analysis in logistic regression models, see e.g.,(Johnson and Albert 1999, Sec 3.4).\n8.5 Online learning and stochastic optimization\nTraditionally machine learning is performed offline, which means we have a batchof data, and\nwe optimize an equation of the following form\nf(\u03b8)=1\nNN/summationdisplay\ni=1f(\u03b8,zi) (8.74)\nwherezi=(xi,yi)in the supervised case, or just xiin the unsupervised case, and f(\u03b8,zi)is\nsome kind of loss function. For example, we might use\nf(\u03b8,zi)=\u2212logp(yi|xi,\u03b8) (8.75)\nin which case we are trying to maximize the likelihood. Alternatively, we might use\nf(\u03b8,zi)=L(yi,h(xi,\u03b8)) (8.76)\nwhereh(xi,\u03b8)is a prediction function, and L(y,\u02c6y)is some other loss function such as squared\nerror or the Huber loss. In frequentist decision theory, the average loss is called the risk (seeSection 6.3), so this overall approach is called empirical risk minimization or ERM (see Section 6.5for details).\nHowever, if we have streaming data, we need to perform online learning, so we can update\nour estimates as each new data point arrives rather than waiting until \u201cthe end\u201d (which maynever occur). And even if we have a batch of data, we might want to treat it like a stream if it istoo large to hold in main memory. Below we discuss learning methods for this kind of scenario.\n1\n1. A simple implementation trick can be used to speed up batch learning algorithms when applied to data sets that\nare too large to hold in memory. First note that the naive implementation makes a pass over the data \ufb01le, from the\nbeginning to end, accumulating the sufficient statistics and gradients as it goes; then an update is performed and theprocess repeats. Unfortunately, at the end of each pass, the data from the beginning of the \ufb01le will have been evicted\nfrom the cache (since are are assuming it cannot all \ufb01t into memory). Rather than going back to the beginning of the\n\ufb01le and reloading it, we can simply work backwards from the end of the \ufb01le, which is already in memory. We then\nrepeat this forwards-backwards pattern over the data. This simple trick is known as rocking.", "292": "262 Chapter 8. Logistic regression\n8.5.1 Online learning and regret minimization\nSuppose that at each step, \u201cnature\u201d presents a sample zkand the \u201clearner\u201d must respond with\na parameter estimate \u03b8k. In the theoretical machine learning community, the objective used in\nonline learning is the regret, which is the averaged loss incurred relative to the best we could\nhave gotten in hindsight using a single \ufb01xed parameter value:\nregretk/defines1\nkk/summationdisplay\nt=1f(\u03b8t,zt)\u2212min\n\u03b8\u2217\u2208\u03981\nkk/summationdisplay\nt=1f(\u03b8\u2217,zt) (8.77)\nFor example, imagine we are investing in the stock-market. Let \u03b8jbe the amount we invest in\nstockj, and letzjbe the return on this stock. Our loss function is f(\u03b8,z)=\u2212\u03b8Tz. The regret\nis how much better (or worse) we did by trading at each step, rather than adopting a \u201cbuy and\nhold\u201d strategy using an oracle to choose which stocks to buy.\nOne simple algorithm for online learning is online gradient descent (Zinkevich 2003), which\nis as follows: at each step k, update the parameters using\n\u03b8k+1=p r oj\u0398(\u03b8k\u2212\u03b7kgk) (8.78)\nwhereprojV(v) = argminw\u2208V||w\u2212v||2is the projection of vector vonto space V,gk=\n\u2207f(\u03b8k,zk)is the gradient, and \u03b7kis the step size. (The projection step is only needed if\nthe parameter must be constrained to live in a certain subset of RD. See Section 13.4.3 for\ndetails.) Below we will see how this approach to regret minimization relates to more traditionalobjectives, such as MLE.\nThere are a variety of other approaches to regret minimization which are beyond the scope\nof this book (see e.g., Cesa-Bianchi and Lugosi (2006) for details).\n8.5.2 Stochastic optimization and risk minimization\nNow suppose that instead of minimizing regret with respect to the past, we want to minimizeexpected loss in the future, as is more common in (frequentist) statistical learning theory. Thatis, we want to minimize\nf(\u03b8)=E[f(\u03b8,z)] (8.79)\nwhere the expectation is taken over future data. Optimizing functions where some of thevariables in the objective are random is called stochastic optimization.\n2\nSuppose we receive an in\ufb01nite stream of samples from the distribution. One way to optimize\nstochastic objectives such as Equation 8.79 is to perform the update in Equation 8.78 at eachstep. This is called stochastic gradient descent orSGD(Nemirovski and Yudin 1978). Since we\ntypically want a single parameter estimate, we can use a running average:\n\u03b8k=1\nkk/summationdisplay\nt=1\u03b8t (8.80)\n2. Note that in stochastic optimization, the objective is stochastic, and therefore the algorithms will be, too. However,\nit is also possible to apply stochastic optimization algorithms to deterministic objectives. Examples include simulated\nannealing (Section 24.6.1) and stochastic gradient descent applied to the empirical risk minimization problem. There are\nsome interesting theoretical connections between online learning and stochastic optimization (Cesa-Bianchi and Lugosi\n2006), but this is beyond the scope of this book.", "293": "8.5. Online learning and stochastic optimization 263\nThis is called Polyak-Ruppert averaging, and can be implemented recursively as follows:\n\u03b8k=\u03b8k\u22121\u22121\nk(\u03b8k\u22121\u2212\u03b8k) (8.81)\nSee e.g., (Spall 2003; Kushner and Yin 2003) for details.\n8.5.2.1 Setting the step size\nWe now discuss some sufficient conditions on the learning rate to guarantee convergence of\nSGD. These are known as the Robbins-Monro conditions:\n\u221e/summationdisplay\nk=1\u03b7k=\u221e,\u221e/summationdisplay\nk=1\u03b72\nk<\u221e. (8.82)\nThe set of values of \u03b7kover time is called the learning rate schedule. Various formulas are\nused, such as \u03b7k=1/k, or the following (Bottou 1998; Bach and Moulines 2011):\n\u03b7k=(\u03c40+k)\u2212\u03ba(8.83)\nwhere\u03c40\u22650slows down early iterations of the algorithm, and \u03ba\u2208(0.5,1]controls the rate at\nwhich old values of are forgotten.\nThe need to adjust these tuning parameters is one of the main drawback of stochastic\noptimization. One simple heuristic (Bottou 2007) is as follows: store an initial subset of thedata, and try a range of \u03b7values on this subset; then choose the one that results in the fastest\ndecrease in the objective and apply it to all the rest of the data. Note that this may not resultin convergence, but the algorithm can be terminated when the performance improvement on ahold-out set plateaus (this is called early stopping).\n8.5.2.2 Per-parameter step sizes\nOne drawback of SGD is that it uses the same step size for all parameters. We now brie\ufb02ypresent a method known as adagrad (short for adaptive gradient) (Duchi et al. 2010), which is\nsimilar in spirit to a diagonal Hessian approximation. (See also (Schaul et al. 2012) for a similarapproach.) In particular, if \u03b8\ni(k)is parameter iat timek, andgi(k)is its gradient, then we\nmake an update as follows:\n\u03b8i(k+1)=\u03b8i(k)\u2212\u03b7gi(k)\n\u03c40+/radicalbig\nsi(k)(8.84)\nwhere the diagonal step size vector is the gradient vector squared, summed over all time steps.This can be recursively updated as follows:\ns\ni(k)=si(k\u22121)+gi(k)2(8.85)\nThe result is a per-parameter step size that adapts to the curvature of the loss function. Thismethod was original derived for the regret minimization case, but it can be applied moregenerally.", "294": "264 Chapter 8. Logistic regression\n8.5.2.3 SGD compared to batch learning\nIf we don\u2019t have an in\ufb01nite data stream, we can \u201csimulate\u201d one by sampling data points at\nrandom from our training set. Essentially we are optimizing Equation 8.74 by treating it as anexpectation with respect to the empirical distribution.\nAlgorithm 8.3: Stochastic gradient descent\n1Initialize\u03b8,\u03b7;\n2repeat\n3Randomly permute data;\n4fori=1:Ndo\n5 g=\u2207f(\u03b8,zi);\n6 \u03b8\u2190proj\u0398(\u03b8\u2212\u03b7g);\n7 Update\u03b7;\n8until converged ;\nIn theory, we should sample with replacement, although in practice it is usually better to\nrandomly permute the data and sample without replacement, and then to repeat. A single suchpass over the entire data set is called an epoch. See Algorithm 8 for some pseudocode.\nIn this offline case, it is often better to compute the gradient of a mini-batch ofBdata cases.\nIfB=1, this is standard SGD, and if B=N, this is standard steepest descent. Typically\nB\u223c100is used.\nAlthough a simple \ufb01rst-order method, SGD performs surprisingly well on some problems,\nespecially ones with large data sets (Bottou 2007). The intuitive reason for this is that one canget a fairly good estimate of the gradient by looking at just a few examples. Carefully evaluatingprecise gradients using large datasets is often a waste of time, since the algorithm will haveto recompute the gradient again anyway at the next step. It is often a better use of computertime to have a noisy estimate and to move rapidly through parameter space. As an extremeexample, suppose we double the training set by duplicating every example. Batch methods willtake twice as long, but online methods will be unaffected, since the direction of the gradienthas not changed (doubling the size of the data changes the magnitude of the gradient, but thatis irrelevant, since the gradient is being scaled by the step size anyway).\nIn addition to enhanced speed, SGD is often less prone to getting stuck in shallow local\nminima, because it adds a certain amount of \u201cnoise\u201d. Consequently it is quite popular in themachine learning community for \ufb01tting models with non-convex objectives, such as neuralnetworks (Section 16.5) and deep belief networks (Section 28.1).\n8.5.3 The LMS algorithm\nAs an example of SGD, let us consider how to compute the MLE for linear regression in anonline fashion. We derived the batch gradient in Equation 7.14. The online gradient at iterationkis given by\ng\nk=xi(\u03b8T\nkxi\u2212yi) (8.86)", "295": "8.5. Online learning and stochastic optimization 265\nblack line = LMS trajectory towards LS soln (red cross)\nw0w1\n\u22121 0 1 2 3\u22121\u22120.500.511.522.53\n(a)0 5 10 15 20 25 30345678910RSS vs iteration\n(b)\nFigure 8.8 Illustration of the LMS algorithm. Left: we start from \u03b8=(\u22120.5,2)and slowly converging\nto the least squares solution of \u02c6\u03b8=( 1.45,0.92)(red cross). Right: plot of objective function over time.\nNote that it does not decrease monotonically. Figure generated by LMSdemo.\nwherei=i(k)is the training example to use at iteration k. If the data set is streaming, we use\ni(k)=k; we shall assume this from now on, for notational simplicity. Equation 8.86 is easy\nto interpret: it is the feature vector xkweighted by the difference between what we predicted,\n\u02c6yk=\u03b8T\nkxk, and the true response, yk; hence the gradient acts like an error signal.\nAfter computing the gradient, we take a step along it as follows:\n\u03b8k+1=\u03b8k\u2212\u03b7k(\u02c6yk\u2212yk)xk (8.87)\n(There is no need for a projection step, since this is an unconstrained optimization problem.)\nThis algorithm is called the least mean squares orLMSalgorithm, and is also known as the\ndelta rule,o rt h eWidrow-Hoff rule.\nFigure 8.8 shows the results of applying this algorithm to the data shown in Figure 7.2. We\nstart at\u03b8=(\u22120.5,2)and converge (in the sense that ||\u03b8k\u2212\u03b8k\u22121||2\n2drops below a threshold\nof10\u22122) in about 26 iterations.\nNote that LMS may require multiple passes through the data to \ufb01nd the optimum. By\ncontrast, the recursive least squares algorithm, which is based on the Kalman \ufb01lter and which\nuses second-order information, \ufb01nds the optimum in a single pass (see Section 18.2.3). See alsoExercise 7.7.\n8.5.4 The perceptron algorithm\nNow let us consider how to \ufb01t a binary logistic regression model in an online manner. Thebatch gradient was given in Equation 8.5. In the online case, the weight update has the simpleform\n\u03b8\nk=\u03b8k\u22121\u2212\u03b7kgi=\u03b8k\u22121\u2212\u03b7k(\u03bci\u2212yi)xi (8.88)\nwhere\u03bci=p(yi=1|xi,\u03b8k)=E[yi|xi,\u03b8k]. We see that this has exactly the same form as the\nLMS algorithm. Indeed, this property holds for all generalized linear models (Section 9.3).", "296": "266 Chapter 8. Logistic regression\nWe now consider an approximation to this algorithm. Speci\ufb01cally, let\n\u02c6yi=a r g m a x\ny\u2208{0,1}p(y|xi,\u03b8) (8.89)\nrepresent the most probable class label. We replace \u03bci=p(y=1|xi,\u03b8) = sigm(\u03b8Txi)in the\ngradient expression with \u02c6yi. Thus the approximate gradient becomes\ngi\u2248(\u02c6yi\u2212yi)xi (8.90)\nIt will make the algebra simpler if we assume y\u2208{ \u22121,+1}rather than y\u2208{0,1}. In this\ncase, our prediction becomes\n\u02c6yi=s i g n (\u03b8Txi) (8.91)\nThen if\u02c6yiyi=\u22121, we have made an error, but if \u02c6yiyi=+ 1, we guessed the right label.\nAt each step, we update the weight vector by adding on the gradient. The key observation is\nthat, if we predicted correctly, then \u02c6yi=yi, so the (approximate) gradient is zero and we do\nnot change the weight vector. But if xiis misclassi\ufb01ed, we update the weights as follows: If\n\u02c6yi=1butyi=\u22121, then the negative gradient is \u2212(\u02c6yi\u2212yi)xi=\u22122xi; and if\u02c6yi=\u22121but\nyi=1, then the negative gradient is \u2212(\u02c6yi\u2212yi)xi=2xi. We can absorb the factor of 2 into\nthe learning rate \u03b7and just write the update, in the case of a misclassi\ufb01cation, as\n\u03b8k=\u03b8k\u22121+\u03b7kyixi (8.92)\nSince it is only the sign of the weights that matter, not the magnitude, we will set \u03b7k=1. See\nAlgorithm 11 for the pseudocode.\nOne can show that this method, known as the perceptron algorithm (Rosenblatt 1958), will\nconverge, provided the data is linearly separable, i.e., that there exist parameters \u03b8such that\npredicting with sign(\u03b8Tx)achieves 0 error on the training set. However, if the data is not\nlinearly separable, the algorithm will not converge, and even if it does converge, it may take\na long time. There are much better ways to train logistic regression models (such as usingproper SGD, without the gradient approximation, or IRLS, discussed in Section 8.3.4). However,the perceptron algorithm is historically important: it was one of the \ufb01rst machine learningalgorithms ever derived (by Frank Rosenblatt in 1957), and was even implemented in analoghardware. In addition, the algorithm can be used to \ufb01t models where computing marginalsp(y\ni|x,\u03b8)is more expensive than computing the MAP output, argmax yp(y|x,\u03b8); this arises\nin some structured-output classi\ufb01cation problems. See Section 19.7 for details.\n8.5.5 A Bayesian view\nAnother approach to online learning is to adopt a Bayesian view. This is conceptually quitesimple: we just apply Bayes rule recursively:\np(\u03b8|D\n1:k)\u221dp(Dk|\u03b8)p(\u03b8|D1:k\u22121) (8.93)\nThis has the obvious advantage of returning a posterior instead of just a point estimate. It alsoallows for the online adaptation of hyper-parameters, which is important since cross-validationcannot be used in an online setting. Finally, it has the (less obvious) advantage that it can be", "297": "8.6. Generative vs discriminative classi\ufb01ers 267\nAlgorithm 8.4: Perceptron algorithm\n1Input: linearly separable data set xi\u2208RD,yi\u2208{ \u22121,+1}fori=1:N;\n2Initialize\u03b80;\n3k\u21900;\n4repeat\n5k\u2190k+1;\n6i\u2190kmodN;\n7if\u02c6yi/negationslash=yithen\n8 \u03b8k+1\u2190\u03b8k+yixi\n9else\n10 no-op\n11until converged ;\nquicker than SGD. To see why, note that by modeling the posterior variance of each parameter\nin addition to its mean, we effectively associate a different learning rate for each parameter(de Freitas et al. 2000), which is a simple way to model the curvature of the space. Thesevariances can then be adapted using the usual rules of probability theory. By contrast, gettingsecond-order optimization methods to work online is more tricky (see e.g., (Schraudolph et al.2007; Sunehag et al. 2009; Bordes et al. 2009, 2010)).\nAs a simple example, in Section 18.2.3 we show how to use the Kalman \ufb01lter to \ufb01t a linear\nregression model online. Unlike the LMS algorithm, this converges to the optimal (offline) answerin a single pass over the data. An extension which can learn a robust non-linear regression modelin an online fashion is described in (Ting et al. 2010). For the GLM case, we can use an assumeddensity \ufb01lter (Section 18.5.3), where we approximate the posterior by a Gaussian with a diagonalcovariance; the variance terms serve as a per-parameter step-size. See Section 18.5.3.2 for details.Another approach is to use particle \ufb01ltering (Section 23.5); this was used in (Andrieu et al. 2000)for sequentially learning a kernelized linear/logistic regression model.\n8.6 Generative vs discriminative classi\ufb01ers\nIn Section 4.2.2, we showed that the posterior over class labels induced by Gaussian discrim-inant analysis (GDA) has exactly the same form as logistic regression, namely p(y=1|x)=\nsigm(w\nTx). The decision boundary is therefore a linear function of xin both cases. Note,\nhowever, that many generative models can give rise to a logistic regression posterior, e.g., if eachclass-conditional density is Poisson, p(x|y=c)=P o i (x|\u03bb\nc). So the assumptions made by GDA\nare much stronger than the assumptions made by logistic regression.\nA further difference between these models is the way they are trained. When \ufb01tting a discrim-\ninative model, we usually maximize the conditional log likelihood/summationtextN\ni=1logp(yi|xi,\u03b8), whereas\nwhen\ufb01ttingagenerativemodel, weusuallymaximizethejointloglikelihood,/summationtextNi=1logp(yi,xi|\u03b8).\nIt is clear that these can, in general, give different results (see Exercise 4.20).\nWhen the Gaussian assumptions made by GDA are correct, the model will need less training\ndata than logistic regression to achieve a certain level of performance, but if the Gaussian", "298": "268 Chapter 8. Logistic regression\nassumptions are incorrect, logistic regression will do better (Ng and Jordan 2002). This is\nbecause discriminative models do not need to model the distribution of the features. This isillustrated in Figure 8.10. We see that the class conditional densities are rather complex; inparticular, p(x|y=1 )is a multimodal distribution, which might be hard to estimate. However,\nthe class posterior, p(y=c|x), is a simple sigmoidal function, centered on the threshold value\nof 0.55. This suggests that, in general, discriminative methods will be more accurate, since their\u201cjob\u201d is in some sense easier. However, accuracy is not the only important factor when choosinga method. Below we discuss some other advantages and disadvantages of each approach.\n8.6.1 Pros and cons of each approach\n\u2022Easy to \ufb01t? As we have seen, it is usually very easy to \ufb01t generative classi\ufb01ers. For example,\nin Sections 3.5.1.1 and 4.2.4, we show that we can \ufb01t a naive Bayes model and an LDA modelby simple counting and averaging. By contrast, logistic regression requires solving a convexoptimization problem (see Section 8.3.4 for the details), which is much slower.\n\u2022Fit classes separately? In a generative classi\ufb01er, we estimate the parameters of each class\nconditional density independently, so we do not have to retrain the model when we addmore classes. In contrast, in discriminative models, all the parameters interact, so the wholemodel must be retrained if we add a new class. (This is also the case if we train a generativemodel to maximize a discriminative objective Salojarvi et al. (2005).)\n\u2022Handle missing features easily? Sometimes some of the inputs (components of x) are not\nobserved. In a generative classi\ufb01er, there is a simple method for dealing with this, as wediscuss in Section 8.6.2. However, in a discriminative classi\ufb01er, there is no principled solutionto this problem, since the model assumes that xis always available to be conditioned on\n(although see (Marlin 2008) for some heuristic approaches).\n\u2022Can handle unlabeled training data? There is much interest in semi-supervised learning,\nwhich uses unlabeled data to help solve a supervised task. This is fairly easy to do usinggenerative models (see e.g., (Lasserre et al. 2006; Liang et al. 2007)), but is much harder to dowith discriminative models.\n\u2022Symmetric in inputs and outputs? We can run a generative model \u201cbackwards\u201d, and\ninfer probable inputs given the output by computing p(x|y). This is not possible with a\ndiscriminative model. The reason is that a generative model de\ufb01nes a joint distribution on x\nandy, and hence treats both inputs and outputs symmetrically.\n\u2022Can handle feature preprocessing? A big advantage of discriminative methods is that they\nallow us to preprocess the input in arbitrary ways, e.g., we can replace xwith\u03c6(x), which\ncould be some basis function expansion, as illustrated in Figure 8.9. It is often hard tode\ufb01ne a generative model on such pre-processed data, since the new features are correlatedin complex ways.\n\u2022Well-calibrated probabilities? Some generative models, such as naive Bayes, make strong\nindependence assumptions which are often not valid. This can result in very extreme poste-rior class probabilities (very near 0 or 1). Discriminative models, such as logistic regression,are usually better calibrated in terms of their probability estimates.\nWe see that there are arguments for and against both kinds of models. It is therefore useful\nto have both kinds in your \u201ctoolbox\u201d. See Table 8.1 for a summary of the classi\ufb01cation and", "299": "8.6. Generative vs discriminative classi\ufb01ers 269\n\u22121 \u22120.5 0 0.5 1\u22121\u22120.500.51Linear Multinomial Logistic Regression\n(a)\u22121 \u22120.5 0 0.5 1\u22121\u22120.500.51Kernel\u2212RBF Multinomial Logistic Regression\n(b)\nFigure 8.9 (a) Multinomial logistic regression for 5 classes in the original feature space. (b) After basis\nfunction expansion, using RBF kernels with a bandwidth of 1, and using all the data points as centers.\nFigure generated by logregMultinomKernelDemo .\n0 0.2 0.4 0.6 0.8 1012345\nxclass conditional densitiesp(x|y=1)p(x|y=2)\n(a)0 0.2 0.4 0.6 0.8 100.20.40.60.811.2\nxp(y=1|x) p(y=2|x)\n(b)\nFigure 8.10 The class-conditional densities p(x|y=c)(left) may be more complex than the\nclass posteriors p(y=c|x)(right). Based on Figure 1.27 of (Bishop 2006a). Figure generated by\ngenerativeVsDiscrim .\nregression techniques we cover in this book.\n8.6.2 Dealing with missing data\nSometimes some of the inputs (components of x) are not observed; this could be due to a\nsensor failure, or a failure to complete an entry in a survey, etc. This is called the missing data\nproblem (Little. and Rubin 1987). The ability to handle missing data in a principled way is one\nof the biggest advantages of generative models.\nTo formalize our assumptions, we can associate a binary response variable ri\u2208{0,1},\nthat speci\ufb01es whether each value xiis observed or not. The joint model has the form\np(xi,ri|\u03b8,\u03c6)=p(ri|xi,\u03c6)p(xi|\u03b8),w h e r e\u03c6are the parameters controlling whether the item", "300": "270 Chapter 8. Logistic regression\nModel Classif/regr Gen/Discr Param/Non Section\nDiscriminant analysis Classif Gen Param Sec. 4.2.2, 4.2.4\nNaive Bayes classi\ufb01er Classif Gen Param Sec. 3.5, 3.5.1.2\nTree-augmented Naive Bayes classi\ufb01er Classif Gen Param Sec. 10.2.1\nLinear regression Regr Discrim Param Sec. 1.4.5, 7.3, 7.6,\nLogistic regression Classif Discrim Param Sec. 1.4.6, 8.3.4, 8.4.3, 21.8.1.1\nSparse linear/ logistic regression Both Discrim Param Ch. 13\nMixture of experts Both Discrim Param Sec. 11.2.4\nMultilayer perceptron (MLP)/ Neural network Both Discrim Param Ch. 16\nConditional random \ufb01eld (CRF) Classif Discrim Param Sec. 19.6\nKnearest neighbor classi\ufb01er Classif Gen Non Sec. 1.4.2, 14.7.3\n(In\ufb01nite) Mixture Discriminant analysis Classif Gen Non Sec. 14.7.3Classi\ufb01cation and regression trees (CART) Both Discrim Non Sec. 16.2\nBoosted model Both Discrim Non Sec. 16.4\nSparse kernelized lin/logreg (SKLR) Both Discrim Non Sec. 14.3.2\nRelevance vector machine (RVM) Both Discrim Non Sec. 14.3.2\nSupport vector machine (SVM) Both Discrim Non Sec. 14.5\nGaussian processes (GP) Both Discrim Non Ch. 15\nSmoothing splines Regr Discrim Non Section 15.4.6\nTable 8.1 List of various models for classi\ufb01cation and regression which we discuss in this book. Columns\nare as follows: Model name; is the model suitable for classi\ufb01cation, regression, or both; is the model\ngenerative or discriminative; is the model parametric or non-parametric; list of sections in book whichdiscuss the model. See also http://pmtk3 .googlecode .com/svn/trunk/docs/tutorial/html/tu\ntSupervised .htmlfor the PMTK equivalents of these models. Any generative probabilistic model (e.g.,\nHMMs, Boltzmann machines, Bayesian networks, etc.) can be turned into a classi\ufb01er by using it as a classconditional density.\nis observed or not. If we assume p(ri|xi,\u03c6)=p(ri|\u03c6), we say the data is missing completely\nat random orMCAR. If we assume p(ri|xi,\u03c6)=p(ri|xo\ni,\u03c6),w h e r exoiis the observed part of\nxi, we say the data is missing at random orMAR. If neither of these assumptions hold, we say\nthe data is not missing at random orNMAR. In this case, we have to model the missing data\nmechanism, since the pattern of missingness is informative about the values of the missing data\nand the corresponding parameters. This is the case in most collaborative \ufb01ltering problems, forexample. See e.g., (Marlin 2008) for further discussion. We will henceforth assume the data isMAR.\nWhen dealing with missing data, it is helpful to distinguish the cases when there is missing-\nness only at test time (so the training data is complete data), from the harder case when there\nis missingness also at training time. We will discuss these two cases below. Note that the classlabel is always missing at test time, by de\ufb01nition; if the class label is also sometimes missing attraining time, the problem is called semi-supervised learning.", "301": "8.6. Generative vs discriminative classi\ufb01ers 271\n8.6.2.1 Missing data at test time\nIn a generative classi\ufb01er, we can handle features that are MAR by marginalizing them out. For\nexample, if we are missing the value of x1, we can compute\np(y=c|x2:D,\u03b8)\u221dp(y=c|\u03b8)p(x2:D|y=c,\u03b8) (8.94)\n=p(y=c|\u03b8)/summationdisplay\nx1p(x1,x2:D|y=c,\u03b8) (8.95)\nIf we make the naive Bayes assumption, the marginalization can be performed as follows:\n/summationdisplay\nx1p(x1,x2:D|y=c,\u03b8)=/bracketleftBigg/summationdisplay\nx1p(x1|\u03b81c)/bracketrightBiggD/productdisplay\nj=2p(xj|\u03b8jc)=D/productdisplay\nj=2p(xj|\u03b8jc) (8.96)\nwhere we exploited the fact that/summationtext\nx1p(x1|y=c,\u03b8)=1. Hence in a naive Bayes classi\ufb01er, we\ncan simply ignore missing features at test time. Similarly, in discriminant analysis, no matterwhat regularization method was used to estimate the parameters, we can always analyticallymarginalize out the missing variables (see Section 4.3):\np(x\n2:D|y=c,\u03b8)=N(x2:D|\u03bcc,2:D,\u03a3c,2:D,2:D) (8.97)\n8.6.2.2 Missing data at training time\nMissing data at training time is harder to deal with. In particular, computing the MLE or MAPestimate is no longer a simple optimization problem, for reasons discussed in Section 11.3.2.However, soon we will study are a variety of more sophisticated algorithms (such as EM algo-rithm, in Section 11.4) for \ufb01nding approximate ML or MAP estimates in such cases.\n8.6.3 Fisher\u2019s linear discriminant analysis (FLDA) *\nDiscriminant analysis is a generative approach to classi\ufb01cation, which requires \ufb01tting an MVN tothe features. As we have discussed, this can be problematic in high dimensions. An alternativeapproach is to reduce the dimensionality of the features x\u2208R\nDand then \ufb01t an MVN to the\nresulting low-dimensional features z\u2208RL. The simplest approach is to use a linear projection\nmatrix,z=Wx,w h e r e Wis aL\u00d7Dmatrix. One approach to \ufb01nding Wwould be to use\nPCA (Section 12.2); the result would be very similar to RDA (Section 4.2.6), since SVD and PCAare essentially equivalent. However, PCA is an unsupervised technique that does not take classlabels into account. Thus the resulting low dimensional features are not necessarily optimalfor classi\ufb01cation, as illustrated in Figure 8.11. An alternative approach is to \ufb01nd the matrixWsuch that the low-dimensional data can be classi\ufb01ed as well as possible using a Gaussian\nclass-conditional density model. The assumption of Gaussianity is reasonable since we arecomputing linear combinations of (potentially non-Gaussian) features. This approach is calledFisher\u2019s linear discriminant analysis,o rFLDA.\nFLDA is an interesting hybrid of discriminative and generative techniques. The drawback of\nthis technique is that it is restricted to using L\u2264C\u22121dimensions, regardless of D, for reasons\nthat we will explain below. In the two-class case, this means we are seeking a single vector w\nonto which we can project the data. Below we derive the optimal win the two-class case. We", "302": "272 Chapter 8. Logistic regression\n\u22124 \u22122 0 2 4 6 801234means\nfisher\npca\n(a)\n\u221245 \u221240 \u221235 \u221230 \u221225 \u221220 \u221215 \u221210 \u22125 005101520fisher\n(b)\n\u22128 \u22126 \u22124 \u22122 0 2 40510152025pca\n(c)\nFigure 8.11 Example of Fisher\u2019s linear discriminant. (a) Two class data in 2D. Dashed green line = \ufb01rst\nprincipal basis vector. Dotted red line = Fisher\u2019s linear discriminant vector. Solid black line joins the\nclass-conditional means. (b) Projection of points onto Fisher\u2019s vector shows good class separation. (c)Projection of points onto PCA vector shows poor class separation. Figure generated by fisherLDAdemo .", "303": "8.6. Generative vs discriminative classi\ufb01ers 273\nthen generalize to the multi-class case, and \ufb01nally we give a probabilistic interpretation of this\ntechnique.\n8.6.3.1 Derivation of the optimal 1d projection\nWe now derive this optimal direction w, for the two-class case, following the presentation of\n(Bishop 2006b, Sec 4.1.4). De\ufb01ne the class-conditional means as\n\u03bc1=1\nN1/summationdisplay\ni:yi=1xi,\u03bc2=1\nN2/summationdisplay\ni:yi=2xi (8.98)\nLetmk=wT\u03bckbe the projection of each mean onto the line w. Also, let zi=wTxibe the\nprojection of the data onto the line. The variance of the projected points is proportional to\ns2\nk=/summationdisplay\ni:yi=k(zi\u2212mk)2(8.99)\nThe goal is to \ufb01nd wsuch that we maximize the distance between the means, m2\u2212m1, while\nalso ensuring the projected clusters are \u201ctight\u201d:\nJ(w)=(m2\u2212m1)2\ns2\n1+s22(8.100)\nWe can rewrite the right hand side of the above in terms of was follows\nJ(w)=wTSBw\nwTSWw(8.101)\nwhereSBis the between-class scatter matrix given by\nSB=(\u03bc2\u2212\u03bc1)(\u03bc2\u2212\u03bc1)T(8.102)\nandSWis the within-class scatter matrix, given by\nSW=/summationdisplay\ni:yi=1(xi\u2212\u03bc1)(xi\u2212\u03bc1)T+/summationdisplay\ni:yi=2(xi\u2212\u03bc2)(xi\u2212\u03bc2)T(8.103)\nTo see this, note that\nwTSBw=wT(\u03bc2\u2212\u03bc1)(\u03bc2\u2212\u03bc1)Tw=(m2\u2212m1)(m2\u2212m1) (8.104)\nand\nwTSWw=/summationdisplay\ni:yi=1wT(xi\u2212\u03bc1)(xi\u2212\u03bc1)Tw+/summationdisplay\ni:yi=2wT(xi\u2212\u03bc2)(xi\u2212\u03bc2)Tw(8.105)\n=/summationdisplay\ni:yi=1(zi\u2212m1)2+/summationdisplay\ni:yi=2(zi\u2212m2)2(8.106)\nEquation 8.101 is a ratio of two scalars; we can take its derivative with respect to wand equate\nto zero. One can show (Exercise 12.6) that that J(w)is maximized when\nSBw=\u03bbSWw (8.107)", "304": "274 Chapter 8. Logistic regression\n\u22124 \u22123 \u22122 \u22121 0 1 2 3 4\u22123\u22122\u221210123\n(a)\u22124. 5 \u22124 \u22123.5 \u22123 \u22122. 5 \u22122 \u22121. 5 \u22121 \u22120.5 0\u22123.5\u22123\u22122.5\u22122\u22121.5\u22121\u22120.500.5\n(b)\nFigure 8.12 (a) PCA projection of vowel data to 2d. (b) FLDA projection of vowel data to 2d. We see there\nis better class separation in the FLDA case. Based on Figure 4.11 of (Hastie et al. 2009). Figure generated by\nfisherDiscrimVowelDemo , by Hannes Bretschneider.\nwhere\n\u03bb=wTSBw\nwTSWw(8.108)\nEquation 8.107 is called a generalized eigenvalue problem. If SWis invertible, we can convert\nit to a regular eigenvalue problem:\nS\u22121\nWSBw=\u03bbw (8.109)\nHowever, in the two class case, there is a simpler solution. In particular, since\nSBw=(\u03bc2\u2212\u03bc1)(\u03bc2\u2212\u03bc1)Tw=(\u03bc2\u2212\u03bc1)(m2\u2212m1) (8.110)\nthen, from Equation 8.109 we have\n\u03bbw=S\u22121\nW(\u03bc2\u2212\u03bc1)(m2\u2212m1) (8.111)\nw\u221dS\u22121\nW(\u03bc2\u2212\u03bc1) (8.112)\nSince we only care about the directionality, and not the scale factor, we can just set\nw=S\u22121\nW(\u03bc2\u2212\u03bc1) (8.113)\nThis is the optimal solution in the two-class case. If SW\u221dI, meaning the pooled covariance\nmatrix is isotropic, then wis proportional to the vector that joins the class means. This is an\nintuitively reasonable direction to project onto, as shown in Figure 8.11.\n8.6.3.2 Extension to higher dimensions and multiple classes\nWe can extend the above idea to multiple classes, and to higher dimensional subspaces, by\n\ufb01nding a projection matrixWwhich maps from DtoLso as to maximize\nJ(W)=|W\u03a3BWT|\n|W\u03a3WWT|(8.114)", "305": "8.6. Generative vs discriminative classi\ufb01ers 275\nwhere\n\u03a3B/defines/summationdisplay\ncNc\nN(\u03bcc\u2212\u03bc)(\u03bcc\u2212\u03bc)T(8.115)\n\u03a3W/defines/summationdisplay\ncNc\nN\u03a3c (8.116)\n\u03a3c/defines1\nNc/summationdisplay\ni:yi=c(xi\u2212\u03bcc)(xi\u2212\u03bcc)T(8.117)\nThe solution can be shown to be\nW=\u03a3\u22121\n2\nWU (8.118)\nwhereUare theLleading eigenvectors of \u03a3\u22121\n2\nW\u03a3B\u03a3\u22121\n2\nW, assuming \u03a3Wis non-singular. (If it\nis singular, we can \ufb01rst perform PCA on all the data.)\nFigure 8.12 gives an example of this method applied to some D=1 0dimensional speech\ndata, representing C=1 1different vowel sounds. We see that FLDA gives better class separation\nthan PCA.\nNote that FLDA is restricted to \ufb01nding at most a L\u2264C\u22121dimensional linear subspace,\nno matter how large D, because the rank of the between class covariance matrix \u03a3BisC\u22121.\n(The -1 term arises because of the \u03bcterm, which is a linear function of the \u03bcc.) This is a rather\nsevere restriction which limits the usefulness of FLDA.\n8.6.3.3 Probabilistic interpretation of FLDA *\nTo \ufb01nd a valid probabilistic interpretation of FLDA, we follow the approach of (Kumar and Andreo\n1998; Zhou et al. 2009). They proposed a model known as heteroscedastic LDA (HLDA), which\nworks as follows. Let Wbe aD\u00d7Dinvertible matrix, and let zi=Wxibe a transformed\nversion of the data. We now \ufb01t full covariance Gaussians to the transformed data, one per class,but with the constraint that only the \ufb01rst Lcomponents will be class-speci\ufb01c; the remaining\nH=D\u2212Lcomponents will be shared across classes, and will thus not be discriminative. That\nis, we use\np(z\ni|\u03b8,yi=c)=N (zi|\u03bcc,\u03a3c) (8.119)\n\u03bcc/defines(mc;m0) (8.120)\n\u03a3c/defines/parenleftbiggSc0\n0S0/parenrightbigg\n(8.121)\nwherem0is the shared Hdimensional mean and S0is the shared H\u00d7Hcovariace. The pdf\nof the original (untransformed) data is given by\np(xi|yi=c,W,\u03b8)=|W |N(Wxi|\u03bcc,\u03a3c) (8.122)\n=|W|N(WLxi|mc,Sc)N(WHxi|m0,S0) (8.123)\nwhereW=/parenleftbiggWL\nWH/parenrightbigg\n. For \ufb01xed W, it is easy to derive the MLE for \u03b8. One can then optimize\nWusing gradient methods.", "306": "276 Chapter 8. Logistic regression\nIn the special case that the \u03a3care diagonal, there is a closed-form solution for W(Gales\n1999). And in the special case the \u03a3care all equal, we recover classical LDA (Zhou et al. 2009).\nIn view of this this result, it should be clear that HLDA will outperform LDA if the class\ncovariances are not equal within the discriminative subspace (i.e., if the assumption that \u03a3cis\nindependent of cis a poor assumption). This is easy to demonstrate on synthetic data, and is\nalso the case on more challenging tasks such as speech recognition (Kumar and Andreo 1998).\nFurthermore, we can extend the model by allowing each class to use its own projection matrix;this is known as multiple LDA (Gales 2002).\nExercises\nExercise 8.1 Spam classi\ufb01cation using logistic regression\nConsider the email spam data set discussed on p300 of (Hastie et al. 2009). This consists of 4601 email\nmessages, from which 57 features have been extracted. These are as follows:\n\u2022 48 features, in [0,100], giving the percentage of words in a given message which match a given word\non the list. The list contains words such as \u201cbusiness\u201d, \u201cfree\u201d, \u201cgeorge\u201d, etc. (The data was collected by\nGeorge Forman, so his name occurs quite a lot.)\n\u2022 6 features, in [0,100], giving the percentage of characters in the email that match a given character on\nthe list. The characters are ;([!$#\n\u2022 Feature 55: The average length of an uninterrupted sequence of capital letters (max is 40.3, mean is 4.9)\n\u2022 Feature 56: The length of the longest uninterrupted sequence of capital letters (max is 45.0, mean is\n52.6)\n\u2022 Feature 57: The sum of the lengts of uninterrupted sequence of capital letters (max is 25.6, mean is\n282.2)\nLoad the data from spamData.mat , which contains a training set (of size 3065) and a test set (of size\n1536).One can imagine performing several kinds of preprocessing to this data. Try each of the following\nseparately:\na. Standardize the columns so they all have mean 0 and unit variance.\nb. Transform the features using log(x\nij+0.1).\nc. Binarize the features using I(xij>0).\nFor each version of the data, \ufb01t a logistic regression model. Use cross validation to choose the strengthof the/lscript\n2regularizer. Report the mean error rate on the training and test sets. You should get numbers\nsimilar to this:\nmethod train test\nstnd 0.082 0.079\nlog 0.052 0.059\nbinary 0.065 0.072\n(The precise values will depend on what regularization value you choose.) Turn in your code and numericalresults.\n(See also Exercise 8.2.\nExercise 8.2 Spam classi\ufb01cation using naive Bayes\nWe will re-examine the dataset from Exercise 8.1.", "307": "8.6. Generative vs discriminative classi\ufb01ers 277\na. Use naiveBayesFit andnaiveBayesPredict on the binarized spam data. What is the training and\ntest error? (You can try different settings of the pseudocount \u03b1if you like (this corresponds to the\nBeta(\u03b1,\u03b1) prior each \u03b8jc), although the default of \u03b1=1is probably \ufb01ne.) Turn in your error rates.\nb. Modify the code so it can handle real-valued features. Use a Gaussian density for each feature; \ufb01t it\nwith maximum likelihood. What are the training and test error rates on the standardized data and the\nlog transformed data? Turn in your 4 error rates and code.\nExercise 8.3 Gradient and Hessian of log-likelihood for logistic regression\na. Let\u03c3(a)=1\n1+e\u2212abe the sigmoid function. Show that\nd\u03c3(a)\nda=\u03c3(a)(1\u2212\u03c3(a)) (8.124)\nb. Using the previous result and the chain rule of calculus, derive an expression for the gradient of the\nlog likelihood (Equation 8.5).\nc. The Hessian can be written as H=XTSX,w h e r e S/definesdiag(\u03bc 1(1\u2212\u03bc1),...,\u03bc n(1\u2212\u03bcn)). Show\nthatHis positive de\ufb01nite. (You may assume that 0<\u03bci<1, so the elements of Swill be strictly\npositive, and that Xis full rank.)\nExercise 8.4 Gradient and Hessian of log-likelihood for multinomial logistic regression\na. Let\u03bcik=S(\u03b7i)k. Prove that the Jacobian of the softmax is\n\u2202\u03bcik\n\u2202\u03b7ij=\u03bcik(\u03b4kj\u2212\u03bcij) (8.125)\nwhere\u03b4kj=I(k=j).\nb. Hence show that\n\u2207wc/lscript=/summationdisplay\ni(yic\u2212\u03bcic)xi (8.126)\nHint: use the chain rule and the fact that/summationtext\ncyic=1.\nc. Show that the block submatrix of the Hessian for classes candc/primeis given by\nHc,c/prime=\u2212/summationdisplay\ni\u03bcic(\u03b4c,c/prime\u2212\u03bci,c/prime)xixT\ni (8.127)\nExercise 8.5 Symmetric version of /lscript2regularized multinomial logistic regression\n(Source: Ex 18.3 of (Hastie et al. 2009).)\nMulticlass logistic regression has the form\np(y=c|x,W)=exp(wc0+wT\ncx)/summationtextC\nk=1exp(w k0+wT\nkx)(8.128)\nwhereWis a(D+1)\u00d7Cweight matrix. We can arbitrarily de\ufb01ne wc=0for one of the classes, say\nc=C, sincep(y=C|x,W)=1\u2212/summationtextC\u22121\nc=1p(y=c|x,w). In this case, the model has the form\np(y=c|x,W)=exp(wc0+wT\ncx)\n1+/summationtextC\u22121\nk=1exp(wk0+wT\nkx)(8.129)", "308": "278 Chapter 8. Logistic regression\nIf we don\u2019t \u201cclamp\u201d one of the vectors to some constant value, the parameters will be unidenti\ufb01able.\nHowever, suppose we don\u2019t clamp wc=0, so we are using Equation 8.128, but we add /lscript2regularization\nby optimizing\nN/summationdisplay\ni=1logp(yi|xi,W)\u2212\u03bbC/summationdisplay\nc=1||wc||2\n2 (8.130)\nShow that at the optimum we have/summationtextC\nc=1\u02c6wcj=0forj=1:D. (For the unregularized \u02c6wc0terms, we\nstill need to enforce that w0C=0to ensure identi\ufb01ability of the offset.)\nExercise 8.6 Elementary properties of /lscript2regularized logistic regression\n(Source: Jaaakkola.). Consider minimizing\nJ(w)=\u2212/lscript(w,Dtrain)+\u03bb||w||2\n2 (8.131)\nwhere\n/lscript(w,D)=1\n|D|/summationdisplay\ni\u2208Dlog\u03c3(yixTiw) (8.132)\nis the average log-likelihood on data set D,f o ryi\u2208{ \u22121,+1}. Answer the following true/ false questions.\na.J(w)has multiple locally optimal solutions: T/F?\nb. Let\u02c6w= argmin wJ(w)be a global optimum. \u02c6wis sparse (has many zero entries): T/F?\nc. If the training data is linearly separable, then some weights wjmight become in\ufb01nite if \u03bb=0: T/F?\nd./lscript(\u02c6w,Dtrain)always increases as we increase \u03bb: T/F?\ne./lscript(\u02c6w,Dtest)always increases as we increase \u03bb: T/F?\nExercise 8.7 Regularizing separate terms in 2d logistic regression\n(Source: Jaaakkola.)\na. Consider the data in Figure 8.13, where we \ufb01t the model p(y=1|x,w)=\u03c3(w0+w1x1+w2x2).\nSuppose we \ufb01t the model by maximum likelihood, i.e., we minimize\nJ(w)=\u2212/lscript(w,Dtrain) (8.133)\nwhere/lscript(w,Dtrain)is the log likelihood on the training set. Sketch a possible decision boundary\ncorresponding to \u02c6w. (Copy the \ufb01gure \ufb01rst (a rough sketch is enough), and then superimpose your\nanswer on your copy, since you will need multiple versions of this \ufb01gure). Is your answer (decision\nboundary) unique? How many classi\ufb01cation errors does your method make on the training set?\nb. Now suppose we regularize only the w0parameter, i.e., we minimize\nJ0(w)=\u2212/lscript(w,Dtrain)+\u03bbw2\n0 (8.134)\nSuppose\u03bbis a very large number, so we regularize w0all the way to 0, but all other parameters are\nunregularized. Sketch a possible decision boundary. How many classi\ufb01cation errors does your methodmake on the training set? Hint: consider the behavior of simple linear regression, w\n0+w1x1+w2x2\nwhenx1=x2=0.\nc. Now suppose we heavily regularize only the w1parameter, i.e., we minimize\nJ1(w)=\u2212/lscript(w,Dtrain)+\u03bbw2\n1 (8.135)\nSketch a possible decision boundary. How many classi\ufb01cation errors does your method make on thetraining set?", "309": "8.6. Generative vs discriminative classi\ufb01ers 279\nFigure 8.13 Data for logistic regression question.\nd. Now suppose we heavily regularize only the w2parameter. Sketch a possible decision boundary. How\nmany classi\ufb01cation errors does your method make on the training set?", "310": "", "311": "9Generalized linear models and the\nexponential family\n9.1 Introduction\nWe have now encountered a wide variety of probability distributions: the Gaussian, the Bernoulli,\nthe Student t, the uniform, the gamma, etc. It turns out that most of these are members of a\nbroader class of distributions known as the exponential family.1In this chapter, we discuss\nvarious properties of this family. This allows us to derive theorems and algorithms with verybroad applicability.\nWe will see how we can easily use any member of the exponential family as a class-conditional\ndensity in order to make a generative classi\ufb01er. In addition, we will discuss how to builddiscriminative models, where the response variable has an exponential family distribution, whosemean is a linear function of the inputs; this is known as a generalized linear model, andgeneralizes the idea of logistic regression to other kinds of response variables.\n9.2 The exponential family\nBefore de\ufb01ning the exponential family, we mention several reasons why it is important:\n\u2022 It can be shown that, under certain regularity conditions, the exponential family is the only\nfamily of distributions with \ufb01nite-sized sufficient statistics, meaning that we can compress\nthe data into a \ufb01xed-sized summary without loss of information. This is particularly usefulfor online learning, as we will see later.\n\u2022 The exponential family is the only family of distributions for which conjugate priors exist,\nwhich simpli\ufb01es the computation of the posterior (see Section 9.2.5).\n\u2022 The exponential family can be shown to be the family of distributions that makes the least\nset of assumptions subject to some user-chosen constraints (see Section 9.2.6).\n\u2022 The exponential family is at the core of generalized linear models, as discussed in Section 9.3.\n\u2022 The exponential family is at the core of variational inference, as discussed in Section 21.2.\n1. The exceptions are the Student t, which does not have the right form, and the uniform distribution, which does not\nhave \ufb01xed support independent of the parameter values.", "312": "282 Chapter 9. Generalized linear models and the exponential family\n9.2.1 De\ufb01nition\nA pdf or pmf p(x|\u03b8),f o rx=(x1,...,x m)\u2208Xmand\u03b8\u2208\u0398\u2286Rd, is said to be in the\nexponential family if it is of the form\np(x|\u03b8)=1\nZ(\u03b8)h(x)exp[\u03b8T\u03c6(x)] (9.1)\n=h(x)exp[\u03b8T\u03c6(x)\u2212A(\u03b8)] (9.2)\nwhere\nZ(\u03b8)=/integraldisplay\nXmh(x)exp[\u03b8T\u03c6(x)]dx (9.3)\nA(\u03b8)=l o g Z(\u03b8) (9.4)\nHere\u03b8are called the natural parameters orcanonical parameters, \u03c6(x)\u2208Rdis called a\nvector of sufficient statistics, Z(\u03b8)is called the partition function, A(\u03b8)is called the log\npartition function orcumulant function, and h(x)is the a scaling constant, often 1. If\n\u03c6(x)=x, we say it is a natural exponential family.\nEquation 9.2 can be generalized by writing\np(x|\u03b8)=h(x)exp[\u03b7(\u03b8)T\u03c6(x)\u2212A(\u03b7(\u03b8))] (9.5)\nwhere\u03b7is a function that maps the parameters \u03b8to the canonical parameters \u03b7=\u03b7(\u03b8).I f\ndim(\u03b8)<dim(\u03b7(\u03b8)), it is called a curved exponential family , which means we have more\nsufficient statistics than parameters. If \u03b7(\u03b8)=\u03b8, the model is said to be in canonical form.\nWe will assume models are in canonical form unless we state otherwise.\n9.2.2 Examples\nLet us consider some examples to make things clearer.\n9.2.2.1 Bernoulli\nThe Bernoulli for x\u2208{0,1}can be written in exponential family form as follows:\nBer(x|\u03bc)=\u03bcx(1\u2212\u03bc)1\u2212x=e x p [xlog(\u03bc)+(1\u2212 x)log(1\u2212\u03bc) ]=e x p [ \u03c6(x)T\u03b8](9.6)\nwhere\u03c6(x)=[ I(x=0 ),I(x=1 ) ]and\u03b8= [log(\u03bc),log(1\u2212\u03bc)]. However, this representation\nisover-complete since there is a linear dependendence between the features:\n1T\u03c6(x)=I(x=0 )+ I(x=1 )=1 (9.7)\nConsequently \u03b8is not uniquely identi\ufb01able. It is common to require that the representation be\nminimal, which means there is a unique \u03b8associated with the distribution. In this case, we\ncan just de\ufb01ne\nBer(x|\u03bc)=( 1\u2212\u03bc)exp/bracketleftbigg\nxlog/parenleftbigg\u03bc\n1\u2212\u03bc/parenrightbigg/bracketrightbigg\n(9.8)", "313": "9.2. The exponential family 283\nNow we have \u03c6(x)=x,\u03b8=l o g/parenleftBig\n\u03bc\n1\u2212\u03bc/parenrightBig\n, which is the log-odds ratio, and Z=1/(1\u2212\u03bc).W e\ncan recover the mean parameter \u03bcfrom the canonical parameter using\n\u03bc=s i g m (\u03b8)=1\n1+e\u2212\u03b8(9.9)\n9.2.2.2 Multinoulli\nWe can represent the multinoulli as a minimal exponential family as follows (where xk=I(x=\nk)):\nCat(x|\u03bc)=K/productdisplay\nk=1\u03bcxk\nk=e x p/bracketleftBiggK/summationdisplay\nk=1xklog\u03bck/bracketrightBigg\n(9.10)\n=e x p/bracketleftBiggK\u22121/summationdisplay\nk=1xklog\u03bck+/parenleftBigg\n1\u2212K\u22121/summationdisplay\nk=1xk/parenrightBigg\nlog(1\u2212K\u22121/summationdisplay\nk=1\u03bck)/bracketrightBigg\n(9.11)\n=e x p/bracketleftBiggK\u22121/summationdisplay\nk=1xklog/parenleftBigg\n\u03bck\n1\u2212/summationtextK\u22121\nj=1\u03bcj/parenrightBigg\n+log(1\u2212K\u22121/summationdisplay\nk=1\u03bck)/bracketrightBigg\n(9.12)\n=e x p/bracketleftBiggK\u22121/summationdisplay\nk=1xklog/parenleftbigg\u03bck\n\u03bcK/parenrightbigg\n+log\u03bcK/bracketrightBigg\n(9.13)\nwhere\u03bcK=1\u2212/summationtextK\u22121\nk=1\u03bck. We can write this in exponential family form as follows:\nCat(x|\u03b8)=e x p ( \u03b8T\u03c6(x)\u2212A(\u03b8)) (9.14)\n\u03b8= [log\u03bc1\n\u03bcK,...,log\u03bcK\u22121\n\u03bcK] (9.15)\n\u03c6(x)=[ I(x=1 ),...,I(x=K\u22121)] (9.16)\nWe can recover the mean parameters from the canonical parameters using\n\u03bck=e\u03b8k\n1+/summationtextK\u22121\nj=1e\u03b8j(9.17)\nFrom this, we \ufb01nd\n\u03bcK=1\u2212/summationtextK\u22121\nj=1e\u03b8j\n1+/summationtextK\u22121\nj=1e\u03b8j=1/summationtextK\u22121\nj=1e\u03b8j(9.18)\nand hence\nA(\u03b8)=l o g/parenleftBigg\n1+K\u22121/summationdisplay\nk=1e\u03b8k/parenrightBigg\n(9.19)\nIf we de\ufb01ne \u03b8K=0, we can write \u03bc=S(\u03b8)andA(\u03b8)=l o g/summationtextK\nk=1e\u03b8k,w h e r eSis the\nsoftmax function in Equation 4.39.", "314": "284 Chapter 9. Generalized linear models and the exponential family\n9.2.2.3 Univariate Gaussian\nThe univariate Gaussian can be written in exponential family form as follows:\nN(x|\u03bc,\u03c32)=1\n(2\u03c0\u03c32)1\n2exp[\u22121\n2\u03c32(x\u2212\u03bc)2] (9.20)\n=1\n(2\u03c0\u03c32)1\n2exp[\u22121\n2\u03c32x2+\u03bc\n\u03c32x\u22121\n2\u03c32\u03bc2] (9.21)\n=1\nZ(\u03b8)exp(\u03b8T\u03c6(x)) (9.22)\nwhere\n\u03b8=/parenleftbigg\u03bc/\u03c32\n\u22121\n2\u03c32/parenrightbigg\n(9.23)\n\u03c6(x)=/parenleftbiggx\nx2/parenrightbigg\n(9.24)\nZ(\u03bc,\u03c32)=\u221a\n2\u03c0\u03c3exp[\u03bc2\n2\u03c32] (9.25)\nA(\u03b8)=\u2212\u03b82\n1\n4\u03b82\u22121\n2log(\u22122\u03b8 2)\u22121\n2log(2\u03c0) (9.26)\n9.2.2.4 Non-examples\nNot all distributions of interest belong to the exponential family. For example, the uniform\ndistribution, X\u223cUnif(a,b ), does not, since the support of the distribution depends on the\nparameters. Also, the Student T distribution (Section 11.4.5) does not belong, since it does nothave the required form.\n9.2.3 Log partition function\nAn important property of the exponential family is that derivatives of the log partition functioncan be used to generate cumulants of the sufficient statistics.\n2For this reason, A(\u03b8)is\nsometimes called a cumulant function. We will prove this for a 1-parameter distribution;\nthis can be generalized to a K-parameter distribution in a straightforward way. For the \ufb01rst\n2. The \ufb01rst and second cumulants of a distribution are its mean E[X]and variance var[X], whereas the \ufb01rst and\nsecond moments are its mean E[X]andE/bracketleftbig\nX2/bracketrightbig\n.", "315": "9.2. The exponential family 285\nderivative we have\ndA\nd\u03b8=d\nd\u03b8/parenleftbigg\nlog/integraldisplay\nexp(\u03b8\u03c6(x))h(x)dx/parenrightbigg\n(9.27)\n=d\nd\u03b8/integraltext\nexp(\u03b8\u03c6(x))h(x)dx/integraltext\nexp(\u03b8\u03c6(x))h(x)dx(9.28)\n=/integraltext\n\u03c6(x)exp(\u03b8\u03c6(x))h(x)dx\nexp(A(\u03b8))(9.29)\n=/integraldisplay\n\u03c6(x)exp(\u03b8\u03c6(x)\u2212A(\u03b8))h(x)dx (9.30)\n=/integraldisplay\n\u03c6(x)p(x)dx=E[\u03c6(x)] (9.31)\nFor the second derivative we have\nd2A\nd\u03b82=/integraldisplay\n\u03c6(x)exp(\u03b8\u03c6(x)\u2212A(\u03b8))h(x)(\u03c6(x)\u2212A/prime(\u03b8))dx (9.32)\n=/integraldisplay\n\u03c6(x)p(x)(\u03c6(x)\u2212A/prime(\u03b8))dx (9.33)\n=/integraldisplay\n\u03c62(x)p(x)dx\u2212A/prime(\u03b8)/integraldisplay\n\u03c6(x)p(x)dx (9.34)\n=E/bracketleftbig\n\u03c62(X)/bracketrightbig\n\u2212E[\u03c6(x)]2= var[\u03c6(x)] (9.35)\nwhere we used the fact that A/prime(\u03b8)=dA\nd\u03b8=E[\u03c6(x)].\nIn the multivariate case, we have that\n\u22022A\n\u2202\u03b8i\u2202\u03b8j=E[\u03c6i(x)\u03c6j(x)]\u2212E[\u03c6i(x)]E[\u03c6j(x)] (9.36)\nand hence\n\u22072A(\u03b8)=c o v[\u03c6(x)] (9.37)\nSince the covariance is positive de\ufb01nite, we see that A(\u03b8)is a convex function (see Section 7.3.3).\n9.2.3.1 Example: the Bernoulli distribution\nFor example, consider the Bernoulli distribution. We have A(\u03b8)=l o g ( 1+ e\u03b8), so the mean is\ngiven by\ndA\nd\u03b8=e\u03b8\n1+e\u03b8=1\n1+e\u2212\u03b8=s i g m (\u03b8)=\u03bc (9.38)\nThe variance is given by\nd2A\nd\u03b82=d\nd\u03b8(1+e\u2212\u03b8)\u22121=( 1+e\u2212\u03b8)\u22122.e\u2212\u03b8(9.39)\n=e\u2212\u03b8\n1+e\u2212\u03b81\n1+e\u2212\u03b8=1\ne\u03b8+11\n1+e\u2212\u03b8=( 1\u2212\u03bc)\u03bc (9.40)", "316": "286 Chapter 9. Generalized linear models and the exponential family\n9.2.4 MLE for the exponential family\nThe likelihood of an exponential family model has the form\np(D|\u03b8)=/bracketleftBiggN/productdisplay\ni=1h(xi)/bracketrightBigg\ng(\u03b8)Nexp/parenleftBigg\n\u03b7(\u03b8)T[N/summationdisplay\ni=1\u03c6(xi)]/parenrightBigg\n(9.41)\nWe see that the sufficient statistics are Nand\n\u03c6(D)=[N/summationdisplay\ni=1\u03c61(xi),...,N/summationdisplay\ni=1\u03c6K(xi)] (9.42)\nFor example, for the Bernoulli model we have \u03c6=[/summationtext\niI(xi=1 ) ], and for the univariate\nGaussian, we have \u03c6=[/summationtext\nixi,/summationtext\nix2\ni]. (We also need to know the sample size, N.)\nThePitman-Koopman-Darmois theorem states that, under certain regularity conditions, the\nexponential family is the only family of distributions with \ufb01nite sufficient statistics. (Here, \ufb01nite\nmeans of a size independent of the size of the data set.)\nOne of the conditions required in this theorem is that the support of the distribution not be\ndependent on the parameter. For a simple example of such a distribution, consider the uniformdistribution\np(x|\u03b8)=U(x|\u03b8)=1\n\u03b8I(0\u2264x\u2264\u03b8) (9.43)\nThe likelihood is given by\np(D|\u03b8)=\u03b8\u2212NI(0\u2264max{x i}\u2264\u03b8) (9.44)\nSo the sufficient statistics are Nands(D)=m a x ixi. This is \ufb01nite in size, but the uni-\nform distribution is not in the exponential family because its support set, X, depends on the\nparameters.\nWe now descibe how to compute the MLE for a canonical exponential family model. Given\nNiid data points D=(x1,...,x N), the log-likelihood is\nlogp(D|\u03b8)=\u03b8T\u03c6(D)\u2212NA(\u03b8) (9.45)\nSince\u2212A(\u03b8)is concave in \u03b8, and\u03b8T\u03c6(D)is linear in \u03b8, we see that the log likelihood is\nconcave, and hence has a unique global maximum. To derive this maximum, we use the factthat the derivative of the log partition function yields the expected value of the sufficient statisticvector (Section 9.2.3):\n\u2207\n\u03b8logp(D|\u03b8)=\u03c6(D)\u2212NE[\u03c6(X)] (9.46)\nSetting this gradient to zero, we see that at the MLE, the empirical average of the sufficient\nstatistics must equal the model\u2019s theoretical expected sufficient statistics, i.e., \u02c6\u03b8must satisfy\nE[\u03c6(X)] =1\nNN/summationdisplay\ni=1\u03c6(xi) (9.47)", "317": "9.2. The exponential family 287\nThis is called moment matching. For example, in the Bernoulli distribution, we have \u03c6(X)=\nI(X=1 ), so the MLE satis\ufb01es\nE[\u03c6(X)] =p(X=1 )=\u02c6\u03bc=1\nNN/summationdisplay\ni=1I(xi=1 ) (9.48)\n9.2.5 Bayes for the exponential family *\nWe have seen that exact Bayesian analysis is considerably simpli\ufb01ed if the prior is conjugate to\nthe likelihood. Informally this means that the prior p(\u03b8|\u03c4)has the same form as the likelihood\np(D|\u03b8). For this to make sense, we require that the likelihood have \ufb01nite sufficient statistics, so\nthat we can write p(D|\u03b8)=p(s(D)|\u03b8). This suggests that the only family of distributions for\nwhich conjugate priors exist is the exponential family. We will derive the form of the prior andposterior below.\n9.2.5.1 Likelihood\nThe likelihood of the exponential family is given by\np(D|\u03b8)\u221dg(\u03b8)\nNexp/parenleftbig\n\u03b7(\u03b8)TsN/parenrightbig\n(9.49)\nwheresN=/summationtextN\ni=1s(xi). In terms of the canonical parameters this becomes\np(D|\u03b7)\u221dexp(N\u03b7Ts\u2212NA(\u03b7)) (9.50)\nwheres=1\nNsN.\n9.2.5.2 Prior\nThe natural conjugate prior has the form\np(\u03b8|\u03bd0,\u03c40)\u221dg(\u03b8)\u03bd0exp/parenleftbig\n\u03b7(\u03b8)T\u03c40/parenrightbig\n(9.51)\nLet us write \u03c40=\u03bd0\u03c40, to separate out the size of the prior pseudo-data, \u03bd0, from the mean of\nthe sufficient statistics on this pseudo-data, \u03c40. In canonical form, the prior becomes\np(\u03b7|\u03bd0,\u03c40)\u221dexp(\u03bd0\u03b7T\u03c40\u2212\u03bd0A(\u03b7)) (9.52)\n9.2.5.3 Posterior\nThe posterior is given by\np(\u03b8|D)=p(\u03b8|\u03bdN,\u03c4N)=p(\u03b8|\u03bd0+N,\u03c40+sN) (9.53)\nSo we see that we just update the hyper-parameters by adding. In canonical form, this becomes\np(\u03b7|D)\u221dexp/parenleftbig\n\u03b7T(\u03bd0\u03c40+Ns)\u2212(\u03bd0+N)A(\u03b7))/parenrightbig\n(9.54)\n=p(\u03b7|\u03bd0+N,\u03bd0\u03c40+Ns\n\u03bd0+N) (9.55)\nSo we see that the posterior hyper-parameters are a convex combination of the prior mean\nhyper-parameters and the average of the sufficient statistics.", "318": "288 Chapter 9. Generalized linear models and the exponential family\n9.2.5.4 Posterior predictive density\nLet us derive a generic expression for the predictive density for future observables D/prime=\n(\u02dcx1,...,\u02dcxN/prime)given past data D=(x1,...,xN)as follows. For notational brevity, we\nwill combine the sufficient statistics with the size of the data, as follows: \u02dc\u03c40=(\u03bd0,\u03c40),\n\u02dcs(D)=(N,s(D)), and\u02dcs(D/prime)=(N/prime,s(D/prime)). So the prior becomes\np(\u03b8|\u02dc\u03c40)=1\nZ(\u02dc\u03c40)g(\u03b8)\u03bd0exp(\u03b7(\u03b8)T\u03c40) (9.56)\nThe likelihood and posterior have a similar form. Hence\np(D/prime|D)=/integraldisplay\np(D/prime|\u03b8)p(\u03b8|D)d\u03b8 (9.57)\n=\u23a1\n\u23a3N/prime/productdisplay\ni=1h(\u02dcxi)\u23a4\u23a6Z(\u02dc\u03c4\n0+\u02dcs(D))\u22121/integraldisplay\ng(\u03b8)\u03bd0+N+N/primed\u03b8 (9.58)\n\u00d7exp\u239b\u239d/summationdisplay\nk\u03b7k(\u03b8)(\u03c4k+N/summationdisplay\ni=1sk(xi)+N/prime/summationdisplay\ni=1sk(\u02dcxi)\u239e\u23a0d\u03b8 (9.59)\n=\u23a1\u23a3\nN/prime/productdisplay\ni=1h(\u02dcxi)\u23a4\u23a6Z(\u02dc\u03c4\n0+\u02dcs(D)+\u02dcs(D/prime))\nZ(\u02dc\u03c40+\u02dcs(D))(9.60)\nIfN=0, this becomes the marginal likelihood of D/prime, which reduces to the familiar form of\nnormalizer of the posterior divided by the normalizer of the prior, multiplied by a constant.\n9.2.5.5 Example: Bernoulli distribution\nAs a simple example, let us revisit the Beta-Bernoulli model in our new notation.\nThe likelihood is given by\np(D|\u03b8)=( 1\u2212\u03b8)Nexp/parenleftBigg\nlog(\u03b8\n1\u2212\u03b8)/summationdisplay\nixi/parenrightBigg\n(9.61)\nHence the conjugate prior is given by\np(\u03b8|\u03bd0,\u03c40)\u221d(1\u2212\u03b8)\u03bd0exp/parenleftbigg\nlog(\u03b8\n1\u2212\u03b8)\u03c40/parenrightbigg\n(9.62)\n=\u03b8\u03c40(1\u2212\u03b8)\u03bd0\u2212\u03c40(9.63)\nIf we de\ufb01ne \u03b1=\u03c40+1and\u03b2=\u03bd0\u2212\u03c40+1, we see that this is a beta distribution.\nWe can derive the posterior as follows, where s=/summationtext\niI(xi=1 )is the sufficient statistic:\np(\u03b8|D)\u221d\u03b8\u03c40+s(1\u2212\u03b8)\u03bd0\u2212\u03c40+n\u2212s(9.64)\n=\u03b8\u03c4n(1\u2212\u03b8)\u03bdn\u2212\u03c4n(9.65)\nWe can derive the posterior predictive distribution as follows. Assume p(\u03b8)=B e t a ( \u03b8|\u03b1,\u03b2),\nand lets=s(D)be the number of heads in the past data. We can predict the probability of a", "319": "9.2. The exponential family 289\ngiven sequence of future heads, D/prime=(\u02dcx1,...,\u02dcxm), with sufficient statistic s/prime=/summationtextm\ni=1I(\u02dcxi=\n1), as follows:\np(D/prime|D)=/integraldisplay1\n0p(D/prime|\u03b8|Beta(\u03b8|\u03b1n,\u03b2n)d\u03b8 (9.66)\n=\u0393(\u03b1n+\u03b2n)\n\u0393(\u03b1n)\u0393(\u03b2n)/integraldisplay1\n0\u03b8\u03b1n+t/prime\u22121(1\u2212\u03b8)\u03b2n+m\u2212t/prime\u22121d\u03b8 (9.67)\n=\u0393(\u03b1n+\u03b2n)\n\u0393(\u03b1n)\u0393(\u03b2n)\u0393(\u03b1n+m)\u0393(\u03b2n+m)\n\u0393(\u03b1n+m+\u03b2n+m)(9.68)\nwhere\n\u03b1n+m=\u03b1n+s/prime=\u03b1+s+s/prime(9.69)\n\u03b2n+m=\u03b2n+(m\u2212s/prime)=\u03b2+(n\u2212s)+(m\u2212s/prime) (9.70)\n9.2.6 Maximum entropy derivation of the exponential family *\nAlthough the exponential family is convenient, is there any deeper justi\ufb01cation for its use? It\nturns out that there is: it is the distribution that makes the least number of assumptions aboutthe data, subject to a speci\ufb01c set of user-speci\ufb01ed constraints, as we explain below. In particular,suppose all we know is the expected values of certain features or functions:\n/summationdisplay\nxfk(x)p(x)=Fk (9.71)\nwhereFkare known constants, and fk(x)is an arbitrary function. The principle of maximum\nentropyormaxentsays we should pick the distribution with maximum entropy (closest to\nuniform), subject to the constraints that the moments of the distribution match the empiricalmoments of the speci\ufb01ed functions.\nTo maximize entropy subject to the constraints in Equation 9.71, and the constraints that\np(x)\u22650and/summationtext\nxp(x)=1, we need to use Lagrange multipliers. The Lagrangian is given by\nJ(p,\u03bb)=\u2212/summationdisplay\nxp(x)logp(x)+\u03bb0(1\u2212/summationdisplay\nxp(x))+/summationdisplay\nk\u03bbk(Fk\u2212/summationdisplay\nxp(x)fk(x))(9.72)\nWe can use the calculus of variations to take derivatives wrt the function p, but we will adopt\na simpler approach and treat pas a \ufb01xed length vector (since we are assuming xis discrete).\nThen we have\n\u2202J\n\u2202p(x)=\u22121\u2212logp(x)\u2212\u03bb0\u2212/summationdisplay\nk\u03bbkfk(x) (9.73)\nSetting\u2202J\n\u2202p(x)=0yields\np(x)=1\nZexp(\u2212/summationdisplay\nk\u03bbkfk(x)) (9.74)", "320": "290 Chapter 9. Generalized linear models and the exponential family\nxiw\n\u03b7i \u03bci \u03b8ig\u22121\ng\u03a8\n\u03a8\u22121\nFigure 9.1 A visualization of the various features of a GLM. Based on Figure 8.3 of (Jordan 2007).\nwhereZ=e1+\u03bb0. Using the sum to one constraint, we have\n1=/summationdisplay\nxp(x)=1\nZ/summationdisplay\nxexp(\u2212/summationdisplay\nk\u03bbkfk(x)) (9.75)\nHence the normalization constant is given by\nZ=/summationdisplay\nxexp(\u2212/summationdisplay\nk\u03bbkfk(x)) (9.76)\nThus the maxent distribution p(x)has the form of the exponential family (Section 9.2), also\nknown as the Gibbs distribution.\n9.3 Generalized linear models (GLMs)\nLinear and logistic regression are examples of generalized linear models, or GLMs (McCullagh\nand Nelder 1989). These are models in which the output density is in the exponential family\n(Section 9.2), and in which the mean parameters are a linear combination of the inputs, passedthrough a possibly nonlinear function, such as the logistic function. We describe GLMs in moredetail below. We focus on scalar outputs for notational simplicity. (This excludes multinomiallogistic regression, but this is just to simplify the presentation.)\n9.3.1 Basics\nTo understand GLMs, let us \ufb01rst consider the case of an unconditional dstribution for a scalarresponse variable:\np(y\ni|\u03b8,\u03c32)=e x p/bracketleftbiggyi\u03b8\u2212A(\u03b8)\n\u03c32+c(yi,\u03c32)/bracketrightbigg\n(9.77)\nwhere\u03c32is thedispersion parameter (often set to 1), \u03b8is the natural parameter, Ais the\npartition function, and cis a normalization constant. For example, in the case of logistic\nregression, \u03b8is the log-odds ratio, \u03b8= log(\u03bc\n1\u2212\u03bc),w h e r e\u03bc=E[y]=p(y=1 )is the mean\nparameter (see Section 9.2.2.1). To convert from the mean parameter to the natural parameter", "321": "9.3. Generalized linear models (GLMs) 291\nDistrib. Link g(\u03bc)\u03b8=\u03c8(\u03bc)\u03bc=\u03c8\u22121(\u03b8)=E[y]\nN(\u03bc,\u03c32)identity \u03b8=\u03bc\u03bc =\u03b8\nBin(N,\u03bc )logit \u03b8= log(\u03bc\n1\u2212\u03bc)\u03bc=s i g m (\u03b8)\nPoi(\u03bc)log \u03b8= log(\u03bc)\u03bc=e\u03b8\nTable 9.1 Canonical link functions \u03c8and their inverses for some common GLMs.\nwe can use a function \u03c8,s o\u03b8=\u03a8 (\u03bc). This function is uniquely determined by the form of the\nexponential family distribution. In fact, this is an invertible mapping, so we have \u03bc=\u03a8\u22121(\u03b8).\nFurthermore, we know from Section 9.2.3 that the mean is given by the derivative of the partition\nfunction, so we have \u03bc=\u03a8\u22121(\u03b8)=A/prime(\u03b8).\nNow let us add inputs/ covariates. We \ufb01rst de\ufb01ne a linear function of the inputs:\n\u03b7i=wTxi (9.78)\nWe now make the mean of the distribution be some invertible monotonic function of this linearcombination. By convention, this function, known as the mean function, is denoted by g\n\u22121,s o\n\u03bci=g\u22121(\u03b7i)=g\u22121(wTxi) (9.79)\nSee Figure 9.1 for a summary of the basic model.\nThe inverse of the mean function, namely g(), is called the link function.W ea r ef r e et o\nchoose almost any function we like for g, so long as it is invertible, and so long as g\u22121has the\nappropriate range. For example, in logistic regression, we set \u03bci=g\u22121(\u03b7i) = sigm(\u03b7 i).\nOne particularly simple form of link function is to use g=\u03c8; this is called the canonical\nlink function. In this case, \u03b8i=\u03b7i=wTxi, so the model becomes\np(yi|xi,w,\u03c32)=e x p/bracketleftbiggyiwTxi\u2212A(wTxi)\n\u03c32+c(yi,\u03c32)/bracketrightbigg\n(9.80)\nIn Table 9.1, we list some distributions and their canonical link functions. We see that for theBernoulli/ binomial distribution, the canonical link is the logit function, g(\u03bc)=l o g (\u03b7/(1\u2212\u03b7)),\nwhose inverse is the logistic function, \u03bc=s i g m (\u03b7).\nBased on the results in Section 9.2.3, we can show that the mean and variance of the response\nvariable are as follows:\nE/bracketleftbig\ny|x\ni,w,\u03c32/bracketrightbig\n=\u03bci=A/prime(\u03b8i) (9.81)\nvar/bracketleftbig\ny|xi,w,\u03c32/bracketrightbig\n=\u03c32\ni=A/prime/prime(\u03b8i)\u03c32(9.82)\nTo make the notation clearer, let us consider some simple examples.\n\u2022 For linear regression, we have\nlogp(yi|xi,w,\u03c32)=yi\u03bci\u2212\u03bc2\ni\n2\n\u03c32\u22121\n2/parenleftbiggy2\ni\n\u03c32+log(2\u03c0\u03c32)/parenrightbigg\n(9.83)\nwhereyi\u2208R, and\u03b8i=\u03bci=wTxiHereA(\u03b8)=\u03b82/2,s oE[yi]=\u03bciandvar[yi]=\u03c32.", "322": "292 Chapter 9. Generalized linear models and the exponential family\n\u2022 For binomial regression, we have\nlogp(yi|xi,w)=y ilog(\u03c0i\n1\u2212\u03c0i)+Nilog(1\u2212\u03c0i)+log/parenleftbigg\nNi\nyi/parenrightbigg\n(9.84)\nwhereyi\u2208{0,1,...,N i},\u03c0i=s i g m (wTxi),\u03b8i= log(\u03c0 i/(1\u2212\u03c0i)) =wTxi, and\u03c32=1.\nHereA(\u03b8)=Nilog(1+e\u03b8), soE[yi]=Ni\u03c0i=\u03bci,var[yi]=Ni\u03c0i(1\u2212\u03c0i).\n\u2022F o rpoisson regression,w eh a v e\nlogp(yi|xi,w)=y ilog\u03bci\u2212\u03bci\u2212log(yi!) (9.85)\nwhereyi\u2208{0,1,2,...},\u03bci=e x p (wTxi),\u03b8i= log(\u03bci)=wTxi, and\u03c32=1.H e r e\nA(\u03b8)=e\u03b8,s oE[yi]=v a r[yi]=\u03bci. Poisson regression is widely used in bio-statistical\napplications, where yimight represent the number of diseases of a given person or place,\nor the number of reads at a genomic location in a high-throughput sequencing context (see\ne.g., (Kuan et al. 2009)).\n9.3.2 ML and MAP estimation\nOne of the appealing properties of GLMs is that they can be \ufb01t using exactly the same methodsthat we used to \ufb01t logistic regression. In particular, the log-likelihood has the following form:\n/lscript(w)=l o gp(D|w)=1\n\u03c32N/summationdisplay\ni=1/lscripti (9.86)\n/lscripti/defines\u03b8iyi\u2212A(\u03b8i) (9.87)\nWe can compute the gradient vector using the chain rule as follows:\nd/lscripti\ndwj=d/lscripti\nd\u03b8id\u03b8i\nd\u03bcid\u03bci\nd\u03b7id\u03b7i\ndwj(9.88)\n=(yi\u2212A/prime(\u03b8i))d\u03b8i\nd\u03bcid\u03bci\nd\u03b7ixij (9.89)\n=(yi\u2212\u03bci)d\u03b8i\nd\u03bcid\u03bci\nd\u03b7ixij (9.90)\nIf we use a canonical link, \u03b8i=\u03b7i, this simpli\ufb01es to\n\u2207w/lscript(w)=1\n\u03c32/bracketleftBiggN/summationdisplay\ni=1(yi\u2212\u03bci)xi/bracketrightBigg\n(9.91)\nwhich is a sum of the input vectors, weighted by the errors. This can be used inside a (stochastic)gradient descent procedure, discussed in Section 8.5.2. However, for improved efficiency, weshould use a second-order method. If we use a canonical link, the Hessian is given by\nH=\u22121\n\u03c32N/summationdisplay\ni=1d\u03bci\nd\u03b8ixixT\ni=\u22121\n\u03c32XTSX (9.92)", "323": "9.4. Probit regression 293\nName Formula\nLogistic g\u22121(\u03b7)=s i g m ( \u03b7)=e\u03b7\n1+e\u03b7\nProbit g\u22121(\u03b7)=\u03a6 (\u03b7)\nLog-log g\u22121(\u03b7)=e x p (\u2212exp(\u2212\u03b7))\nComplementary log-log g\u22121(\u03b7)=1\u2212exp(\u2212exp(\u03b7))\nTable 9.2 Summary of some possible mean functions for binary regression.\nwhereS=d i a g (d\u03bc1\nd\u03b81,...,d\u03bcN\nd\u03b8N)is a diagonal weighting matrix. This can be used inside the\nIRLS algorithm (Section 8.3.4). Speci\ufb01cally, we have the following Newton update:\nwt+1=(XTStX)\u22121XTStzt (9.93)\nzt=\u03b8t+S\u22121\nt(y\u2212\u03bct) (9.94)\nwhere\u03b8t=Xwtand\u03bct=g\u22121(\u03b7t).\nIf we extend the derivation to handle non-canonical links, we \ufb01nd that the Hessian has another\nterm. However, it turns out that the expected Hessian is the same as in Equation 9.92; using\nthe expected Hessian (known as the Fisher information matrix) instead of the actual Hessian isknown as the Fisher scoring method.\nIt is straightforward to modify the above procedure to perform MAP estimation with a Gaus-\nsian prior: we just modify the objective, gradient and Hessian, just as we added /lscript\n2regularization\nto logistic regression in Section 8.3.6.\n9.3.3 Bayesian inference\nBayesian inference for GLMs is usually conducted using MCMC (Chapter 24). Possible methodsinclude Metropolis Hastings with an IRLS-based proposal (Gamerman 1997), Gibbs samplingusing adaptive rejection sampling (ARS) for each full-conditional (Dellaportas and Smith 1993),etc. See e.g., (Dey et al. 2000) for futher information. It is also possible to use the Gaussianapproximation (Section 8.4.1) or variational inference (Section 21.8.1.1).\n9.4 Probit regression\nIn (binary) logistic regression, we use a model of the form p(y=1|xi,w) = sigm( wTxi).I n\ngeneral, we can write p(y=1|xi,w)=g\u22121(wTxi), for any function g\u22121that maps [\u2212\u221e,\u221e]\nto[0,1]. Several possible mean functions are listed in Table 9.2.\nIn this section, we focus on the case where g\u22121(\u03b7)=\u03a6 (\u03b7),w h e r e\u03a6(\u03b7)is the cdf of the\nstandard normal. This is known as probit regression. The probit function is very similar to\nthe logistic function, as shown in Figure 8.7(b). However, this model has some advantages overlogistic regression, as we will see.", "324": "294 Chapter 9. Generalized linear models and the exponential family\n9.4.1 ML/MAP estimation using gradient-based optimization\nWe can \ufb01nd the MLE for probit regression using standard gradient methods. Let \u03bci=wTxi,\nand let\u02dcyi\u2208{ \u22121,+1}. Then the gradient of the log-likelihod for a speci\ufb01c case is given by\ngi/definesd\ndwlogp(\u02dcyi|wTxi)=d\u03bci\ndwd\nd\u03bcilogp(\u02dcyi|wTxi)=xi\u02dcyi\u03c6(\u03bci)\n\u03a6(\u02dcyi\u03bci)(9.95)\nwhere\u03c6is the standard normal pdf, and \u03a6is its cdf. Similarly, the Hessian for a single case is\ngiven by\nHi=d\ndw2logp(\u02dcyi|wTxi)=\u2212xi/parenleftbigg\u03c6(\u03bci)2\n\u03a6(\u02dcyi\u03bci)2+\u02dcyi\u03bci\u03c6(\u03bci)\n\u03a6(\u02dcyi\u03bci)/parenrightbigg\nxT\ni (9.96)\nWe can modify these expressions to compute the MAP estimate in a straightforward manner. In\nparticular, if we use the prior p(w)=N(0,V0), the gradient and Hessian of the penalized\nlog likelihood have the form/summationtext\nigi+2V\u22121\n0wand/summationtext\niHi+2V\u22121\n0. These expressions can be\npassed to any gradient-based optimizer. See probitRegDemo for a demo.\n9.4.2 Latent variable interpretation\nWe can interpret the probit (and logistic) model as follows. First, let us associate each itemx\niwith two latent utilities, u0iandu1i, corresponding to the possible choices of yi=0and\nyi=1. We then assume that the observed choice is whichever action has larger utility. More\nprecisely, the model is as follows:\nu0i/defineswT\n0xi+\u03b40i (9.97)\nu1i/defineswT\n1xi+\u03b41i (9.98)\nyi=I(u1i>u10) (9.99)\nwhere\u03b4\u2019s are error terms, representing all the other factors that might be relevant in decision\nmaking that we have chosen not to (or are unable to) model. This is called a random utility\nmodelorRUM(McFadden 1974; Train 2009).\nSince it is only the difference in utilities that matters, let us de\ufb01ne zi=u1i\u2212u0i+/epsilon1i,w h e r e\n/epsilon1i=\u03b41i\u2212\u03b40i. If the\u03b4\u2019s have a Gaussian distribution, then so does /epsilon1i. Thus we can write\nzi/defineswTxi+/epsilon1i (9.100)\n/epsilon1i\u223cN(0,1) (9.101)\nyi=1 = I(zi\u22650) (9.102)\nFollowing (Fruhwirth-Schnatter and Fruhwirth 2010), we call this the difference RUM or dRUM\nmodel.\nWhen we marginalize out zi, we recover the probit model:\np(yi=1|xi,w)=/integraldisplay\nI(zi\u22650)N(zi|wTxi,1)dzi (9.103)\n=p(wTxi+/epsilon1\u22650) =p(/epsilon1\u2265\u2212wTxi) (9.104)\n=1\u2212\u03a6(\u2212wTxi)=\u03a6 (wTxi) (9.105)", "325": "9.4. Probit regression 295\nwhere we used the symmetry of the Gaussian.3This latent variable interpretation provides an\nalternative way to \ufb01t the model, as discussed in Section 11.4.6.\nInterestingly, if we use a Gumbel distribution for the \u03b4\u2019s, we induce a logistic distibution for\n/epsilon1i, and the model reduces to logistic regression. See Section 24.5.1 for further details.\n9.4.3 Ordinal probit regression *\nOne advantage of the latent variable interpretation of probit regression is that it is easy to extend\nto the case where the response variable is ordinal, that is, it can take on Cdiscrete values which\ncan be ordered in some way, such as low, medium and high. This is called ordinal regression.\nThe basic idea is as follows. We introduce C+1thresholds \u03b3jand set\nyi=jif\u03b3j\u22121<zi\u2264\u03b3j (9.106)\nwhere\u03b30\u2264\u00b7\u00b7\u00b7\u2264\u03b3C. For identi\ufb01ability reasons, we set \u03b30=\u2212\u221e,\u03b31=0and\u03b3C=\u221e.F o r\nexample, if C=2, this reduces to the standard binary probit model, whereby zi<0produces\nyi=0andzi\u22650produces yi=1.I fC=3, we partition the real line into 3 intervals:\n(\u2212\u221e,0],(0,\u03b32],(\u03b32,\u221e). We can vary the parameter \u03b32to ensure the right relative amount\nof probability mass falls in each interval, so as to match the empirical frequencies of each classlabel.\nFinding the MLEs for this model is a bit trickier than for binary probit regression, since\nwe need to optimize for wand\u03b3, and the latter must obey an ordering constraint. See e.g.,\n(Kawakatsu and Largey 2009) for an approach based on EM. It is also possible to derive a simpleGibbs sampling algorithm for this model (see e.g., (Hoff 2009, p216)).\n9.4.4 Multinomial probit models *\nNow consider the case where the response variable can take on Cunordered categorical values,\nyi\u2208{1,...,C}. Themultinomial probit model is de\ufb01ned as follows:\nzic=wTxic+/epsilon1ic (9.107)\n/epsilon1\u223cN(0,R) (9.108)\nyi=a r g m a x\nczic (9.109)\nSee e.g., (Dow and Endersby 2004; Scott 2009; Fruhwirth-Schnatter and Fruhwirth 2010) formore details on the model and its connection to multinomial logistic regression. (By de\ufb01ningw=[w\n1,...,wC], andxic=[0,...,0,xi,0,...,0], we can recover the more familiar\nformulation zic=xT\niwc.) Since only relative utilities matter, we constrain Rto be a correlation\nmatrix. If instead of setting yi=a r g m a xczicwe useyic=I(zic>0), we get a model known\nasmultivariate probit, which is one way to model Ccorrelated binary outcomes (see e.g.,\n(Talhouk et al. 2011)).\n3. Note that the assumption that the Gaussian noise term is zero mean and unit variance is made without loss of\ngenerality. To see why, suppose we used some other mean \u03bcand variance \u03c32. Then we could easily rescale wand add\nan offset term without changing the likelihood. since P(N(0,1)\u2265\u2212wTx)=P(N(\u03bc,\u03c32)\u2265\u2212(wTx+\u03bc)/\u03c3).", "326": "296 Chapter 9. Generalized linear models and the exponential family\n9.5 Multi-task learning\nSometimes we want to \ufb01t many related classi\ufb01cation or regression models. It is often reasonable\nto assume the input-output mapping is similar across these different models, so we can getbetter performance by \ufb01tting all the parameters at the same time. In machine learning, thissetup is often called multi-task learning (Caruana 1998), transfer learning (e.g., (Raina et al.\n2005)), or learning to learn (Thrun and Pratt 1997). In statistics, this is usually tackled using\nhierarchical Bayesian models (Bakker and Heskes 2003), as we discuss below, although there areother possible methods (see e.g., (Chai 2010)).\n9.5.1 Hierarchical Bayes for multi-task learning\nLetyijbe the response of the i\u2019 t hi t e mi ng r o u pj ,f o ri=1:N jandj=1:J. For example,\njmight index schools, imight index students within a school, and yijmight be the test score,\nas in Section 5.6.2. Or jmight index people, and imight index purchaes, and yijmight be\nthe identity of the item that was purchased (this is known as discrete choice modeling (Train\n2009)). Let xijbe a feature vector associated with yij. The goal is to \ufb01t the models p(yj|xj)\nfor allj.\nAlthough some groups may have lots of data, there is often a long tail, where the majority\nof groups have little data. Thus we can\u2019t reliably \ufb01t each model separately, but we don\u2019t wantto use the same model for all groups. As a compromise, we can \ufb01t a separate model foreach group, but encourage the model parameters to be similar across groups. More precisely,suppose E[y\nij|xij]=g(xT\nij\u03b2j),w h e r egis the link function for the GLM. Furthermore, suppose\n\u03b2j\u223cN(\u03b2\u2217,\u03c32\njI), and that \u03b2\u2217\u223cN(\u03bc,\u03c32\n\u2217I). In this model, groups with small sample\nsize borrow statistical strength from the groups with larger sample size, because the \u03b2j\u2019s are\ncorrelated via the latent common parents \u03b2\u2217(see Section 5.5 for further discussion of this point).\nThe term \u03c32\njcontrols how much group jdepends on the common parents and the \u03c32\n\u2217term\ncontrols the strength of the overall prior.\nSuppose, for simplicity, that \u03bc=0, and that \u03c32\njand\u03c32\n\u2217are all known (e.g., they could be set\nby cross validation). The overall log probability has the form\nlogp(D|\u03b2)+logp(\u03b2)=/summationdisplay\nj/bracketleftBigg\nlogp(Dj|\u03b2j)\u2212||\u03b2j\u2212\u03b2\u2217||2\n2\u03c32\nj/bracketrightBigg\n\u2212||\u03b2\u2217||2\n2\u03c32\u2217(9.110)\nWe can perform MAP estimation of \u03b2=(\u03b21:J,\u03b2\u2217)using standard gradient methods. Alter-\nnatively, we can perform an iterative optimization scheme, alternating between optimizing the\n\u03b2jand the\u03b2\u2217; since the likelihood and prior are convex, this is guaranteed to converge to the\nglobal optimum. Note that once the models are trained, we can discard \u03b2\u2217, and use each model\nseparately.\n9.5.2 Application to personalized email spam \ufb01ltering\nAn interesting application of multi-task learning is personalized spam \ufb01ltering. Suppose we\nwant to \ufb01t one classi\ufb01er per user, \u03b2j. Since most users do not label their email as spam or not,\nit will be hard to estimate these models independently. So we will let the \u03b2jhave a common\nprior\u03b2\u2217, representing the parameters of a generic user.", "327": "9.5. Multi-task learning 297\nIn this case, we can emulate the behavior of the above model with a simple trick (Daume\n2007b; Attenberg et al. 2009; Weinberger et al. 2009): we make two copies of each feature xi,\none concatenated with the user id, and one not. The effect will be to learn a predictor of the\nform\nE[yi|xi,u]=(\u03b2\u2217,w1,\u00b7\u00b7\u00b7,wJ)T[xi,I(u=1 )xi,\u00b7\u00b7\u00b7,I(u=J)xi] (9.111)\nwhereuis the user id. In other words,\nE[yi|xi,u=j]=(\u03b2T\n\u2217+wj)Txi (9.112)\nThus\u03b2\u2217will be estimated from everyone\u2019s email, whereas wjwill just be estimated from user\nj\u2019s email.\nTo see the correspondence with the above hierarchical Bayesian model, de\ufb01ne wj=\u03b2j\u2212\u03b2\u2217.\nThen the log probability of the original model can be rewritten as\n/summationdisplay\nj/bracketleftBigg\nlogp(Dj|\u03b2\u2217+wj)\u2212||wj||2\n2\u03c32\nj/bracketrightBigg\n\u2212||\u03b2\u2217||2\n2\u03c32\u2217(9.113)\nIf we assume \u03c32\nj=\u03c32\n\u2217, the effect is the same as using the augmented feature trick, with the\nsame regularizer strength for both wjand\u03b2\u2217. However, one typically gets better performance\nby not requiring that \u03c32\njbe equal to \u03c32\n\u2217(Finkel and Manning 2009).\n9.5.3 Application to domain adaptation\nDomain adaptation is the problem of training a set of classi\ufb01ers on data drawn from different\ndistributions, such as email and newswire text. This problem is obviously a special case of\nmulti-task learning, where the tasks are the same.\n(Finkel and Manning 2009) used the above hierarchical Bayesian model to perform domain\nadaptation for two NLP tasks, namely named entity recognition and parsing. They report reason-ably large improvements over \ufb01tting separate models to each dataset, and small improvementsover the approach of pooling all the data and \ufb01tting a single model.\n9.5.4 Other kinds of prior\nIn multi-task learning, it is common to assume that the prior is Gaussian. However, sometimesother priors are more suitable. For example, consider the task of conjoint analysis, which\nrequires \ufb01guring out which features of a product customers like best. This can be modelledusing the same hierarchical Bayesian setup as above, but where we use a sparsity-promotingprior on\u03b2\nj, rather than a Gaussian prior. This is called multi-task feature selection. See e.g.,\n(Lenk et al. 1996; Argyriou et al. 2008) for some possible approaches.\nIt is not always reasonable to assume that all tasks are all equally similar. If we pool the\nparameters across tasks that are qualitatively different, the performance will be worse than notusing pooling, because the inductive bias of our prior is wrong. Indeed, it has been foundexperimentally that sometimes multi-task learning does worse than solving each task separately(this is called negative transfer).", "328": "298 Chapter 9. Generalized linear models and the exponential family\nOne way around this problem is to use a more \ufb02exible prior, such as a mixture of Gaussians.\nSuch \ufb02exible priors can provide robustness against prior mis-speci\ufb01cation. See e.g., (Xue et al.\n2007; Jacob et al. 2008) for details. One can of course combine mixtures with sparsity-promotingpriors (Ji et al. 2009). Many other variants are possible.\n9.6 Generalized linear mixed models *\nSuppose we generalize the multi-task learning scenario to allow the response to include infor-mation at the group level, x\nj, as well as at the item level, xij. Similarly, we can allow the\nparameters to vary across groups, \u03b2j, or to be tied across groups, \u03b1. This gives rise to the\nfollowing model:\nE[yij|xij,xj]=g/parenleftbig\n\u03c61(xij)T\u03b2j+\u03c62(xj)T\u03b2/prime\nj+\u03c63(xij)T\u03b1+\u03c64(xj)T\u03b1/prime/parenrightbig\n(9.114)\nwhere the \u03c6kare basis functions. This model can be represented pictorially as shown in\nFigure 9.2(a). (Such \ufb01gures will be explained in Chapter 10.) Note that the number of \u03b2j\nparameters grows with the number of groups, whereas the size of \u03b1is \ufb01xed.\nFrequentists call the terms \u03b2jrandom effects, since they vary randomly across groups, but\nthey call \u03b1a\ufb01xed effect, since it is viewed as a \ufb01xed but unknown constant. A model with\nboth \ufb01xed and random effects is called a mixed model.I f p(y|x)is a GLM, the overall model\nis called a generalized linear mixed effects model orGLMM. Such models are widely used in\nstatistics.\n9.6.1 Example: semi-parametric GLMMs for medical data\nConsider the following example from (Wand 2009). Suppose yijis the amount of spinal bone\nmineral density (SBMD) for person jat measurement i.L e txijbe the age of person, and let\nxjbe their ethnicity, which can be one of: White, Asian, Black, or Hispanic. The primary goal\nis to determine if there are signi\ufb01cant differences in the mean SBMD among the four ethnic\ngroups, after accounting for age. The data is shown in the light gray lines in Figure 9.2(b). Wesee that there is a nonlinear effect of SBMD vs age, so we will use a semi-parametric model\nwhich combines linear regression with non-parametric regression (Ruppert et al. 2003). We alsosee that there is variation across individuals within each group, so we will use a mixed effectsmodel. Speci\ufb01cally, we will use \u03c6\n1(xij)=1to account for the random effect of each person;\n\u03c62(xij)=0since no other coefficients are person-speci\ufb01c; \u03c63(xij)=[bk(xij)],w h e r e bkis\nthek\u2019th spline basis functions (see Section 15.4.6.2), to account for the nonlinear effect of age;\nand\u03c64(xj)=[I(xj=w),I(xj=a),I(xj=b),I(xj=h)]to account for the effect of the\ndifferent ethnicities. Furthermore, we use a linear link function. The overall model is therefore\nE[yij|xij,xj]=\u03b2 j+\u03b1Tb(xij)+/epsilon1ij (9.115)\n+\u03b1/prime\nwI(xj=w)+\u03b1/prime\naI(xj=a)+\u03b1/prime\nbI(xj=b)+\u03b1/prime\nhI(xj=h)(9.116)\nwhere/epsilon1ij\u223cN(0,\u03c32\ny).\u03b1contains the non-parametric part of the model related to age, \u03b1/prime\ncontains the parametric part of the model related to ethnicity, and \u03b2jis a random offset\nfor person j. We endow all of these regression coefficients with separate Gaussian priors.\nWe can then perform posterior inference to compute p(\u03b1,\u03b1/prime,\u03b2,\u03c32|D)(see Section 9.6.2 for", "329": "9.6. Generalized linear mixed models * 299\nxjxijyij\nNj\u03b2j\nJ\u03bc\u03b2\u03c32\n\u03b2\n\u03b1 \u03bc\u03b1\n\u03c32\n\u03b1\u03c32\ny\n(a)\nage in yearsspinal bone mineral density (g/cm2)\n0.60.81.01.21.4\n10 15 20 25Asian BlackHispanic10 15 20 25\n0.60.81.01.21.4White\n(b)\nFigure 9.2 (a) Directed graphical model for generalized linear mixed effects model with Jgroups. (b)\nSpinal bone mineral density vs age for four different ethnic groups. Raw data is shown in the light gray\nlines. Fitted model shown in black (solid is the posterior predicted mean, dotted is the posterior predictivevariance). From Figure 9 of (Wand 2009). Used with kind permission of Matt Wand", "330": "300 Chapter 9. Generalized linear models and the exponential family\ncomputational details). After \ufb01tting the model, we can compute the prediction for each group.\nSee Figure 9.2(b) for the results. We can also perform signi\ufb01cance testing, by computing p(\u03b1g\u2212\n\u03b1w|D)for each ethnic group grelative to some baseline (say, White), as we did in Section 5.2.3.\n9.6.2 Computational issues\nThe principle problem with GLMMs is that they can be difficult to \ufb01t, for two reasons. First,p(y\nij|\u03b8)may not be conjugate to the prior p(\u03b8)where\u03b8=(\u03b1,\u03b2). Second, there are two levels\nof unknowns in the model, namely the regression coefficients \u03b8and the means and variances\nof the priors \u03b7=(\u03bc,\u03c3).\nOne approach is to adopt fully Bayesian inference methods, such as variational Bayes (Hall\net al. 2011) or MCMC (Gelman and Hill 2007). We discuss VB in Section 21.5, and MCMC inSection 24.1.\nAn alternative approach is to use empirical Bayes, which we discuss in general terms in\nSection 5.6. In the context of a GLMM, we can use the EM algorithm (Section 11.4), where in theE step we compute p(\u03b8|\u03b7,D), and in the M step we optimize \u03b7. If the linear regression setting,\nthe E step can be performed exactly, but in general we need to use approximations. Traditionalmethods use numerical quadrature or Monte Carlo (see e.g., (Breslow and Clayton 1993)). Afaster approach is to use variational EM; see (Braun and McAuliffe 2010) for an application ofvariational EM to a multi-level discrete choice modeling problem.\nIn frequentist statistics, there is a popular method for \ufb01tting GLMMs called generalized\nestimating equations orGEE(Hardin and Hilbe 2003). However, we do not recommend this\napproach, since it is not as statistically efficient as likelihood-based methods (see Section 6.4.3).In addition, it can only provide estimates of the population parameters \u03b1, but not the random\neffects\u03b2\nj, which are sometimes of interest in themselves.\n9.7 Learning to rank *\nIn this section, we discuss the learning to rank orLETORproblem. That is, we want to learn a\nfunction that can rank order a set of items (we will be more precise below). The most commonapplication is to information retrieval. Speci\ufb01cally, suppose we have a query qand a set of\ndocuments d\n1,...,dmthat might be relevant to q(e.g., all documents that contain the string q).\nWe would like to sort these documents in decreasing order of relevance and show the top kto\nthe user. Similar problems arise in other areas, such as collaborative \ufb01ltering. (Ranking playersin a game or tournament setting is a slightly different kind of problem; see Section 22.5.5.)\nBelow we summarize some methods for solving this problem, following the presentation of\n(Liu 2009). This material is not based on GLMs, but we include it in this chapter anyway forlack of a better place.\nA standard way to measure the relevance of a document dto a query qis to use a probabilistic\nlanguage model based on a bag of words model. That is, we de\ufb01ne sim(q,d)/definesp(q|d)=/producttext\nn\ni=1p(qi|d),w h e r eqiis thei\u2019th word or term, and p(qi|d)is a multinoulli distribution\nestimated from document d. In practice, we need to smooth the estimated distribution, for\nexample by using a Dirichlet prior, representing the overall frequency of each word. This can be", "331": "9.7. Learning to rank * 301\nestimated from all documents in the system. More precisely, we can use\np(t|d)=( 1\u2212\u03bb)TF(t,d)\nLEN(d)+\u03bbp(t|background) (9.117)\nwhere TF(t,d)is the frequency of term tin document d, LEN(d)is the number of words in d,\nand0<\u03bb<1is a smoothing parameter (see e.g., Zhai and Lafferty (2004) for details).\nHowever, there might be many other signals that we can use to measure relevance. For\nexample, the PageRank of a web document is a measure of its authoritativeness, derived from\nthe web\u2019s link structure (see Section 17.2.4 for details). We can also compute how often andwhere the query occurs in the document. Below we discuss how to learn how to combine allthese signals.\n4\n9.7.1 The pointwise approach\nSuppose we collect some training data representing the relevance of a set of documents for eachquery. Speci\ufb01cally, for each query q, suppose that we retrieve mpossibly relevant documents\nd\nj,f o rj=1:m. For each query document pair, we de\ufb01ne a feature vector, x(q,d).F o r\nexample, this might contain the query-document similarity score and the page rank score of thedocument. Furthermore, suppose we have a set of labels y\njrepresenting the degree of relevance\nof document djto queryq. Such labels might be binary (e.g., relevant or irrelevant), or they may\nrepresent a degree of relevance (e.g., very relevant, somewhat relevant, irrelevant). Such labelscan be obtained from query logs, by thresholding the number of times a document was clickedon for a given query.\nIf we have binary relevance labels, we can solve the problem using a standard binary clas-\nsi\ufb01cation scheme to estimate, p(y=1|x(q,d)). If we have ordered relevancy labels, we can\nuse ordinal regression to predict the rating, p(y=r|x(q,d)). In either case, we can then sort\nthe documents by this scoring metric. This is called the pointwise approach to LETOR, and\nis widely used because of its simplicity. However, this method does not take into account thelocation of each document in the list. Thus it penalizes errors at the end of the list just as muchas errors at the beginning, which is often not the desired behavior. In addition, each decisionabout relevance is made very myopically.\n9.7.2 The pairwise approach\nThere is evidence (e.g., (Carterette et al. 2008)) that people are better at judging the relativerelevance of two items rather than absolute relevance. Consequently, the data might tell usthatd\njis more relevant than dkfor a given query, or vice versa. We can model this kind of\ndata using a binary classi\ufb01er of the form p(yjk|x(q,dj),x(q,dk)), where we set yjk=1if\nrel(dj,q)>rel(dk,q)andyjk=0otherwise.\nOne way to model such a function is as follows:\np(yjk=1|xj,xk) = sigm(f (xj)\u2212f(xk)) (9.118)\n4. Rather surprisingly, Google does not (or at least, did not as of 2008) using such learning methods in its search engine.\nSource: Peter Norvig, quoted in http://anand .typepad .com/datawocky/2008/05/are-human-experts-less-p\nrone-to-catastrophic-errors-than-machine-learned-models .html.", "332": "302 Chapter 9. Generalized linear models and the exponential family\nwheref(x)is a scoring function, often taken to be linear, f(x)=wTx. This is a special\nkind of neural network known as RankNet (Burges et al. 2005) (see Section 16.5 for a general\ndiscussion of neural networks). We can \ufb01nd the MLE of wby maximizing the log likelihood, or\nequivalently, by minimizing the cross entropy loss, given by\nL=N/summationdisplay\ni=1mi/summationdisplay\nj=1mi/summationdisplay\nk=j+1Lijk (9.119)\n\u2212Lijk=I(yijk=1 )l o gp(yijk=1|xij,xik,w)\n+I(yijk=0 )l o gp(yijk=0|xij,xik,w) (9.120)\nThis can be optimized using gradient descent. A variant of RankNet is used by Microsoft\u2019s Bing\nsearch engine.5\n9.7.3 The listwise approach\nThe pairwise approach suffers from the problem that decisions about relevance are made justbased on a pair of items (documents), rather than considering the full context. We now considermethods that look at the entire list of items at the same time.\nWe can de\ufb01ne a total order on a list by specifying a permutation of its indices, \u03c0. To model\nour uncertainty about \u03c0, we can use the Plackett-Luce distribution, which derives its name\nfrom independent work by (Plackett 1975) and (Luce 1959). This has the following form:\np(\u03c0|s)=\nm/productdisplay\nj=1sj/summationtextm\nu=jsu(9.121)\nwheresj=s(\u03c0\u22121(j))is the score of the document ranked at the j\u2019th position.\nTo understand Equation 9.121, let us consider a simple example. Suppose \u03c0=(A,B,C).\nThen we have that p(\u03c0)is the probability of Abeing ranked \ufb01rst, times the probability of B\nbeing ranked second given that Ais ranked \ufb01rst, times the probabilty of Cbeing ranked third\ngiven that AandBare ranked \ufb01rst and second. In other words,\np(\u03c0|s)=sA\nsA+sB+sC\u00d7sB\nsB+sC\u00d7sC\nsC(9.122)\nTo incorporate features, we can de\ufb01ne s(d)=f(x(q,d)), where we often take fto be a\nlinear function, f(x)=wTx. This is known as the ListNetmodel (Cao et al. 2007). To train\nthis model, let yibe the relevance scores of the documents for query i. We then minimize the\ncross entropy term\n\u2212/summationdisplay\ni/summationdisplay\n\u03c0p(\u03c0|yi)logp(\u03c0|si) (9.123)\nOf course, as stated, this is intractable, since the i\u2019th term needs to sum over mi!permutations.\nTo make this tractable, we can consider permutations over the top kpositions only:\np(\u03c01:k|s1:m)=k/productdisplay\nj=1sj/summationtextmu=1su(9.124)\n5. Source: http://www .bing.com/community/site_blogs/b/search/archive/2009/06/01/user-needs-f\neatures-and-the-science-behind-bing .aspx.", "333": "9.7. Learning to rank * 303\nThere are only m!/(m\u2212k)!such permutations. If we set k=1, we can evaluate each cross\nentropy term (and its derivative) in O(m)time.\nIn the special case where only one document from the presented list is deemed relevant, say\nyi=c, we can instead use multinomial logistic regression:\np(yi=c|x)=exp(sc)/summationtextm\nc/prime=1exp(sc/prime)(9.125)\nThis often performs at least as well as ranking methods, at least in the context of collaborative\n\ufb01ltering (Yang et al. 2011).\n9.7.4 Loss functions for ranking\nThere are a variety of ways to measure the performance of a ranking system, which we summa-rize below.\n\u2022Mean reciprocal rank (MRR). For a query q, let the rank position of its \ufb01rst relevant\ndocument be denoted by r(q). Then we de\ufb01ne the mean reciprocal rank to be1/r(q).\nThis is a very simple performance measure.\n\u2022Mean average precision (MAP). In the case of binary relevance labels, we can de\ufb01ne the\nprecision at k of some ordering as follows:\nP@k(\u03c0)/definesnum. relevant documents in the top kpositions of \u03c0\nk(9.126)\nWe then de\ufb01ne the average precision as follows:\nAP(\u03c0)/defines/summationtext\nkP@k(\u03c0)\u00b7Ik\nnum. relevant documents(9.127)\nwhereIkis 1 iff document kis relevant. For example, if we have the relevancy labels\ny=( 1,0,1,0,1), then the AP is1\n3(1\n1+2\n3+3\n5)\u22480.76. Finally, we de\ufb01ne the mean average\nprecision as the AP averaged over all queries.\n\u2022Normalized discounted cumulative gain (NDCG). Suppose the relevance labels have multi-ple levels. We can de\ufb01ne the discounted cumulative gain of the \ufb01rst kitems in an ordering\nas follows:\nDCG@k(r)=r\n1+k/summationdisplay\ni=2ri\nlog2i(9.128)\nwhereriis the relevance of item iand thelog2term is used to discount items later in\nthe list. Table 9.3 gives a simple numerical example. An alternative de\ufb01nition, that placesstronger emphasis on retrieving relevant documents, uses\nDCG@k(r)=k/summationdisplay\ni=12ri\u22121\nlog2(1+i)(9.129)\nThe trouble with DCG is that it varies in magnitude just because the length of a returnedlist may vary. It is therefore common to normalize this measure by the ideal DCG, which is", "334": "304 Chapter 9. Generalized linear models and the exponential family\ni12 3 4 5 6\nri32 3 0 1 2\nlog2i0 1 1.59 2.0 2.32 2.59\nri\nlog2iN/A 2 1.887 0 0.431 0.772\nTable 9.3 Illustration of how to compute NDCG, from http://en .wikipedia .org/wiki/Discounted\n_cumulative_gain . The value riis the relevance score of the item in position i. From this, we see\nthat DCG@6 =3+( 2+1 .887+0+0 .431+0.772) = 8.09. The maximum DCG is obtained using the\nordering with scores 3, 3, 2, 2, 1, 0. Hence the ideal DCG is 8.693, and so the normalized DCG is 8.09 /\n8.693 = 0.9306.\nthe DCG obtained by using the optimal ordering: IDCG@k( r) = argmax\u03c0DCG@k(r). This\ncan be easily computed by sorting r1:mand then computing DCG@k. Finally, we de\ufb01ne\nthenormalized discounted cumulative gain orNDCGas DCG/IDCG. Table 9.3 gives a\nsimple numerical example. The NDCG can be averaged over queries to give a measure of\nperformance.\n\u2022Rank correlation . We can measure the correlation between the ranked list, \u03c0, and the\nrelevance judegment, \u03c0\u2217, using a variety of methods. One approach, known as the (weighted)\nKendall\u2019s \u03c4statistics, is de\ufb01ned in terms of the weighted pairwise inconsistency between the\ntwo lists:\n\u03c4(\u03c0,\u03c0\u2217)=/summationtext\nu<vwuv[1+sgn(\u03c0 u\u2212\u03c0v)sgn(\u03c0\u2217\nu\u2212\u03c0\u2217\nv)]\n2/summationtext\nu<vwuv(9.130)\nA variety of other measures are commonly used.\nThese loss functions can be used in different ways. In the Bayesian approach, we \ufb01rst \ufb01t the\nmodel using posterior inference; this depends on the likelihood and prior, but not the loss. Wethen choose our actions at test time to minimize the expected future loss. One way to do this isto sample parameters from the posterior, \u03b8\ns\u223cp(\u03b8|D), and then evaluate, say, the precision@k\nfor different thresholds, averaging over \u03b8s. See (Zhang et al. 2010) for an example of such an\napproach.\nIn the frequentist approach, we try to minimize the empirical loss on the training set. The\nproblem is that these loss functions are not differentiable functions of the model parameters.We can either use gradient-free optimization methods, or we can minimize a surrogate lossfunction instead. Cross entropy loss (i.e., negative log likelihood) is an example of a widely usedsurrogate loss function.\nAnother loss, known as weighted approximate-rank pairwise orWARPloss, proposed in\n(Usunier et al. 2009) and extended in (Weston et al. 2010), provides a better approximation tothe precision@k loss. WARP is de\ufb01ned as follows:\nWARP(f(x,:),y)/definesL(rank(f(x,:),y)) (9.131)\nrank(f(x,:),y)=/summationdisplay\ny/prime/negationslash=yI(f(x,y/prime)\u2265f(x,y)) (9.132)\nL(k)/definesk/summationdisplay\nj=1\u03b1j,with\u03b11\u2265\u03b12\u2265\u00b7\u00b7\u00b7\u22650 (9.133)", "335": "9.7. Learning to rank * 305\nHeref(x,:) = [f(x,1),...,f(x,|y|)]is the vector of scores for each possible output label,\nor, in IR terms, for each possible document corresponding to input query x. The expression\nrank(f(x,:),y)measures the rank of the true label yassigned by this scoring function. Finally,\nLtransforms the integer rank into a real-valued penalty. Using \u03b11=1and\u03b1j>1=0would\noptimize the proportion of top-ranked correct labels. Setting \u03b11:kto be non-zero values would\noptimize the top kin the ranked list, which will induce good performance as measured by\nMAP or precision@k. As it stands, WARP loss is still hard to optimize, but it can be further\napproximated by Monte Carlo sampling, and then optimized by gradient descent, as describedin (Weston et al. 2010).\nExercises\nExercise 9.1 Conjugate prior for univariate Gaussian in exponential family form\nDerive the conjugate prior for \u03bcand\u03bb=1/\u03c32for a univariate Gaussian using the exponential family,\nby analogy to Section 9.2.5.5. By suitable reparameterization, show that the prior has the form p(\u03bc,\u03bb)=\nN(\u03bc|\u03b3,\u03bb(2\u03b1\u22121))Ga(\u03bb|\u03b1,\u03b2), and thus only has 3 free parameters.\nExercise 9.2 The MVN is in the exponential family\nShow that we can write the MVN in exponential family form. Hint: use the information form de\ufb01ned in\nSection 4.3.3.", "336": "", "337": "10 Directed graphical models (Bayes nets)\n10.1 Introduction\nI basically know of two principles for treating complicated systems in simple ways: the\n\ufb01rst is the principle of modularity and the second is the principle of abstraction. Iam an apologist for computational probability in machine learning because I believe thatprobability theory implements these two principles in deep and intriguing ways \u2014 namelythrough factorization and through averaging. Exploiting these two mechanisms as fullyas possible seems to me to be the way forward in machine learning. \u2014 Michael Jordan,1997 (quoted in (Frey 1998)).\nSuppose we observe multiple correlated variables, such as words in a document, pixels in animage, or genes in a microarray. How can we compactly representthejoint distribution p(x|\u03b8)?\nHow can we use this distribution to inferone set of variables given another in a reasonable\namount of computation time? And how can we learnthe parameters of this distribution with a\nreasonable amount of data? These questions are at the core of probabilistic modeling, inferenceand learning, and form the topic of this chapter.\n10.1.1 Chain rule\nBy thechain rule of probability, we can always represent a joint distribution as follows, using\nany ordering of the variables:\np(x1:V)=p(x1)p(x2|x1)p(x3|x2,x1)p(x4|x1,x2,x3)...p(xV|x1:V\u22121) (10.1)\nwhereVis the number of variables, the Matlab-like notation 1:Vdenotes the set {1,2,...,V},\nand where we have dropped the conditioning on the \ufb01xed parameters \u03b8for brevity. The problem\nwith this expression is that it becomes more and more complicated to represent the conditionaldistributions p(x\nt|x1:t\u22121)astgets large.\nFor example, suppose all the variables have Kstates. We can represent p(x1)as a table\nofO(K)numbers, representing a discrete distribution (there are actually only K\u22121free\nparameters, due to the sum-to-one constraint, but we write O(K)for simplicity). Similarly, we\ncan represent p(x2|x1)as a table of O(K2)numbers by writing p(x2=j|x1=i)=Tij;w e\nsay thatTis astochastic matrix, since it satis\ufb01es the constraint/summationtext\njTij=1for all rows i,\nand0\u2264Tij\u22641for all entries. Similarly, we can represent p(x3|x1,x2)as a 3d table with", "338": "308 Chapter 10. Directed graphical models (Bayes nets)\nO(K3)numbers. These are called conditional probability tables orCPTs. We see that there\nareO(KV)parameters in the model. We would need an awful lot of data to learn so many\nparameters.\nOne solution is to replace each CPT with a more parsimonius conditional probability distri-\nbutionorCPD, such as multinomial logistic regression, i.e., p(xt=k|x1:t\u22121)=S(Wtx1:t\u22121)k.\nThe total number of parameters is now only O(K2V2), making this a compact density model\n(Neal 1992; Frey 1998). This is adequate if all we want to do is evaluate the probability of a fully\nobserved vector x1:T. For example, we can use this model to de\ufb01ne a class-conditional density,\np(x|y=c), thus making a generative classi\ufb01er (Bengio and Bengio 2000). However, this model\nis not useful for other kinds of prediction tasks, since each variable depends on all the previousvariables. So we need another approach.\n10.1.2 Conditional independence\nThe key to efficiently representing large joint distributions is to make some assumptions aboutconditional independence (CI). Recall from Section 2.2.4 that XandYare conditionally inde-\npendent given Z, denoted X\u22a5Y|Z, if and only if (iff) the conditional joint can be written as\na product of conditional marginals, i.e.,\nX\u22a5Y|Z\u21d0\u21d2p(X,Y|Z)=p(X|Z)p(Y|Z) (10.2)\nLet us see why this might help. Suppose we assume that x\nt+1\u22a5x1:t\u22121|xt, or in words,\n\u201cthe future is independent of the past given the present\u201d. This is called the (\ufb01rst order) Markov\nassumption. Using this assumption, plus the chain rule, we can write the joint distribution asfollows:\np(x\n1:V)=p(x1)V/productdisplay\nt=1p(xt|xt\u22121) (10.3)\nThis is called a (\ufb01rst-order) Markov chain. They can be characterized by an initial distribution\nover states, p(x1=i), plus astate transition matrix p(xt=j|xt\u22121=i). See Section 17.2 for\nmore information.\n10.1.3 Graphical models\nAlthough the \ufb01rst-order Markov assumption is useful for de\ufb01ning distributions on 1d sequences,how can we de\ufb01ne distributions on 2d images, or 3d videos, or, in general, arbitrary collectionsof variables (such as genes belonging to some biological pathway)? This is where graphicalmodels come in.\nAgraphical model (GM) is a way to represent a joint distribution by making CI assumptions.\nInparticular, thenodesinthegraphrepresentrandomvariables, andthe(lackof)edgesrepresentCI assumptions. (A better name for these models would in fact be \u201cindependence diagrams\u201d,but the term \u201cgraphical models\u201d is now entrenched.) There are several kinds of graphical model,depending on whether the graph is directed, undirected, or some combination of directed andundirected. In this chapter, we just study directed graphs. We consider undirected graphs inChapter 19.", "339": "10.1. Introduction 309\n45231\n(a)45231\n(b)\nFigure 10.1 (a) A simple DAG on 5 nodes, numbered in topological order. Node 1 is the root, nodes 4 and\n5 are the leaves. (b) A simple undirected graph, with the following maximal cliques: {1,2,3},{2,3,4},\n{3,5}.\n10.1.4 Graph terminology\nBefore we continue, we must de\ufb01ne a few basic terms, most of which are very intuitive.\nAgraphG=(V,E)consists of a set of nodesorvertices, V={1,...,V }, and a set\nofedges,E={(s,t):s,t\u2208V }. We can represent the graph by its adjacency matrix,i n\nwhich we write G(s,t)=1to denote (s,t)\u2208E, that is, if s\u2192tis an edge in the graph.\nIfG(s,t)=1iffG(t,s)=1, we say the graph is undirected, otherwise it is directed.W e\nusually assume G(s,s)=0, which means there are no self loops.\nHere are some other terms we will commonly use:\n\u2022ParentFor a directed graph, the parentsof a node is the set of all nodes that feed into it:\npa(s)/defines{t:G(t,s)=1}.\n\u2022ChildFor a directed graph, the children of a node is the set of all nodes that feed out of it:\nch(s)/defines{t:G(s,t)=1} .\n\u2022FamilyFor a directed graph, the familyof a node is the node and its parents, fam(s)=\n{s}\u222apa(s).\n\u2022RootFor a directed graph, a rootis a node with no parents.\n\u2022LeafFor a directed graph, a leafis a node with no children.\n\u2022Ancestors For a directed graph, the ancestors are the parents, grand-parents, etc of a node.\nThat is, the ancestors of tis the set of nodes that connect to tvia a trail: anc(t) /defines{s:s;\nt}.\n\u2022Descendants For a directed graph, the descendants are the children, grand-children, etc of\na node. That is, the descendants of sis the set of nodes that can be reached via trails from\ns:desc(s)/defines{t:s;t}.\n\u2022Neighbors For any graph, we de\ufb01ne the neighbors of a node as the set of all immediately\nconnected nodes, nbr(s)/defines{t:G(s,t)=1\u2228G(t,s)=1}. For an undirected graph, we", "340": "310 Chapter 10. Directed graphical models (Bayes nets)\nwrites\u223ctto indicate that sandtare neighbors (so (s,t)\u2208Eis an edge in the graph).\n\u2022DegreeThedegreeof a node is the number of neighbors. For directed graphs, we speak of\nthein-degree andout-degree, which count the number of parents and children.\n\u2022Cycle or loop For any graph, we de\ufb01ne a cycleorloopto be a series of nodes such that\nwe can get back to where we started by following edges, s1\u2212s2\u00b7\u00b7\u00b7\u2212sn\u2212s1,n\u22652. If the\ngraph is directed, we may speak of a directed cycle. For example, in Figure 10.1(a), there are\nno directed cycles, but 1\u21922\u21924\u21923\u21921is an undirected cycle.\n\u2022DAGAdirected acyclic graph orDAGis a directed graph with no directed cycles. See\nFigure 10.1(a) for an example.\n\u2022Topological ordering For a DAG, a topological ordering ortotal ordering is a numbering\nof the nodes such that parents have lower numbers than their children. For example, inFigure 10.1(a), we can use (1,2,3,4,5),o r(1,3,2,5,4), etc.\n\u2022Path or trail Apathortrails;tis a series of directed edges leading from stot.\n\u2022TreeAn undirected treeis an undirectecd graph with no cycles. A directed tree is a DAG in\nwhich there are no directed cycles. If we allow a node to have multiple parents, we call it apolytree, otherwise we call it a moral directed tree.\n\u2022ForestAforestis a set of trees.\n\u2022Subgraph A (node-induced) subgraph G\nAis the graph created by using the nodes in Aand\ntheir corresponding edges, GA=(VA,EA).\n\u2022CliqueFor an undirected graph, a cliqueis a set of nodes that are all neighbors of each\nother. A maximal clique is a clique which cannot be made any larger without losing the\nclique property. For example, in Figure 10.1(b), {1,2}is a clique but it is not maximal, since\nwe can add 3 and still maintain the clique property. In fact, the maximal cliques are asfollows:{1,2,3},{2,3,4},{3,5}.\n10.1.5 Directed graphical models\nAdirected graphical model orDGMis a GM whose graph is a DAG. These are more commonly\nknown as Bayesian networks. However, there is nothing inherently \u201cBayesian\u201d about Bayesian\nnetworks: they are just a way of de\ufb01ning probability distributions. These models are also calledbelief networks. The term \u201cbelief\u201d here refers to subjective probability. Once again, there isnothing inherently subjective about the kinds of probability distributions represented by DGMs.Finally, these models are sometimes called causal networks, because the directed arrows are\nsometimes interpreted as representing causal relations. However, there is nothing inherentlycausal about DGMs. (See Section 26.6.1 for a discussion of causal DGMs.) For these reasons, weuse the more neutral (but less glamorous) term DGM.\nThe key property of DAGs is that the nodes can be ordered such that parents come before\nchildren. This is called a topological ordering, and it can be constructed from any DAG. Givensuch an order, we de\ufb01ne the ordered Markov property to be the assumption that a node only\ndepends on its immediate parents, not on all predecessors in the ordering, i.e.,\nx\ns\u22a5xpred(s)\\pa(s) |xpa(s) (10.4)\nwherepa(s)are the parents of node s, andpred(s)are the predecessors of node sin the\nordering. This is a natural generalization of the \ufb01rst-order Markov property to from chains togeneral DAGs.", "341": "10.2. Examples 311\nY\nX1X2X3X4\n(a)Y\nX1 X2 X3\nX4\n(b)\nFigure 10.2 (a) A naive Bayes classi\ufb01er represented as a DGM. We assume there are D=4features,\nfor simplicity. Shaded nodes are observed, unshaded nodes are hidden. (b) Tree-augmented naive Bayes\nclassi\ufb01er for D=4features. In general, the tree topology can change depending on the value of y.\nFor example, the DAG in Figure 10.1(a) encodes the following joint distribution:\np(x1:5)=p( x1)p(x2|x1)p(x3|x1,\b\bx2)p(x4|\b\bx1,x2,x3)p(x5|\b\bx1,\b\bx2,x3,\b\bx4) (10.5)\n=p(x1)p(x2|x1)p(x3|x1)p(x4|x2,x3)p(x5|x3) (10.6)\nIn general, we have\np(x1:V|G)=V/productdisplay\nt=1p(xt|xpa(t)) (10.7)\nwhere each term p(xt|xpa(t))is a CPD. We have written the distribution as p(x|G)to emphasize\nthat this equation only holds if the CI assumptions encoded in DAG Gare correct. However,\nwe will usual drop this explicit conditioning for brevity. If each node has O(F)parents and\nKstates, the number of parameters in the model is O(VKF), which is much less than the\nO(KV)needed by a model which makes no CI assumptions.\n10.2 Examples\nIn this section, we show a wide variety of commonly used probabilistic models can be conve-\nniently represented as DGMs.\n10.2.1 Naive Bayes classi\ufb01ers\nIn Section 3.5, we introduced the naive Bayes classi\ufb01er. This assumes the features are condi-tionally independent given the class label. This assumption is illustrated in Figure 10.2(a). Thisallows us to write the joint distirbution as follows:\np(y,x)=p(y)\nD/productdisplay\nj=1p(xj|y) (10.8)\nThe naive Bayes assumption is rather naive, since it assumes the features are conditionally\nindependent. One way to capture correlation between the features is to use a graphical model.In particular, if the model is a tree, the method is known as a tree-augmented naive Bayes", "342": "312 Chapter 10. Directed graphical models (Bayes nets)\nx1 x2 x3\u00b7\u00b7\u00b7\n(a)x1 x2 x3 x4\u00b7\u00b7\u00b7\n(b)\nFigure 10.3 A \ufb01rst and second order Markov chain.\nx1 x2 xTz1 z2 zT\nFigure 10.4 A \ufb01rst-order HMM.\nclassi\ufb01er orTANmodel (Friedman et al. 1997). This is illustrated in Figure 10.2(b). The reason\nto use a tree, as opposed to a generic graph, is two-fold. First, it is easy to \ufb01nd the optimal\ntree structure using the Chow-Liu algorithm, as explained in Section 26.3. Second, it is easy tohandle missing features in a tree-structured model, as we explain in Section 20.2.\n10.2.2 Markov and hidden Markov models\nFigure 10.3(a) illustrates a \ufb01rst-order Markov chain as a DAG. Of course, the assumption that theimmediate past, x\nt\u22121, captures everything we need to know about the entire history, x1:t\u22122,i s\na bit strong. We can relax it a little by adding a dependence from xt\u22122toxtas well; this is\ncalled asecond order Markov chain, and is illustrated in Figure 10.3(b). The corresponding\njoint has the following form:\np(x1:T)=p(x1,x2)p(x3|x1,x2)p(x4|x2,x3)...=p(x1,x2)T/productdisplay\nt=3p(xt|xt\u22121,xt\u22122)(10.9)\nWe can create higher-order Markov models in a similar way. See Section 17.2 for a more detaileddiscussion of Markov models.\nUnfortunately, even the second-order Markov assumption may be inadequate if there are long-\nrange correlations amongst the observations. We can\u2019t keep building ever higher order models,since the number of parameters will blow up. An alternative approach is to assume that thereis an underlying hidden process, that can be modeled by a \ufb01rst-order Markov chain, but thatthe data is a noisy observation of this process. The result is known as a hidden Markov model\norHMM, and is illustrated in Figure 10.4. Here z\ntis known as a hidden variable at \u201ctime\u201d t,\nandxtis the observed variable. (We put \u201ctime\u201d in quotation marks, since these models can be\napplied to any kind of sequence data, such as genomics or language, where trepresents location\nrather than time.) The CPD p(zt|zt\u22121)is thetransition model, and the CPD p(xt|zt)is the\nobservation model.", "343": "10.2. Examples 313\nh0h1h2P(v=0|h1,h2)P(v=1|h1,h2)\n100 \u03b80 1\u2212\u03b80\n110 \u03b80\u03b81 1\u2212\u03b80\u03b81\n101 \u03b80\u03b82 1\u2212\u03b80\u03b82\n111 \u03b80\u03b81\u03b82 1\u2212\u03b80\u03b81\u03b82\nTable 10.1 Noisy-OR CPD for 2 parents augmented with leak node. We have omitted the tsubscript for\nbrevity.\nThe hidden variables often represent quantities of interest, such as the identity of the word\nthat someone is currently speaking. The observed variables are what we measure, such as the\nacoustic waveform. What we would like to do is estimate the hidden state given the data, i.e., tocompute p(z\nt|x1:t,\u03b8). This is called state estimation, and is just another form of probabilistic\ninference. See Chapter 17 for further details on HMMs.\n10.2.3 Medical diagnosis\nConsider modeling the relationship between various variables that are measured in an intensivecare unit (ICU), such as the breathing rate of a patient, their blood pressure, etc. The alarm\nnetwork in Figure 10.5(a) is one way to represent these (in)dependencies (Beinlich et al. 1989).\nThis model has 37 variables and 504 parameters.\nSince this model was created by hand, by a process called knowledge engineering,i ti s\nknown as a probabilistic expert system. In Section 10.4, we discuss how to learn the parameters\nof DGMs from data, assuming the graph structure is known, and in Chapter 26, we discuss howto learn the graph structure itself.\nA different kind of medical diagnosis network, known as the quick medical reference or\nQMRnetwork (Shwe et al. 1991), is shown in Figure 10.5(b). This was designed to model infectious\ndiseases. The QMR model is a bipartite graph structure, with diseases (causes) at the top and\nsymptoms or \ufb01ndings at the bottom. All nodes are binary. We can write the distribution asfollows:\np(v,h)=/productdisplay\nsp(hs)/productdisplay\ntp(vt|hpa(t)) (10.10)\nwherehsrepresent the hidden nodes (diseases), and vtrepresent the visible nodes (symptoms).\nThe CPD for the root nodes are just Bernoulli distributions, representing the prior probability\nof that disease. Representing the CPDs for the leaves (symptoms) using CPTs would requiretoo many parameters, because the fan-in(number of parents) of many leaf nodes is very\nhigh. A natural alternative is to use logistic regression to model the CPD, p(v\nt=1|hpa(t))=\nsigm(wT\nthpa(t)). (A DGM in which the CPDs are logistic regression distributions is known as a\nsigmoid belief net (Neal 1992).) However, since the parameters of this model were created by\nhand, an alternative CPD, known as the noisy-OR model, was used.\nThe noisy-OR model assumes that if a parent is on, then the child will usually also be on\n(since it is an or-gate), but occasionally the \u201clinks\u201d from parents to child may fail, independentlyat random. In this case, even if the parent is on, the child may be off. To model this moreprecisely, let \u03b8\nst=1\u2212qstbe the probability that the s\u2192tlink fails, so qst=1\u2212\u03b8st=p(vt=", "344": "314 Chapter 10. Directed graphical models (Bayes nets)\nHRBPErrCauter\nHRSATTPRMinVol\nPVSATPAP Pulm  \nEmbolus\nShuntIntubation\nPressDisconnect VentMach\nVentTube\nVentLung\nVentAlv\nArtco2\nBPAnaphy\nLaxis  Hypo  \nVolemia\nPCWPCOLvFailure\n Lved \nVolumeStroke\nVolume\nHistory\nCVPErrlow\nOutput\nHrEKGHRInsuff\nAnesth\nCatecholSAO2\nExpCo2MinVolset\nKinked\n Tube \nFIO2\n(a)\nIOXKHDUW\u0003\nGLVHDVHERWXOLVP\nVH[ )DEGRPHQ\u0003\nSDLQ\u0018\u001a\u0013\u0003GLVHDVHV\n\u0017\u0013\u001a\u0018\u0003V\\PSWRPV:%&\u0003\nFRXQW\n(b)\nFigure 10.5 (a) The alarm network. Figure generated by visualizeAlarmNetwork . (b) The QMR\nnetwork.", "345": "10.2. Examples 315\nGpGmp(X=a)p(X=b)p(X=o)p(X=ab)\naa 1000\nab 000 1\nao 1000\nba 000 1\nbb 0100\nbo 0100\noa 1000\nob 0100\noo 001 0\nTable 10.2 CPT which encodes a mapping from genotype to phenotype (bloodtype). This is a determin-\nistic, but many-to-one, mapping.\n1|hs=1,h\u2212s=0 )is the probability that scan activate ton its own (its \u201ccausal power\u201d). The\nonly way for the child to be off is if all the links from all parents that are on fail independently\nat random. Thus\np(vt=0|h)=/productdisplay\ns\u2208pa(t)\u03b8I(hs=1)\nst (10.11)\nObviously, p(vt=1|h)=1\u2212p(vt=0|h).\nIf we observe that vt=1but all its parents are off, then this contradicts the model. Such\na data case would get probability zero under the model, which is problematic, because it ispossible that someone exhibits a symptom but does not have any of the speci\ufb01ed diseases. Tohandle this, we add a dummy leak node h\n0, which is always on; this represents \u201call other\ncauses\u201d. The parameter q0trepresents the probability that the background leak can cause the\neffect on its own. The modi\ufb01ed CPD becomes p(vt=0|h)=\u03b80t/producttext\ns\u2208pa(t)\u03b8hs\nst. See Table 10.1\nfor a numerical example.\nIf we de\ufb01ne wst/defineslog(\u03b8st), we can rewrite the CPD as\np(vt=1|h)=1\u2212exp/parenleftBigg\nw0t+/summationdisplay\nshswst/parenrightBigg\n(10.12)\nWe see that this is similar to a logistic regression model.\nBipartite models with noisy-OR CPDs are called BN2Omodels. It is relatively easy to set the\n\u03b8stparameters by hand, based on domain expertise. However, it is also possible to learn them\nfrom data (see e.g, (Neal 1992; Meek and Heckerman 1997)). Noisy-OR CPDs have also proveduseful in modeling human causal learning (Griffiths and Tenenbaum 2005), as well as generalbinary classi\ufb01cation settings (Yuille and Zheng 2009).\n10.2.4 Genetic linkage analysis *\nAnother important (and historically very early) application of DGMs is to the problem of genetic\nlinkage analysis. We start with a pedigree graph, which is a DAG that representing the\nrelationship between parents and children, as shown in Figure 10.6(a). We then convert this to aDGM, as we explain below. Finally we perform probabilistic inference in the resulting model.", "346": "316 Chapter 10. Directed graphical models (Bayes nets)\n\u0014 \u0015\n\u0016 \u0017\n\u0018 \u0019\n(a)\n\u0014 \u0015\n\u0016 \u0017\n\u0018 \u0019\u0015\n\u0016 \u0017\n\u0018 \u0019\u0014/RFXV\u0003\u0006\u0003\u0014 /RFXV\u0003\u0006\u0003\u0015\n(b)\nFigure 10.6 Left: family tree, circles are females, squares are males. Individuals with the disease of\ninterest are highlighted. Right: DGM for two loci. Blue nodes Xijis the observed phenotype for individual\niat locusj. All other nodes are hidden. Orange nodes Gp/m\nijis the paternal/ maternal allele. Small\nred nodes zp/m\nijlare the paternal/ maternal selection switching variables. These are linked across loci,\nzm\nij\u2192zm\ni,j+1andzp\nij\u2192zp\ni,j+1. The founder (root) nodes do not have any parents, and hence do no need\nswitching variables. Based on Figure 3 from (Friedman et al. 2000).", "347": "10.2. Examples 317\nIn more detail, for each person (or animal) iand location or locus jalong the genome, we\ncreate three nodes: the observed markerXij(which can be a property such as blood type,\nor just a fragment of DNA that can be measured), and two hidden alleles,Gm\nijandGp\nij, one\ninherited from i\u2019s mother (maternal allele) and the other from i\u2019s father (paternal allele). Together,\nthe ordered pair Gij=(Gm\nij,Gp\nij)constitutes i\u2019s hidden genotype at locusj.\nObviously we must add Gm\nij\u2192XijandGp\nij\u2192Xijarcs representing the fact that genotypes\ncause phenotypes (observed manifestations of genotypes). The CPD p(Xij|Gm\nij,Gp\nij)is called\nthepenetrance model. As a very simple example, suppose Xij\u2208{A,B,O,AB }represents\npersoni\u2019s observed bloodtype, and Gm\nij,Gp\nij\u2208{A,B,O}is their genotype. We can repre-\nsent the penetrance model using the deterministic CPD shown in Table 10.2. For example, A\ndominates O, so if a person has genotype AO or OA, their phenotype will be A.\nIn addition, we add arcs from i\u2019s mother and father into Gij, re\ufb02ecting the Mendelian\ninheritance of genetic material from one\u2019s parents. More precisely, let mi=kbei\u2019s mother.\nThenGm\nijcould either be equal to Gm\nkjorGp\nkj, that is,i\u2019s maternal allele is a copy of one of its\nmother\u2019s two alleles. Let Zm\nijbe a hidden variable than speci\ufb01es the choice. We can model this\nusing the following CPD, known as the inheritance model:\np(Gm\nij|Gmkj,Gp\nkj,Zm\nij)=/braceleftbiggI(Gm\nij=Gm\nkj)ifZm\nij=m\nI(Gm\nij=Gp\nkj)ifZm\nij=p(10.13)\nWe can de\ufb01ne p(Gp\nij|Gm\nkj,Gp\nkj,Zp\nij)similarly, where k=piisi\u2019s father. The values of the Zij\nare said to specify the phaseof the genotype. The values of Gp\ni,j,Gm\ni,j,Zp\ni,jandZm\ni,jconstitute\nthehaplotype of person iat locusj.1\nNext, we need to specify the prior for the root nodes, p(Gmij)andp(Gp\nij). This is called\nthefounder model, and represents the overall prevalence of difference kinds of alleles in the\npopulation. We usually assume independence between the loci for these founder alleles.\nFinally, we need to specify priors for the switch variables that control the inheritance process.\nThese variables are spatially correlated, since adjacent sites on the genome are typically inherited\ntogether (recombination events are rare). We can model this by imposing a two-state Markovchain on the Z\u2019s, where the probability of switching state at locus jis given by \u03b8\nj=1\n2(1\u2212\ne\u22122dj),w h e r edjis the distance between loci jandj+1. This is called the recombination\nmodel.\nThe resulting DGM is shown in Figure 10.6(b): it is a series of replicated pedigree DAGs,\naugmented with switching Zvariables, which are linked using Markov chains. (There is a\nrelated model known as phylogenetic HMM (Siepel and Haussler 2003), which is used to model\nevolution amongst phylogenies.)\nAs a simpli\ufb01ed example of how this model can be used, suppose we only have one locus,\ncorresponding to blood type. For brevity, we will drop the jindex. Suppose we observe xi=A.\nThen there are 3 possible genotypes: Giis(A,A),(A,O)or(O,A). There is ambiguity\nbecause the genotype to phenotype mapping is many-to-one. We want to reverse this mapping.This is known as an inverse problem. Fortunately, we can use the blood types of relatives to\nhelp disambiguate the evidence. Information will \u201c\ufb02ow\u201d from the other x\ni/prime\u2019s up to their Gi/prime\u2019s,\nthen across to i\u2019sGivia the pedigree DAG. Thus we can combine our local evidence p(xi|Gi)\n1. Sometimes the observed marker is equal to the unphased genotype, which is the unordered set {Gp\nij,Gm\nij}; however,\nthe phased or hidden genotype is not directly measurable.", "348": "318 Chapter 10. Directed graphical models (Bayes nets)\nwith an informative prior, p(Gi|x\u2212i), conditioned on the other data, to get a less entropic local\nposterior, p(Gi|x)\u221dp(xi|Gi)p(Gi|x\u2212i).\nIn practice, the model is used to try to determine where along the genome a given disease-\ncausing gene is assumed to lie \u2014 this is the genetic linkage analysis task. The method works as\nfollows. First, suppose all the parameters of the model, including the distance between all themarker loci, are known. The only unknown is the location of the disease-causing gene. If thereareLmarker loci, we construct L+1models: in model /lscript, we postulate that the disease gene\ncomes after marker /lscript,f o r0</lscript<L+1. We can estimate the Markov switching parameter \u02c6\u03b8\n/lscript,\nand hence the distance d/lscriptbetween the disease gene and its nearest known locus. We measure\nthe quality of that model using its likelihood, p(D|\u02c6\u03b8/lscript). We then can then pick the model with\nhighest likelihood (which is equivalent to the MAP model under a uniform prior).\nNote, however, that computing the likelihood requires marginalizing out all the hidden Z\nandGvariables. See (Fishelson and Geiger 2002) and the references therein for some exact\nmethods for this task; these are based on the variable elimination algorithm, which we discussin Section 20.3. Unfortunately, for reasons we explain in Section 20.5, exact methods can becomputationally intractable if the number of individuals and/or loci is large. See (Albers et al.2006) for an approximate method for computing the likelihood; this is based on a form ofvariational inference, which we will discuss in Section 22.4.1.\n10.2.5 Directed Gaussian graphical models *\nConsider a DGM where all the variables are real-valued, and all the CPDs have the followingform:\np(x\nt|xpa(t))=N(xt|\u03bct+wT\ntxpa(t),\u03c32\nt) (10.14)\nThis is called a linear Gaussian CPD. As we show below, multiplying all these CPDs together\nresults in a large joint Gaussian distribution of the form p(x)=N(x|\u03bc,\u03a3). This is called a\ndirected GGM, or a Gaussian Bayes net.\nWe now explain how to derive \u03bcand\u03a3from the CPD parameters, following (Shachter and\nKenley 1989, App. B). For convenience, we will rewrite the CPDs in the following form:\nxt=\u03bct+/summationdisplay\ns\u2208pa(t)wts(xs\u2212\u03bcs)+\u03c3tzt (10.15)\nwherezt\u223cN(0,1),\u03c3tis the conditional standard deviation of xtgiven its parents, wtsis the\nstrength of the s\u2192tedge, and \u03bctis the local mean.2\nIt is easy to see that the global mean is just the concatenation of the local means, \u03bc=\n(\u03bc1,...,\u03bc D). We now derive the global covariance, \u03a3.L e tS/definesdiag(\u03c3)be a diagonal matrix\ncontaining the standard deviations. We can rewrite Equation 10.15 in matrix-vector form asfollows:\n(x\u2212\u03bc)=W(x\u2212\u03bc)+Sz (10.16)\n2. If we do not subtract off the parent\u2019s mean (i.e., if we use xt=\u03bct+/summationtext\ns\u2208pa(t)wtsxs+\u03c3tzt), the derivation of \u03a3\nis much messier, as can be seen by looking at (Bishop 2006b, p370).", "349": "10.3. Inference 319\nNow letebe a vector of noise terms:\ne/definesSz (10.17)\nWe can rearrange this to get\ne=(I\u2212W)(x\u2212\u03bc) (10.18)\nSinceWis lower triangular (because wts=0ift>sin the topological ordering), we have that\nI\u2212Wis lower triangular with 1s on the diagonal. Hence\n\u239b\n\u239c\u239c\u239c\u239de\n1\ne2\n...\ned\u239e\n\u239f\u239f\u239f\u23a0=\u239b\n\u239c\u239c\u239c\u239c\u239c\u239d1\n\u2212w\n211\n\u2212w32\u2212w311\n......\n\u2212wd1\u2212wd2...\u2212wd,d\u221211\u239e\n\u239f\u239f\u239f\u239f\u239f\u23a0\u239b\n\u239c\u239c\u239c\u239dx\n1\u2212\u03bc1\nx2\u2212\u03bc2\n...\nxd\u2212\u03bcd\u239e\n\u239f\u239f\u239f\u23a0(10.19)\nSinceI\u2212Wis always invertible, we can write\nx\u2212\u03bc=(I\u2212W)\u22121e/definesUe=USz (10.20)\nwhere we de\ufb01ned U=(I\u2212W)\u22121. Thus the regression weights correspond to a Cholesky\ndecomposition of \u03a3,a sw en o ws h o w :\n\u03a3=c o v [x]=c o v[x\u2212\u03bc] (10.21)\n=c o v [USz]=UScov[z]SUT=US2UT(10.22)\n10.3 Inference\nWe have seen that graphical models provide a compact way to de\ufb01ne joint probability distribu-\ntions. Given such a joint distribution, what can we do with it? The main use for such a jointdistribution is to perform probabilistic inference. This refers to the task of estimating unknown\nquantities from known quantities. For example, in Section 10.2.2, we introduced HMMs, andsaid that one of the goals is to estimate the hidden states (e.g., words) from the observations(e.g., speech signal). And in Section 10.2.4, we discussed genetic linkage analysis, and said thatone of the goals is to estimate the likelihood of the data under various DAGs, corresponding todifferent hypotheses about the location of the disease-causing gene.\nIn general, we can pose the inference problem as follows. Suppose we have a set of correlated\nrandom variables with joint distribution p(x\n1:V|\u03b8). (In this section, we are assuming the\nparameters \u03b8of the model are known. We discuss how to learn the parameters in Section 10.4.)\nLet us partition this vector into the visible variables xv, which are observed, and the hidden\nvariables, xh, which are unobserved. Inference refers to computing the posterior distribution\nof the unknowns given the knowns:\np(xh|xv,\u03b8)=p(xh,xv|\u03b8)\np(xv|\u03b8)=p(xh,xv|\u03b8)/summationtext\nx/prime\nhp(x/prime\nh,xv|\u03b8)(10.23)\nEssentially we are conditioning on the data by clamping the visible variables to their observed\nvalues,xv, and then normalizing, to go from p(xh,xv)top(xh|xv). The normalization constant\np(xv|\u03b8)is the likelihood of the data, also called the probability of the evidence.", "350": "320 Chapter 10. Directed graphical models (Bayes nets)\nSometimes only some of the hidden variables are of interest to us. So let us partition the\nhidden variables into query variables, xq, whose value we wish to know, and the remaining\nnuisance variables, xn, which we are not interested in. We can compute what we are interested\nin bymarginalizing out the nuisance variables:\np(xq|xv,\u03b8)=/summationdisplay\nxnp(xq,xn|xv,\u03b8) (10.24)\nIn Section 4.3.1, we saw how to perform all these operations for a multivariate Gaussian in\nO(V3)time, where Vis the number of variables. What if we have discrete random variables,\nwith say Kstates each? If the joint distribution is represented as a multi-dimensional table,\nwe can always perform these operations exactly, but this will take O(KV)time. In Chapter 20,\nwe explain how to exploit the factorization encoded by the GM to perform these operations inO(VK\nw+1)time, where wis a quantity known as the treewidth of the graph. This measures\nhow \u201ctree-like\u201d the graph is. If the graph is a tree (or a chain), we have w=1, so for these\nmodels, inference takes time linear in the number of nodes. Unfortunately, for more generalgraphs, exact inference can take time exponential in the number of nodes, as we explain inSection 20.5. We will therefore examine various approximate inference schemes later in thebook.\n10.4 Learning\nIn the graphical models literature, it is common to distinguish between inference and learning.Inference means computing (functions of) p(x\nh|xv,\u03b8),w h e r evare the visible nodes, hare the\nhidden nodes, and \u03b8are the parameters of the model, assumed to be known. Learning usually\nmeans computing a MAP estimate of the parameters given data:\n\u02c6\u03b8=a r g m a x\n\u03b8N/summationdisplay\ni=1logp(xi,v|\u03b8)+logp(\u03b8) (10.25)\nwherexi,vare the visible variables in case i. If we have a uniform prior, p(\u03b8)\u221d1, this reduces\nto the MLE, as usual.\nIf we adopt a Bayesian view, the parameters are unknown variables and should also be\ninferred. Thus to a Bayesian, there is no distinction between inference and learning. In fact, wecan just add the parameters as nodes to the graph, condition on D, and then infer the values\nof all the nodes. (We discuss this in more detail below.)\nIn this view, the main difference between hidden variables and parameters is that the number\nof hidden variables grows with the amount of training data (since there is usually a set of hiddenvariables for each observed data case), whereas the number of parameters in usually \ufb01xed (atleast in a parametric model). This means that we must integrate out the hidden variables to avoidover\ufb01tting, but we may be able to get away with point estimation techniques for parameters,which are fewer in number.\n10.4.1 Plate notation\nWhen inferring parameters from data, we often assume the data is iid. We can represent thisassumption explicitly using a graphical model, as shown in Figure 10.7(a). This illustrates the", "351": "10.4. Learning 321\n\u03b8\nX1XN\nN\u03b8\nXi\nFigure 10.7 Left: data points xiare conditionally independent given \u03b8. Right: Plate notation. This\nrepresents the same model as the one on the left, except the repeated xinodes are inside a box, known as\na plate; the number in the lower right hand corner, N, speci\ufb01es the number of repetitions of the Xinode.\nassumption that each data case was generated independently but from the same distribution.\nNotice that the data cases are only independent conditional on the parameters \u03b8; marginally,\nthe data cases are dependent. Nevertheless, we can see that, in this example, the order in whichthe data cases arrive makes no difference to our beliefs about \u03b8, since all orderings will have\nthe same sufficient statistics. Hence we say the data is exchangeable.\nTo avoid visual clutter, it is common to use a form of syntactic sugar calledplates:w e\nsimply draw a little box around the repeated variables, with the convention that nodes withinthe box will get repeated when the model is unrolled. We often write the number of copies or\nrepetitions in the bottom right corner of the box. See Figure 10.7(b) for a simple example. Thecorresponding joint distribution has the form\np(\u03b8,D)=p(\u03b8)/bracketleftBigg\nN/productdisplay\ni=1p(xi|\u03b8)/bracketrightBigg\n(10.26)\nThis DGM represents the CI assumptions behind the models we considered in Chapter 5.\nA slightly more complex example is shown in Figure 10.8. On the left we show a naive Bayes\nclassi\ufb01er that has been \u201cunrolled\u201d for Dfeatures, but uses a plate to represent repetition over\ncasesi=1:N. The version on the right shows the same model using nested plate notation.\nWhen a variable is inside two plates, it will have two sub-indices. For example, we write \u03b8jc\nto represent the parameter for feature jin class-conditional density c. Note that plates can\nbe nested or crossing. Notational devices for modeling more complex parameter tying patternscan be devised (e.g., (Heckerman et al. 2004)), but these are not widely used. What is not clearfrom the \ufb01gure is that \u03b8\njcis used to generate xijiffyi=c, otherwise it is ignored. This is an\nexample of context speci\ufb01c independence, since the CI relationship xij\u22a5\u03b8jconly holds if\nyi/negationslash=c.", "352": "322 Chapter 10. Directed graphical models (Bayes nets)\n\u03b8c1 ... \u03b8cD\nCXi1 ... XiDYi\nN\u03c0\n(a)DCN\n\u03b8jcXijYi\u03c0\n(b)\nFigure 10.8 Naive Bayes classi\ufb01er as a DGM. (a) With single plates. (b) WIth nested plates.\n10.4.2 Learning from complete data\nIf all the variables are fully observed in each case, so there is no missing data and there are no\nhidden variables, we say the data is complete. For a DGM with complete data, the likelihood is\ngiven by\np(D|\u03b8)=N/productdisplay\ni=1p(xi|\u03b8)=N/productdisplay\ni=1V/productdisplay\nt=1p(xit|xi,pa(t),\u03b8t)=V/productdisplay\nt=1p(Dt|\u03b8t) (10.27)\nwhereDtis the data associated with node tand its parents, i.e., the t\u2019th family. This is a\nproduct of terms, one per CPD. We say that the likelihood decomposes according to the graph\nstructure.\nNow suppose that the prior factorizes as well:\np(\u03b8)=V/productdisplay\nt=1p(\u03b8t) (10.28)\nThen clearly the posterior also factorizes:\np(\u03b8|D)\u221dp(D|\u03b8)p(\u03b8)=V/productdisplay\nt=1p(Dt|\u03b8t)p(\u03b8t) (10.29)\nThis means we can compute the posterior of each CPD independently. In other words,\nfactored prior plus factored likelihood implies factored posterior (10.30)\nLet us consider an example, where all CPDs are tabular, thus extending the earlier results of\nSecion 3.5.1.2, where discussed Bayesian naive Bayes. We have a separate row (i.e., a separatemultinoulli distribution) for each conditioning case, i.e., for each combination of parent values,\nas in Table 10.2. Formally, we can write the t\u2019th CPT as x\nt|xpa(t)=c\u223cCat(\u03b8tc),w h e r e\n\u03b8tck/definesp(xt=k|xpa(t)=c),f o rk=1:Kt,c=1:Ctandt=1:T.H e r eKtis the number", "353": "10.4. Learning 323\nof states for node t,Ct/defines/producttext\ns\u2208pa(t)Ksis the number of parent combinations, and Tis the\nnumber of nodes. Obviously/summationtext\nk\u03b8tck=1for each row of each CPT.\nLet us put a separate Dirichlet prior on each row of each CPT, i.e., \u03b8tc\u223cDir(\u03b1tc). Then we\ncan compute the posterior by simply adding the pseudo counts to the empirical counts to get\n\u03b8tc|D \u223cDir(N tc+\u03b1tc),w h e r eN tckis the number of times that node tis in state kwhile its\nparents are in state c:\nNtck/definesN/summationdisplay\ni=1I(xi,t=k,xi,pa(t)=c) (10.31)\nFrom Equation 2.77, the mean of this distribution is given by the following:\n\u03b8tck=Ntck+\u03b1tck/summationtext\nk/prime(Ntck/prime+\u03b1tck/prime)(10.32)\nFor example, consider the DGM in Figure 10.1(a). Suppose the training data consists of the\nfollowing 5 cases:\nx1x2x3x4x5\n0010001111\n11010\n0110001110\nBelow we list all the sufficient statistics N\ntck, and the posterior mean parameters \u03b8ickunder\na Dirichlet prior with \u03b1ick=1(corresponding to add-one smoothing) for the t=4node:\nx2x3Ntck=1Ntck=0\u03b8tck=1\u03b8tck=0\n00 00 1/2 1/2\n10 10 2/3 1/3\n01 01 1/3 2/3\n11 21 3/5 2/5\nIt is easy to show that the MLE has the same form as Equation 10.32, except without the \u03b1tck\nterms, i.e.,\n\u02c6\u03b8tck=Ntck/summationtext\nk/primeNtck/prime(10.33)\nOf course, the MLE suffers from the zero-count problem discussed in Section 3.3.4.1, so it isimportant to use a prior to regularize the estimation problem.\n10.4.3 Learning with missing and/or latent variables\nIf we have missing data and/or hidden variables, the likelihood no longer factorizes, and indeedit is no longer convex, as we explain in detail in Section 11.3. This means we will usually canonly compute a locally optimal ML or MAP estimate. Bayesian inference of the parameters iseven harder. We discuss suitable approximate inference techniques in later chapters.", "354": "324 Chapter 10. Directed graphical models (Bayes nets)\n10.5 Conditional independence properties of DGMs\nAt the heart of any graphical model is a set of conditional indepence (CI) assumptions. We write\nxA\u22a5GxB|xCifAis independent of BgivenCin the graph G, using the semantics to be\nde\ufb01ned below. Let I(G)be the set of all such CI statements encoded by the graph.\nWe say that Gis anI-map(independence map) for p, or that pisMarkovwrtG,i ff\nI(G)\u2286I(p),w h e r eI(p)is the set of all CI statements that hold for distribution p. In other\nwords, the graph is an I-map if it does not make any assertions of CI that are not true of thedistribution. This allows us to use the graph as a safe proxy for pwhen reasoning about p\u2019s CI\nproperties. This is helpful for designing algorithms that work for large classes of distributions,regardless of their speci\ufb01c numerical parameters \u03b8.\nNote that the fully connected graph is an I-map of all distributions, since it makes no CI\nassertions at all (since it is not missing any edges). We therefore say Gis aminimal I-map of\npifGis an I-map of p, and if there is no G\n/prime\u2286Gwhich is an I-map of p.\nIt remains to specify how to determine if xA\u22a5GxB|xC. Deriving these independencies\nfor undirected graphs is easy (see Section 19.2), but the DAG situation is somewhat complicated,because of the need to respect the orientation of the directed edges. We give the details below.\n10.5.1 d-separation and the Bayes Ball algorithm (global Markov properties)\nFirst, we introduce some de\ufb01nitions. We say an undirected path Pisd-separated b yas e to f\nnodesE(containing the evidence) iff at least one of the following conditions hold:\n1. P contains a chain, s\u2192m\u2192tors\u2190m\u2190t,w h e r em \u2208E\n2. P contains a tent or fork, s/arrowsouthwestm/arrowsoutheastt,w h e r em \u2208E\n3. P contains a colliderorv-structure, s/arrowsoutheastm/arrowsouthwestt,w h e r e mis not in Eand nor is any\ndescendant of m.\nNext, we say that a set of nodes Ais d-separated from a different set of nodes Bgiven a\nthird observed set Eiff each undirected path from every node a\u2208Ato every node b\u2208Bis\nd-separated by E. Finally, we de\ufb01ne the CI properties of a DAG as follows:\nxA\u22a5GxB|xE\u21d0\u21d2A is d-separated from B given E (10.34)\nTheBayes ball algorithm (Shachter 1998) is a simple way to see if Ais d-separated from B\ngivenE, based on the above de\ufb01nition. The idea is this. We \u201cshade\u201d all nodes in E, indicating\nthat they are observed. We then place \u201cballs\u201d at each node in A, let them \u201cbounce around\u201d\naccording to some rules, and then ask if any of the balls reach any of the nodes in B. The three\nmain rules are shown in Figure 10.9. Notice that balls can travel opposite to edge directions.We see that a ball can pass through a chain, but not if it is shaded in the middle. Similarly, aball can pass through a fork, but not if it is shaded in the middle. However, a ball cannot passthrough a v-structure, unless it is shaded in the middle.\nWe can justify the 3 rules of Bayes ball as follows. First consider a chain structure X\u2192Y\u2192\nZ, which encodes\np(x,y,z)=p(x)p(y|x)p(z|y) (10.35)", "355": "10.5. Conditional independence properties of DGMs 325\nXYZ\n(a)XY\nZ\n(b)\nX\nYZ\n(c)XYZ\n(d)\nXY\nZ\n(e)X\nYZ\n(f)\nFigure 10.9 Bayes ball rules. A shaded node is one we condition on. If there is an arrow hitting a bar, it\nmeans the ball cannot pass through; otherwise the ball can pass through. Based on (Jordan 2007).\nWhen we condition on y,a r exandzindependent? We have\np(x,z|y)=p(x)p(y|x)p(z|y)\np(y)=p(x,y)p(z|y)\np(y)=p(x|y)p(z|y) (10.36)\nand therefore x\u22a5z|y. So observing the middle node of chain breaks it in two (as in a Markov\nchain).\nNow consider the tent structure X\u2190Y\u2192Z. The joint is\np(x,y,z)=p(y)p(x|y)p(z|y) (10.37)", "356": "326 Chapter 10. Directed graphical models (Bayes nets)\nxy\n(a)xy\n(b)y/primeyxz\n(c)\nFigure 10.10 (a-b) Bayes ball boundary conditions. (c) Example of why we need boundary conditions. y/prime\nis an observed child of y, rendering y\u201ceffectively observed\u201d, so the ball bounces back up on its way from\nxtoz.\nWhen we condition on y,a r exandzindependent? We have\np(x,z|y)=p(x,y,z)\np(y)=p(y)p(x|y)p(z|y)\np(y)=p(x|y)p(z|y) (10.38)\nand therefore x\u22a5z|y. So observing a root node separates its children (as in a naive Bayes\nclassi\ufb01er: see Section 3.5).\nFinally consider a v-structure X\u2192Y\u2190Z. The joint is\np(x,y,z)=p(x)p(z)p(y|x,z) (10.39)\nWhen we condition on y,a r exandzindependent? We have\np(x,z|y)=p(x)p(z)p(y|x,z)\np(y)(10.40)\nsox/negationslash\u22a5z|y. However, in the unconditional distribution, we have\np(x,z)=p(x)p(z) (10.41)\nso we see that xandzare marginally independent. So we see that conditioning on a common\nchild at the bottom of a v-structure makes its parents become dependent. This important effect\nis calledexplaining away, inter-causal reasoning,o r Berkson\u2019s paradox. As an example of\nexplaining away, suppose we toss two coins, representing the binary numbers 0 and 1, and weobserve the \u201csum\u201d of their values. A priori, the coins are independent, but once we observetheir sum, they become coupled (e.g., if the sum is 1, and the \ufb01rst coin is 0, then we know thesecond coin is 1).\nFinally, Bayes Ball also needs the \u201cboundary conditions\u201d shown in Figure 10.10(a-b). To\nunderstand where these rules come from, consider Figure 10.10(c). Suppose Y\n/primeis a noise-free\ncopy ofY. Then if we observe Y/prime, we effectively observe Yas well, so the parents XandZ\nhave to compete to explain this. So if we send a ball down X\u2192Y\u2192Y/prime, it should \u201cbounce\nback\u201d up along Y/prime\u2192Y\u2192Z. However, if Yand all its children are hidden, the ball does not\nbounce back.", "357": "10.5. Conditional independence properties of DGMs 327\n12\n354\n67\nFigure 10.11 A DGM.\nFor example, in Figure 10.11, we see that x2\u22a5x6|x5, since the 2\u21925\u21926path is blocked\nbyx5(which is observed), the 2\u21924\u21927\u21926path is blocked by x7(which is hidden), and\nthe2\u21921\u21923\u21926path is blocked by x1(which is hidden). However, we also see that\nx2/negationslash\u22a5x6|x5,x7, since now the 2\u21924\u21927\u21926path is no longer blocked by x7(which is\nobserved). Exercise 10.2 gives you some more practice in determining CI relationships for DGMs.\n10.5.2 Other Markov properties of DGMs\nFrom the d-separation criterion, one can conclude that\nt\u22a5nd(t)\\pa(t)|pa(t) (10.42)\nwhere the non-descendants of a node nd(t)are all the nodes except for its descendants,\nnd(t)=V\\{ t\u222adesc(t)}. Equation 10.42 is called the directed local Markov property.F o r\nexample, in Figure 10.11, we have nd(3) ={2,4}, andpa(3) = 1,s o3 \u22a52,4|1.\nA special case of this property is when we only look at predecessors of a node according to\nsome topological ordering. We have\nt\u22a5pred(t)\\pa(t)|pa(t) (10.43)\nwhich follows since pred(t)\u2286nd(t). This is called the ordered Markov property, which\njusti\ufb01es Equation 10.7. For example, in Figure 10.11, if we use the ordering 1,2,...,7. we \ufb01nd\npred(3) = {1,2}andpa(3) = 1,s o3 \u22a52|1.\nWe\nhave now described three Markov properties for DAGs: the directed global Markov property\nG in Equation 10.34, the ordered Markov property O in Equation 10.43, and the directed local\nMarkov property L in Equation 10.42. It is obvious that G=\u21d2L=\u21d2O. What is less\nobvious, but nevertheless true, is that O=\u21d2L=\u21d2G(see e.g., (Koller and Friedman 2009)\nfor the proof). Hence all these properties are equivalent.\nFurthermore, any distribution pthat is Markov wrt Gcan be factorized as in Equation 10.7;\nthis is called the factorization property F. It is obvious that O=\u21d2F, but one can show that\nthe converse also holds (see e.g., (Koller and Friedman 2009) for the proof).\n10.5.3 Markov blanket and full conditionals\nThe set of nodes that renders a node tconditionally independent of all the other nodes in\nthe graph is called t\u2019sMarkov blanket; we will denote this by mb(t). One can show that the\nMarkov blanket of a node in a DGM is equal to the parents, the children, and the co-parents,", "358": "328 Chapter 10. Directed graphical models (Bayes nets)\ni.e., other nodes who are also parents of its children:\nmb(t)/definesch(t)\u222apa(t)\u222acopa(t) (10.44)\nFor example, in Figure 10.11, we have\nmb(5) = {6,7}\u222a{2,3}\u222a{4}={2,3,4,6,7} (10.45)\nwhere 4 is a co-parent of 5 because they share a common child, namely 7.\nTo see why the co-parents are in the Markov blanket, note that when we derive p(xt|x\u2212t)=\np(xt,x\u2212t)/p(x\u2212t), all the terms that do not involve xtwill cancel out between numerator and\ndenominator, so we are left with a product of CPDs which contain xtin theirscope. Hence\np(xt|x\u2212t)\u221dp(xt|xpa(t))/productdisplay\ns\u2208ch(t)p(xs|xpa(s)) (10.46)\nFor example, in Figure 10.11 we have\np(x5|x\u22125)\u221dp(x5|x2,x3)p(x6|x3,x5)p(x7|x4,x5,x6) (10.47)\nThe resulting expression is called t\u2019sfull conditional, and will prove to be important when we\nstudy Gibbs sampling (Section 24.2).\n10.6 In\ufb02uence (decision) diagrams *\nWe can represent multi-stage (Bayesian) decision problems by using a graphical notation known\nas adecision diagram or anin\ufb02uence diagram (Howard and Matheson 1981; Kjaerulff and\nMadsen 2008). This extends directed graphical models by adding decision nodes (also called ac-\ntion nodes), represented by rectangles, and utility nodes (also called value nodes), represented\nby diamonds. The original random variables are called chance nodes, and are represented by\novals, as usual.\nFigure 10.12(a) gives a simple example, illustrating the famous oil wild-catter problem.3In\nthis problem, you have to decide whether to drill an oil well or not. You have two possibleactions:d=1means drill, d=0means don\u2019t drill. You assume there are 3 states of nature:\no=0means the well is dry, o=1means it is wet (has some oil), and o=2means it is\nsoaking (has a lot of oil). Suppose your prior beliefs are p(o)=[ 0.5,0.3,0.2]. Finally, you must\nspecify the utility function U(d,o). Since the states and actions are discrete, we can represent\nit as a table (analogous to a CPT in a DGM). Suppose we use the following numbers, in dollars:\no=0o=1o=2\nd=0000\nd=1-70 50 200\nWe see that if you don\u2019t drill, you incur no costs, but also make no money. If you drill a dry\nwell, you lose $70; if you drill a wet well, you gain $50; and if you drill a soaking well, you gain$200. Your prior expected utility if you drill is given by\nEU(d=1 )= 2/summationdisplay\no=0p(o)U(d,o)=0.5\u00b7(\u221270)+0.3\u00b750+0.2\u00b7200 = 20 (10.48)\n3. This example is originally from (Raiffa 1968). Our presentation is based on some notes by Daphne Koller.", "359": "10.6. In\ufb02uence (decision) diagrams * 329\nOil\nUtilityDrill\n(a)Oil\nUtilityDrillSound\n(b)\nOil\nUtilityDrillSoundTest\nCost\n(c)\nFigure 10.12 (a) In\ufb02uence diagram for basic oil wild catter problem. (b) An extension in which we have\nan information arc from the Sound chance node to the Drill decision node. (c) An extension in which we\nget to decide whether to perform the test or not.\nYour expected utility if you don\u2019t drill is 0. So your maximum expected utility is\nMEU=m a x{EU(d=0 ),EU(d=1 )}=m a x{0,20}=2 0 (10.49)\nand therefore the optimal action is to drill:\nd\u2217=a r gm a x {EU(d=0 ),EU(d=1 )}=1 (10.50)\nNow let us consider a slight extension to the model. Suppose you perform a sounding to\nestimate the state of the well. The sounding observation can be in one of 3 states: s=0is\na diffuse re\ufb02ection pattern, suggesting no oil; s=1is an open re\ufb02ection pattern, suggesting\nsome oil; and s=2is a closed re\ufb02ection pattern, indicating lots of oil. Since Sis caused by O,\nwe add an O\u2192Sarc to our model. In addition, we assume that the outcome of the sounding\ntest will be available before we decide whether to drill or not; hence we add an information\narcfromStoD. This is illustrated in Figure 10.12(b).\nLet us model the reliability of our sensor using the following conditional distribution for\np(s|o):", "360": "330 Chapter 10. Directed graphical models (Bayes nets)\ns=0s=1s=2\no=00.6 0.3 0.1\no=10.3 0.4 0.3\no=20.1 0.4 0.5\nSuppose we do the sounding test and we observe s=0. The posterior over the oil state is\np(o|s=0 )=[ 0 .732,0.219,0.049] (10.51)\nNow your posterior expected utility of performing action dis\nEU(d|s=0 )=2/summationdisplay\no=0p(o|s=0 )U(o,d) (10.52)\nIfd=1, this gives\nEU(d=1|s=0 )=0 .732\u00d7(\u221270)+0.219\u00d750+0.049\u00d7200 =\u221230.5 (10.53)\nHowever, if d=0, thenEU(d=0|s=0 )=0 , since not drilling incurs no cost. So if we\nobserves=0, we are better off not drilling, which makes sense.\nNow suppose we do the sounding test and we observe s=1. By similar reasoning, one\ncan show that EU(d=1|s=1 )=3 2 .9, which is higher than EU(d=0|s=1 )=0 .\nSimilarly, if we observe s=2,w eh a v e EU(d=1|s=2 )=8 7 .5which is much higher\nthanEU(d=0|s=2 )=0 .Hence the optimal policy d\u2217(s)is as follows: if s=0, choose\nd\u2217(0) = 0and get $0; if s=1, choosed\u2217(1) = 1and get $32.9; and if s=2, choosed\u2217(2) = 1\nand get $87.5.\nYou can compute your expected pro\ufb01t or maximum expected utility as follows:\nMEU=/summationdisplay\nsp(s)EU(d\u2217(s)|s) (10.54)\nThis is the expected utility given possible outcomes of the sounding test, assuming you act\noptimally given the outcome. The prior marginal on the outcome of the test is\np(s)=/summationdisplay\nop(o)p(s|o)=[ 0.41,0.35,0.24] (10.55)\nHence your maximum expected utility is\nMEU=0.41\u00d70+0.35\u00d732.9+0.24\u00d787.5=3 2.2 (10.56)\nNow suppose you can choose whether to do the test or not. This can be modelled as shown\nin Figure 10.12(c), where we add a new test node T.I fT=1, we do the test, and Scan enter 1\nof 3 states, determined by O, exactly as above. If T=0, we don\u2019t do the test, and Senters a\nspecial unknown state. There is also some cost associated with performing the test.\nIs it worth doing the test? This depends on how much our MEU changes if we know the\noutcome of the test (namely the state of S). If you don\u2019t do the test, we have MEU=2 0\nfrom Equation 10.49. If you do the test, you have MEU=3 2.2from Equation 10.56. So the\nimprovement in utility if you do the test (and act optimally on its outcome) is $12.2. This is", "361": "10.6. In\ufb02uence (decision) diagrams * 331\nat\nzt zt+1xt xt+1\nRt\n(a)at\nxt xt+1\nRt\n(b)\nFigure 10.13 (a) A POMDP, shown as an in\ufb02uence diagram. ztare hidden world states. We implicitly\nmake the no forgetting assumption, which effectively means that athas arrows coming into it from all\nprevious observations, x1:t. (b) An MDP, shown as an in\ufb02uence diagram.\ncalled the value of perfect information (VPI). So we should do the test as long as it costs less\nthan $12.2.\nIn terms of graphical models, the VPI of a variable Tcan be determined by computing the\nMEU for the base in\ufb02uence diagram, I, and then computing the MEU for the same in\ufb02uence\ndiagram where we add information arcs from Tto the action nodes, and then computing the\ndifference. In other words,\nVPI=MEU(I+T\u2192D)\u2212MEU(I) (10.57)\nwhereDis the decision node and Tis the variable we are measuring.\nIt is possible to modify the variable elimination algorithm (Section 20.3) so that it computes\nthe optimal policy given an in\ufb02uence diagram. These methods essentially work backwards from\nthe \ufb01nal time-step, computing the optimal decision at each step assuming all following actionsare chosen optimally. See e.g., (Lauritzen and Nilsson 2001; Kjaerulff and Madsen 2008) fordetails.\nWe could continue to extend the model in various ways. For example, we could imagine a\ndynamical system in which we test, observe outcomes, perform actions, move on to the nextoil well, and continue drilling (and polluting) in this way. In fact, many problems in robotics,business, medicine, public policy, etc. can be usefully formulated as in\ufb02uence diagrams unrolledover time (Raiffa 1968; Lauritzen and Nilsson 2001; Kjaerulff and Madsen 2008).\nA generic model of this form is shown in Figure 10.13(a). This is known as a partially\nobserved Markov decision process orPOMDP(pronounced \u201cpom-d-p\u201d). This is basically a\nhidden Markov model (Section 17.3) augmented with action and reward nodes. This can be usedto model the perception-action cycle that all intelligent agents use (see e.g., (Kaelbling et al.\n1998) for details).\nA special case of a POMDP, in which the states are fully observed, is called a Markov decision\nprocessorMDP, shown in Figure 10.13(b). This is much easier to solve, since we only have\nto compute a mapping from observed states to actions. This can be solved using dynamicprogramming (see e.g., (Sutton and Barto 1998) for details).\nIn the POMDP case, the information arc from x\nttoatis not sufficient to uniquely determine", "362": "332 Chapter 10. Directed graphical models (Bayes nets)\n(a)AB CE FI DG H\n(b)JGA D\nH IE FB C\n(c)\nFigure 10.14 Some DGMs.\nthe best action, since the state is not fully observed. Instead, we need to choose actions based\non ourbelief state ,p(zt|x1:t,a1:t). Since the belief updating process is deterministic (see\nSection 17.4.2), we can compute a belief state MDP . For details on to compute the policies for\nsuch models, see e.g., (Kaelbling et al. 1998; Spaan and Vlassis 2005).\nExercises\nExercise 10.1 Marginalizing a node in a DGM\n(Source: Koller.)\nConsider the DAG Gin Figure 10.14(a). Assume it is a minimal I-map for p(A,B,C,D,E,F,X ).N o w\nconsider marginalizing out X. Construct a new DAG G/primewhich is a minimal I-map for p(A,B,C,D,E,F ).\nSpecify (and justify) which extra edges need to be added.\nExercise 10.2 Bayes Ball\n(Source: Jordan.)\nHere we compute some global independence statements from some directed graphical models. You can\nuse the \u201cBayes ball\u201d algorithm, the d-separation criterion, or the method of converting to an undirected\ngraph (all should give the same results).\na. Consider the DAG in Figure 10.14(b). List all variables that are independent of Agiven evidence on B.\nb. Consider the DAG in Figure 10.14(c). List all variables that are independent of Agiven evidence on J.\nExercise 10.3 Markov blanket for a DGM\nProve that the full conditional for node iin a DGM is given by\np(Xi|X\u2212i)\u221dp(Xi|Pa(Xi))/productdisplay\nYj\u2208ch(Xi)p(Yj|Pa(Yj)) (10.58)\nwherech(Xi)are the children of XiandPa(Yj)are the parents of Yj.\nExercise 10.4 Hidden variables in DGMs\nConsider the DGMs in Figure 11.1 which both de\ufb01ne p(X1:6), where we number empty nodes left to right,\ntop to bottom. The graph on the left de\ufb01nes the joint as\np(X1:6)=/summationdisplay\nhp(X1)p(X2)p(X3)p(H=h|X1:3)p(X4|H=h)p(X5|H=h)p(X6|H=h)(10.59)", "363": "10.6. In\ufb02uence (decision) diagrams * 333\n(a)\n (b)\nFigure 10.15 (a) Weather BN. (b) Fishing BN.\nwhere we have marginalized over the hidden variable H. The graph on the right de\ufb01nes the joint as\np(X1:6)=p(X1)p(X2)p(X3)p(X4|X1:3)p(X5|X1:4)p(X6|X1:5) (10.60)\na. (5 points) Assuming all nodes (including H) are binary and all CPDs are tabular, prove that the model\non the left has 17 free parameters.\nb. (5 points) Assuming all nodes are binary and all CPDs are tabular, prove that the model on the right\nhas 59 free parameters.\nc. (5 points) Suppose we have a data set D=Xn\n1:6forn=1:N, where we observe the Xs but not H,\nand we want to estimate the parameters of the CPDs using maximum likelihood. For which model is\nthis easier? Explain your answer.\nExercise 10.5 B a y e sn e t sf o rar a i n yd a y\n(Source: Nando de Freitas.). In this question you must model a problem with 4 binary variables: G=\u201dgray\u201d,\nV=\u201dVancouver\u201d, R=\u201drain\u201d and S=\u201dsad\u201d. Consider the directed graphical model describing the relation-\nship between these variables shown in Figure 10.15(a).\na. Write down an expression for P(S=1|V=1 )in terms of \u03b1,\u03b2,\u03b3,\u03b4.\nb. Write down an expression for P(S=1|V=0 ). Is this the same or different to P(S=1|V=1 )?\nExplain why.\nc. Find maximum likelihood estimates of \u03b1,\u03b2,\u03b3using the following data set, where each row is a training\ncase. (You may state your answers without proof.)\nVGRS\n1111\n1101\n1000(10.61)\nExercise 10.6 Fishing nets\n(Source: (Duda et al. 2001)..) Consider the Bayes net shown in Figure 10.15(b). Here, the nodes represent\nthe following variables\nX1\u2208{winter, spring, summer, autumn },X2\u2208{salmon, sea bass } (10.62)\nX3\u2208{light, medium, dark },X4\u2208{wide, thin } (10.63)", "364": "334 Chapter 10. Directed graphical models (Bayes nets)\nX1 X2 X3 X4 X5Z1 Z2 Z3\n(a)X1 X2 X4Z1 Z2 Z3\n(b)\nFigure 10.16 (a) A QMR-style network with some hidden leaves. (b) Removing the barren nodes.\nThe corresponding conditional probability tables are\np(x1)=/parenleftbig\n.25.25.25.25/parenrightbig\n,p(x2|x1)=\u239b\n\u239c\u239c\u239d.9.1\n.3.7\n.4.6\n.8.2\u239e\n\u239f\u239f\u23a0(10.64)\np(x3|x2)=/parenleftbigg.33.33.34\n.8.1.1/parenrightbigg\n,p(x4|x2)=/parenleftbigg.4.6\n.95.05/parenrightbigg\n(10.65)\nNote that in p(x4|x2), the rows represent x2and the columns x4(so each row sums to one and represents\nthe child of the CPD). Thus p(x4=thin|x2=sea bass)=0 .05,p(x4=thin|x2=salmon)=0 .6, etc.\nAnswer the following queries. You may use matlab or do it by hand. In either case, show your work.\na. Suppose the \ufb01sh was caught on December 20 \u2014 the end of autumn and the beginning of winter \u2014\nand thus let p(x1)=(.5,0,0,.5)instead of the above prior. (This is called soft evidence, since we\ndo not know the exact value of X1, but we have a distribution over it.) Suppose the lightness has not\nbeen measured but it is known that the \ufb01sh is thin. Classify the \ufb01sh as salmon or sea bass.\nb. Suppose all we know is that the \ufb01sh is thin and medium lightness. What season is it now, most likely?\nUsep(x1)=/parenleftbig\n.25.25.25.25/parenrightbig\nExercise 10.7 Removing leaves in BN20 networks\na. Consider the QMR network, where only some of the symtpoms are observed. For example, in Fig-\nure 10.16(a), X4andX5are hidden. Show that we can safely remove all the hidden leaf nodes without\naffecting the posterior over the disease nodes, i.e., prove that we can compute p(z1:3|x1,x2,x4)using\nthe network in Figure 10.16(b). This is called barren node removal, and can be applied to any DGM.\nb. Now suppose we partition the leaves into three groups: on, off and unknown. Clearly we can remove the\nunknown leaves, since they are hidden and do not affect their parents. Show that we can analytically\nremove the leaves that are in the \u201coff state\u201d, by absorbing their effect into the prior of the parents.(This trick only works for noisy-OR CPDs.)\nExercise 10.8 Handling negative \ufb01ndings in the QMR network\nConsider the QMR network. Let dbe the hidden diseases, f\u2212be the negative \ufb01ndings (leaf nodes that are\noff), andf\u2212be the positive \ufb01ndings (leaf nodes that are on). We can compute the posterior p(d|f,f+)in\ntwo steps: \ufb01rst absorb the negative \ufb01ndings, p(d|f\u2212)\u221dp(d)p(f\u2212|d), then absorb the positive \ufb01ndings,\np(d|f\u2212,f+)\u221dp(d|f\u2212)p(f+|d). Show that the \ufb01rst step can be done in O(|d||f\u2212|)time, where |d|is\nthe number of dieases and |f\u2212|is the number of negative \ufb01ndings. For simplicity, you can ignore leak\nnodes. (Intuitively, the reason for this is that there is no correlation induced amongst the parents whenthe \ufb01nding is off, since there is no explaining away.)", "365": "10.6. In\ufb02uence (decision) diagrams * 335\nExercise 10.9 Moralization does not introduce new independence statements\nRecall that the process of moralizing a DAG means connecting together all \u201cunmarried\u201d parents that share\na common child, and then dropping all the arrows. Let Mbe the moralization of DAG G. Show that\nCI(M)\u2286CI(G), where CI are the set of conditional independence statements implied by the model.", "366": "", "367": "11 Mixture models and the EM algorithm\n11.1 Latent variable models\nIn Chapter 10 we showed how graphical models can be used to de\ufb01ne high-dimensional joint\nprobability distributions. The basic idea is to model dependence between two variables byadding an edge between them in the graph. (Technically the graph represents conditionalindependence, but you get the point.)\nAn alternative approach is to assume that the observed variables are correlated because they\narise from a hidden common \u201ccause\u201d. Model with hidden variables are also known as latent\nvariable models orLVMs. As we will see in this chapter, such models are harder to \ufb01t than\nmodels with no latent variables. However, they can have signi\ufb01cant advantages, for two mainreasons. First, LVMs often have fewer parameters than models that directly represent correlationin the visible space. This is illustrated in Figure 11.1. If all nodes (including H) are binary and allCPDs are tabular, the model on the left has 17 free parameters, whereas the model on the righthas 59 free parameters.\nSecond, the hidden variables in an LVM can serve as a bottleneck, which computes a\ncompressed representation of the data. This forms the basis of unsupervised learning, as wewill see. Figure 11.2 illustrates some generic LVM structures that can be used for this purpose.In general there are Llatent variables, z\ni1,...,z IL, andDvisible variables, xi1,...,x iD,\nwhere usually D/greatermuchL.I fw eh a v e L>1, there are many latent factors contributing to each\nobservation, so we have a many-to-many mapping. If L=1, we we only have a single latent\nvariable; in this case, ziis usually discrete, and we have a one-to-many mapping. We can\nalso have a many-to-one mapping, representing different competing factors or causes for eachobserved variable; such models form the basis of probabilistic matrix factorization, discussedin Section 27.6.2. Finally, we can have a one-to-one mapping, which can be represented asz\ni\u2192xi. By allowing ziand/orxito be vector-valued, this representation can subsume all the\nothers. Depending on the form of the likelihood p(xi|zi)and the prior p(zi), we can generate\na variety of different models, as summarized in Table 11.1.\n11.2 Mixture models\nThe simplest form of LVM is when zi\u2208{1,...,K}, representing a discrete latent state. We will\nuse a discrete prior for this, p(zi)=C a t (\u03c0). For the likelihood, we use p(xi|zi=k)=pk(xi),", "368": "338 Chapter 11. Mixture models and the EM algorithm\n+\n\u0014\u001a\u0003SDUDPHWHUV \u0018\u001c\u0003SDUDPHWHUV\nFigure 11.1 A DGM with and without hidden variables. The leaves represent medical symptoms. The\nroots represent primary causes, such as smoking, diet and exercise. The hidden variable can represent\nmediating factors, such as heart disease, which might not be directly visible.\nxi1... xiDzi1...ziL\n(a)xi1... xiDzi\n(b)\nxizi1...ziL\n(c)xizi\n(d)\nFigure 11.2 A latent variable model represented as a DGM. (a) Many-to-many. (b) One-to-many. (c)\nMany-to-one. (d) One-to-one.\nwherepkis thek\u2019thbase distribution for the observations; this can be of any type. The overall\nmodel is known as a mixture model, since we are mixing together the Kbase distributions as\nfollows:\np(xi|\u03b8)=K/summationdisplay\nk=1\u03c0kpk(xi|\u03b8) (11.1)\nThis is a convex combination of thepk\u2019s, since we are taking a weighted sum, where the\nmixing weights \u03c0ksatisfy0\u2264\u03c0k\u22641and/summationtextK\nk=1\u03c0k=1. We give some examples below.", "369": "11.2. Mixture models 339\np(xi|zi) p(zi) Name Section\nMVN Discrete Mixture of Gaussians 11.2.1\nProd. Discrete Discrete Mixture of multinomials 11.2.2\nProd. Gaussian Prod. Gaussian Factor analysis/ probabilistic PCA 12.1.5\nProd. Gaussian Prod. Laplace Probabilistic ICA/ sparse coding 12.6\nProd. Discrete Prod. Gaussian Multinomial PCA 27.2.3\nProd. Discrete Dirichlet Latent Dirichlet allocation 27.3\nProd. Noisy-OR Prod. Bernoulli BN20/ QMR 10.2.3\nProd. Bernoulli Prod. Bernoulli Sigmoid belief net 27.7\nTable 11.1 Summary of some popular directed latent variable models. Here \u201cProd\u201d means product, so\n\u201cProd. Discrete\u201d in the likelihood means a factored distribution of the form/producttext\njCat(xij|zi), and \u201cProd.\nGaussian\u201dmeansafactoreddistributionoftheform/producttext\njN(xij|zi). \u201cPCA\u201dstandsfor\u201cprincipalcomponents\nanalysis\u201d. \u201cICA\u201d stands for \u201cindepedendent components analysis\u201d.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.20.30.40.50.60.70.8\n(a)\n (b)\nFigure 11.3 A mixture of 3 Gaussians in 2d. (a) We show the contours of constant probability for each\ncomponent in the mixture. (b) A surface plot of the overall density. Based on Figure 2.23 of (Bishop 2006a).\nFigure generated by mixGaussPlotDemo .\n11.2.1 Mixtures of Gaussians\nThe most widely used mixture model is the mixture of Gaussians (MOG), also called a Gaussian\nmixture model orGMM. In this model, each base distribution in the mixture is a multivariate\nGaussian with mean \u03bckand covariance matrix \u03a3k. Thus the model has the form\np(xi|\u03b8)=K/summationdisplay\nk=1\u03c0kN(xi|\u03bck,\u03a3k) (11.2)\nFigure 11.3 shows a mixture of 3 Gaussians in 2D. Each mixture component is represented by a\ndifferent set of eliptical contours. Given a sufficiently large number of mixture components, a\nGMM can be used to approximate any density de\ufb01ned on RD.", "370": "340 Chapter 11. Mixture models and the EM algorithm\n11.2.2 Mixture of multinoullis\nWe can use mixture models to de\ufb01ne density models on many kinds of data. For example,\nsuppose our data consist of D-dimensional bit vectors. In this case, an appropriate class-\nconditional density is a product of Bernoullis:\np(xi|zi=k,\u03b8)=D/productdisplay\nj=1Ber(xij|\u03bcjk)=D/productdisplay\nj=1\u03bcxij\njk(1\u2212\u03bcjk)1\u2212xij(11.3)\nwhere\u03bcjkis the probability that bit jturns on in cluster k.\nThe latent variables do not have to any meaning, we might simply introduce latent variables\nin order to make the model more powerful. For example, one can show (Exercise 11.8) that themean and covariance of the mixture distribution are given by\nE[x]=/summationdisplay\nk\u03c0k\u03bck (11.4)\ncov[x]=/summationdisplay\nk\u03c0k[\u03a3k+\u03bck\u03bcT\nk]\u2212E[x]E[x]T(11.5)\nwhere\u03a3k=d i a g (\u03bcjk(1\u2212\u03bcjk)). So although the component distributions are factorized,\nthe joint distribution is not. Thus the mixture distribution can capture correlations between\nvariables, unlike a single product-of-Bernoullis model.\n11.2.3 Using mixture models for clustering\nThere are two main applications of mixture models. The \ufb01rst is to use them as a black-box\ndensity model, p(xi). This can be useful for a variety of tasks, such as data compression, outlier\ndetection, and creating generative classi\ufb01ers, where we model each class-conditional densityp(x|y=c)by a mixture distribution (see Section 14.7.3).\nThe second, and more common, application of mixture models is to use them for clustering.\nWe discuss this topic in detail in Chapter 25, but the basic idea is simple. We \ufb01rst \ufb01t the mixturemodel, and then compute p(z\ni=k|xi,\u03b8), which represents the posterior probability that point\nibelongs to cluster k. This is known as the responsibility of cluster kfor point i, and can be\ncomputed using Bayes rule as follows:\nrik/definesp(zi=k|xi,\u03b8)=p(zi=k|\u03b8)p(xi|zi=k,\u03b8)/summationtextK\nk/prime=1p(zi=k/prime|\u03b8)p(xi|zi=k/prime,\u03b8)(11.6)\nThis procedure is called soft clustering, and is identical to the computations performed when\nusing a generative classi\ufb01er. The difference between the two models only arises at training time:\nin the mixture case, we never observe zi, whereas with a generative classi\ufb01er, we do observe yi\n(which plays the role of zi).\nWe can represent the amount of uncertainty in the cluster assignment by using 1\u2212maxkrik.\nAssuming this is small, it may be reasonable to compute a hard clustering using the MAP\nestimate, given by\nz\u2217\ni=a r gm a x\nkrik=a r gm a x\nklogp(xi|zi=k,\u03b8)+logp(zi=k|\u03b8) (11.7)", "371": "11.2. Mixture models 341\n0 9.5 11.5 13.5 15.5 18.5 20.5\u22125\u22124\u22123\u22122\u22121012345\ntimegenesyeast microarray data\n(a)K\u2212Means centroids\n(b)\nFigure 11.4 (a) Some yeast gene expression data plotted as a time series. (c) Visualizing the 16 cluster\ncenters produced by K-means. Figure generated by kmeansYeastDemo .\n0.12\n 0.14\n 0.12\n 0.06\n 0.13\n0.07\n 0.05\n 0.15\n 0.07\n 0.09\nFigure 11.5 We \ufb01t a mixture of 10 Bernoullis to the binarized MNIST digit data. We show the MLE for the\ncorresponding cluster means, \u03bck. The numbers on top of each image represent the mixing weights \u02c6\u03c0k.\nNo labels were used when training the model. Figure generated by mixBerMnistEM .\nHard clustering using a GMM is illustrated in Figure 1.8, where we cluster some data rep-\nresenting the height and weight of people. The colors represent the hard assignments. Note\nthat the identity of the labels (colors) used is immaterial; we are free to rename all the clusters,\nwithout affecting the partitioning of the data; this is called label switching .\nAnother example is shown in Figure 11.4. Here the data vectors xi\u2208R7represent the\nexpression levels of different genes at 7 different time points. We clustered them using a GMM.\nWe see that there are several kinds of genes, such as those whose expression level goes up\nmonotonically over time (in response to a given stimulus), those whose expression level goes\ndown monotonically, and those with more complex response patterns. We have clustered the\nseries into K=1 6groups. (See Section 11.5 for details on how to choose K.) For example, we\ncan represent each cluster by a prototype orcentroid. This is shown in Figure 11.4(b).\nAs an example of clustering binary data, consider a binarized version of the MNIST handwrit-\nten digit dataset (see Figure 1.5(a)), where we ignore the class labels. We can \ufb01t a mixture of", "372": "342 Chapter 11. Mixture models and the EM algorithm\nBernoullis to this, using K=1 0, and then visualize the resulting centroids, \u02c6\u03bck, as shown in\nFigure 11.5. We see that the method correctly discovered some of the digit classes, but overall the\nresults aren\u2019t great: it has created multiple clusters for some digits, and no clusters for others.There are several possible reasons for these \u201cerrors\u201d:\n\u2022 The model is very simple and does not capture the relevant visual characteristics of a digit.\nFor example, each pixel is treated independently, and there is no notion of shape or a stroke.\n\u2022 Although we think there should be 10 clusters, some of the digits actually exhibit a fair degree\nof visual variety. For example, there are two ways of writing 7\u2019s (with and without the cross\nbar). Figure 1.5(a) illustrates some of the range in writing styles. Thus we need K/greatermuch10\nclusters to adequately model this data. However, if we set Kto be large, there is nothing\nin the model or algorithm preventing the extra clusters from being used to create multipleversions of the same digit, and indeed this is what happens. We can use model selectionto prevent too many clusters from being chosen but what looks visually appealing and whatmakes a good density estimator may be quite different.\n\u2022 The likelihood function is not convex, so we may be stuck in a local optimum, as we explain\nin Section 11.3.2.\nThis example is typical of mixture modeling, and goes to show one must be very cautious\ntrying to \u201cinterpret\u201d any clusters that are discovered by the method. (Adding a little bit ofsupervision, or using informative priors, can help a lot.)\n11.2.4 Mixtures of experts\nSection 14.7.3 described how to use mixture models in the context of generative classi\ufb01ers. Wecan also use them to create discriminative models for classi\ufb01cation and regression. For example,consider the data in Figure 11.6(a). It seems like a good model would be three different linearregression functions, each applying to a different part of the input space. We can model this byallowing the mixing weights and the mixture densities to be input-dependent:\np(y\ni|xi,zi=k,\u03b8)=N (yi|wT\nkxi,\u03c32\nk) (11.8)\np(zi|xi,\u03b8)=C a t ( zi|S(VTxi)) (11.9)\nSee Figure 11.7(a) for the DGM.\nThis model is called a mixture of experts or MoE (Jordan and Jacobs 1994). The idea is that\neach submodel is considered to be an \u201cexpert\u201d in a certain region of input space. The functionp(z\ni=k|xi,\u03b8)is called a gating function, and decides which expert to use, depending on\nthe input values. For example, Figure 11.6(b) shows how the three experts have \u201ccarved up\u201d the1d input space, Figure 11.6(a) shows the predictions of each expert individually (in this case, theexperts are just linear regression models), and Figure 11.6(c) shows the overall prediction of themodel, obtained using\np(y\ni|xi,\u03b8)=/summationdisplay\nkp(zi=k|xi,\u03b8)p(yi|xi,zi=k,\u03b8) (11.10)\nWe discuss how to \ufb01t this model in Section 11.4.3.", "373": "11.2. Mixture models 343\n\u22121 \u22120.5 0 0.5 1\u22121.5\u22121\u22120.500.511.5expert predictions, fixed mixing weights=0\n(a)\u22121 \u22120.5 0 0.5 100.10.20.30.40.50.60.70.80.91gating functions, fixed mixing weights=0\n(b)\n\u22121.5 \u22121 \u22120.5 0 0.5 1\u22122\u22121.5\u22121\u22120.500.511.5predicted mean and var, fixed mixing weights=0\n(c)\nFigure 11.6 (a) Some data \ufb01t with three separate regression lines. (b) Gating functions for three different\n\u201cexperts\u201d. (c) The conditionally weighted average of the three expert predictions. Figure generated by\nmixexpDemo .\nyizixi\n(a)yiz1\ni\nz2\nixi\n(b)\nFigure 11.7 (a) A mixture of experts. (b) A hierarchical mixture of experts.", "374": "344 Chapter 11. Mixture models and the EM algorithm\n0 0.2 0.4 0.6 0.8 1\u22120.200.20.40.60.811.2forwards problem\n(a)\u22120.2 0 0.2 0.4 0.6 0.8 1 1.2\u22120.200.20.40.60.811.2expert predictions\n(b)\n\u22120.2 0 0.2 0.4 0.6 0.8 1 1.200.10.20.30.40.50.60.70.80.91prediction\n  \nmean\nmode\n(c)\nFigure 11.8 (a) Some data from a simple forwards model. (b) Some data from the inverse model, \ufb01t\nwith a mixture of 3 linear regressions. Training points are color coded by their responsibilities. (c) The\npredictive mean (red cross) and mode (black square). Based on Figures 5.20 and 5.21 of (Bishop 2006b).Figure generated by mixexpDemoOneToMany .\nIt should be clear that we can \u201cplug in\u201d any model for the expert. For example, we can use\nneural networks (Chapter 16) to represent both the gating functions and the experts. The result\nis known as a mixture density network. Such models are slower to train, but can be more\n\ufb02exible than mixtures of experts. See (Bishop 1994) for details.\nIt is also possible to make each expert be itself a mixture of experts. This gives rise to a\nmodel known as the hierarchical mixture of experts. See Figure 11.7(b) for the DGM, and\nSection 16.2.6 for further details.\n11.2.4.1 Application to inverse problems\nMixtures of experts are useful in solving inverse problems. These are problems where we have\nto invert a many-to-one mapping. A typical example is in robotics, where the location of theend effector (hand) yis uniquely determined by the joint angles of the motors, x. However,\nfor any given location y, there are many settings of the joints xthat can produce it. Thus the\ninverse mapping x=f\n\u22121(y)is not unique. Another example is kinematic tracking of people\nfrom video (Bo et al. 2008), where the mapping from image appearance to pose is not unique,due to self occlusion, etc.", "375": "11.3. Parameter estimation for mixture models 345\n\u03b8z\nz1 zN\n\u00b7\u00b7\u00b7\nx1 xN\n\u03b8x\n(a)\u03b8z\nNzi\nxi\n\u03b8x\n(b)\nFigure 11.9 A LVM represented as a DGM. Left: Model is unrolled for Nexamples. Right: same model\nusing plate notation.\nA simpler example, for illustration purposes, is shown in Figure 11.8(a). We see that this\nde\ufb01nes a function, y=f(x), since for every value xalong the horizontal axis, there is a\nunique response y. This is sometimes called the forwards model. Now consider the problem\nof computing x=f\u22121(y). The corresponding inverse model is shown in Figure 11.8(b); this is\nobtained by simply interchanging the xandyaxes. Now we see that for some values along\nthe horizontal axis, there are multiple possible outputs, so the inverse is not uniquely de\ufb01ned.\nFor example, if y=0.8, thenxcould be 0.2 or 0.8. Consequently, the predictive distribution,\np(x|y,\u03b8)is multimodal.\nWe can \ufb01t a mixture of linear experts to this data. Figure 11.8(b) shows the prediction of each\nexpert, and Figure 11.8(c) shows (a plugin approximation to) the posterior predictive mode andmean. Note that the posterior mean does not yield good predictions. In fact, any model whichis trained to minimize mean squared error \u2014 even if the model is a \ufb02exible nonlinear model,such as neural network \u2014 will work poorly on inverse problems such as this. However, theposterior mode, where the mode is input dependent, provides a reasonable approximation.\n11.3 Parameter estimation for mixture models\nWe have seen how to compute the posterior over the hidden variables given the observedvariables, assuming the parameters are known. In this section, we discuss how to learn theparameters.\nIn Section 10.4.2, we showed that when we have complete data and a factored prior, the\nposterior over the parameters also factorizes, making computation very simple. Unfortunatelythis is no longer true if we have hidden variables and/or missing data. The reason is apparentfrom looking at Figure 11.9. If the z\niwere observed, then by d-separation, we see that \u03b8z\u22a5\u03b8x|D,\nand hence the posterior will factorize. But since, in an LVM, the ziare hidden, the parameters\nare no longer independent, and the posterior does not factorize, making it much harder to", "376": "346 Chapter 11. Mixture models and the EM algorithm\n\u221225 \u221220 \u221215 \u221210 \u22125 0 5 10 15 20 2505101520253035\n(a)\u03bc1\u03bc2\n\u221215.5 \u221210.5 \u22125.5 \u22120.5 4.5 9.5 14.5 19.5\u221215.5\u221210.5\u22125.5\u22120.54.59.514.519.5\n(b)\nFigure 11.10 Left:N= 200data points sampled from a mixture of 2 Gaussians in 1d, with \u03c0k=0.5,\n\u03c3k=5,\u03bc1=\u221210and\u03bc2=1 0. Right: Likelihood surface p(D|\u03bc1,\u03bc2), with all other parameters set\nto their true values. We see the two symmetric modes, re\ufb02ecting the unidenti\ufb01ability of the parameters.\nFigure generated by mixGaussLikSurfaceDemo .\ncompute. This also complicates the computation of MAP and ML estimates, as we discus below.\n11.3.1 Unidenti\ufb01ability\nThe main problem with computing p(\u03b8|D)for an LVM is that the posterior may have multiple\nmodes. To see why, consider a GMM. If the ziwere all observed, we would have a unimodal\nposterior for the parameters:\np(\u03b8|D) = Dir(\u03c0 |D)K/productdisplay\nk=1NIW(\u03bck,\u03a3k|D) (11.11)\nConsequently we can easily \ufb01nd the globally optimal MAP estimate (and hence globally optimal\nMLE).\nBut now suppose the zi\u2019s are hidden. In this case, for each of the possible ways of \u201c\ufb01lling in\u201d\nthezi\u2019s, we get a different unimodal likelihood. Thus when we marginalize out over the zi\u2019s, we\nget a multi-modal posterior for p(\u03b8|D).1These modes correspond to different labelings of the\nclusters. This is illustrated in Figure 11.10(b), where we plot the likelihood function, p(D|\u03bc1,\u03bc2),\nfor a 2D GMM with K=2for the data is shown in Figure 11.10(a). We see two peaks, one\ncorresponding to the case where \u03bc1=\u221210,\u03bc2=1 0, and the other to the case where \u03bc1=1 0,\n\u03bc2=\u221210. We say the parameters are not identi\ufb01able, since there is not a unique MLE.\nTherefore there cannot be a unique MAP estimate (assuming the prior does not rule out certainlabelings), and hence the posterior must be multimodal. The question of how many modes there\n1. Do not confuse multimodality of the parameter posterior, p(\u03b8|D), with the multimodality de\ufb01ned by the model,\np(x|\u03b8). In the latter case, if we have Kclusters, we would expect to only get Kpeaks, although it is theoretically\npossible to get more than K, at least if D>1(Carreira-Perpinan and Williams 2003).", "377": "11.3. Parameter estimation for mixture models 347\nare in the parameter posterior is hard to answer. There are K!possible labelings, but some of\nthe peaks might get merged. Nevertheless, there can be an exponential number, since \ufb01nding\nthe optimal MLE for a GMM is NP-hard (Aloise et al. 2009; Drineas et al. 2004).\nUnidenti\ufb01ability can cause a problem for Bayesian inference. For example, suppose we\ndraw some samples from the posterior, \u03b8(s)\u223cp(\u03b8|D), and then average them, to try to\napproximate the posterior mean, \u03b8=1\nS/summationtextS\ns=1\u03b8(s). (This kind of Monte Carlo approach is\nexplained in more detail in Chapter 24.) If the samples come from different modes, the average\nwill be meaningless. Note, however, that it is reasonable to average the posterior predictive\ndistributions, p(x)\u22481\nS/summationtextS\ns=1p(x|\u03b8(s)), since the likelihood function is invariant to which\nmode the parameters came from.\nA variety of solutions have been proposed to the unidenti\ufb01ability problem. These solutions\ndepend on the details of the model and the inference algorithm that is used. For example, see\n(Stephens 2000) for an approach to handling unidenti\ufb01ability in mixture models using MCMC.\nThe approach we will adopt in this chapter is much simpler: we just compute a single\nlocal mode, i.e., we perform approximate MAP estimation. (We say \u201capproximate\u201d since \ufb01ndingthe globally optimal MLE, and hence MAP estimate, is NP-hard, at least for mixture models(Aloise et al. 2009).) This is by far the most common approach, because of its simplicity. Itis also a reasonable approximation, at least if the sample size is sufficiently large. To see why,consider Figure 11.9(a). We see that there are Nlatent variables, each of which gets to \u201csee\u201d\none data point each. However, there are only two latent parameters, each of which gets toseeNdata points. So the posterior uncertainty about the parameters is typically much less\nthan the posterior uncertainty about the latent variables. This justi\ufb01es the common strategyof computing p(z\ni|xi,\u02c6\u03b8), but not bothering to compute p(\u03b8|D). In Section 5.6, we will study\nhierarchical Bayesian models, which essentially put structure on top of the parameters. In suchmodels, it is important to model p(\u03b8|D), so that the parameters can send information between\nthemselves. If we used a point estimate, this would not be possible.\n11.3.2 Computing a MAP estimate is non-convex\nIn the previous sections, we have argued, rather heuristically, that the likelihood function hasmultiple modes, and hence that \ufb01nding an MAP or ML estimate will be hard. In this section, weshow this result by more algebraic means, which sheds some additional insight into the problem.Our presentation is based in part on (Rennie 2004).\nConsider the log-likelihood for an LVM:\nlogp(D|\u03b8)=/summationdisplay\nilog/bracketleftBigg/summationdisplay\nzip(xi,zi|\u03b8)/bracketrightBigg\n(11.12)\nUnfortunately, this objective is hard to maximize. since we cannot push the log inside the sum.This precludes certain algebraic simplications, but does not prove the problem is hard.\nNow suppose the joint probability distribution p(z\ni,xi|\u03b8)is in the exponential family, which\nmeans it can be written as follows:\np(x,z|\u03b8)=1\nZ(\u03b8)exp[\u03b8T\u03c6(x,z)] (11.13)", "378": "348 Chapter 11. Mixture models and the EM algorithm\nwhere\u03c6(x,z)are the sufficient statistics, and Z(\u03b8)is the normalization constant (see Sec-\ntion 9.2 for more details). It can be shown (Exercise 9.2) that the MVN is in the exponential\nfamily, as are nearly all of the distributions we have encountered so far, including Dirichlet,multinomial, Gamma, Wishart, etc. (The Student distribution is a notable exception.) Further-more, mixtures of exponential families are also in the exponential family, providing the mixingindicator variables are observed (Exercise 11.11).\nWith this assumption, the complete data log likelihood can be written as follows:\n/lscript\nc(\u03b8)=/summationdisplay\nilogp(xi,zi|\u03b8)=\u03b8T(/summationdisplay\ni\u03c6(xi,zi))\u2212NZ(\u03b8) (11.14)\nThe \ufb01rst term is clearly linear in \u03b8. One can show that Z(\u03b8)is a convex function (Boyd and\nVandenberghe 2004), so the overall objective is concave (due to the minus sign), and hence hasa unique maximum.\nNow consider what happens when we have missing data. The observed data log likelihood\nis given by\n/lscript(\u03b8)=/summationdisplay\nilog/summationdisplay\nzip(xi,zi|\u03b8)=/summationdisplay\nilog/bracketleftBigg/summationdisplay\nzie\u03b8T\u03c6(zi,xi)/bracketrightBigg\n\u2212NlogZ(\u03b8) (11.15)\nOne can show that the log-sum-exp function is convex (Boyd and Vandenberghe 2004), and weknow that Z(\u03b8)is convex. However, the difference of two convex functions is not, in general,\nconvex. So the objective is neither convex nor concave, and has local optima.\nThe disadvantage of non-convex functions is that it is usually hard to \ufb01nd their global op-\ntimum. Most optimization algorithms will only \ufb01nd a local optimum; which one they \ufb01nddepends on where they start. There are some algorithms, such as simulated annealing (Sec-tion 24.6.1) or genetic algorithms, that claim to always \ufb01nd the global optimum, but this is onlyunder unrealistic assumptions (e.g., if they are allowed to be cooled \u201cin\ufb01nitely slowly\u201d, or al-lowed to run \u201cin\ufb01nitely long\u201d). In practice, we will run a local optimizer, perhaps using multiple\nrandom restarts to increase out chance of \ufb01nding a \u201cgood\u201d local optimum. Of course, careful\ninitialization can help a lot, too. We give examples of how to do this on a case-by-case basis.\nNote that a convex method for \ufb01tting mixtures of Gaussians has been proposed. The idea\nis to assign one cluster per data point, and select from amongst them, using a convex /lscript\n1-type\npenalty, rather than trying to optimize the locations of the cluster centers. See (Lashkari andGolland 2007) for details. This is essentially an unsupervised version of the approach used insparse kernel logistic regression, which we will discuss in Section 14.3.2. Note, however, that the/lscript\n1penalty, although convex, is not necessarily a good way to promote sparsity, as discussed in\nChapter 13. In fact, as we will see in that Chapter, some of the best sparsity-promoting methodsuse non-convex penalties, and use EM to optimie them! The moral of the story is: do not beafraid of non-convexity.\n11.4 The EM algorithm\nFor many models in machine learning and statistics, computing the ML or MAP parameterestimate is easy provided we observe all the values of all the relevant random variables, i.e., if", "379": "11.4. The EM algorithm 349\nModel Section\nMix. Gaussians 11.4.2\nMix. experts 11.4.3\nFactor analysis 12.1.5\nStudent T 11.4.5\nProbit regression 11.4.6\nDGM with hidden variables 11.4.4\nMVN with missing data 11.6.1\nHMMs 17.5.2\nShrinkage estimates of Gaussian means Exercise 11.13\nTable 11.2 Some models discussed in this book for which EM can be easily applied to \ufb01nd the ML/ MAP\nparameter estimate.\nwe have complete data. However, if we have missing data and/or latent variables, then computing\nthe ML/MAP estimate becomes hard.\nOne approach is to use a generic gradient-based optimizer to \ufb01nd a local minimum of the\nnegative log likelihood orNLL, given by\nNLL(\u03b8)=\u2212/defines1\nNlogp(D|\u03b8) (11.16)\nHowever, we often have to enforce constraints, such as the fact that covariance matrices must bepositive de\ufb01nite, mixing weights must sum to one, etc., which can be tricky (see Exercise 11.5). Insuchcases, itis oftenmuchsimpler (butnotalways faster)to useanalgorithm called expectation\nmaximization,o r EMfor short (Dempster et al. 1977; Meng and van Dyk 1997; McLachlan and\nKrishnan 1997). This is a simple iterative algorithm, often with closed-form updates at each step.Furthermore, the algorithm automatically enforce the required constraints.\nEM exploits the fact that if the data were fully observed, then the ML/ MAP estimate would be\neasy to compute. In particular, EM is an iterative algorithm which alternates between inferringthe missing values given the parameters (E step), and then optimizing the parameters given the\u201c\ufb01lled in\u201d data (M step). We give the details below, followed by several examples. We end witha more theoretical discussion, where we put the algorithm in a larger context. See Table 11.2 fora summary of the applications of EM in this book.\n11.4.1 Basic idea\nLetxibe the visible or observed variables in case i, and let zibe the hidden or missing\nvariables. The goal is to maximize the log likelihood of the observed data:\n/lscript(\u03b8)=N/summationdisplay\ni=1logp(xi|\u03b8)=N/summationdisplay\ni=1log/bracketleftBigg/summationdisplay\nzip(xi,zi|\u03b8)/bracketrightBigg\n(11.17)\nUnfortunately this is hard to optimize, since the log cannot be pushed inside the sum.", "380": "350 Chapter 11. Mixture models and the EM algorithm\nEM gets around this problem as follows. De\ufb01ne the complete data log likelihood to be\n/lscriptc(\u03b8)/definesN/summationdisplay\ni=1logp(xi,zi|\u03b8) (11.18)\nThis cannot be computed, since ziis unknown. So let us de\ufb01ne the expected complete data\nlog likelihood as follows:\nQ(\u03b8,\u03b8t\u22121)=E/bracketleftbig\n/lscriptc(\u03b8)/vextendsingle/vextendsingleD,\u03b8t\u22121/bracketrightbig\n(11.19)\nwheretis the current iteration number. Qis called the auxiliary function. The expectation\nis taken wrt the old parameters, \u03b8t\u22121, and the observed data D. The goal of the Es t e pis to\ncompute Q(\u03b8,\u03b8t\u22121), or rather, the terms inside of it which the MLE depends on; these are\nknown as the expected sufficient statistics or ESS. In the Ms t e p, we optimize the Q function\nwrt\u03b8:\n\u03b8t=a r gm a x\n\u03b8Q(\u03b8,\u03b8t\u22121) (11.20)\nTo perform MAP estimation, we modify the M step as follows:\n\u03b8t=a r g m a x\n\u03b8Q(\u03b8,\u03b8t\u22121)+logp(\u03b8) (11.21)\nThe E step remains unchanged.\nIn Section 11.4.7 we show that the EM algorithm monotonically increases the log likelihood of\nthe observed data (plus the log prior, if doing MAP estimation), or it stays the same. So if the\nobjective ever goes down, there must be a bug in our math or our code. (This is a surprisinglyuseful debugging tool!)\nBelow we explain how to perform the E and M steps for several simple models, that should\nmake things clearer.\n11.4.2 EM for GMMs\nIn this section, we discuss how to \ufb01t a mixture of Gaussians using EM. Fitting other kinds ofmixture models requires a straightforward modi\ufb01cation \u2014 see Exercise 11.3. We assume thenumber of mixture components, K, is known (see Section 11.5 for discussion of this point).", "381": "11.4. The EM algorithm 351\n11.4.2.1 Auxiliary function\nThe expected complete data log likelihood is given by\nQ(\u03b8,\u03b8(t\u22121))/definesE/bracketleftBigg/summationdisplay\nilogp(xi,zi|\u03b8)/bracketrightBigg\n(11.22)\n=/summationdisplay\niE/bracketleftBigg\nlog/bracketleftBiggK/productdisplay\nk=1(\u03c0kp(xi|\u03b8k))I(zi=k)/bracketrightBigg/bracketrightBigg\n(11.23)\n=/summationdisplay\ni/summationdisplay\nkE[I(zi=k)]log[\u03c0 kp(xi|\u03b8k)] (11.24)\n=/summationdisplay\ni/summationdisplay\nkp(zi=k|xi,\u03b8t\u22121)log[\u03c0 kp(xi|\u03b8k)] (11.25)\n=/summationdisplay\ni/summationdisplay\nkriklog\u03c0k+/summationdisplay\ni/summationdisplay\nkriklogp(xi|\u03b8k) (11.26)\nwhererik/definesp(zi=k|xi,\u03b8(t\u22121))is theresponsibility that cluster ktakes for data point i.\nThis is computed in the E step, described below.\n11.4.2.2 E step\nThe E step has the following simple form, which is the same for any mixture model:\nrik=\u03c0kp(xi|\u03b8(t\u22121)\nk)\n/summationtext\nk/prime\u03c0k/primep(xi|\u03b8(t\u22121)\nk/prime)(11.27)\n11.4.2.3 M step\nIn the M step, we optimize Qwrt\u03c0and the\u03b8k.F o r\u03c0, we obviously have\n\u03c0k=1\nN/summationdisplay\nirik=rk\nN(11.28)\nwhererk/defines/summationtext\nirikis the weighted number of points assigned to cluster k.\nTo derive the M step for the \u03bckand\u03a3kterms, we look at the parts of Qthat depend on \u03bck\nand\u03a3k. We see that the result is\n/lscript(\u03bck,\u03a3k)=/summationdisplay\nk/summationdisplay\niriklogp(xi|\u03b8k) (11.29)\n=\u22121\n2/summationdisplay\nirik/bracketleftbig\nlog|\u03a3k|+(xi\u2212\u03bck)T\u03a3\u22121\nk(xi\u2212\u03bck)/bracketrightbig\n(11.30)\nThis is just a weighted version of the standard problem of computing the MLEs of an MVN (see\nSection 4.1.3). One can show (Exercise 11.2) that the new parameter estimates are given by\n\u03bck=/summationtext\nirikxi\nrk(11.31)\n\u03a3k=/summationtext\nirik(xi\u2212\u03bck)(xi\u2212\u03bck)T\nrk=/summationtext\nirikxixT\ni\nrk\u2212\u03bck\u03bcT\nk (11.32)", "382": "352 Chapter 11. Mixture models and the EM algorithm\nThese equations make intuitive sense: the mean of cluster kis just the weighted average of all\npoints assigned to cluster k, and the covariance is proportional to the weighted empirical scatter\nmatrix.\nAfter computing the new estimates, we set \u03b8t=(\u03c0k,\u03bck,\u03a3k)fork=1:K, and go to the\nnext E step.\n11.4.2.4 Example\nAn example of the algorithm in action is shown in Figure 11.11. We start with \u03bc1=(\u22121,1),\n\u03a31=I,\u03bc2=( 1,\u22121),\u03a32=I. We color code points such that blue points come from cluster\n1 and red points from cluster 2. More precisely, we set the color to\ncolor(i)=ri1blue+ri2red (11.33)\nso ambiguous points appear purple. After 20 iterations, the algorithm has converged on a good\nclustering. (The data was standardized, by removing the mean and dividing by the standarddeviation, before processing. This often helps convergence.)\n11.4.2.5 K-means algorithm\nThere is a popular variant of the EM algorithm for GMMs known as the K-means algorithm,\nwhich we now discuss. Consider a GMM in which we make the following assumptions: \u03a3\nk=\n\u03c32IDis \ufb01xed, and \u03c0k=1/Kis \ufb01xed, so only the cluster centers, \u03bck\u2208RD,h a v et ob e\nestimated. Now consider the following delta-function approximation to the posterior computedduring the E step:\np(z\ni=k|xi,\u03b8)\u2248I(k=z\u2217\ni) (11.34)\nwherezi\u2217=a r g m a xkp(zi=k|xi,\u03b8). This is sometimes called hard EM, since we are making\na hard assignment of points to clusters. Since we assumed an equal spherical covariance matrixfor each cluster, the most probable cluster for x\nican be computed by \ufb01nding the nearest\nprototype:\nz\u2217\ni=a r gm i n\nk||xi\u2212\u03bck||2\n2 (11.35)\nHence in each E step, we must \ufb01nd the Euclidean distance between Ndata points and Kcluster\ncenters, which takes O(NKD)time. However, this can be sped up using various techniques,\nsuch as applying the triangle inequality to avoid some redundant computations (Elkan 2003).\nGiven the hard cluster assignments, the M step updates each cluster center by computing themean of all points assigned to it:\n\u03bc\nk=1\nNk/summationdisplay\ni:zi=kxi (11.36)\nSee Algorithm 5 for the pseudo-code.", "383": "11.4. The EM algorithm 353\n\u22122 0 2\u2212202\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 11.11 Illustration of the EM for a GMM applied to the Old Faithful data. (a) Initial (random) values\nof the parameters. (b) Posterior responsibility of each point computed in the \ufb01rst E step. The degree of\nredness indicates the degree to which the point belongs to the red cluster, and similarly for blue; this\npurple points have a roughly uniform posterior over clusters. (c) We show the updated parameters after\nthe \ufb01rst M step. (d) After 3 iterations. (e) After 5 iterations. (f) After 16 iterations. Based on (Bishop 2006a)\nFigure 9.8. Figure generated by mixGaussDemoFaithful .", "384": "354 Chapter 11. Mixture models and the EM algorithm\nAlgorithm 11.1: K-means algorithm\n1initialize mk;\n2repeat\n3Assign each data point to its closest cluster center: zi=a r gm i n k||xi\u2212\u03bck||2\n2;\n4Update each cluster center by computing the mean of all points assigned to it:\n\u03bck=1\nNk/summationtext\ni:zi=kxi;\n5until converged ;\nK=2\n50 100 150 200 250 30020\n40\n60\n80\n100\n120\n140\n160\n180\n200\n(a)\n (b)\nFigure 11.12 An image compressed using vector quantization with a codebook of size K. (a)K=2. (b)\nK=4. Figure generated by vqDemo.\n11.4.2.6 Vector quantization\nSince K-means is not a proper EM algorithm, it is not maximizing likelihood. Instead, it can be\ninterpreted as a greedy algorithm for approximately minimizing a loss function related to data\ncompression, as we now explain.\nSuppose we want to perform lossy compression of some real-valued vectors, xi\u2208RD.Av e r y\nsimple approach to this is to use vector quantization orVQ. The basic idea is to replace each\nreal-valued vector xi\u2208RDwith a discrete symbol zi\u2208{1,...,K}, which is an index into a\ncodebook ofKprototypes, \u03bck\u2208RD. Each data vector is encoded by using the index of the\nmost similar prototype, where similarity is measured in terms of Euclidean distance:\nencode(xi) = argmin\nk||xi\u2212\u03bck||2(11.37)\nWe can de\ufb01ne a cost function that measures the quality of a codebook by computing the\nreconstruction error ordistortion it induces:\nJ(\u03bc,z|K,X)/defines1\nNN/summationdisplay\ni=1||xi\u2212decode(encode(xi))||2=1\nNN/summationdisplay\ni=1||xi\u2212\u03bczi||2(11.38)\nwhere decode (k)=\u03bck. The K-means algorithm can be thought of as a simple iterative scheme\nfor minimizing this objective.\nOf course, we can achieve zero distortion if we assign one prototype to every data vector,\nbut that takes O(NDC)space, where Nis the number of real-valued data vectors, each of", "385": "11.4. The EM algorithm 355\nlengthD, andCis the number of bits needed to represent a real-valued scalar (the quantization\naccuracy). However, in many data sets, we see similar vectors repeatedly, so rather than storing\nthem many times, we can store them once and then create pointers to them. Hence we canreduce the space requirement to O(Nlog\n2K+KDC): theO(Nlog2K)term arises because\neach of the Ndata vectors needs to specify which of the Kcodewords it is using (the pointers);\nand theO(KDC)term arises because we have to store each codebook entry, each of which is\naD-dimensional vector. Typically the \ufb01rst term dominates the second, so we can approximate\ntherateof the encoding scheme (number of bits needed per object) as O(log2K), which is\ntypically much less than O(DC).\nOne application of VQ is to image compression. Consider the N= 200\u00d7320 = 64 ,000pixel\nimage in Figure 11.12; this is gray-scale, so D=1. If we use one byte to represent each pixel\n(a gray-scale intensity of 0 to 255), then C=8, so we need NC= 512,000bits to represent\nthe image. For the compressed image, we need Nlog2K+KCbits. ForK=4, this is about\n128kb, a factor of 4 compression. For K=8, this is about 192kb, a factor of 2.6 compression,\nat negligible perceptual loss (see Figure 11.12(b)). Greater compression could be achieved if wemodelled spatial correlation between the pixels, e.g., if we encoded 5x5 blocks (as used by JPEG).This is because the residual errors (differences from the model\u2019s predictions) would be smaller,and would take fewer bits to encode.\n11.4.2.7 Initialization and avoiding local minima\nBoth K-means and EM need to be initialized. It is common to pick Kdata points at random, and\nto make these be the initial cluster centers. Or we can pick the centers sequentially so as to tryto \u201ccover\u201d the data. That is, we pick the initial point uniformly at random. Then each subsequentpoint is picked from the remaining points with probability proportional to its squared distanceto the points\u2019s closest cluster center. This is known as farthest point clustering (Gonzales 1985),\nork-means++ (Arthur and Vassilvitskii 2007; Bahmani et al. 2012). Surprisingly, this simple trick\ncan be shown to guarantee that the distortion is never more than O(logK)worse than optimal\n(Arthur and Vassilvitskii 2007).\nAn heuristic that is commonly used in the speech recognition community is to incrementally\n\u201cgrow\u201d GMMs: we initially give each cluster a score based on its mixture weight; after eachround of training, we consider splitting the cluster with the highest score into two, with the newcentroids being random perturbations of the original centroid, and the new scores being half ofthe old scores. If a new cluster has too small a score, or too narrow a variance, it is removed.We continue in this way until the desired number of clusters is reached. See (Figueiredo andJain 2002) for a similar incremental approach.\n11.4.2.8 MAP estimation\nAs usual, the MLE may over\ufb01t. The over\ufb01tting problem is particularly severe in the case ofGMMs. To understand the problem, suppose for simplicity that \u03a3\nk=\u03c32\nkI, and that K=2.I t\nis possible to get an in\ufb01nite likelihood by assigning one of the centers, say \u03bc2, to a single data\npoint, say x1, since then the 1st term makes the following contribution to the likelihood:\nN(x1|\u03bc2,\u03c32\n2I)=1/radicalbig\n2\u03c0\u03c32\n2e0(11.39)", "386": "356 Chapter 11. Mixture models and the EM algorithm\nxp(x)\n(a)10 20 30 40 50 60 70 80 90 10000.10.20.30.40.50.60.70.80.91\ndimensionalityfraction of times EM for GMM fails\n  \nMLE\nMAP\n(b)\nFigure 11.13 (a) Illustration of how singularities can arise in the likelihood function of GMMs. Based on\n(Bishop 2006a) Figure 9.7. Figure generated by mixGaussSingularity . (b) Illustration of the bene\ufb01t of\nMAP estimation vs ML estimation when \ufb01tting a Gaussian mixture model. We plot the fraction of times\n(out of 5 random trials) each method encounters numerical problems vs the dimensionality of the problem,forN=1 0 0samples. Solid red (upper curve): MLE. Dotted black (lower curve): MAP. Figure generated by\nmixGaussMLvsMAP .\nHence we can drive this term to in\ufb01nity by letting \u03c32\u21920, as shown in Figure 11.13(a). We will\ncall this the \u201ccollapsing variance problem\u201d.\nAn easy solution to this is to perform MAP estimation. The new auxiliary function is the\nexpected complete data log-likelihood plus the log prior:\nQ/prime(\u03b8,\u03b8old)=/bracketleftBigg/summationdisplay\ni/summationdisplay\nkriklog\u03c0ik+/summationdisplay\ni/summationdisplay\nkriklogp(xi|\u03b8k)/bracketrightBigg\n+logp(\u03c0)+/summationdisplay\nklogp(\u03b8k)(11.40)\nNote that the E step remains unchanged, but the M step needs to be modi\ufb01ed, as we now\nexplain.\nFor the prior on the mixture weights, it is natural to use a Dirichlet prior, \u03c0\u223cDir(\u03b1), since\nthis is conjugate to the categorical distribution. The MAP estimate is given by\n\u03c0k=rk+\u03b1k\u22121\nN+/summationtext\nk\u03b1k\u2212K(11.41)\nIf we use a uniform prior, \u03b1k=1, this reduces to Equation 11.28.\nThe prior on the parameters of the class conditional densities, p(\u03b8k), depends on the form of\nthe class conditional densities. We discuss the case of GMMs below, and leave MAP estimationfor mixtures of Bernoullis to Exercise 11.3.\nFor simplicity, let us consider a conjugate prior of the form\np(\u03bc\nk,\u03a3k) = NIW( \u03bck,\u03a3k|m0,\u03ba0,\u03bd0,S0) (11.42)", "387": "11.4. The EM algorithm 357\nFrom Section 4.6.3, the MAP estimate is given by\n\u02c6\u03bck=rkxk+\u03ba0m0\nrk+\u03ba0(11.43)\n(11.44)\nxk/defines/summationtext\nirikxi\nrk(11.45)\n\u02c6\u03a3k=S0+Sk+\u03ba0rk\n\u03ba0+rk(xk\u2212m0)(xk\u2212m0)T\n\u03bd0+rk+D+2(11.46)\nSk/defines/summationdisplay\nirik(xi\u2212xk)(xi\u2212xk)T(11.47)\nWe now illustrate the bene\ufb01ts of using MAP estimation instead of ML estimation in the\ncontext of GMMs. We apply EM to some synthetic data in Ddimensions, using either ML or\nMAP estimation. We count the trial as a \u201cfailure\u201d if there are numerical issues involving singular\nmatrices. For each dimensionality, we conduct 5 random trials. The results are illustrated inFigure 11.13(b) using N= 100. We see that as soon as Dbecomes even moderately large, ML\nestimation crashes and burns, whereas MAP estimation never encounters numerical problems.\nWhen using MAP estimation, we need to specify the hyper-parameters. Here we mention\nsome simple heuristics for setting them (Fraley and Raftery 2007, p163). We can set \u03ba\n0=0,\nso that the \u03bckare unregularized, since the numerical problems only arise from \u03a3k. In this\ncase, the MAP estimates simplify to \u02c6\u03bck=xkand\u02c6\u03a3k=S0+Sk\n\u03bd0+rk+D+2, which is not quite so\nscary-looking.\nNow we discuss how to set S0. One possibility is to use\nS0=1\nK1/Ddiag(s2\n1,...,s2D) (11.48)\nwheresj=( 1/N)/summationtextN\ni=1(xij\u2212xj)2is the pooled variance for dimension j. (The reason\nfor the1\nK1/Dterm is that the resulting volume of each ellipsoid is then given by |S0|=\n1\nK|diag(s2\n1,...,s2\nD)|.) The parameter \u03bd0controls how strongly we believe this prior. The\nweakest prior we can use, while still being proper, is to set \u03bd0=D+2, so this is a common\nchoice.\n11.4.3 EM for mixture of experts\nWe can \ufb01t a mixture of experts model using EM in a straightforward manner. The expected\ncomplete data log likelihood is given by\nQ(\u03b8,\u03b8old)=N/summationdisplay\ni=1K/summationdisplay\nk=1riklog[\u03c0ikN(yi|wT\nkxi,\u03c32\nk)] (11.49)\n\u03c0i,k/definesS(VTxi)k (11.50)\nrik\u221d\u03c0old\nikN(yi|xT\niwold\nk,(\u03c3old\nk)2) (11.51)\nSo the E step is the same as in a standard mixture model, except we have to replace \u03c0kwith\n\u03c0i,kwhen computing rik.", "388": "358 Chapter 11. Mixture models and the EM algorithm\nIn the M step, we need to maximize Q(\u03b8,\u03b8old)wrtwk,\u03c32\nkandV. For the regression\nparameters for model k, the objective has the form\nQ(\u03b8k,\u03b8old)=N/summationdisplay\ni=1rik/braceleftbigg\n\u22121\n\u03c32\nk(yi\u2212wT\nkxi)/bracerightbigg\n(11.52)\nWe recognize this as a weighted least squares problem, which makes intuitive sense: if rikis\nsmall, then data point iwill be downweighted when estimating model k\u2019s parameters. From\nSection 8.3.4 we can immediately write down the MLE as\nwk=(XTRkX)\u22121XTRky (11.53)\nwhereRk=d i a g (r:,k). The MLE for the variance is given by\n\u03c32\nk=/summationtextN\ni=1rik(yi\u2212wT\nkxi)2\n/summationtextNi=1rik(11.54)\nWe replace the estimate of the unconditional mixing weights \u03c0with the estimate of the gating\nparameters, V. The objective has the form\n/lscript(V)=/summationdisplay\ni/summationdisplay\nkriklog\u03c0i,k (11.55)\nWe recognize this as equivalent to the log-likelihood for multinomial logistic regression in\nEquation 8.34, except we replace the \u201chard\u201d 1-of-C encoding yiwith the \u201csoft\u201d 1-of-K encoding\nri. Thus we can estimate Vby \ufb01tting a logistic regression model to soft target labels.\n11.4.4 EM for DGMs with hidden variables\nWe can generalize the ideas behind EM for mixtures of experts to compute the MLE or MAPestimate for an arbitrary DGM. We could use gradient-based methods (Binder et al. 1997), but itis much simpler to use EM (Lauritzen 1995): in the E step, we just estimate the hidden variables,and in the M step, we will compute the MLE using these \ufb01lled-in values. We give the detailsbelow.\nFor simplicity of presentation, we will assume all CPDs are tabular. Based on Section 10.4.2,\nlet us write each CPT as follows:\np(x\nit|xi,pa(t),\u03b8t)=Kpa(t )/productdisplay\nc=1Kt/productdisplay\nk=1\u03b8I(xit=i,xi,pa(t )=c)\ntck(11.56)\nThe log-likelihood of the complete data is given by\nlogp(D|\u03b8)=V/summationdisplay\nt=1Kpa(t )/summationdisplay\nc=1Kt/summationdisplay\nk=1Ntcklog\u03b8tck (11.57)\nwhereNtck=/summationtextN\ni=1I(xit=i,xi,pa(t)=c)are the empirical counts. Hence the expected\ncomplete data log-likelihood has the form\nE[logp(D|\u03b8)] =/summationdisplay\nt/summationdisplay\nc/summationdisplay\nkNtcklog\u03b8tck (11.58)", "389": "11.4. The EM algorithm 359\nwhere\nNtck=N/summationdisplay\ni=1E/bracketleftbig\nI(xit=i,xi,pa(t)=c)/bracketrightbig\n=/summationdisplay\nip(xit=k,xi,pa(t)=c|Di) (11.59)\nwhereDiare all the visible variables in case i.\nThe quantity p(xit,xi,pa(t)|Di,\u03b8)is known as a family marginal, and can be computed\nusing any GM inference algorithm. The Ntjkare the expected sufficient statistics, and constitute\nthe output of the E step.\nGiven these ESS, the M step has the simple form\n\u02c6\u03b8tck=Ntck/summationtext\nk/primeNtjk/prime(11.60)\nThis can be proved by adding Lagrange multipliers (to enforce the constraint/summationtext\nk\u03b8tjk=1)\nto the expected complete data log likelihood, and then optimizing each parameter vector \u03b8tc\nseparately. We can modify this to perform MAP estimation with a Dirichlet prior by simply\nadding pseudo counts to the expected counts.\n11.4.5 EM for the Student distribution *\nOne problem with the Gaussian distribution is that it is sensitive to outliers, since the log-probability only decays quadratically with distance from the center. A more robust alternative isthe Student t distribution, as discussed in Section ??.\nUnlike the case of a Gaussian, there is no closed form formula for the MLE of a Student, even\nif we have no missing data, so we must resort to iterative optimization methods. The easiestone to use is EM, since it automatically enforces the constraints that \u03bdis positive and that \u03a3\nis symmetric positive de\ufb01nite. In addition, the resulting algorithm turns out to have a simpleintuitive form, as we see below.\nAt \ufb01rst blush, it might not be apparent why EM can be used, since there is no missing data.\nThe key idea is to introduce an \u201carti\ufb01cial\u201d hidden or auxiliary variable in order to simplify thealgorithm. In particular, we will exploit the fact that a Student distribution can be written as aGaussian scale mixture:\nT(x\ni|\u03bc,\u03a3,\u03bd)=/integraldisplay\nN(xi|\u03bc,\u03a3/zi)Ga(zi|\u03bd\n2,\u03bd\n2)dzi (11.61)\n(See Exercise 11.1 for a proof of this in the 1d case.) This can be thought of as an \u201cin\ufb01nite\u201dmixture of Gaussians, each one with a slightly different covariance matrix.\nTreating the z\nias missing data, we can write the complete data log likelihood as\n/lscriptc(\u03b8)=N/summationdisplay\ni=1[logN(xi|\u03bc,\u03a3/zi)+logGa( zi|\u03bd/2,\u03bd/2)] (11.62)\n=N/summationdisplay\ni=1/bracketleftbigg\n\u2212D\n2log(2\u03c0)\u22121\n2log|\u03a3|\u2212zi\n2\u03b4i+\u03bd\n2log\u03bd\n2\u2212log\u0393(\u03bd\n2) (11.63)\n+\u03bd\n2(logzi\u2212zi)+(D\n2\u22121)logzi/bracketrightbigg\n(11.64)", "390": "360 Chapter 11. Mixture models and the EM algorithm\nwhere we have de\ufb01ned the Mahalanobis distance to be\n\u03b4i=(xi\u2212\u03bc)T\u03a3\u22121(xi\u2212\u03bc) (11.65)\nWe can partition this into two terms, one involving \u03bcand\u03a3, and the other involving \u03bd.W e\nhave, dropping irrelevant constants,\n/lscriptc(\u03b8)=L N(\u03bc,\u03a3)+L G(\u03bd) (11.66)\nLN(\u03bc,\u03a3)/defines\u22121\n2Nlog|\u03a3|\u22121\n2N/summationdisplay\ni=1zi\u03b4i (11.67)\nLG(\u03bd)/defines\u2212Nlog\u0393(\u03bd/2)+1\n2N\u03bdlog(\u03bd/2)+1\n2\u03bdN/summationdisplay\ni=1(logzi\u2212zi) (11.68)\n11.4.5.1 EM with \u03bdknown\nLet us \ufb01rst derive the algorithm with \u03bdassumed known, for simplicity. In this case, we can\nignore the LGterm, so we only need to \ufb01gure out how to compute E[zi]wrt the old parameters.\nFrom Section 4.6.2.2 we have\np(zi|xi,\u03b8)=G a ( zi|\u03bd+D\n2,\u03bd+\u03b4i\n2) (11.69)\nNow ifzi\u223cGa(a,b), then E[zi]=a/b. Hence the E step at iteration tis\nz(t)\ni/definesE/bracketleftBig\nzi|xi,\u03b8(t)/bracketrightBig\n=\u03bd(t)+D\n\u03bd(t)+\u03b4(t)\ni(11.70)\nThe M step is obtained by maximizing E[LN(\u03bc,\u03a3)]to yield\n\u02c6\u03bc(t+1)=/summationtext\niz(t)ixi/summationtext\niz(t)\ni(11.71)\n\u02c6\u03a3(t+1)=1\nN/summationdisplay\niz(t)\ni(xi\u2212\u02c6\u03bc(t+1))(xi\u2212\u02c6\u03bc(t+1))T(11.72)\n=1\nN/bracketleftBigg/summationdisplay\niz(t)ixixT\ni\u2212/parenleftBiggN/summationdisplay\ni=1z(t)\ni/parenrightBigg\n\u02c6\u03bc(t+1)(\u02c6\u03bc(t+1))T/bracketrightBigg\n(11.73)\nThese results are quite intuitive: the quantity ziis the precision of measurement i,s oi fi ti s\nsmall, the corresponding data point is down-weighted when estimating the mean and covariance.\nThis is how the Student achieves robustness to outliers.\n11.4.5.2 EM with \u03bdunknown\nTo compute the MLE for the degrees of freedom, we \ufb01rst need to compute the expectation ofL\nG(\u03bd), which involves ziandlogzi.N o wi fzi\u223cGa(a,b), then one can show that\n/lscript(t)\ni/definesE/bracketleftBig\nlogzi|\u03b8(t)/bracketrightBig\n=\u03a8 (a)\u2212logb (11.74)", "391": "11.4. The EM algorithm 361\n\u22125 \u22124 \u22123 \u22122 \u22121 0 1 2\u22127\u22126\u22125\u22124\u22123\u22122\u22121012314 errors using gauss (red=error)\nBankrupt\nSolvent\n(a)\u22125 \u22124 \u22123 \u22122 \u22121 0 1 2\u22127\u22126\u22125\u22124\u22123\u22122\u221210124 errors using student (red=error)\nBankrupt\nSolvent\n(b)\nFigure 11.14 Mixture modeling on the bankruptcy data set. Left: Gaussian class conditional densities.\nRight: Student class conditional densities. Points that belong to class 1 are shown as triangles, points that\nbelong to class 2 are shown as circles The estimated labels, based on the posterior probability of belongingto each mixture component, are computed. If these are incorrect, the point is colored red, otherwise it iscolored blue. (Training data is in black.) Figure generated by mixStudentBankruptcyDemo .\nwhere\u03a8(x)=d\ndxlog\u0393(x) is the digamma function. Hence, from Equation 11.69, we have\n/lscript(t)\ni=\u03a8 (\u03bd(t)+D\n2)\u2212log(\u03bd(t)+\u03b4(t)\ni\n2) (11.75)\n= log(z(t)\ni)+\u03a8(\u03bd(t)+D\n2)\u2212log(\u03bd(t)+D\n2) (11.76)\nSubstituting into Equation 11.68, we have\nE[LG(\u03bd)] =\u2212Nlog\u0393(\u03bd/2)+N\u03bd\n2log(\u03bd/2)+\u03bd\n2/summationdisplay\ni(/lscript(t)\ni\u2212z(t)\ni) (11.77)\nThe gradient of this expression is equal to\nd\nd\u03bdE[LG(\u03bd)] =\u2212N\n2\u03a8(\u03bd/2)+N\n2log(\u03bd/2)+N\n2+1\n2/summationdisplay\ni(/lscript(t)\ni\u2212z(t)\ni) (11.78)\nThis has a unique solution in the interval (0,+\u221e]which can be found using a 1d constrained\noptimizer.\nPerforming a gradient-based optimization in the M step, rather than a closed-form update, is\nan example of what is known as the generalized EM algorithm. One can show that EM will still\nconverge to a local optimum even if we only perform a \u201cpartial\u201d improvement to the parameters\nin the M step.\n11.4.5.3 Mixtures of Student distributions\nIt is easy to extend the above methods to \ufb01t a mixture of Student distributions. See Exercise 11.4for the details.\nLet us consider a small example from (Lo 2009, ch3). We have a N=6 6,D=2data\nset regarding the bankrupty patterns of certain companies. The \ufb01rst feature speci\ufb01es the ratio", "392": "362 Chapter 11. Mixture models and the EM algorithm\nof retained earnings (RE) to total assets, and the second feature speci\ufb01es the ratio of earnings\nbefore interests and taxes (EBIT) to total assets. We \ufb01t two models to this data, ignoring theclass labels: a mixture of 2 Gaussians, and a mixture of 2 Students. We then use each \ufb01ttedmodel to classify the data. We compute the most probable cluster membership and treat thisas\u02c6y\ni. We then compare \u02c6yito the true labels yiand compute an error rate. If this is more\nthan 50%, we permute the latent labels (i.e., we consider cluster 1 to represent class 2 and viceversa), and then recompute the error rate. Points which are misclassi\ufb01ed are then shown in red.The result is shown in Figure 11.14. We see that the Student model made 4 errors, the Gaussianmodel made 21. This is because the class-conditional densities contain some extreme values,causing the Gaussian to be a poor choice.\n11.4.6 EM for probit regression *\nIn Section 9.4.2, we described the latent variable interpretation of probit regression. Recall thatthis has the form p(y\ni=1|zi)=I(zi>0),w h e r ezi\u223cN(wTxi,1)is latent. We now show\nhow to \ufb01t this model using EM. (Although it is possible to \ufb01t probit regression models usinggradient based methods, as shown in Section 9.4.1, this EM-based approach has the advantagethat it generalized to many other kinds of models, as we will see later on.)\nThe complete data log likelihood has the following form, assuming a N(0,V\n0)prior onw:\n/lscript(z,w|V0)=l o g p(y|z)+logN(z|Xw,I)+logN(w|0,V0) (11.79)\n=/summationdisplay\nilogp(yi|zi)\u22121\n2(z\u2212Xw)T(z\u2212Xw)\u22121\n2wTV\u22121\n0w+const(11.80)\nThe posterior in the E step is a truncated Gaussian:\np(zi|yi,xi,w)=/braceleftbiggN(zi|wTxi,1)I(zi>0)ifyi=1\nN(zi|wTxi,1)I(zi<0)ifyi=0(11.81)\nIn Equation 11.80, we see that wonly depends linearly on z, so we just need to compute\nE[zi|yi,xi,w]. Exercise 11.15 asks you to show that the posterior mean is given by\nE[zi|w,xi]=/braceleftBigg\n\u03bci+\u03c6(\u03bci)\n1\u2212\u03a6(\u2212\u03bci)=\u03bci+\u03c6(\u03bci)\n\u03a6(\u03bci)ifyi=1\n\u03bci\u2212\u03c6(\u03bci)\n\u03a6(\u2212\u03bci)=\u03bci\u2212\u03c6(\u03bci)\n1\u2212\u03a6(\u03bci)ifyi=0(11.82)\nwhere\u03bci=wTxi.\nIn the M step, we estimate wusing ridge regression, where \u03bc=E[z]is the output we are\ntrying to predict. Speci\ufb01cally, we have\n\u02c6w=(V\u22121\n0+XTX)\u22121XT\u03bc (11.83)\nThe EM algorithm is simple, but can be much slower than direct gradient methods, as\nillustrated in Figure 11.15. This is because the posterior entropy in the E step is quite high, sincewe only observe that zis positive or negative, but are given no information from the likelihood\nabout its magnitude. Using a stronger regularizer can help speed convergence, because itconstrains the range of plausible zvalues. In addition, one can use various speedup tricks, such\nas data augmentation (van Dyk and Meng 2001), but we do not discuss that here.", "393": "11.4. The EM algorithm 363\n0 20 40 60 80 100 120010203040506070\niterpenalized NLLprobit regression with L2 regularizer of 0.100\n  \nem\nminfunc\nFigure 11.15 Fitting a probit regression model in 2d using a quasi-Newton method or EM. Figure generated\nbyprobitRegDemo .\n11.4.7 Theoretical basis for EM *\nIn this section, we show that EM monotonically increases the observed data log likelihood until\nit reaches a local maximum (or saddle point, although such points are usually unstable). Ourderivation will also serve as the basis for various generalizations of EM that we will discuss later.\n11.4.7.1 Expected complete data log likelihood is a lower bound\nConsider an arbitrary distribution q(z\ni)over the hidden variables. The observed data log\nlikelihood can be written as follows:\n/lscript(\u03b8)/definesN/summationdisplay\ni=1log/bracketleftBigg/summationdisplay\nzip(xi,zi|\u03b8)/bracketrightBigg\n=N/summationdisplay\ni=1log/bracketleftBigg/summationdisplay\nziq(zi)p(xi,zi|\u03b8)\nq(zi)/bracketrightBigg\n(11.84)\nNowlog(u)is a concavefunction, so from Jensen\u2019s inequality (Equation 2.113) we have the\nfollowing lower bound:\n/lscript(\u03b8)\u2265/summationdisplay\ni/summationdisplay\nziqi(zi)logp(xi,zi|\u03b8)\nqi(zi)(11.85)\nLet us denote this lower bound as follows:\nQ(\u03b8,q)/defines/summationdisplay\niEqi[logp(xi,zi|\u03b8)]+H(qi) (11.86)\nwhere H(qi)is the entropy of qi.\nThe above argument holds for any positive distribution q. Which one should we choose?\nIntuitively we should pick the qthat yields the tightest lower bound. The lower bound is a sum", "394": "364 Chapter 11. Mixture models and the EM algorithm\noveriof terms of the following form:\nL(\u03b8,qi)=/summationdisplay\nziqi(zi)logp(xi,zi|\u03b8)\nqi(zi)(11.87)\n=/summationdisplay\nziqi(zi)logp(zi|xi,\u03b8)p(xi|\u03b8)\nqi(zi)(11.88)\n=/summationdisplay\nziqi(zi)logp(zi|xi,\u03b8)\nqi(zi)+/summationdisplay\nziqi(zi)logp(xi|\u03b8) (11.89)\n=\u2212KL(qi(zi)||p(zi|xi,\u03b8))+logp(xi|\u03b8) (11.90)\nThep(xi|\u03b8)term is independent of qi, so we can maximize the lower bound by setting qi(zi)=\np(zi|xi,\u03b8). Of course, \u03b8is unknown, so instead we use qt\ni(zi)=p(zi|xi,\u03b8t),w h e r e\u03b8tis our\nestimate of the parameters at iteration t. This is the output of the E step.\nPlugging this in to the lower bound we get\nQ(\u03b8,qt)=/summationdisplay\niEqt\ni[logp(xi,zi|\u03b8)]+H/parenleftbig\nqt\ni/parenrightbig\n(11.91)\nWe recognize the \ufb01rst term as the expected complete data log likelihood. The second term is a\nconstant wrt \u03b8. So the M step becomes\n\u03b8t+1=a r gm a x\n\u03b8Q(\u03b8,\u03b8t) = argmax\n\u03b8/summationdisplay\niEqt\ni[logp(xi,zi|\u03b8)] (11.92)\nas usual.\nNow comes the punchline. Since we used qt\ni(zi)=p(zi|xi,\u03b8t), the KL divergence becomes\nzero, soL(\u03b8t,qi)=l o gp(xi|\u03b8t), and hence\nQ(\u03b8t,\u03b8t)=/summationdisplay\nilogp(xi|\u03b8t)=/lscript(\u03b8t) (11.93)\nWe see that the lower bound is tight after the E step. Since the lower bound \u201ctouches\u201d thefunction, maximizing the lower bound will also \u201cpush up\u201d on the function itself. That is, theM step is guaranteed to modify the parameters so as to increase the likelihood of the observeddata (unless it is already at a local maximum).\nThis process is sketched in Figure 11.16. The dashed red curve is the original function (the\nobserved data log-likelihood). The solid blue curve is the lower bound, evaluated at \u03b8\nt; this\ntouches the objective function at \u03b8t. We then set \u03b8t+1to the maximum of the lower bound\n(blue curve), and \ufb01t a new bound at that point (dotted green curve). The maximum of this newbound becomes \u03b8\nt+2, etc. (Compare this to Newton\u2019s method in Figure 8.4(a), which repeatedly\n\ufb01ts and then optimizes a quadratic approximation.)\n11.4.7.2 EM monotonically increases the observed data log likelihood\nWe now prove that EM monotonically increases the observed data log likelihood until it reachesa local optimum. We have\n/lscript(\u03b8\nt+1)\u2265Q(\u03b8t+1,\u03b8t)\u2265Q(\u03b8t,\u03b8t)=/lscript(\u03b8t) (11.94)", "395": "11.4. The EM algorithm 365\n  \nQ(\u03b8,\u03b8t)\nQ(\u03b8,\u03b8t+1)\nl(\u03b8)\n\u03b8t\u03b8t+1\u03b8t+2\nFigure 11.16 Illustration of EM as a bound optimization algorithm. Based on Figure 9.14 of (Bishop 2006a).\nFigure generated by emLogLikelihoodMax .\nwhere the \ufb01rst inequality follows since Q(\u03b8,\u00b7)is a lower bound on /lscript(\u03b8); the second inequality\nfollows since, by de\ufb01nition, Q(\u03b8t+1,\u03b8t) = max \u03b8Q(\u03b8,\u03b8t)\u2265Q(\u03b8t,\u03b8t); and the \ufb01nal equality\nfollows Equation 11.93.\nAs a consequence of this result, if you do not observe monotonic increase of the observed\ndata log likelihood, you must have an error in your math and/or code. (If you are performing\nMAP estimation, you must add on the log prior term to the objective.) This is a surprisinglypowerful debugging tool.\n11.4.8 Online EM\nWhen dealing with large or streaming datasets, it is important to be able to learn online, aswe discussed in Section 8.5. There are two main approaches to online EM in the literature.\nThe \ufb01rst approach, known as incremental EM (Neal and Hinton 1998), optimizes the lower\nboundQ(\u03b8,q\n1,...,q N)oneqiat a time; however, this requires storing the expected sufficient\nstatistics for each data case. The second approach, known as stepwise EM (Sato and Ishii 2000;\nCappe and Mouline 2009; Cappe 2010), is based on stochastic approximation theory, and onlyrequires constant memory use. We explain both approaches in more detail below, following thepresentation of (Liang and Klein Liang and Klein).\n11.4.8.1 Batch EM review\nBefore explaining online EM, we review batch EM in a more abstract setting. Let \u03c6(x,z)be a\nvector of sufficient statistics for a single data case. (For example, for a mixture of multinoullis,this would be the count vector a(j), which is the number of cluster jwas used in z, plus the\nmatrixB(j,v), which is of the number of times the hidden state was jand the observed letter\nwasv.) Lets\ni=/summationtext\nzp(z|xi,\u03b8)\u03c6(xi,z)be the expected sufficient statistics for case i, and\n\u03bc=/summationtextN\ni=1sibe the sum of the ESS. Given \u03bc, we can derive an ML or MAP estimate of the\nparameters in the M step; we will denote this operation by \u03b8(\u03bc). (For example, in the case of\nmixtures of multinoullis, we just need to normalize aand each row of B.) With this notation\nunder our belt, the pseudo code for batch EM is as shown in Algorithm 8.", "396": "366 Chapter 11. Mixture models and the EM algorithm\nAlgorithm 11.2: Batch EM algorithm\n1initialize \u03bc;\n2repeat\n3\u03bcnew=0;\n4foreach example i=1:Ndo\n5 si:=/summationtext\nzp(z|xi,\u03b8(\u03bc))\u03c6(xi,z);\n6 \u03bcnew:=\u03bcnew+si;;\n7\u03bc:=\u03bcnew;\n8until converged ;\n11.4.8.2 Incremental EM\nIn incremental EM (Neal and Hinton 1998), we keep track of \u03bcas well as the si. When we come\nto a data case, we swap out the old siand replace it with the new snew\ni, as shown in the code\nin Algorithm 8. Note that we can exploit the sparsity of snewito speedup the computation of \u03b8,\nsince most components of \u03bcwil not have changed.\nAlgorithm 11.3: Incremental EM algorithm\n1initialize sifori=1:N;\n2\u03bc=/summationtext\nisi;\n3repeat\n4foreach example i=1:N in a random order do\n5 snewi:=/summationtext\nzp(z|xi,\u03b8(\u03bc))\u03c6(xi,z);\n6 \u03bc:=\u03bc+snewi\u2212si;\n7 si:=snewi;\n8until converged ;\nThis can be viewed as maximizing the lower bound Q(\u03b8,q1,...,q N)by optimizing q1, then\n\u03b8, thenq2, then\u03b8, etc. As such, this method is guaranteed to monotonically converge to a local\nmaximum of the lower bound and to the log likelihood itself.\n11.4.8.3 Stepwise EM\nIn stepwise EM, whenever we compute a new si,w em o v e \u03bctowards it, as shown in Algorithm 7.2\nAt iteration k, the stepsize has value \u03b7k, which must satisfy the Robbins-Monro conditions in\nEquation 8.82. For example, (Liang and Klein Liang and Klein) use \u03b7k=( 2+k)\u2212\u03bafor\n0.5<\u03ba\u22641. We can get somewhat better behavior by using a minibatch of size mbefore\neach update. It is possible to optimize mand\u03bato maximize the training set likelihood, by\n2. A detail: As written the update for \u03bcdoes not exploit the sparsity of si. We can \ufb01x this by storing m=\u03bc/producttext\nj<k(1\u2212\u03b7j)\ninstead of \u03bc, and then using the sparse update m:=m+\u03b7k/producttext\nj<k(1\u2212\u03b7j)si. This will not affect the results (i.e.,\n\u03b8(\u03bc)=\u03b8(m)), since scaling the counts by a global constant has no effect.", "397": "11.4. The EM algorithm 367\n\u0014\n\u0015\n\u0016\n\u0011\u0011\u0011\nFigure 11.17 Illustration of deterministic annealing. Based on http://en .wikipedia .org/wiki/Grad\nuated_optimization .\ntrying different values in parallel for an initial trial period; this can signi\ufb01cantly speed up the\nalgorithm.\nAlgorithm 11.4: Stepwise EM algorithm\n1initialize \u03bc;k=0;\n2repeat\n3foreach example i=1:N in a random order do\n4 si:=/summationtext\nzp(z|xi,\u03b8(\u03bc))\u03c6(xi,z);\n5 \u03bc:= (1\u2212\u03b7k)\u03bc+\u03b7ksi;\n6 k:=k+1\n7until converged ;\n(Liang and Klein Liang and Klein) compare batch EM, incremental EM, and stepwise EM\non four different unsupervised language modeling tasks. They found that stepwise EM (using\u03ba\u22480.7andm\u22481000) was faster than incremental EM, and both were much faster than batch\nEM. In terms of accuracy, stepwise EM was usually as good or sometimes even better than batchEM; incremental EM was often worse than either of the other methods.\n11.4.9 Other EM variants *\nEM is one of the most widely used algorithms in statistics and machine learning. Not surpris-ingly, many variations have been proposed. We brie\ufb02y mention a few below, some of which wewill use in later chapters. See (McLachlan and Krishnan 1997) for more information.\n\u2022Annealed EM In general, EM will only converge to a local maximum. To increase the chance\nof \ufb01nding the global maximum, we can use a variety of methods. One approach is to use\na method known as deterministic annealing (Rose 1998). The basic idea is to \u201csmooth\u201d\nthe posterior \u201clandscape\u201d by raising it to a temperature, and then gradually cooling it, all thewhile slowly tracking the global maximum. See Figure 11.17. for a sketch. (A stochastic version", "398": "368 Chapter 11. Mixture models and the EM algorithm\ntrue log\u2212likelihood\nlower bound\ntraining time\n(a)true log\u2212likelihood\nlower bound\ntraining time\n(b)\nFigure 11.18 Illustration of possible behaviors of variational EM. (a) The lower bound increases at each\niteration, and so does the likelihood. (b) The lower bound increases but the likelihood decreases. In\nthis case, the algorithm is closing the gap between the approximate and true posterior. This can have aregularizing effect. Based on Figure 6 of (Saul et al. 1996). Figure generated by varEMbound .\nof this algorithm is described in Section 24.6.1.) An annealed version of EM is described in\n(Ueda and Nakano 1998).\n\u2022Variational EM In Section 11.4.7, we showed that the optimal thing to do in the E step is to\nmakeqibe the exact posterior over the latent variables, qt\ni(zi)=p(zi|xi,\u03b8t). In this case,\nthe lower bound on the log likelihood will be tight, so the M step will \u201cpush up\u201d on thelog-likelihood itself. However, sometimes it is computationally intractable to perform exactinference in the E step, but we may be able to perform approximate inference. If we canensure that the E step is performing inference based onaal o w e rbound to the likelihood,then the M step can be seen as monotonically increasing this lower bound (see Figure 11.18).This is called variational EM (Neal and Hinton 1998). See Chapter 21 for some variational\ninference methods that can be used in the E step.\n\u2022Monte Carlo EM Another approach to handling an intractable E step is to use a Monte\nCarlo approximation to the expected sufficient statistics. That is, we draw samples from theposterior, z\ns\ni\u223cp(zi|xi,\u03b8t), and then compute the sufficient statistics for each completed\nvector,(xi,zsi), and then average the results. This is called Monte Carlo EM orMCEM(Wei\nand Tanner 1990). (If we only draw a single sample, it is called stochastic EM (Celeux and\nDiebolt 1985).) One way to draw samples is to use MCMC (see Chapter 24). However, if we\nhave to wait for MCMC to converge inside each E step, the method becomes very slow. Analternative is to use stochastic approximation, and only perform \u201cbrief\u201d sampling in the Estep, followed by a partial parameter update. This is called stochastic approximation EM\n(Delyon et al. 1999) and tends to work better than MCEM. Another alternative is to applyMCMC to infer the parameters as well as the latent variables (a fully Bayesian approach), thuseliminating the distinction between E and M steps. See Chapter 24 for details.\n\u2022Generalized EM Sometimes we can perform the E step exactly, but we cannot perform the\nM step exactly. However, we can still monotonically increase the log likelihood by performinga \u201cpartial\u201d M step, in which we merely increase the expected complete data log likelihood,rather than maximizing it. For example, we might follow a few gradient steps. This is called", "399": "11.4. The EM algorithm 369\n0 2 4 6 8 10 12 14 16 18\u221242\u221241.5\u221241\u221240.5\u221240\u221239.5\u221239\u221238.5\niterationsloglikK=5, D=15, N=5000\n  \nEM (1.080)\nOR(1) (1.358)\nOR(1.25) (1.141)\nOR(2) (1.219)\nOR(5) (1.433)\n(a)0 2 4 6 8 10 12 14 16 18\u221242\u221241\u221240\u221239\u221238\u221237\u221236\niterationsloglikK=5, D=15, N=5000\n  \nEM (1.315)\nOR(1) (1.368)\nOR(1.25) (1.381)\nOR(2) (1.540)\nOR(5) (1.474)\n(b)\nFigure 11.19 Illustration of adaptive over-relaxed EM applied to a mixture of 5 Gaussians in 15 dimensions.\nWe show the algorithm applied to two different datasets, randomly sampled from a mixture of 10 Gaussians.\nWe plot the convergence for different update rates \u03b7. Using\u03b7=1gives the same results as regular EM.\nThe actual running time is printed in the legend. Figure generated by mixGaussOverRelaxedEmDemo .\nthegeneralized EM orGEMalgorithm. (This is an unfortunate term, since there are many\nways to generalize EM....)\n\u2022ECM(E) algorithm TheECMalgorithm stands for \u201cexpectation conditional maximization\u201d,\nand refers to optimizing the parameters in the M step sequentially, if they turn out to be\ndependent. The ECMEalgorithm, which stands for \u201cECM either\u201d (Liu and Rubin 1995), is\na variant of ECM in which we maximize the expected complete data log likelihood (the Q\nfunction) as usual, or the observed data log likelihood, during one or more of the conditionalmaximization steps. The latter can be much faster, since it ignores the results of the E step,and directly optimizes the objective of interest. A standard example of this is when \ufb01ttingthe Student T distribution. For \ufb01xed \u03bd, we can update \u03a3as usual, but then to update \u03bd,\nwe replace the standard update of the form \u03bd\nt+1= argmax \u03bdQ((\u03bct+1,\u03a3t+1,\u03bd),\u03b8t)with\n\u03bdt+1= argmax \u03bdlogp(D|\u03bct+1,\u03a3t+1,\u03bd). See (McLachlan and Krishnan 1997) for more\ninformation.\n\u2022Over-relaxed EM Vanilla EM can be quite slow, especially if there is lots of missing data. The\nadaptiveoverrelaxed EM algorithm (Salakhutdinov and Roweis 2003) performs an update\nof the form \u03b8t+1=\u03b8t+\u03b7(M(\u03b8t)\u2212\u03b8t),w h e r e\u03b7is a step-size parameter, and M(\u03b8t)is\nthe usual update computed during the M step. Obviously this reduces to standard EM if\u03b7=1, but using larger values of \u03b7can result in faster convergence. See Figure 11.19 for an\nillustration. Unfortunately, using too large a value of \u03b7can cause the algorithm to fail to\nconverge.\nFinally, note that EM is in fact just a special case of a larger class of algorithms known as\nbound optimization orMMalgorithms (MM stands for minorize-maximize). See (Hunter and\nLange 2004) for further discussion.", "400": "370 Chapter 11. Mixture models and the EM algorithm\n11.5 Model selection for latent variable models\nWhen using LVMs, we must specify the number of latent variables, which controls the model\ncomplexity. In particuarl, in the case of mixture models, we must specify K, the number\nof clusters. Choosing these parameters is an example of model selection. We discuss someapproaches below.\n11.5.1 Model selection for probabilistic models\nThe optimal Bayesian approach, discussed in Section 5.3, is to pick the model with the largestmarginal likelihood, K\n\u2217=a r g m a xkp(D|K).\nThere are two problems with this. First, evaluating the marginal likelihood for LVMs is\nquite difficult. In practice, simple approximations, such as BIC, can be used (see e.g., (Fraleyand Raftery 2002)). Alternatively, we can use the cross-validated likelihood as a performancemeasure, although this can be slow, since it requires \ufb01tting each model Ftimes, where Fis the\nnumber of CV folds.\nThe second issue is the need to search over a potentially large number of models. The usual\napproach is to perform exhaustive search over all candidate values of K. However, sometimes\nwe can set the model to its maximal size, and then rely on the power of the Bayesian Occam\u2019srazor to \u201ckill off\u201d unwanted components. An example of this will be shown in Section 21.6.1.6,when we discuss variational Bayes.\nAn alternative approach is to perform stochastic sampling in the space of models. Traditional\napproaches, such as (Green 1998, 2003; Lunn et al. 2009), are based on reversible jump MCMC,and use birth moves to propose new centers, and death moves to kill off old centers. However,this can be slow and difficult to implement. A simpler approach is to use a Dirichlet processmixture model, which can be \ufb01t using Gibbs sampling, but still allows for an unbounded numberof mixture components; see Section 25.2 for details.\nPerhaps surprisingly, these sampling-based methods can be faster than the simple approach\nof evaluating the quality of each Kseparately. The reason is that \ufb01tting the model for each\nKis often slow. By contrast, the sampling methods can often quickly determine that a certain\nvalue ofKis poor, and thus they need not waste time in that part of the posterior.\n11.5.2 Model selection for non-probabilistic methods\nWhat if we are not using a probabilistic model? For example, how do we choose Kfor theK-\nmeans algorithm? Since this does not correspond to a probability model, there is no likelihood,so none of the methods described above can be used.\nAn obvious proxy for the likelihood is the reconstruction error. De\ufb01ne the squared recon-\nstruction error of a data set D, using model complexity K, as follows:\nE(D,K)=1\n|D|/summationdisplay\ni\u2208D||xi\u2212\u02c6xi||2(11.95)\nIn the case of K-means, the reconstruction is given by \u02c6xi=\u03bczi,w h e r ezi=a r g m i nk||xi\u2212\n\u03bck||2\n2, as explained in Section 11.4.2.6.\nFigure 11.20(a) plots the reconstruction error on the test setfor K-means. We notice that the\nerror decreases with increasing model complexity! The reason for this behavior is as follows:", "401": "11.5. Model selection for latent variable models 371\n2 4 6 8 10 12 14 1600.050.10.150.20.25MSE on test vs K for K\u2212means\n(a)2 4 6 8 10 12 14 1611951200120512101215122012251230123512401245NLL on test set vs K for GMM\n(b)\nFigure 11.20 Test set performance vs Kfor data generated from a mixture of 3 Gaussians in 1d (data is\nshown in Figure 11.21(a)). (a) MSE on test set for K-means. (b) Negative log likelihood on test set for GMM.\nFigure generated by kmeansModelSel1d .\n\u22123 \u22122 \u22121 0 1 2 30102030405060Xtrain\n(a)\u22121 0 100.20.40.60.81K=2, mse=0.2023\n\u22122 0 200.20.40.60.81K=3, mse=0.0818\n\u22122 0 200.20.40.60.81K=4, mse=0.0562\n\u22122 0 200.20.40.60.81K=5, mse=0.0368\n\u22122 0 200.20.40.60.81K=6, mse=0.0275\n\u22122 0 200.20.40.60.81K=10, mse=0.0111\n(b)\n\u22122 0 200.10.20.30.40.5K=2, nll=1244.7882\n\u22122 0 200.10.20.30.40.5K=3, nll=1198.9738\n\u22122 0 200.10.20.30.40.5K=4, nll=1196.9937\n\u22122 0 200.10.20.30.40.5K=5, nll=1202.5869\n\u22122 0 200.10.20.30.40.5K=6, nll=1199.5574\n\u22122 0 200.10.20.30.40.5K=10, nll=1203.2931\n(c)\nFigure 11.21 Synthetic data generated from a mixture of 3 Gaussians in 1d. (a) Histogram of training data.\n(Test data looks essentially the same.) (b) Centroids estimated by K-means for K\u2208{2,3,4,5,6,10}.\n(c) GMM density model estimated by EM for for the same values of K. Figure generated by\nkmeansModelSel1d .", "402": "372 Chapter 11. Mixture models and the EM algorithm\nwhen we add more and more centroids to K-means, we can \u201ctile\u201d the space more densely, as\nshown in Figure 11.21(b). Hence any given test vector is more likely to \ufb01nd a close prototype to\naccurately represent it as Kincreases, thus decreasing reconstruction error. However, if we use\na probabilistic model, such as the GMM, and plot the negative log-likelihood, we get the usualU-shaped curve on the test set, as shown in Figure 11.20(b).\nIn supervised learning, we can always use cross validation to select between non-probabilistic\nmodels of different complexity, but this is not the case with unsupervised learning. Althoughthis is not a novel observation (e.g., it is mentioned in passing in (Hastie et al. 2009, p519), oneof the standard references in this \ufb01eld), it is perhaps not as widely appreciated as it should be.In fact, it is one of the more compelling arguments in favor of probabilistic models.\nGiven that cross validation doesn\u2019t work, and supposing one is unwilling to use probabilistic\nmodels (for some bizarre reason...), how can one choose K? The most common approach is to\nplot the reconstruction error on the training set versus K, and to try to identify a kneeorkink\nin the curve. The idea is that for K<K\n\u2217,w h e r eK\u2217is the \u201ctrue\u201d number of clusters, the rate\nof decrease in the error function will be high, since we are splitting apart things that shouldnot be grouped together. However, for K>K\n\u2217, we are splitting apart \u201cnatural\u201d clusters, which\ndoes not reduce the error by as much.\nThis kink-\ufb01nding process can be automated by use of the gap statistic (Tibshirani et al.\n2001). Nevertheless, identifying such kinks can be hard, as shown in Figure 11.20(a), since theloss function usually drops off gradually. A different approach to \u201ckink \ufb01nding\u201d is described inSection 12.3.2.1.\n11.6 Fitting models with missing data\nSuppose we want to \ufb01t a joint density model by maximum likelihood, but we have \u201choles\u201d in ourdata matrix, due to missing data (usually represented by NaNs). More formally, let O\nij=1if\ncomponent jof data case iis observed, and let Oij=0otherwise. Let Xv={xij:Oij=1}\nbe the visible data, and Xh={xij:Oij=0}be the missing or hidden data. Our goal is to\ncompute\n\u02c6\u03b8=a r g m a x\n\u03b8p(Xv|\u03b8,O) (11.96)\nUnder the missing at random assumption (see Section 8.6.2), we have\np(Xv|\u03b8,O)=N/productdisplay\ni=1p(xiv|\u03b8) (11.97)\nwherexivis a vector created from row iand the columns indexed by the set {j:Oij=1}.\nHence the log-likelihood has the form\nlogp(Xv|\u03b8)=/summationdisplay\nilogp(xiv|\u03b8) (11.98)\nwhere\np(xiv|\u03b8)=/summationdisplay\nxihp(xiv,xih|\u03b8) (11.99)", "403": "11.6. Fitting models with missing data 373\nandxihis the vector of hidden variables for case i(assumed discrete for notational simplicity).\nSubstituting in, we get\nlogp(Xv|\u03b8)=/summationdisplay\nilog/bracketleftBigg/summationdisplay\nxihp(xiv,xih|\u03b8)/bracketrightBigg\n(11.100)\nUnfortunately, this objective is hard to maximize. since we cannot push the log inside the sum.\nHowever, we can use the EM algorithm to compute a local optimum. We give an example ofthis below.\n11.6.1 EM for the MLE of an MVN with missing data\nSuppose we want to \ufb01t an MVN by maximum likelihood, but we have missing data. We can useEM to \ufb01nd a local maximum of the objective, as we explain below.\n11.6.1.1 Getting started\nTo get the algorithm started, we can compute the MLE based on those rows of the data ma-trix that are fully observed. If there are no such rows, we can use some ad-hoc imputationprocedures, and then compute an initial MLE.\n11.6.1.2 E step\nOnce we have \u03b8\nt\u22121, we can compute the expected complete data log likelihood at iteration tas\nfollows:\nQ(\u03b8,\u03b8t\u22121)=E/bracketleftBiggN/summationdisplay\ni=1logN(xi|\u03bc,\u03a3)|D,\u03b8t\u22121/bracketrightBigg\n(11.101)\n=\u2212N\n2log|2\u03c0\u03a3|\u22121\n2/summationdisplay\niE/bracketleftbig\n(xi\u2212\u03bc)T\u03a3\u22121(xi\u2212\u03bc)/bracketrightbig\n(11.102)\n=\u2212N\n2log|2\u03c0\u03a3|\u22121\n2tr(\u03a3\u22121/summationdisplay\niE/bracketleftbig\n(xi\u2212\u03bc)(xi\u2212\u03bc)T/bracketrightbig\n(11.103)\n=\u2212N\n2log|\u03a3|\u2212ND\n2log(2\u03c0)\u22121\n2tr(\u03a3\u22121E[S(\u03bc)]) (11.104)\nwhere\nE[S(\u03bc)]/defines/summationdisplay\ni/parenleftBig\nE/bracketleftbig\nxixT\ni/bracketrightbig\n+\u03bc\u03bcT\u22122\u03bcE[xi]T/parenrightBig\n(11.105)\n(We drop the conditioning of the expectation on Dand\u03b8t\u22121for brevity.) We see that we need\nto compute/summationtext\niE[xi]and/summationtext\niE/bracketleftbig\nxixT\ni/bracketrightbig\n; these are the expected sufficient statistics.\nTo compute these quantities, we use the results from Section 4.3.1. Speci\ufb01cally, consider case\ni, where components vare observed and components hare unobserved. We have\nxih|xiv,\u03b8\u223cN(mi,Vi) (11.106)\nmi/defines\u03bch+\u03a3hv\u03a3\u22121\nvv(xiv\u2212\u03bcv) (11.107)\nVi/defines\u03a3hh\u2212\u03a3hv\u03a3\u22121\nvv\u03a3vh (11.108)", "404": "374 Chapter 11. Mixture models and the EM algorithm\nHence the expected sufficient statistics are\nE[xi]=(E[xih];xiv)=(mi;xiv) (11.109)\nwhere we have assumed (without loss of generality) that the unobserved variables come before\nthe observed variables in the node ordering.\nTo compute E/bracketleftbig\nxixT\ni/bracketrightbig\n, we use the result that cov[x]=E/bracketleftbig\nxxT/bracketrightbig\n\u2212E[x]E/bracketleftbig\nxT/bracketrightbig\n. Hence\nE/bracketleftbig\nxixT\ni/bracketrightbig\n=E/bracketleftbigg/parenleftbiggxih\nxiv/parenrightbigg/parenleftbig\nxT\nihxT\niv/parenrightbig/bracketrightbigg\n=/parenleftbiggE/bracketleftbig\nxihxT\nih/bracketrightbig\nE[xih]xT\niv\nxivE[xih]TxivxTiv/parenrightbigg\n(11.110)\nE/bracketleftbig\nxihxT\nih/bracketrightbig\n=E[xih]E[xih]T+Vi (11.111)\n11.6.1.3 M step\nBy solving \u2207Q(\u03b8,\u03b8(t\u22121))=0, we can show that the M step is equivalent to plugging these\nESS into the usual MLE equations to get\n\u03bct=1\nN/summationdisplay\niE[xi] (11.112)\n\u03a3t=1\nN/summationdisplay\niE/bracketleftbig\nxixTi/bracketrightbig\n\u2212\u03bct(\u03bct)T(11.113)\nThus we see that EM is notequivalent to simply replacing variables by their expectations and\napplying the standard MLE formula; that would ignore the posterior variance and would result\nin an incorrect estimate. Instead we must compute the expectation of the sufficient statistics,and plug that into the usual equation for the MLE. We can easily modify the algorithm toperform MAP estimation, by plugging in the ESS into the equation for the MAP estimate. For animplementation, see gaussMissingFitEm .\n11.6.1.4 Example\nAs an example of this procedure in action, let us reconsider the imputation problem fromSection 4.3.2.3, which had N= 10010-dimensional data cases, with 50% missing data. Let\nus \ufb01t the parameters using EM. Call the resulting parameters \u02c6\u03b8. We can use our model for\npredictions by computing E/bracketleftBig\nx\nih|xiv,\u02c6\u03b8/bracketrightBig\n. Figure 11.22(a-b) indicates that the results obtained\nusing the learned parameters are almost as good as with the true parameters. Not surprisingly,performance improves with more data, or as the fraction of missing data is reduced.\n11.6.1.5 Extension to the GMM case\nIt is straightforward to \ufb01t a mixture of Gaussians in the presence of partially observed datavectorsx\ni. We leave the details as an exercise.\nExercises\nExercise 11.1 Student T as in\ufb01nite mixture of Gaussians\nDerive Equation 11.61. For simplicity, assume a one-dimensional distribution.", "405": "11.6. Fitting models with missing data 375\n\u221210 0 10\u221210\u221250510\ntruthimputedR2 = 0.260\n\u221220 0 20\u22121001020\ntruthimputedR2 = 0.685\n\u221210 0 10\u221210\u221250510\ntruthimputedR2 = 0.399imputation with true params\n\u221210 0 10\u221210\u221250510\ntruthimputedR2 = 0.531\n(a)\u221210 0 10\u221210\u221250510\ntruthimputedR2 = 0.220\n\u221220 0 20\u22121001020\ntruthimputedR2 = 0.609\n\u221210 0 10\u221210\u221250510\ntruthimputedR2 = 0.113imputation with em\n\u221210 0 10\u221210\u221250510\ntruthimputedR2 = 0.532\n(b)\nFigure 11.22 Illustration of data imputation. (a) Scatter plot of true values vs imputed values us-\ning true parameters. (b) Same as (b), but using parameters estimated with EM. Figure generated by\ngaussImputationDemo .\nExercise 11.2 EM for mixtures of Gaussians\nShow that the M step for ML estimation of a mixture of Gaussians is given by\n\u03bck=/summationtext\nirikxi\nrk(11.114)\n\u03a3k=/summationtext\nirik(xi\u2212\u03bck)(xi\u2212\u03bck)T\nrk=/summationtext\nirikxixT\ni\u2212rk\u03bck\u03bcT\nk\nrk(11.115)\nExercise 11.3 EM for mixtures of Bernoullis\n\u2022 Show that the M step for ML estimation of a mixture of Bernoullis is given by\n\u03bckj=/summationtext\nirikxij/summationtext\nirik(11.116)\n\u2022 Show that the M step for MAP estimation of a mixture of Bernoullis with a \u03b2(\u03b1,\u03b2)prior is given by\n\u03bckj=(/summationtext\nirikxij)+\u03b1\u22121\n(/summationtext\nirik)+\u03b1+\u03b2\u22122(11.117)\nExercise 11.4 EM for mixture of Student distributions\nDerive the EM algorithm for ML estimation of a mixture of multivariate Student T distributions.\nExercise 11.5 Gradient descent for \ufb01tting GMM\nConsider the Gaussian mixture model\np(x|\u03b8)=/summationdisplay\nk\u03c0kN(x|\u03bck,\u03a3k) (11.118)\nDe\ufb01ne the log likelihood as\n/lscript(\u03b8)=N/summationdisplay\nn=1logp(xn|\u03b8) (11.119)", "406": "376 Chapter 11. Mixture models and the EM algorithm\nN\nm l\u03bcj \u03c3kxnJn Knpq\nFigure 11.23 A mixture of Gaussians with two discrete latent indicators. Jnspeci\ufb01es which mean to use,\nandKnspeci\ufb01es which variance to use.\nDe\ufb01ne the posterior responsibility that cluster khas for datapoint nas follows:\nrnk/definesp(zn=k|xn,\u03b8)=\u03c0kN(xn|\u03bck,\u03a3k)/summationtextK\nk/prime=1\u03c0k/primeN(xn|\u03bck/prime,\u03a3k/prime)(11.120)\na. Show that the gradient of the log-likelihood wrt \u03bckis\nd\nd\u03bck/lscript(\u03b8)=/summationdisplay\nnrnk\u03a3\u22121\nk(xn\u2212\u03bck) (11.121)\nb. Derive the gradient of the log-likelihood wrt \u03c0k. (For now, ignore any constraints on \u03c0k.)\nc. One way to handle the constraint that/summationtextKk=1\u03c0k=1is to reparameterize using the softmax function:\n\u03c0k/definesewk\n/summationtextKk\n/prime=1ewk/prime(11.122)\nHerewk\u2208Rare unconstrained parameters. Show that\nd\ndwk/lscript(\u03b8)=/summationdisplay\nnrnk\u2212\u03c0k (11.123)\n(There may be a constant factor missing in the above expression...) Hint: use the chain rule and the\nfact that\nd\u03c0j\ndwk=/braceleftbigg\u03c0j(1\u2212\u03c0j)ifj=k\n\u2212\u03c0j\u03c0kifj/negationslash=k(11.124)\nwhich follows from Exercise 8.4(1).\nd. Derive the gradient of the log-likelihood wrt \u03a3k. (For now, ignore any constraints on \u03a3k.)\ne. One way to handle the constraint that \u03a3kbe a symmetric positive de\ufb01nite matrix is to reparame-\nterize using a Cholesky decomposition, \u03a3k=RT\nkR,w h e r eRis an upper-triangular, but otherwise\nunconstrained matrix. Derive the gradient of the log-likelihood wrt Rk.\nExercise 11.6 EM for a \ufb01nite scale mixture of Gaussians\n(Source: Jaakkola..) Consider the graphical model in Figure 11.23 which de\ufb01nes the following:\np(xn|\u03b8)=m/summationdisplay\nj=1pj/bracketleftBiggl/summationdisplay\nk=1qkN(xn|\u03bcj,\u03c32\nk)/bracketrightBigg\n(11.125)", "407": "11.6. Fitting models with missing data 377\nwhere\u03b8={p1,...,p m,\u03bc1,...,\u03bc m,q1,...,q l,\u03c32\n1,...,\u03c32\nl}are all the parameters. Here pj/definesP(Jn=\nj)andqk/definesP(Kn=k)are the equivalent of mixture weights. We can think of this as a mixture\nofmnon-Gaussian components, where each component distribution is a scale mixture, p(x|j;\u03b8)=/summationtextl\nk=1qkN(x;\u03bcj,\u03c32\nk), combining Gaussians with different variances (scales).\nWe will now derive a generalized EM algorithm for this model. (Recall that in generalized EM, we do a\npartial update in the M step, rather than \ufb01nding the exact maximum.)\na. Derive an expression for the responsibilities, P(Jn=j,Kn=k|xn,\u03b8), needed for the E step.\nb. Write out a full expression for the expected complete log-likelihood\nQ(\u03b8new,\u03b8old)=E\u03b8oldN/summationdisplay\nn=1logP(Jn,Kn,xn|\u03b8new) (11.126)\nc. Solving the M-step would require us to jointly optimize the means \u03bc1,...,\u03bc mand the variances\n\u03c32\n1,...,\u03c32\nl. It will turn out to be simpler to \ufb01rst solve for the \u03bcj\u2019s given \ufb01xed \u03c32\nj\u2019s, and subsequently\nsolve for \u03c32\nj\u2019s given the new values of \u03bcj\u2019s. For brevity, we will just do the \ufb01rst part. Derive an\nexpression for the maximizing \u03bcj\u2019s given \ufb01xed \u03c32\n1:l, i.e., solve\u2202Q\n\u2202\u03bcnew=0.\nExercise 11.7 Manual calculation of the M step for a GMM\n(Source: de Freitas.) In this question we consider clustering 1D data with a mixture of 2 Gaussians usingthe EM algorithm. You are given the 1-D data points x= [ 11 02 0 ] . Suppose the output of the E\nstep is the following matrix:\nR=\u23a1\n\u23a310\n0.40.6\n01\u23a4\u23a6 (11.127)\nwhere entry r\ni,cis the probability of obervation xibelonging to cluster c(the responsibility of cluster cfor\ndata point i). You just have to compute the M step. You may state the equations for maximum likelihood\nestimates of these quantities (which you should know) without proof; you just have to apply the equations\nto this data set. You may leave your answer in fractional form. Show your work.\na. Write down the likelihood function you are trying to optimize.\nb. After performing the M step for the mixing weights \u03c01,\u03c02, what are the new values?\nc. After performing the M step for the means \u03bc1and\u03bc2, what are the new values?\nExercise 11.8 Moments of a mixture of Gaussians\nConsider a mixture of KGaussians\np(x)=K/summationdisplay\nk=1\u03c0kN(x|\u03bck,\u03a3k) (11.128)\na. Show that\nE[x]=/summationdisplay\nk\u03c0k\u03bck (11.129)", "408": "378 Chapter 11. Mixture models and the EM algorithm\n0 2 4 6 8 10 12 14 16 18\u22122\u22121012345\nFigure 11.24 Some data points in 2d. Circles represent the initial guesses for m1andm2.\nb. Show that\ncov[x]=/summationdisplay\nk\u03c0k[\u03a3k+\u03bck\u03bcT\nk]\u2212E[x]E[x]T(11.130)\nHint: use the fact that cov[x]= E/bracketleftbig\nxxT/bracketrightbig\n\u2212E[x]E[x]T.\nExercise 11.9 K-means clustering by hand\n(Source: Jaakkola.)\nIn Figure 11.24, we show some data points which lie on the integer grid. (Note that the x-axis has been\ncompressed; distances should be measured using the actual grid coordinates.) Suppose we apply the K-means algorithm to this data, using K=2and with the centers initialized at the two circled data points.\nDraw the \ufb01nal clusters obtained after K-means converges (show the approximate location of the new centersand group together all the points assigned to each center). Hint: think about shortest Euclidean distance.\nExercise 11.10 Deriving the K-means cost function\nShow that\nJ\nW(z)=1\n2K/summationdisplay\nk=1/summationdisplay\ni:zi=k/summationdisplay\ni/prime:zi/prime=k(xi\u2212xi/prime)2=K/summationdisplay\nk=1nk/summationdisplay\ni:zi=k(xi\u2212xk)2(11.131)\nHint: note that, for any \u03bc,\n/summationdisplay\ni(xi\u2212\u03bc)2=/summationdisplay\ni[(xi\u2212x)\u2212(\u03bc\u2212x)]2(11.132)\n=/summationdisplay\ni(xi\u2212x)2+/summationdisplay\ni(x\u2212\u03bc)2\u22122/summationdisplay\ni(xi\u2212x)(\u03bc\u2212x) (11.133)\n=ns2+n(x\u2212\u03bc)2(11.134)\nwheres2=1\nn/summationtextn\ni=1(xi\u2212x)2, since\n/summationdisplay\ni(xi\u2212x)(\u03bc\u2212x)=( \u03bc\u2212x)/parenleftBigg\n(/summationdisplay\nixi)\u2212nx/parenrightBigg\n=(\u03bc\u2212x)(nx\u2212nx)=0 (11.135)\nExercise 11.11 Visible mixtures of Gaussians are in the exponential family\nShow that the joint distribution p(x,z|\u03b8)for a 1d GMM can be represented in exponential family form.", "409": "11.6. Fitting models with missing data 379\n2 2.05 2.1 2.15 2.2 2.25 2.3 2.35 2.42.62.833.23.43.63.844.24.44.6\ninverse temperaturesurvival timeregression with censored data; red x = censored, green * = predicted\n  \nEM\nOLS\nFigure 11.25 Example of censored linear regression. Black circles are observed training points, red crosses\nare observed but censored training points. Green stars are predicted values of the censored training points.\nWe also show the lines \ufb01t by least squares (ignoring censoring) and by EM. Based on Figure 5.6 of (Tanner1996). Figure generated by linregCensoredSchmeeHahnDemo , written by Hannes Bretschneider.\nExercise 11.12 EM for robust linear regression with a Student t likelihood\nConsider a model of the form\np(y\ni|xi,w,\u03c32,\u03bd)=T(yi|wTxi,\u03c32,\u03bd) (11.136)\nDerive an EM algorithm to compute the MLE for w. You may assume \u03bdand\u03c32are \ufb01xed, for simplicity.\nHint: see Section 11.4.5.\nExercise 11.13 EM for EB estimation of Gaussian shrinkage model\nExtend the results of Section 5.6.2.2 to the case where the \u03c32\njare not equal (but are known). Hint: treat\nthe\u03b8jas hidden variables, and then to integrate them out in the E step, and maximize \u03b7=(\u03bc,\u03c42)in the\nM step.Exercise 11.14 EM for censored linear regression\nCensored regression refers to the case where one knows the outcome is at least (or at most) a certain\nvalue, but the precise value is unknown. This arises in many different settings. For example, suppose one\nis trying to learn a model that can predict how long a program will take to run, for different settings ofits parameters. One may abort certain runs if they seem to be taking too long; the resulting run times aresaid to be right censored. For such runs, all we know is that y\ni\u2265ci,w h e r eciis the censoring time,\nthat is,yi=m i n (zi,ci),w h e r eziis the true running time and yiis the observed running time. We\ncan also de\ufb01ne left censored andinterval censored models.3Derive an EM algorithm for \ufb01tting a linear\nregression model to right-censored data. Hint: use the results from Exercise 11.15. See Figure 11.25 for anexample, based on the data from (Schmee and Hahn 1979). We notice that the EM line is tilted upwardsmore, since the model takes into account the fact that the truncated values are actually higher than theobserved values.\n3. There is a closely related model in econometrics called the Tobit model, in which yi=m a x (zi,0), so we only\nget to observe positive outcomes. An example of this is when zirepresents \u201cdesired investment\u201d, and yiis actual\ninvestment. Probit regression (Section 9.4) is another example.", "410": "380 Chapter 11. Mixture models and the EM algorithm\nExercise 11.15 Posterior mean and variance of a truncated Gaussian\nLetzi=\u03bci+\u03c3/epsilon1i,w h e r e/epsilon1i\u223cN(0,1). Sometimes, such as in probit regression or censored regression,\nwe do not observe zi, but we observe the fact that it is above some threshold, namely we observe the event\nE=I(zi\u2265ci)=I(/epsilon1i\u2265ci\u2212\u03bci\n\u03c3). (See Exercise 11.14 for details on censored regression, and Section 11.4.6\nfor probit regression.) Show that\nE[zi|zi\u2265ci]=\u03bc i+\u03c3H/parenleftBigci\u2212\u03bci\n\u03c3/parenrightBig\n(11.137)\nand\nE/bracketleftbig\nz2\ni|zi\u2265ci/bracketrightbig\n=\u03bc2\ni+\u03c32+\u03c3(ci+\u03bci)H/parenleftBigci\u2212\u03bci\n\u03c3/parenrightBig\n(11.138)\nwhere we have de\ufb01ned\nH(u)/defines\u03c6(u)\n1\u2212\u03a6(u)(11.139)\nand where \u03c6(u)is the pdf of a standard Gaussian, and \u03a6(u)is its cdf.\nHint 1: we have p(/epsilon1i|E)=p(/epsilon1i,E)\np(E),w h e r eE is some event of interest.\nHint 2: It can be shown that\nd\ndwN(w|0,1) =\u2212wN(w|0,1) (11.140)\nand hence\n/integraldisplayc\nbwN(w|0,1) =N(b|0,1)\u2212N(c|0,1) (11.141)", "411": "12 Latent linear models\n12.1 Factor analysis\nOne problem with mixture models is that they only use a single latent variable to generate the\nobservations. In particular, each observation can only come from one of Kprototypes. One can\nthink of a mixture model as using Khidden binary variables, representing a one-hot encoding\nof the cluster identity. But because these variables are mutually exclusive, the model is stilllimited in its representational power.\nAn alternative is to use a vector of real-valued latent variables, z\ni\u2208RL. The simplest prior\nto use is a Gaussian (we will consider other choices later):\np(zi)=N(zi|\u03bc0,\u03a30) (12.1)\nIf the observations are also continuous, so xi\u2208RD, we may use a Gaussian for the likelihood.\nJust as in linear regression, we will assume the mean is a linear function of the (hidden) inputs,thus yielding\np(x\ni|zi,\u03b8)=N(Wzi+\u03bc,\u03a8) (12.2)\nwhereWis aD\u00d7Lmatrix, known as the factor loading matrix, and \u03a8is aD\u00d7Dcovariance\nmatrix. We take \u03a8to be diagonal, since the whole point of the model is to \u201cforce\u201d zito explain\nthe correlation, rather than \u201cbaking it in\u201d to the observation\u2019s covariance. This overall modelis called factor analysis orFA. The special case in which \u03a8=\u03c3\n2Iis called probabilistic\nprincipal components analysis orPPCA. The reason for this name will become apparent later.\nThe generative process, where L=1,D=2and\u03a8is diagonal, is illustrated in Figure 12.1.\nWe take an isotropic Gaussian \u201cspray can\u201d and slide it along the 1d line de\ufb01ned by wzi+\u03bc.\nThis induces an ellongated (and hence correlated) Gaussian in 2d.\n12.1.1 FA is a low rank parameterization of an MVN\nFA can be thought of as a way of specifying a joint density model on xusing a small number\nof parameters. To see this, note that from Equation 4.126, the induced marginal distributionp(x\ni|\u03b8)is a Gaussian:\np(xi|\u03b8)=/integraldisplay\nN(xi|Wzi+\u03bc,\u03a8)N(zi|\u03bc0,\u03a30)dzi (12.3)\n=N(xi|W\u03bc0+\u03bc,\u03a8+W\u03a30WT) (12.4)", "412": "382 Chapter 12. Latent linear models\nZ\n[\u0014[\u0014[\u0015[\u0015\n]S\u000b]\fS\u000b[\f\u021d\nS\u000b]\f]\u0003_Z_S\u000b[_]\f\n\u021d\n\u0003\nFigure 12.1 Illustration of the PPCA generative process, where we have L=1latent dimension generating\nD=2observed dimensions. Based on Figure 12.9 of (Bishop 2006b).\nFrom this, we see that we can set \u03bc0=0without loss of generality, since we can always absorb\nW\u03bc0into\u03bc. Similarly, we can set \u03a30=Iwithout loss of generality, because we can always\n\u201cemulate\u201d a correlated prior by using de\ufb01ning a new weight matrix, \u02dcW=W\u03a3\u22121\n2\n0. Then we\n\ufb01nd\ncov[x|\u03b8]=\u02dcWT+E/bracketleftbig\n/epsilon1/epsilon1T/bracketrightbig\n=(W\u03a3\u22121\n2\n0)\u03a30(W\u03a3\u22121\n2\n0)T+\u03a8=WWT+\u03a8 (12.5)\nWe thus see that FA approximates the covariance matrix of the visible vector using a low-rank\ndecomposition:\nC/definescov[x]=WWT+\u03a8 (12.6)\nThis only uses O(LD)parameters, which allows a \ufb02exible compromise between a full covariance\nGaussian, with O(D2)parameters, and a diagonal covariance, with O(D)parameters. Note that\nif we did not restrict \u03a8to be diagonal, we could trivially set \u03a8to a full covariance matrix; then\nwe could set W=0, in which case the latent factors would not be required.\n12.1.2 Inference of the latent factors\nAlthough FA can be thought of as just a way to de\ufb01ne a density on x, it is often used because\nwe hope that the latent factors zwill reveal something interesting about the data. To do this,\nwe need to compute the posterior over the latent factors. We can use Bayes rule for Gaussians\nto give\np(zi|xi,\u03b8)=N(zi|mi,\u03a3i) (12.7)\n\u03a3i/defines(\u03a3\u22121\n0+WT\u03a8\u22121W)\u22121(12.8)\nmi/defines\u03a3i(WT\u03a8\u22121(xi\u2212\u03bc)+\u03a3\u22121\n0\u03bc0) (12.9)\nNote that in the FA model, \u03a3iis actually independent of i, so we can denote it by \u03a3. Computing\nthis matrix takes O(L3+L2D)time, and computing each mi=E[zi|xi,\u03b8]takesO(L2+LD)\ntime. The miare sometimes called the latent scores,o rl a t e n t factors.", "413": "12.1. Factor analysis 383\n\u22121 \u22120.5 0 0.5 1\u22121\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.81\nRetailDealerEngine\nCylinders\nHorsepower\nCityMPGHighwayMPGWeight WheelbaseLengthWidth\nComponent 1Component 2rotation=none\nPorsche 911 Honda InsightGMC Yukon XL 2500 SLT\nMercedes\u2212Benz CL600Kia Sorento LX\nMercedes\u2212Benz G500\nSaturn Ion1Nissan Pathfinder Armada SE\nFigure 12.2 2D projection of 2004 cars data based on factor analysis. The blue text are the names of cars\ncorresponding to certain chosen points. Figure generated by faBiplotDemo .\nLet us give a simple example, based (Shalizi 2009). We consider a dataset of D=1 1variables\nandN= 387cases describing various aspects of cars, such as the engine size, the number of\ncylinders, the miles per gallon (MPG), the price, etc. We \ufb01rst \ufb01t a L=2dimensional model. We\ncan plot the miscores as points in R2, to visualize the data, as shown in Figure 12.2.\nTo get a better understanding of the \u201cmeaning\u201d of the latent factors, we can project unit vectors\ncorresponding to each of the feature dimensions, e1=( 1,0,...,0),e2=( 0,1,0,...,0), etc.\ninto the low dimensional space. These are shown as blue lines in Figure 12.2; this is known as\nabiplot. We see that the horizontal axis represents price, corresponding to the features labeled\n\u201cdealer\u201d and \u201cretail\u201d, with expensive cars on the right. The vertical axis represents fuel efficiency(measured in terms of MPG) versus size: heavy vehicles are less efficient and are higher up,whereas light vehicles are more efficient and are lower down. We can \u201cverify\u201d this interpretationby clicking on some points, and \ufb01nding the closest exemplars in the training set, and printingtheir names, as in Figure 12.2. However, in general, interpreting latent variable models is fraughtwith difficulties, as we discuss in Section 12.1.3.\n12.1.3 Unidenti\ufb01ability\nJust like with mixture models, FA is also unidenti\ufb01able. To see this, suppose Ris an arbitrary\northogonal rotation matrix, satisfying RRT=I. Let us de\ufb01ne \u02dcW=WR; then the likelihood", "414": "384 Chapter 12. Latent linear models\nfunction of this modi\ufb01ed matrix is the same as for the unmodi\ufb01ed matrix, since\ncov[x]=\u02dcWE/bracketleftbig\nzzT/bracketrightbig\u02dcWT+E/bracketleftbig\n/epsilon1/epsilon1T/bracketrightbig\n(12.10)\n=WRRTWT+\u03a8=WWT+\u03a8 (12.11)\nGeometrically, multiplying Wby an orthogonal matrix is like rotating zbefore generating x;\nbut since zis drawn from an isotropic Gaussian, this makes no difference to the likelihood.\nConsequently, we cannot unique identify W, and therefore cannot uniquely identify the latent\nfactors, either.\nTo ensure a unique solution, we need to remove L(L\u22121)/2degrees of freedom, since that\nis the number of orthonormal matrices of size L\u00d7L.1In total, the FA model has D+LD\u2212\nL(L\u22121)/2free parameters (excluding the mean), where the \ufb01rst term arises from \u03a8. Obviously\nwe require this to be less than or equal to D(D+1)/2, which is the number of parameters in\nan unconstrained (but symmetric) covariance matrix. This gives us an upper bound on L,a s\nfollows:\nLmax=\u230aD+0.5(1\u2212\u221a\n1+8D)\u230b (12.12)\nFor example, D=6impliesL\u22643. But we usually never choose this upper bound, since it\nwould result in over\ufb01tting (see discussion in Section 12.3 on how to choose L).\nUnfortunately, even if we set L<L max, we still cannot uniquely identify the parameters,\nsince the rotational ambiguity still exists. Non-identi\ufb01ability does not affect the predictive per-\nformance of the model. However, it does affect the loading matrix, and hence the interpretationof the latent factors. Since factor analysis is often used to uncover structure in the data, thisproblem needs to be addressed. Here are some commonly used solutions:\n\u2022ForcingWto be orthonormal Perhaps the cleanest solution to the identi\ufb01ability problem\nis to force Wto be orthonormal, and to order the columns by decreasing variance of the\ncorresponding latent factors. This is the approach adopted by PCA, which we will discuss in\nSection 12.2. The result is not necessarily more interpretable, but at least it is unique.\n\u2022ForcingWto be lower triangular One way to achieve identi\ufb01ability, which is popular\nin the Bayesian community (e.g., (Lopes and West 2004)), is to ensure that the \ufb01rst visiblefeature is only generated by the \ufb01rst latent factor, the second visible feature is only generatedby the \ufb01rst two latent factors, and so on. For example, if L=3andD=4, the correspond\nfactor loading matrix is given by\nW=\u239b\n\u239c\u239c\u239dw\n1100\nw21w220\nw31w32w33\nw41w42w43\u239e\n\u239f\u239f\u23a0(12.13)\nWe also require that wjj>0forj=1:L. The total number of parameters in this\nconstrained matrix is D+DL\u2212L(L\u22121)/2, which is equal to the number of uniquely\nidenti\ufb01able parameters. The disadvantage of this method is that the \ufb01rst Lvisible variables,\n1. To see this, note that there are L\u22121free parameters in Rin the \ufb01rst column (since the column vector must be\nnormalized to unit length), there are L\u22122free parameters in the second column (which must be orthogonal to the\n\ufb01rst), and so on.", "415": "12.1. Factor analysis 385\n\u03a8\u03c0\nWk\u03bck\nxiqizi\nN K\nFigure 12.3 Mixture of factor analysers as a DGM.\nknown as the founder variables, affect the interpretation of the latent factors, and so must\nbe chosen carefully.\n\u2022Sparsity promoting priors on the weights Instead of pre-specifying which entries in W\nare zero, we can encourage the entries to be zero, using /lscript1regularization (Zou et al. 2006),\nARD (Bishop 1999; Archambeau and Bach 2008), or spike-and-slab priors (Rattray et al. 2009).\nThis is called sparse factor analysis. This does not necessarily ensure a unique MAP estimate,but it does encourage interpretable solutions. See Section 13.8.\n\u2022Choosing an informative rotation matrix There are a variety of heuristic methods that try\nto \ufb01nd rotation matrices Rwhich can be used to modify W(and hence the latent factors) so\nas to try to increase the interpretability, typically by encouraging them to be (approximately)sparse. One popular method is known as varimax (Kaiser 1958).\n\u2022Use of non-Gaussian priors for the latent factors In Section 12.6, we will dicuss how re-\nplacingp(z\ni)with a non-Gaussian distribution can enable us to sometimes uniquely identify\nWas well as the latent factors. This technique is known as ICA.\n12.1.4 Mixtures of factor analysers\nThe FA model assumes that the data lives on a low dimensional linear manifold. In reality, mostdata is better modeled by some form of low dimensional curvedmanifold. We can approximate\na curved manifold by a piecewise linear manifold. This suggests the following model: let thek\u2019th linear subspace of dimensionality L\nkbe represented by Wk,f o rk=1:K. Suppose we\nhave a latent indicator qi\u2208{1,...,K}specifying which subspace we should use to generate\nthe data. We then sample zifrom a Gaussian prior and pass it through the Wkmatrix (where\nk=qi), and add noise. More precisely, the model is as follows:\np(xi|zi,qi=k,\u03b8)=N (xi|\u03bck+Wkzi,\u03a8) (12.14)\np(zi|\u03b8)=N (zi|0,I) (12.15)\np(qi|\u03b8)=C a t ( qi|\u03c0) (12.16)", "416": "386 Chapter 12. Latent linear models\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2\u22121.5\u22121\u22120.500.511.5\n(a)\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2\u22121.5\u22121\u22120.500.511.5\n(b)\nFigure 12.4 Mixture of 1d PPCAs \ufb01t to a dataset, for K=1,10. Figure generated by\nmixPpcaDemoNetlab .\nThis is called a mixture of factor analysers (MFA) (Hinton et al. 1997). The CI assumptions are\nrepresented in Figure 12.3.\nAnother way to think about this model is as a low-rank version of a mixture of Gaussians. In\nparticular, this model needs O(KLD)parameters instead of the O(KD2)parameters needed\nfor a mixture of full covariance Gaussians. This can reduce over\ufb01tting. In fact, MFA is a good\ngeneric density model for high-dimensional real-valued data.\n12.1.5 EM for factor analysis models\nUsing the results from Chapter 4, it is straightforward to derive an EM algorithm to \ufb01t an FAmodel. With just a little more work, we can \ufb01t a mixture of FAs. Below we state the resultswithout proof. The derivation can be found in (Ghahramani and Hinton 1996a); however, derivingthese equations yourself is a useful exercise if you want to become pro\ufb01cient at the math.\nTo obtain the results for a single factor analyser, just set r\nic=1andc=1in the equations\nbelow. In Section 12.2.5 we will see a further simpli\ufb01cation of these equations that arises when\ufb01tting a PPCA model, where the results will turn out to have a particularly simple and elegantintepretation.\nIn the E step, we compute the posterior responsibility of cluster cfor data point iusing\nr\nic/definesp(qi=c|xi,\u03b8)\u221d\u03c0cN(xi|\u03bcc,WcWT\nc+\u03a8) (12.17)\nThe conditional posterior for ziis given by\np(zi|xi,qi=c,\u03b8)=N (zi|mic,\u03a3ic) (12.18)\n\u03a3ic/defines(IL+WT\nc\u03a8\u22121\ncWc)\u22121(12.19)\nmic/defines\u03a3ic(WT\nc\u03a8\u22121\nc(xi\u2212\u03bcc)) (12.20)\nIn the M step, it is easiest to estimate \u03bccandWcat the same time, by de\ufb01ning \u02dcWc=", "417": "12.2. Principal components analysis (PCA) 387\n(Wc,\u03bcc),\u02dcz=(z,1), Also, de\ufb01ne\nbic/definesE[\u02dc z|xi,qi=c]=[mic;1] (12.21)\nCic/definesE/bracketleftbig\n\u02dcz\u02dczT|xi,qi=c/bracketrightbig\n=/parenleftbiggE/bracketleftbig\nzzT|xi,qi=c/bracketrightbig\nE[z|xi,qi=c]\nE[z|xi,qi=c]T1/parenrightbigg\n(12.22)\nThen the M step is as follows:\n\u02c6\u02dcWc=/bracketleftBigg/summationdisplay\niricxibT\nic/bracketrightBigg/bracketleftBigg/summationdisplay\niricCic/bracketrightBigg\u22121\n(12.23)\n\u02c6\u03a8=1\nNdiag/braceleftBigg/summationdisplay\nicric/parenleftBig\nxi\u2212\u02c6\u02dcWcbic/parenrightBig\nxT\ni/bracerightBigg\n(12.24)\n\u02c6\u03c0c=1\nNN/summationdisplay\ni=1ric (12.25)\nNote that these updates are for \u201cvanilla\u201d EM. A much faster version of this algorithm, based\non ECM, is described in (Zhao and Yu 2008).\n12.1.6 Fitting FA models with missing data\nIn many applications, such as collaborative \ufb01ltering, we have missing data. One virtue of the\nEM approach to \ufb01tting an FA/PPCA model is that it is easy to extend to this case. However,over\ufb01tting can be a problem if there is a lot of missing data. Consequently it is important toperform MAP estimation or to use Bayesian inference. See e.g., (Ilin and Raiko 2010) for details.\n12.2 Principal components analysis (PCA)\nConsider the FA model where we constrain \u03a8=\u03c32I, andWto be orthonormal. It can\nbe shown (Tipping and Bishop 1999) that, as \u03c32\u21920, this model reduces to classical (non-\nprobabilistic) principal components analysis (PCA), also known as the Karhunen Loeve\ntransform. The version where \u03c32>0is known as probabilistic PCA (PPCA) (Tipping and\nBishop 1999), or sensible PCA (Roweis 1997). (An equivalent result was derived independently,\nfrom a different perspective, in (Moghaddam and Pentland 1995).)\nTo make sense of this result, we \ufb01rst have to learn about classical PCA. We then connect PCA\nto the SVD. And \ufb01nally we return to discuss PPCA.\n12.2.1 Classical PCA: statement of the theorem\nThesynthesis view of classical PCA is summarized in the forllowing theorem.\nTheorem 12.2.1. Suppose we want to \ufb01nd an orthogonal set of Llinear basis vectors wj\u2208RD,\nand the corresponding scores zi\u2208RL, such that we minimize the average reconstruction error\nJ(W,Z)=1\nNN/summationdisplay\ni=1||xi\u2212\u02c6xi||2(12.26)", "418": "388 Chapter 12. Latent linear models\n\u22125 0 5\u22123\u22122\u2212101234\n(a)\u22124 \u22123 \u22122 \u22121 0 1 2 3\u22125\u22124\u22123\u22122\u2212101234\n(b)\nFigure 12.5 An illustration of PCA and PPCA where D=2andL=1. Circles are the original data\npoints, crosses are the reconstructions. The red star is the data mean. (a) PCA. The points are orthogonally\nprojected onto the line. Figure generated by pcaDemo2d . (b) PPCA. The projection is no longer orthogonal:\nthe reconstructions are shrunk towards the data mean (red star). Based on Figure 7.6 of (Nabney 2001).Figure generated by ppcaDemo2d .\nwhere\u02c6xi=Wzi, subject to the constraint that Wis orthonormal. Equivalently, we can write this\nobjective as follows:\nJ(W,Z)=||X\u2212WZT||2\nF (12.27)\nwhereZis anN\u00d7Lmatrix with the ziin its rows, and ||A||Fis theFrobenius norm of matrix\nA, de\ufb01ned by\n||A||F=/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay\ni=1n/summationdisplay\nj=1a2\nij=/radicalBig\ntr(ATA)=||A(:)|| 2 (12.28)\nThe optimal solution is obtained by setting \u02c6W=VL, whereVLcontains the Leigenvectors\nwith largest eigenvalues of the empirical covariance matrix, \u02c6\u03a3=1\nN/summationtextN\ni=1xixT\ni. (We assume the\nxihave zero mean, for notational simplicity.) Furthermore, the optimal low-dimensional encoding\nof the data is given by \u02c6zi=WTxi, which is an orthogonal projection of the data onto the column\nspace spanned by the eigenvectors.\nAn example of this is shown in Figure 12.5(a) for D=2andL=1. The diagonal line is the\nvectorw1; this is called the \ufb01rst principal component or principal direction. The data points\nxi\u2208R2are orthogonally projected onto this line to get zi\u2208R. This is the best 1-dimensional\napproximation to the data. (We will discuss Figure 12.5(b) later.)\nIn general, it is hard to visualize higher dimensional data, but if the data happens to be a\nset of images, it is easy to do so. Figure 12.6 shows the \ufb01rst three principal vectors, reshaped\nas images, as well as the reconstruction of a speci\ufb01c image using a varying number of basisvectors. (We discuss how to choose Lin Section 11.5.)\nBelow we will show that the principal directions are the ones along which the data shows\nmaximal variance. This means that PCA can be \u201cmisled\u201d by directions in which the varianceis high merely because of the measurement scale. Figure 12.7(a) shows an example, where thevertical axis (weight) uses a large range than the horizontal axis (height), resulting in a line thatlooks somewhat \u201cunnatural\u201d. It is therefore standard practice to standardize the data \ufb01rst, or", "419": "12.2. Principal components analysis (PCA) 389\nmean\n principal basis 1\nprincipal basis 2\n principal basis 3\n(a)\nreconstructed with 2 bases\n reconstructed with 10 bases\nreconstructed with 100 bases\n reconstructed with 506 bases\n(b)\nFigure 12.6 (a) The mean and the \ufb01rst three PC basis vectors (eigendigits) based on 25 images of the digit\n3 (from the MNIST dataset). (b) Reconstruction of an image based on 2, 10, 100 and all the basis vectors.\nFigure generated by pcaImageDemo .\n55 60 65 70 75 80 8550100150200250300\nheightweight\n(a)\u22124 \u22123 \u22122 \u22121 0 1 2 3 4\u22124\u22123\u22122\u22121012345\nheightweight\n(b)\nFigure 12.7 Effect of standardization on PCA applied to the height/ weight dataset. Left: PCA of raw data.\nRight: PCA of standardized data. Figure generated by pcaDemoHeightWeight .\nequivalently, to work with correlation matrices instead of covariance matrices. The bene\ufb01ts of\nthis are apparent from Figure 12.7(b).\n12.2.2 Proof *\nProof.We usewj\u2208RDto denote the j\u2019th principal direction, xi\u2208RDto denote the i\u2019th\nhigh-dimensional observation, zi\u2208RLto denote the i\u2019th low-dimensional representation, and\n\u02dc zj\u2208RNto denote the [z1j,...,z Nj], which is the j\u2019th component of all the low-dimensional\nvectors.\nLet us start by estimating the best 1d solution, w1\u2208RD, and the corresponding projected\npoints\u02dc z1\u2208RN. We will \ufb01nd the remaining bases w2,w3, etc. later. The reconstruction error", "420": "390 Chapter 12. Latent linear models\nis given by\nJ(w1,z1)=1\nNN/summationdisplay\ni=1||xi\u2212zi1w1||2=1\nNN/summationdisplay\ni=1(xi\u2212zi1w1)T(xi\u2212zi1w1)(12.29)\n=1\nNN/summationdisplay\ni=1[xT\nixi\u22122zi1wT\n1xi+z2\ni1wT\n1w1] (12.30)\n=1\nNN/summationdisplay\ni=1[xTixi\u22122zi1wT\n1xi+z2\ni1] (12.31)\nsincewT\n1w1=1(by the orthonormality assumption). Taking derivatives wrt zi1and equating\nto zero gives\n\u2202\n\u2202zi1J(w1,z1)=1\nN[\u22122wT\n1xi+2zi1]=0\u21d2zi1=wT\n1xi (12.32)\nSo the optimal reconstruction weights are obtained by orthogonally projecting the data onto the\n\ufb01rst principal direction, w1(see Figure 12.5(a)). Plugging back in gives\nJ(w1)=1\nNN/summationdisplay\ni=1[xT\nixi\u2212z2\ni1]=c o n s t \u22121\nNN/summationdisplay\ni=1z2\ni1 (12.33)\nNow the variance of the projected coordinates is given by\nvar[\u02dc z1]=E/bracketleftbig\n\u02dc z21/bracketrightbig\n\u2212(E[\u02dc z1])2=1\nNN/summationdisplay\ni=1z2\ni1\u22120 (12.34)\nsince\nE[zi1]=E/bracketleftbig\nxTiw1/bracketrightbig\n=E[xi]Tw1=0 (12.35)\nbecause the data has been centered. From this, we see that minimizing the reconstruction error\nis equivalent to maximizing the variance of the projected data, i.e.,\nargmin\nw1J(w1) = argmax\nw1var[\u02dc z1] (12.36)\nThis is why it is often said that PCA \ufb01nds the directions of maximal variance. This is called the\nanalysis view of PCA.\nThe variance of the projected data can be written as\n1\nNN/summationdisplay\ni=1z2\ni1=1\nNN/summationdisplay\ni=1wT\n1xixT\niw1=wT\n1\u02c6\u03a3w1 (12.37)\nwhere\u02c6\u03a3=1\nN/summationtextN\ni=1/summationtext\nixixT\niis the empirical covariance matrix (or correlation matrix if the\ndata is standardized).", "421": "12.2. Principal components analysis (PCA) 391\nWe can trivially maximize the variance of the projection (and hence minimize the recon-\nstruction error) by letting ||w1|| \u2192 \u221e, so we impose the constraint ||w1||=1and instead\nmaximize\n\u02dcJ(w1)=wT\n1\u02c6\u03a3w1+\u03bb1(wT\n1w1\u22121) (12.38)\nwhere\u03bb1is the Lagrange multiplier. Taking derivatives and equating to zero we have\n\u2202\n\u2202w1\u02dcJ(w1)=2 \u02c6\u03a3w1\u22122\u03bb1w1=0 (12.39)\n\u02c6\u03a3w1=\u03bb1w1 (12.40)\nHence the direction that maximizes the variance is an eigenvector of the covariance matrix. Left\nmultiplying by w1(and using wT\n1w1=1) we \ufb01nd that the variance of the projected data is\nwT\n1\u02c6\u03a3w1=\u03bb1 (12.41)\nSince we want to maximize the variance, we pick the eigenvector which corresponds to thelargest eigenvalue.\nNow let us \ufb01nd another direction w\n2to further minimize the reconstruction error, subject to\nwT\n1w2=0andwT\n2w2=1. The error is\nJ(w1,z1,w2,z2)=1\nNN/summationdisplay\ni=1||xi\u2212zi1w1\u2212zi2w2||2(12.42)\nOptimizing wrt w1andz1gives the same solution as before. Exercise 12.4 asks you to show\nthat\u2202J\n\u2202z2=0yieldszi2=wT\n2xi. In other words, the second principal encoding is gotten by\nprojecting onto the second principal direction. Substituting in yields\nJ(w2)=1\nnN/summationdisplay\ni=1[xT\nixi\u2212wT\n1xixTiw1\u2212wT\n2xixTiw2]=c o n s t \u2212wT\n2\u02c6\u03a3w2 (12.43)\nDropping the constant term and adding the constraints yields\n\u02dcJ(w2)=\u2212wT\n2\u02c6\u03a3w2+\u03bb2(wT\n2w2\u22121)+\u03bb12(wT\n2w1\u22120) (12.44)\nExercise 12.4 asks you to show that the solution is given by the eigenvector with the second\nlargest eigenvalue:\n\u02c6\u03a3w2=\u03bb2w2 (12.45)\nThe proof continues in this way. (Formally one can use induction.)", "422": "392 Chapter 12. Latent linear models\n12.2.3 Singular value decomposition (SVD)\nWe have de\ufb01ned the solution to PCA in terms of eigenvectors of the covariance matrix. However,\nthere is another way to obtain the solution, based on the singular value decomposition,o r\nSVD. This basically generalizes the notion of eigenvectors from square matrices to any kind of\nmatrix.\nIn particular, any (real) N\u00d7DmatrixXcan be decomposed as follows\nX/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nN\u00d7D=U/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nN\u00d7NS/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nN\u00d7DVT\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nD\u00d7D(12.46)\nwhereUis anN\u00d7Nmatrix whose columns are orthornormal (so UTU=IN),VisD\u00d7D\nmatrix whose rows and columns are orthonormal (so VTV=VVT=ID), andSis aN\u00d7D\nmatrix containing the r=m i n (N,D)singular values \u03c3i\u22650on the main diagonal, with 0s\n\ufb01lling the rest of the matrix. The columns of Uare the left singular vectors, and the columns\nofVare the right singular vectors. See Figure 12.8(a) for an example.\nSince there are at most Dsingular values (assuming N>D), the last N\u2212Dcolumns of U\nare irrelevant, since they will be multiplied by 0. The economy sized SVD,o rthin SVD, avoids\ncomputing these unnecessary elements. Let us denote this decomposition by \u02c6U\u02c6S\u02c6V.I fN>D,\nwe have\nX/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nN\u00d7D=\u02c6U/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nN\u00d7D\u02c6S/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nD\u00d7D\u02c6VT\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nD\u00d7D(12.47)\nas in Figure 12.8(a). If N<D,w eh a v e\nX/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nN\u00d7D=\u02c6U/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nN\u00d7N\u02c6S/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nN\u00d7N\u02c6VT\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nN\u00d7D(12.48)\nComputing the economy-sized SVD takes O(NDmin(N,D ))time (Golub and van Loan 1996,\np254).\nThe connection between eigenvectors and singular vectors is the following. For an arbitrary\nreal matrix X,i fX=USVT,w eh a v e\nXTX=VSTUTUSVT=V(STS)VT=VDVT(12.49)\nwhereD=S2is a diagonal matrix containing the squares singular values. Hence\n(XTX)V=VD (12.50)\nso the eigenvectors of XTXare equal to V, the right singular vectors of X, and the eigenvalues\nofXTXare equal to D, the squared singular values. Similarly\nXXT=USVTVSTUT=U(SST)UT(12.51)\n(XXT)U=U(SST)=UD (12.52)\nso the eigenvectors of XXTare equal to U, the left singular vectors of X. Also, the eigenvalues\nofXXTare equal to the squared singular values. We can summarize all this as follows:\nU=evec(XXT),V=evec(XTX),S2=eval(XXT)=eval(XTX) (12.53)", "423": "12.2. Principal components analysis (PCA) 393\n=\n0\u03c31\n\u03c3D...D\nDD N\u2212D D D\nN\nX= US VT\n(a)\n/similarequalL\n\u03c31...\u03c3LL D\nLD\nN\nX/similarequalULSLVT\nL\n(b)\nFigure 12.8 (a) SVD decomposition of non-square matrices X=USVT. The shaded parts of S, and all\nthe off-diagonal terms, are zero. The shaded entries in UandSare not computed in the economy-sized\nversion, since they are not needed. (b) Truncated SVD approximation of rank L.\nSince the eigenvectors are unaffected by linear scaling of a matrix, we see that the right\nsingular vectors of Xare equal to the eigenvectors of the empirical covariance \u02c6\u03a3. Furthermore,\nthe eigenvalues of \u02c6\u03a3are a scaled version of the squared singular values. This means we can\nperform PCA using just a few lines of code (see pcaPmtk).\nHowever, the connection between PCA and SVD goes deeper. From Equation 12.46, we can\nrepresent a rank rmatrix as follows:\nX=\u03c31\u239b\n\u239d|\nu1\n|\u239e\u23a0/parenleftbig\n\u2212v\nT\n1\u2212/parenrightbig\n+\u00b7\u00b7\u00b7+\u03c3r\u239b\u239d|\nu\nr\n|\u239e\u23a0/parenleftbig\n\u2212v\nT\nr\u2212/parenrightbig\n(12.54)\nIf the singular values die off quickly as in Figure 12.10, we can produce a rank Lapproximation\nto the matrix as follows:\nX\u2248U:,1:LS1:L,1:LVT\n:,1:L (12.55)\nThis is called a truncated SVD (see Figure 12.8(b)). The total number of parameters needed to\nrepresent an N\u00d7Dmatrix using a rank Lapproximation is\nNL+LD+L=L(N+D+1) (12.56)", "424": "394 Chapter 12. Latent linear models\nrank 200\n(a)\nrank 2\n(b)\nrank 5\n(c)\nrank 20\n(d)\nFigure 12.9 Low rank approximations to an image. Top left: The original image is of size 200\u00d7320,s o\nhas rank 200. Subsequent images have ranks 2, 5, and 20. Figure generated by svdImageDemo .\n0 10 20 30 40 50 60 70 80 90 10045678910log(\u03c3i)\ni  \noriginal\nrandomized\nFigure 12.10 First 50 log singular values for the clown image (solid red line), and for a data matrix\nobtained by randomly shuffling the pixels (dotted green line). Figure generated by svdImageDemo .", "425": "12.2. Principal components analysis (PCA) 395\nAs an example, consider the 200\u00d7320pixel image in Figure 12.9(top left). This has 64,000\nnumbers in it. We see that a rank 20 approximation, with only (200+320+1) \u00d720 = 10, 420\nnumbers is a very good approximation.\nOne can show that the error in this approximation is given by\n||X\u2212XL||F\u2248\u03c3L+1 (12.57)\nFurthermore, one can show that the SVD offers the best rank Lapproximation to a matrix (best\nin the sense of minimizing the above Frobenius norm).\nLet us connect this back to PCA. Let X=USVTbe a truncated SVD of X. We know that\n\u02c6W=V, and that \u02c6Z=X\u02c6W,s o\n\u02c6Z=USVTV=US (12.58)\nFurthermore, the optimal reconstruction is given by \u02c6X=Z\u02c6WT,s ow e\ufb01 n d\n\u02c6X=USVT(12.59)\nThis is precisely the same as a truncated SVD approximation! This is another illustration of the\nfact that PCA is the best low rank approximation to the data.\n12.2.4 Probabilistic PCA\nWe are now ready to revisit PPCA. One can show the following remarkable result.\nTheorem 12.2.2 ((Tipping and Bishop 1999)). Consider a factor analysis model in which \u03a8=\u03c32I\nandWis orthogonal. The observed data log likelihood is given by\nlogp(X|W,\u03c32)=\u2212N\n2ln|C|\u22121\n2N/summationdisplay\ni=1xT\niC\u22121xi=\u2212N\n2ln|C|+tr(C\u22121\u02c6\u03a3)(12.60)\nwhereC=WWT+\u03c32IandS=1\nN/summationtextN\ni=1xixT\ni=( 1/N)XTX. (We are assuming centered\ndata, for notational simplicity.) The maxima of the log-likelihood are given by\n\u02c6W=V(\u039b\u2212\u03c32I)1\n2R (12.61)\nwhereRis an arbitrary L\u00d7Lorthogonal matrix, Vis theD\u00d7Lmatrix whose columns are the\n\ufb01rstLeigenvectors of S, and\u039bis the corresponding diagonal matrix of eigenvalues. Without loss\nof generality, we can set R=I. Furthermore, the MLE of the noise variance is given by\n\u02c6\u03c32=1\nD\u2212LD/summationdisplay\nj=L+1\u03bbj (12.62)\nwhich is the average variance associated with the discarded dimensions.\nThus, as\u03c32\u21920,w eh a v e \u02c6W\u2192V, as in classical PCA. What about \u02c6Z? It is easy to see that\nthe posterior over the latent factors is given by\np(zi|xi,\u02c6\u03b8)=N (zi|\u02c6F\u22121\u02c6WTxi,\u03c32\u02c6F\u22121) (12.63)\n\u02c6F/defines\u02c6WT\u02c6W+\u02c6\u03c32I (12.64)", "426": "396 Chapter 12. Latent linear models\n(Do not confuse F=WTW+\u03c32IwithC=WWT+\u03c32I.) Hence, as \u03c32\u21920, we \ufb01nd\n\u02c6W\u2192V,\u02c6F\u2192Iand\u02c6zi\u2192VTxi. Thus the posterior mean is obtained by an orthogonal\nprojection of the data onto the column space of V, as in classical PCA.\nNote, however, that if \u03c32>, the posterior mean is not an orthogonal projection, since it is\nshrunk somewhat towards the prior mean, as illustrated in Figure 12.5(b). This sounds like an\nundesirable property, but it means that the reconstructions will be closer to the overall datamean,\u02c6\u03bc=\nx.\n12.2.5 EM algorithm for PCA\nAlthough the usual way to \ufb01t a PCA model uses eigenvector methods, or the SVD, we can alsouse EM, which will turn out to have some advantages that we discuss below. EM for PCA relieson the probabilistic formulation of PCA. However the algorithm continues to work in the zeronoise limit, \u03c3\n2=0, as shown by (Roweis 1997).\nLet\u02dcZbe aL\u00d7Nmatrix storing the posterior means (low-dimensional representations)\nalong its columns. Similarly, let \u02dcX=XTstore the original data along its columns. From\nEquation 12.63, when \u03c32=0,w eh a v e\n\u02dcZ=(WTW)\u22121WT\u02dcX (12.65)\nThis constitutes the E step. Notice that this is just an orthogonal projection of the data.\nFrom Equation 12.23, the M step is given by\n\u02c6W=/bracketleftBigg/summationdisplay\nixiE[zi]T/bracketrightBigg/bracketleftBigg/summationdisplay\niE[zi]E[zi]T/bracketrightBigg\u22121\n(12.66)\nwhere we exploited the fact that \u03a3=c o v[zi|xi,\u03b8]=0Iwhen\u03c32=0. It is worth comparing\nthis expression to the MLE for multi-output linear regression (Equation 7.89), which has the formW=(/summationtext\niyixT\ni)(/summationtext\nixixTi)\u22121. Thus we see that the M step is like linear regression where we\nreplace the observed inputs by the expected values of the latent variables.\nIn summary, here is the entire algorithm:\n\u2022Es t e p\u02dcZ=(WTW)\u22121WT\u02dcX\n\u2022Ms t e pW =\u02dcX\u02dcZT(\u02dcZ\u02dcZT)\u22121\n(Tipping and Bishop 1999) showed that the only stable \ufb01xed point of the EM algorithm is the\nglobally optimal solution. That is, the EM algorithm converges to a solution where Wspans\nthe same linear subspace as that de\ufb01ned by the \ufb01rst Leigenvectors. However, if we want W\nto be orthogonal, and to contain the eigenvectors in descending order of eigenvalue, we have\nto orthogonalize the resulting matrix (which can be done quite cheaply). Alternatively, we canmodify EM to give the principal basis directly (Ahn and Oh 2003).\nThis algorithm has a simple physical analogy in the case D=2andL=1(Roweis 1997).\nConsider some points in R\n2attached by springs to a rigid rod, whose orientation is de\ufb01ned by a\nvectorw.L e tzibe the location where the i\u2019th spring attaches to the rod. In the E step, we hold\nthe rod \ufb01xed, and let the attachment points slide around so as to minimize the spring energy(which is proportional to the sum of squared residuals). In the M step, we hold the attachment", "427": "12.2. Principal components analysis (PCA) 397\n\u22123 \u22122 \u22121 0 1 2 3\u22122.5\u22122\u22121.5\u22121\u22120.500.511.522.5E step 1\n(a)\u22123 \u22122 \u22121 0 1 2 3\u22122.5\u22122\u22121.5\u22121\u22120.500.511.522.5M step 1\n(b)\n\u22123 \u22122 \u22121 0 1 2 3\u22122.5\u22122\u22121.5\u22121\u22120.500.511.522.5E step 2\n(c)\u22123 \u22122 \u22121 0 1 2 3\u22123\u22122\u221210123M step 2\n(d)\nFigure 12.11 Illustration of EM for PCA when D=2andL=1. Green stars are the original data points,\nblack circles are their reconstructions. The weight vector wis represented by blue line. (a) We start with\na random initial guess of w. The E step is represented by the orthogonal projections. (b) We update the\nrodwin the M step, keeping the projections onto the rod (black circles) \ufb01xed. (c) Another E step. The\nblack circles can \u2019slide\u2019 along the rod, but the rod stays \ufb01xed. (d) Another M step. Based on Figure 12.12 of\n(Bishop 2006b). Figure generated by pcaEmStepByStep .\npoints \ufb01xed and let the rod rotate so as to minimize the spring energy. See Figure 12.11 for an\nillustration.\nApart from this pleasing intuitive interpretation, EM for PCA has the following advantages\nover eigenvector methods:\n\u2022 EM can be faster. In particular, assuming N,D/greatermuchL, the dominant cost of EM is the pro-\njection operation in the E step, so the overall time is O(TLND),w h e r eTis the number of", "428": "398 Chapter 12. Latent linear models\n(a) (b) (c) (d) (e) (f)\nFigure 12.12 Illustration of estimating the effective dimensionalities in a mixture of factor analysers using\nVBEM. The blank columns have been forced to 0 via the ARD mechanism. The data was generated from\n6 clusters with intrinsic dimensionalities of 7,4,3,2,2,1, which the method has successfully estimated.\nSource: Figure 4.4 of (Beal 2003). Used with kind permission of Matt Beal.\niterations. (Roweis 1997) showed experimentally that the number of iterations is usually very\nsmall (the mean was 3.6), regardless of NorD. (This results depends on the ratio of eigenval-\nues of the empirical covariance matrix.) This is much faster than the O(min(ND2,DN2))\ntime required by straightforward eigenvector methods, although more sophisticated eigenvec-tor methods, such as the Lanczos algorithm, have running times comparable to EM.\n\u2022 EM can be implemented in an online fashion, i.e., we can update our estimate of Was the\ndata streams in.\n\u2022 EM can handle missing data in a simple way (see Section 12.1.6).\n\u2022 EM can be extended to handle mixtures of PPCA/ FA models.\u2022 EM can be modi\ufb01ed to variational EM or to variational Bayes EM to \ufb01t more complex models.\n12.3 Choosing the number of latent dimensions\nIn Section 11.5, we discussed how to choose the number of components Kin a mixture model.\nIn this section, we discuss how to choose the number of latent dimensions Lin a FA/PCA model.\n12.3.1 Model selection for FA/PPCA\nIf we use a probabilistic model, we can in principle compute L\u2217= argmaxLp(L|D). However,\nthere are two problems with this. First, evaluating the marginal likelihood for LVMs is quite\ndifficult. In practice, simple approximations, such as BIC or variational lower bounds (seeSection 21.5), can be used (see also (Minka 2000a)). Alternatively, we can use the cross-validatedlikelihood as a performance measure, although this can be slow, since it requires \ufb01tting eachmodelFtimes, where Fis the number of CV folds.\nThe second issue is the need to search over a potentially large number of models. The usual\napproach is to perform exhaustive search over all candidate values of L. However, sometimes\nwe can set the model to its maximal size, and then use a technique called automatic relevancydetermination (Section 13.7), combined with EM, to automatically prune out irrelevant weights.", "429": "12.3. Choosing the number of latent dimensions 399\nnumber of points \n  per cluster 1 7 4 3 2 2\n  8        2       1  8      1      2 16 1       4 2 3 2 163322 6 4 1743221 2 8 174322intrinsic dimensionalities\nFigure 12.13 We show the estimated number of clusters, and their estimated dimensionalities, as a\nfunction of sample size. The VBEM algorithm found two different solutions when N=8. Note that more\nclusters, with larger effective dimensionalities, are discovered as the sample sizes increases. Source: Table\n4.1 of (Beal 2003). Used with kind permission of Matt Beal.\nThis technique will be described in a supervised context in Chapter 13, but can be adapted to\nthe (M)FA context as shown in (Bishop 1999; Ghahramani and Beal 2000).\nFigure 12.12 illustrates this approach applied to a mixture of FAs \ufb01t to a small synthetic dataset.\nThe \ufb01gures visualize the weight matrices for each cluster, using Hinton diagrams, where where\nthe size of the square is proportional to the value of the entry in the matrix.2We see that\nmany of them are sparse. Figure 12.13 shows that the degree of sparsity depends on the amountof training data, in accord with the Bayesian Occam\u2019s razor. In particular, when the samplesize is small, the method automatically prefers simpler models, but as the sample size getssufficiently large, the method converges on the \u201ccorrect\u201d solution, which is one with 6 subspacesof dimensionality 1, 2, 2, 3, 4 and 7.\nAlthough the ARD/ EM method is elegant, it still needs to perform search over K. This is\ndone using \u201cbirth\u201d and \u201cdeath\u201d moves (Ghahramani and Beal 2000). An alternative approach is toperform stochastic sampling in the space of models. Traditional approaches, such as (Lopes andWest 2004), are based on reversible jump MCMC, and also use birth and death moves. However,this can be slow and difficult to implement. More recent approaches use non-parametric priors,combined with Gibbs sampling, see e.g., (Paisley and Carin 2009).\n12.3.2 Model selection for PCA\nSince PCA is not a probabilistic model, we cannot use any of the methods described above. Anobvious proxy for the likelihood is the reconstruction error:\nE(D,L)=1\n|D|/summationdisplay\ni\u2208D||xi\u2212\u02c6xi||2(12.67)\nIn the case of PCA, the reconstruction is given by by \u02c6xi=Wzi+\u03bc,w h e r ezi=WT(xi\u2212\u03bc)\nandWand\u03bcare estimated from Dtrain.\n2. Geoff Hinton is an English professor of computer science at the University of Toronto.", "430": "400 Chapter 12. Latent linear models\n0 100 200 300 400 5000102030405060rmse\nnum PCstrain set reconstruction error\n(a)0 100 200 300 400 5000102030405060rmse\nnum PCstest set reconstruction error\n(b)\nFigure 12.14 Reconstruction error on MNIST vs number of latent dimensions used by PCA. (a) Training\nset. (b) Test set. Figure generated by pcaOverfitDemo .\nFigure 12.14(a) plots E(Dtrain,L)vsLon the MNIST training data in Figure 12.6. We see that\nit drops off quite quickly, indicating that we can capture most of the empirical correlation of the\npixels with a small number of factors, as illustrated qualitatively in Figure 12.6.\nExercise 12.5 asks you to prove that the residual error from only using Lterms is given by the\nsum of the discarded eigenvalues:\nE(Dtrain,L)=D/summationdisplay\nj=L+1\u03bbj (12.68)\nTherefore an alternative to plotting the error is to plot the retained eigenvalues, in decreasingorder. This is called a scree plot, because \u201cthe plot looks like the side of a mountain, and \u2019scree\u2019\nrefers to the debris fallen from a mountain and lying at its base\u201d.\n3This will have the same shape\nas the residual error plot.\nA related quantity is the fraction of variance explained, de\ufb01ned as\nF(Dtrain,L)=/summationtextL\nj=1\u03bbj/summationtextLmax\nj/prime=1\u03bbj/prime(12.69)\nThis captures the same information as the scree plot.\nOf course, if we use L= rank(X) , we get zero reconstruction error on the training set.\nTo avoid over\ufb01tting, it is natural to plot reconstruction error on the test set. This is shown in\nFigure 12.14(b). Here we see that the error continues to go down even as the model becomesmore complex! Thus we do not get the usual U-shaped curve that we typically expect to see.\nWhat is going on? The problem is that PCA is not a proper generative model of the data.\nIt is merely a compression technique. If you give it more latent dimensions, it will be able toapproximate the test data more accurately. By contrast, a probabilistic model enjoys a BayesianOccam\u2019s razor effect (Section 5.3.1), in that it gets \u201cpunished\u201d if it wastes probability mass onparts of the space where there is little data. This is illustrated in Figure 12.15, which plots the\n3. Quotation from http://janda .org/workshop/factoranalysis/SPSSrun/SPSS08 .htm.", "431": "12.3. Choosing the number of latent dimensions 401\n0 100 200 300 400 5001.31.41.51.61.71.81.922.12.2x 106negloglik\nnum PCstrain set negative loglik\n(a)0 100 200 300 400 5001.81.922.12.22.32.42.5x 106negloglik\nnum PCstest set negative loglik\n(b)\nFigure 12.15 Negative log likelihood on MNIST vs number of latent dimensions used by PPCA. (a) Training\nset. (b) Test set. Figure generated by pcaOverfitDemo .\nnegative log likelihood, computed using PPCA, vs L. Here, on the test set, we see the usual\nU-shaped curve.\nThese results are analogous to those in Section 11.5.2, where we discussed the issue of choosing\nKin the K-means algorithm vs using a GMM.\n12.3.2.1 Pro\ufb01le likelihood\nAlthough there is no U-shape, there is sometimes a \u201cregime change\u201d in the plots, from relatively\nlarge errors to relatively small. One way to automate the detection of this is described in (Zhuand Ghodsi 2006). The idea is this. Let \u03bb\nkbe some measure of the error incurred by a model of\nsizek, such that \u03bb1\u2265\u03bb2\u2265\u00b7\u00b7\u00b7\u2265\u03bb Lmax. In PCA, these are the eigenvalues, but the method can\nalso be applied to K-means. Now consider partitioning these values into two groups, dependingon whether k<Lork>L,w h e r eLis some threshold which we will determine. To measure\nthe quality of L, we will use a simple change-point model, where \u03bb\nk\u223cN(\u03bc1,\u03c32)ifk\u2264L,\nand\u03bbk\u223cN(\u03bc2,\u03c32)ifk>L. (It is important that \u03c32be the same in both models, to prevent\nover\ufb01tting in the case where one regime has less data than the other.) Within each of the tworegimes, we assume the \u03bb\nkare iid, which is obviously incorrect, but is adequate for our present\npurposes. We can \ufb01t this model for each L=1:Lmaxby partitioning the data and computing\nthe MLEs, using a pooled estimate of the variance:\n\u03bc1(L)=/summationtext\nk\u2264L\u03bbk\nL,\u03bc2(L)=/summationtext\nk>L\u03bbk\nN\u2212L(12.70)\n\u03c32(L)=/summationtext\nk\u2264L(\u03bbk\u2212\u03bc1(L))2+/summationtext\nk>L(\u03bbk\u2212\u03bc2(L))2\nN(12.71)\nWe can then evaluate the pro\ufb01le log likelihood\n/lscript(L)=L/summationdisplay\nk=1logN(\u03bbk|\u03bc1(L),\u03c32(L))+K/summationdisplay\nk=L+1logN(\u03bbk|\u03bc2(L),\u03c32(L)) (12.72)\nFinally, we choose L\u2217= argmax /lscript(L). This is illustrated in Figure 12.16. On the left, we plot\nthe scree plot, which has the same shape as in Figure 12.14(a). On the right, we plot the pro\ufb01le", "432": "402 Chapter 12. Latent linear models\n0 10 20 30 40 5000.511.522.533.54x 105\nnum PCseigenvaluescree plot\n(a)0 10 20 30 40 50\u22125750\u22125700\u22125650\u22125600\u22125550\u22125500\u22125450\nnum PCsprofile log likelihood\n(b)\nFigure 12.16 (a) Scree plot for training set, corresponding to Figure 12.14(a). (b) Pro\ufb01le likelihood. Figure\ngenerated by pcaOverfitDemo .\nlikelihood. Rather miraculously, we see a fairly well-determined peak.\n12.4 PCA for categorical data\nIn this section, we consider extending the factor analysis model to the case where the observed\ndata is categorical rather than real-valued. That is, the data has the form yij\u2208{1,...,C},\nwherej=1:Ris the number of observed response variables. We assume each yijis generated\nfrom a latent variable zi\u2208RL, with a Gaussian prior, which is passed through the softmax\nfunction as follows:\np(zi)=N (0,I) (12.73)\np(yi|zi,\u03b8)=R/productdisplay\nr=1Cat(yir|S(WT\nrzi+w0r)) (12.74)\nwhereWr\u2208RL\u00d7Mis the factor loading matrix for response j, andw0r\u2208RMis the offset\nterm for response r, and\u03b8=(Wr,w0r)R\nr=1. (We need an explicit offset term, since clamping\none element of zito 1 can cause problems when computing the posterior covariance.) As in\nfactor analysis, we have de\ufb01ned the prior mean to be m0=0and the prior covariance V0=I,\nsince we can capture non-zero mean by changing w0jand non-identity covariance by changing\nWr. We will call this categorical PCA. See Chapter 27 for a discussion of related models.\nIt is interesting to study what kinds of distributions we can induce on the observed variables\nby varying the parameters. For simplicity, we assume there is a single ternary response variable,\nsoyilives in the 3d probability simplex. Figure 12.17 shows what happens when we vary the\nparameters of the prior, m0andV0, which is equivalent to varying the parameters of the\nlikelihood, W1andw01. We see that this can de\ufb01ne fairly complex distributions over the\nsimplex. This induced distribution is known as the logistic normal distribution (Aitchison\n1982).\nWe can \ufb01t this model to data using a modi\ufb01ed version of EM. The basic idea is to infer\na Gaussian approximation to the posterior p(zi|yi,\u03b8)in the E step, and then to maximize \u03b8\nin the M step. The details for the multiclass case, can be found in (Khan et al. 2010) (see", "433": "12.4. PCA for categorical data 403\nFigure 12.17 Some examples of the logistic normal distribution de\ufb01ned on the 3d simplex. (a) Diagonal\ncovariance and non-zero mean. (b) Negative correlation between states 1 and 2. (c) Positive correlation\nbetween states 1 and 2. Source: Figure 1 of (Blei and Lafferty 2007). Used with kind permission of David\nBlei.\n2 4 6 8 10 12 14 1620\n40\n60\n80\n100\n120\n140\n(a)\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\u22121.5\u22121\u22120.500.511.52\n(b)\nFigure 12.18 Left: 150 synthetic 16 dimensional bit vectors. Right: the 2d embedding learned by binary\nPCA, using variational EM. We have color coded points by the identity of the true \u201cprototype\u201d that generated\nthem. Figure generated by binaryFaDemoTipping .\nalso Section 21.8.1.1). The details for the binary case for the the sigmoid link can be found in\nExercise 21.9, and for the probit link in Exercise 21.10.\nOne application of such a model is to visualize high dimensional categorical data. Fig-\nure 12.18(a) shows a simple example where we have 150 6-dimensional bit vectors. It is clear that\neach sample is just a noisy copy of one of three binary prototypes. We \ufb01t a 2d catFA to this\nmodel, yielding approximate MLEs \u02c6\u03b8. In Figure 12.18(b), we plot E/bracketleftBig\nzi|xi,\u02c6\u03b8/bracketrightBig\n. We see that there\nare three distinct clusters, as is to be expected.\nIn (Khan et al. 2010), we show that this model outperforms \ufb01nite mixture models on the task\nof imputing missing entries in design matrices consisting of real and categorical data. This is\nuseful for analysing social science survey data, which often has missing data and variables of\nmixed type.", "434": "404 Chapter 12. Latent linear models\nyixiziwyWx\nN\n(a)xiyizs\ni zxi\nBxWxWy\nN\n(b)\nxiyizs\ni zxi zy\ni\nBx ByWxWy\nN\n(c)\nFigure 12.19 Gaussian latent factor models for paired data. (a) Supervised PCA. (b) Partial least squares.\n(c) Canonical correlation analysis.\n12.5 PCA for paired and multi-view data\nIt is common to have a pair of related datasets, e.g., gene expression and gene copy number, or\nmovie ratings by users and movie reviews. It is natural to want to combine these together into alow-dimensional embedding. This is an example of data fusion. In some cases, we might want\nto predict one element of the pair, say x\ni1, from the other one, xi2, via the low-dimensional\n\u201cbottleneck\u201d.\nBelow we discuss various latent Gaussian models for these tasks, following the presentation\nof (Virtanen 2010). The models easily generalize from pairs to sets of data, xim,f o rm=1:M.\nWe focus on the case where xim\u2208RDm. In this case, the joint distribution is multivariate\nGaussian, so we can easily \ufb01t the models using EM, or Gibbs sampling.\nWe can generalize the models to handle discrete and count data by using the exponential\nfamily as a response distribution instead of the Gaussian, as we explain in Section 27.2.2.However, this will require the use of approximate inference in the E step (or an analogousmodi\ufb01cation to MCMC).", "435": "12.5. PCA for paired and multi-view data 405\n12.5.1 Supervised PCA (latent factor regression)\nConsider the following model, illustrated in Figure 12.19(a):\np(zi)=N (0,IL) (12.75)\np(yi|zi)=N (wT\nyzi+\u03bcy,\u03c32\ny) (12.76)\np(xi|zi)=N (Wxzi+\u03bcx,\u03c32\nxID) (12.77)\nIn (Yu et al. 2006), this is called supervised PCA. In (West 2003), this is called Bayesian factor\nregression. This model is like PCA, except that the target variable yiis taken into account when\nlearning the low dimensional embedding. Since the model is jointly Gaussian, we have\nyi|xi\u223cN(xT\niw,\u03c32\ny+wT\nyCwy) (12.78)\nwherew=\u03a8\u22121WxCwy,\u03a8=\u03c32\nxID, andC\u22121=I+WT\nx\u03a8\u22121Wx. So although this is a\njoint density model of (yi,xi), we can infer the implied conditional distribution.\nWe now show an interesting connection to Zellner\u2019s g-prior. Suppose p(wy)=N(0,1\ng\u03a32),\nand letX=RVTbe the SVD of X,w h e r e VTV=IandRTR=\u03a32=d i a g (\u03c32\nj)contains\nthe squared singular values. Then one can show (West 2003) that\np(w)=N(0,gV\u2212T\u03a3\u22122V\u22121)=N(0,g(XTX)\u22121) (12.79)\nSo the dependence of the prior for wonXarises from the fact that wis derived indirectly by\na joint model of Xandy.\nThe above discussion focussed on regression. (Guo 2009) generalizes CCA to the exponential\nfamily, which is more appropriate if xiand/oryiare discrete. Although we can no longer\ncompute the conditional p(yi|xi,\u03b8)in closed form, the model has a similar interpretation to\nthe regression case, namely that we are predicting the response via a latent \u201cbottleneck\u201d.\nThe basic idea of compressing xito predict yican be formulated using information theory.\nIn particular, we might want to \ufb01nd an encoding distribution p(z|x)such that we minimize\nI(X;Z)\u2212\u03b2I(X;Y) (12.80)\nwhere\u03b2\u22650is some parameter controlling the tradeoff between compression and predictive\naccuracy. This is known as the information bottleneck (Tishby et al. 1999). Often Zis taken to\nbe discrete, as in clustering. However, in the Gaussian case, IB is closely related to CCA (Chechik\net al. 2005).\nWe can easily generalize CCA to the case where yiis a vector of responses to be predicted, as\nin multi-label classi\ufb01cation. (Ma et al. 2008; Williamson and Ghahramani 2008) used this modelto perform collaborative \ufb01ltering, where the goal is to predict y\nij\u2208{1,...,5}, the rating person\nigives to movie j, where the \u201cside information\u201d xitakes the form of a list of i\u2019s friends. The\nintuition behind this approach is that knowledge of who your friends are, as well as the ratingsof all other users, should help predict which movies you will like. In general, any setting wherethe tasks are correlated could bene\ufb01t from CCA. Once we adopt a probabilistic view, variousextensions are straightforward. For example, we can easily generalize to the semi-supervisedcase, where we do not observe y\nifor alli(Yu et al. 2006).", "436": "406 Chapter 12. Latent linear models\n12.5.1.1 Discriminative supervised PCA\nOne problem with this model is that it puts as much weight on predicting the inputs xias the\noutputsyi. This can be partially alleviated by using a weighted objective of the following form\n(Rish et al. 2008):\n/lscript(\u03b8)=/productdisplay\nip(yi|\u03b7iy)\u03b1yp(xi|\u03b7ix)\u03b1x(12.81)\nwhere the \u03b1mcontrol the relative importance of the data sources, and \u03b7im=Wmzi.F o r\nGaussian data, we can see that \u03b1mjust controls the noise variance:\n/lscript(\u03b8)\u221d/productdisplay\niexp(\u22121\n2\u03b1x||xT\ni\u2212\u03b7ix||2)exp(\u22121\n2\u03b1y||yT\ni\u2212\u03b7iy||2) (12.82)\nThis interpretation holds more generally for the exponential family. Note, however, that it is hard\nto estimate the \u03b1mparameters, because changing them changes the normalization constant of\nthe likelihood. We give an alternative approach to weighting ymore heavily below.\n12.5.2 Partial least squares\nThe technique of partial least squares (PLS) (Gustafsson 2001; Sun et al. 2009) is an asym-\nmetric or more \u201cdiscriminative\u201d form of supervised PCA. The key idea is to allow some of the(co)variance in the input features to be explained by its own subspace, z\nx\ni, and to let the rest of\nthe subspace, zsi, be shared between input and output. The model has the form\np(zi)=N (zs\ni|0,ILs)N(zxi|0,ILx) (12.83)\np(yi|zi)=N (Wyzsi+\u03bcy,\u03c32IDy) (12.84)\np(xi|zi)=N (Wxzsi+Bxzxi+\u03bcx,\u03c32IDx) (12.85)\nSee Figure 12.19(b). The corresponding induced distribution on the visible variables has the form\np(vi|\u03b8)=/integraldisplay\nN(vi|Wzi+\u03bc,\u03c32I)N(zi|0,I)dzi=N(vi|\u03bc,WWT+\u03c32I) (12.86)\nwherevi=(xi;yi),\u03bc=(\u03bcy;\u03bcx)and\nW=/parenleftbiggWy0\nWxBx/parenrightbigg\n(12.87)\nWWT=/parenleftbiggWyWT\nyWxWT\nx\nWxWT\nxWxWT\nx+BxBT\nx/parenrightbigg\n(12.88)\nWe should choose Llarge enough so that the shared subspace does not capture covariate-\nspeci\ufb01c variation.\nThis model can be easily generalized to discrete data using the exponential family (Virtanen\n2010).", "437": "12.6. Independent Component Analysis (ICA) 407\n12.5.3 Canonical correlation analysis\nCanonical correlation analysis orCCAis like a symmetric unsupervised version of PLS: it\nallows each view to have its own \u201cprivate\u201d subspace, but there is also a shared subspace. If we\nhave two observed variables, xiandyi, then we have three latent variables, zs\ni\u2208RL0which is\nshared,zxi\u2208RLxandzy\ni\u2208RLywhich are private. We can write the model as follows (Bach\nand Jordan 2005):\np(zi)=N (zs\ni|0,ILs)N(zxi|0,ILx)N(zy\ni|0,ILy) (12.89)\np(xi|zi)=N (xi|Bxzx\ni+Wxzsi+\u03bcx,\u03c32IDx) (12.90)\np(yi|zi)=N (yi|Byzy\ni+Wyzs\ni+\u03bcy,\u03c32IDy) (12.91)\nSee Figure 12.19(c). The corresponding observed joint distribution has the form\np(vi|\u03b8)=/integraldisplay\nN(vi|Wzi+\u03bc,\u03c32I)N(zi|0,I)dzi=N(vi|\u03bc,WWT+\u03c32ID)(12.92)\nwhere\nW=/parenleftbiggWxBx0\nWy0B y/parenrightbigg\n(12.93)\nWWT=/parenleftbiggWxWT\nx+BxBT\nxWxWT\ny\nWyWT\nyWyWT\ny+ByBTy/parenrightbigg\n(12.94)\nOne can compute the MLE for this model using EM. (Bach and Jordan 2005) show that the\nresulting MLE is equivalent (up to rotation and scaling) to the classical, non-probabilistic view.However, the advantages of the probabilistic view are many: we can trivially generalize to M>2\nobserved variables; we can create mixtures of CCA (Viinikanoja et al. 2010); we can create sparseversions of CCA using ARD (Archambeau and Bach 2008); we can generalize to the exponentialfamily (Klami et al. 2010); we can perform Bayesian inference of the parameters (Wang 2007;Klami and Kaski 2008); we can handle non-parametric sparsity-promoting priors for WandB\n(Rai and Daume 2009); and so on.\n12.6 Independent Component Analysis (ICA)\nConsider the following situation. You are in a crowded room and many people are speaking.Your ears essentially act as two microphones, which are listening to a linear combination of thedifferent speech signals in the room. Your goal is to deconvolve the mixed signals into theirconstituent parts. This is known as the cocktail party problem, and is an example of blind\nsignal separation (BSS), orblind source separation, where \u201cblind\u201d means we know \u201cnothing\u201d\nabout the source of the signals. Besides the obvious applications to acoustic signal processing,this problem also arises when analysing EEG and MEG signals, \ufb01nancial data, and any otherdataset (not necessarily temporal) where latent sources or factors get mixed together in a linearway.\nWe can formalize the problem as follows. Let x\nt\u2208RDbe the observed signal at the sensors\nat \u201ctime\u201d t, andzt\u2208RLbe the vector of source signals. We assume that\nxt=Wzt+/epsilon1t (12.95)", "438": "408 Chapter 12. Latent linear models\n0 100 200 300 400 500\u2212202\n0 100 200 300 400 500\u2212505\n0 100 200 300 400 500\u2212202truth\n0 100 200 300 400 500\u221210010\n(a)0 100 200 300 400 500\u221210010\n0 100 200 300 400 500\u2212505\n0 100 200 300 400 500\u221210010observed signals\n0 100 200 300 400 500\u2212505\n(b)\n0 100 200 300 400 500\u221210010\n0 100 200 300 400 500\u2212505\n0 100 200 300 400 500\u2212202PCA estimate\n0 100 200 300 400 500\u2212101\n(c)0 100 200 300 400 500\u2212505\n0 100 200 300 400 500\u221210010\n0 100 200 300 400 500\u2212202ICA estimate\n0 100 200 300 400 500\u2212202\n(d)\nFigure 12.20 Illustration of ICA applied to 500 iid samples of a 4d source signal. (a) Latent signals. (b)\nObservations. (c) PCA estimate. (d) ICA estimate. Figure generated by icaDemo, written by Aapo Hyvarinen.\nwhereWis anD\u00d7Lmatrix, and /epsilon1t\u223cN(0,\u03a8). In this section, we treat each time point\nas an independent observation, i.e., we do not model temporal correlation (so we could replace\nthetindex with i, but we stick with tto be consistent with much of the ICA literature). The\ngoal is to infer the source signals, p(zt|xt,\u03b8), as illustrated in Figure 12.20. In this context, W\nis called the mixing matrix.I f L=D(number of sources = number of sensors), it will be a\nsquare matrix. Often we will assume the noise level, |\u03a8|, is zero, for simplicity.\nSo far, the model is identical to factor analysis (or PCA if there is no noise, except we don\u2019t in\ngeneral require orthogonality of W). However, we will use a different prior for p(zt). In PCA,\nwe assume each source is independent, and has a Gaussian distribution\np(zt)=L/productdisplay\nj=1N(ztj|0,1) (12.96)\nWe will now relax this Gaussian assumption and let the source distributions be any non-Gaussian", "439": "12.6. Independent Component Analysis (ICA) 409\n\u22123 \u22122 \u22121 0 1 2 3\u22123\u22122\u221210123uniform data\n(a)\u22123 \u22122 \u22121 0 1 2 3\u22123\u22122\u221210123uniform data after linear mixing\n(b)\n\u22123 \u22122 \u22121 0 1 2 3\u22123\u22122\u221210123PCA applied to mixed data from uniform source\n(c)\u22123 \u22122 \u22121 0 1 2 3\u22123\u22122\u221210123ICA applied to mixed data from uniform source\n(d)\nFigure 12.21 Illustration of ICA and PCA applied to 100 iid samples of a 2d source signal with a uniform\ndistribution. (a) Latent signals. (b) Observations. (c) PCA estimate. (d) ICA estimate. Figure generated by\nicaDemoUniform , written by Aapo Hyvarinen.\ndistribution\np(zt)=L/productdisplay\nj=1pj(ztj) (12.97)\nWithout loss of generality, we can constrain the variance of the source distributions to be 1,\nbecause any other variance can be modelled by scaling the rows of Wappropriately. The\nresulting model is known as independent component analysis orICA.\nThe reason the Gaussian distribution is disallowed as a source prior in ICA is that it does not\npermit unique recovery of the sources, as illustrated in Figure 12.20(c). This is because the PCAlikelihood is invariant to any orthogonal transformation of the sources z\ntand mixing matrix W.\nPCA can recover the best linear subspace in which the signals lie, but cannot uniquely recoverthe signals themselves.", "440": "410 Chapter 12. Latent linear models\nTo illustrate this, suppose we have two independent sources with uniform distributions, as\nshown in Figure 12.21(a). Now suppose we have the following mixing matrix\nW=/parenleftbigg\n23\n21/parenrightbigg\n(12.98)\nThen we observe the data shown in Figure 12.21(b) (assuming no noise). If we apply PCA followedby scaling to this, we get the result in Figure 12.21(c). This corresponds to a whitening of thedata. To uniquely recover the sources, we need to perform an additional rotation. The troubleis, there is no information in the symmetric Gaussian posterior to tell us which angle to rotateby. In a sense, PCA solves \u201chalf\u201d of the problem, since it identi\ufb01es the linear subspace; allthat ICA has to do is then to identify the appropriate rotation. (Hence we see that ICA is notthat different from methods such as varimax, which seek good rotations of the latent factors toenhance interpretability.)\nFigure 12.21(d) shows that ICA can recover the source, up to a permutation of the indices and\npossible sign change. ICA requires that Wis square and hence invertible. In the non-square\ncase (e.g., where we have more sources than sensors), we cannot uniquely recover the true signal,but we can compute the posterior p(z\nt|xt,\u02c6W), which represents our beliefs about the source.\nIn both cases, we need to estimate Was well as the source distributions pj. We discuss how\nto do this below.\n12.6.1 Maximum likelihood estimation\nIn this section, we discuss ways to estimate square mixing matrices Wfor the noise-free ICA\nmodel. As usual, we will assume that the observations have been centered; hence we can alsoassumezis zero-mean. In addition, we assume the observations have been whitened, which\ncan be done with PCA.\nIf the data is centered and whitened, we have E/bracketleftbig\nxx\nT/bracketrightbig\n=I. But in the noise free case, we\nalso have\ncov[x]=E/bracketleftbig\nxxT/bracketrightbig\n=WE/bracketleftbig\nzzT/bracketrightbig\nWT=WWT(12.99)\nHence we see that Wmust be orthogonal. This reduces the number of parameters we have to\nestimate from D2toD(D\u22121)/2. It will also simplify the math and the algorithms.\nLetV=W\u22121; these are often called the recognition weights, as opposed to W, which are\nthegenerative weights.4\nSincex=Wz, we have, from Equation 2.89,\npx(Wzt)=pz(zt)|det(W\u22121)|=pz(Vxt)|det(V)| (12.100)\nHence we can write the log-likelihood, assuming Tiid samples, as follows:\n1\nTlogp(D|V)=l o g|det(V)|+1\nTL/summationdisplay\nj=1T/summationdisplay\nt=1logpj(vT\njxt) (12.101)\n4. In the literature, it is common to denote the generative weights by Aand the recognition weights by W, but we are\ntrying to be consistent with the notation used earlier in this chapter.", "441": "12.6. Independent Component Analysis (ICA) 411\nwherevjis thej\u2019th row of V. Since we are constraining Vto be orthogonal, the \ufb01rst term is a\nconstant, so we can drop it. We can also replace the average over the data with an expectation\noperator to get the following objective\nNLL(V)=L/summationdisplay\nj=1E[Gj(zj)] (12.102)\nwherezj=vT\njxandGj(z)/defines\u2212logpj(z). We want to minimize this subject to the constraint\nthat the rows of Vare orthogonal. We also want them to be unit norm, since this ensures\nthat the variance of the factors is unity (since, with whitened data, E/bracketleftbig\nvT\njx/bracketrightbig\n=||vj||2), which is\nnecessary to \ufb01x the scale of the weights. In otherwords, Vshould be an orthonormal matrix.\nIt is straightforward to derive a gradient descent algorithm to \ufb01t this model; however, it\nis rather slow. One can also derive a faster algorithm that follows the natural gradient; seee.g., (MacKay 2003, ch 34) for details. A popular alternative is to use an approximate Newtonmethod, which we discuss in Section 12.6.2. Another approach is to use EM, which we discussin Section 12.6.3.\n12.6.2 The FastICA algorithm\nWe now describe the fast ICA algorithm, based on (Hyvarinen and Oja 2000), which we will\nshow is an approximate Newton method for \ufb01tting ICA models.\nFor simplicity of presentation, we initially assume there is only one latent factor. In addition,\nwe initially assume all source distributions are known and are the same, so we can just writeG(z)=\u2212logp(z).L e tg(z)=\nd\ndzG(z). The constrained objective, and its gradient and\nHessian, are given by\nf(v)=E/bracketleftbig\nG(vTx)/bracketrightbig\n+\u03bb(1\u2212vTv) (12.103)\n\u2207f(v)=E/bracketleftbig\nxg(vTx)/bracketrightbig\n\u2212\u03b2v (12.104)\nH(v)=E/bracketleftbig\nxxTg/prime(vTx)/bracketrightbig\n\u2212\u03b2I (12.105)\nwhere\u03b2=2\u03bbis a Lagrange multiplier. Let us make the approximation\nE/bracketleftbig\nxxTg/prime(vTx)/bracketrightbig\n\u2248E/bracketleftbig\nxxT/bracketrightbig\nE/bracketleftbig\ng/prime(vTx)/bracketrightbig\n=E/bracketleftbig\ng/prime(vTx)/bracketrightbig\n(12.106)\nThis makes the Hessian very easy to invert, giving rise to the following Newton update:\nv\u2217/definesv\u2212E/bracketleftbig\nxg(vTx)/bracketrightbig\n\u2212\u03b2v\nE[g/prime(vTx)]\u2212\u03b2(12.107)\nOne can rewrite this in the following way\nv\u2217/definesE/bracketleftbig\nxg(vTx)/bracketrightbig\n\u2212E/bracketleftbig\ng/prime(vTx)/bracketrightbig\nv (12.108)\n(In practice, the expectations can be replaced by Monte Carlo estimates from the training set,which gives an efficient online learning algorithm.) After performing this update, one shouldproject back onto the constraint surface using\nv\nnew/definesv\u2217\n||v\u2217||(12.109)", "442": "412 Chapter 12. Latent linear models\n\u22124 \u22123 \u22122 \u22121 0 1 2 3 400.050.10.150.20.250.30.350.40.450.5\n(a)\u22124 \u22123 \u22122 \u22121 0 1 2 3 4\u22123\u22122\u221210123Gaussian\n(b)\n\u221210 \u22125 0 5 10\u22128\u22126\u22124\u221220246810Laplace\n(c)\u22122 \u22121 0 1 2\u22121.5\u22121\u22120.500.511.5Uniform\n(d)\nFigure 12.22 Illustration of Gaussian, sub-Gaussian (uniform) and super-Gaussian (Laplace) distributions\nin 1d and 2d. Figure generated by subSuperGaussPlot , written by Kevin Swersky.\nOne iterates this algorithm until convergence. (Due to the sign ambiguity of v, the values of v\nmay not converge, but the direction de\ufb01ned by this vector should converge, so one can assess\nconvergence by monitoring |vTvnew|, which should approach 1.)\nSince the objective is not convex, there are multiple local optima. We can use this fact to\nlearn multiple different weight vectors or features. We can either learn the features sequentially\nand then project out the part of vjthat lies in the subspace de\ufb01ned by earlier features, or\nwe can learn them in parallel, and orthogonalize Vin parallel. This latter approach is usually\npreferred, since, unlike PCA, the features are not ordered in any way. So the \ufb01rst feature is not\u201cmore important\u201d than the second, and hence it is better to treat them symmetrically.", "443": "12.6. Independent Component Analysis (ICA) 413\n12.6.2.1 Modeling the source densities\nSo far, we have assumed that G(z)=\u2212logp(z)is known. What kinds of models might be\nreasonable as signal priors? We know that using Gaussians (which correspond to quadratic\nfunctions for G) won\u2019t work. So we want some kind of non-Gaussian distribution. In general,\nthere are several kinds of non-Gaussian distributions, such as the following:\n\u2022Super-Gaussian distributions These are distributions which have a big spike at the mean,\nand hence (in order to ensure unit variance) have heavy tails. The Laplace distribution is\na classic example. See Figure 12.22. Formally, we say a distribution is super-Gaussian or\nleptokurtic (\u201clepto\u201d coming from the Greek for \u201cthin\u201d) if kurt(z)>0,w h e r e kurt(z)is the\nkurtosis of the distribution, de\ufb01ned by\nkurt(z)/defines\u03bc4\n\u03c34\u22123 (12.110)\nwhere\u03c3is the standard deviation, and \u03bckis thek\u2019thcentral moment, or moment about\nthe mean:\n\u03bck/definesE/bracketleftbig\n(X\u2212E[X])k/bracketrightbig\n(12.111)\n(So\u03bc1=\u03bcis the mean, and \u03bc2=\u03c32is the variance.) It is conventional to subtract 3 in the\nde\ufb01nition of kurtosis to make the kurtosis of a Gaussian variable equal to zero.\n\u2022Sub-Gaussian distributions Asub-Gaussian orplatykurtic (\u201cplaty\u201d coming from the Greek\nfor \u201cbroad\u201d) distribution has negative kurtosis. These are distributions which are much \ufb02atterthan a Gaussian. The uniform distribution is a classic example. See Figure 12.22.\n\u2022Skewed distributions Another way to \u201cbe non-Gaussian\u201d is to be asymmetric. One measure\nof this is skewness, de\ufb01ned by\nskew(z)/defines\u03bc\n3\n\u03c33(12.112)\nAn example of a (right) skewed distribution is the gamma distribution (see Figure 2.9).\nWhen one looks at the empirical distribution of many natural signals, such as images and\nspeech, when passed through certain linear \ufb01lters, they tend to be very super-Gaussian. Thisresult holds both for the kind of linear \ufb01lters found in certain parts of the brain, such as thesimple cells found in the primary visual cortex, as well as for the kinds of linear \ufb01lters used insignal processing, such as wavelet transforms. One obvious choice for modeling natural signalswith ICA is therefore the Laplace distribution. For mean zero and variance 1, this has a log pdfgiven by\nlogp(z)=\u2212\u221a\n2|z|\u2212log(\u221a\n2) (12.113)\nSince the Laplace prior is not differentiable at the origin, it is more common to use other,smoother super-Gaussian distributions. One example is the logistic distribution. The corre-sponding log pdf, for the case where the mean is zero and the variance is 1 (so \u03bc=0and\ns=\n\u221a\n3\n\u03c0), is given by the following:\nlogp(z)=\u22122logcosh(\u03c0\n2\u221a\n3z)\u2212log4\u221a\n3\n\u03c0(12.114)", "444": "414 Chapter 12. Latent linear models\n\u03c8Txt1 xtDzt1 ztDqt1 qtD\nW\u03c31k\u03bc1k\n\u03c0\n\u03c3Dk\u03bcDk\nFigure 12.23 Modeling the source distributions using a mixture of univariate Gaussians (the independent\nfactor analysis model of (Moulines et al. 1997; Attias 1999)).\nVarious ways of estimating G(Z)=\u2212logp(z)are discussed in the seminal paper (Pham and\nGarrat 1997). However, when \ufb01tting ICA by maximum likelihood, it is not critical that the exact\nshape of the source distribution be known (although it is important to know whether it is subor super Gaussian). Consequently, it is common to just use G(z)=\u221a\nzorG(z)=l o gc o s h ( z)\ninstead of the more complex expressions above.\n12.6.3 Using EM\nAn alternative to assuming a particular form for G(z), or equivalently for p(z), is to use a\n\ufb02exible non-parametric density estimator, such as a mixture of (uni-variate) Gaussians:\np(qj=k)=\u03c0 k (12.115)\np(zj|qj=k)=N (\u03bcj,k,\u03c32\nj,k) (12.116)\np(x|z)=N (Wz,\u03a8) (12.117)\nThis approach was proposed in (Moulines et al. 1997; Attias 1999), and the corresponding graph-ical model is shown in Figure 12.23.\nIt is possible to derive an exact EM algorithm for this model. The key observation is that\nit is possible to compute E[z\nt|xt,\u03b8]exactly by summing over all KLcombinations of the qt\nvariables, where Kis the number of mixture components per source. (If this is too expensive,\none can use a variational mean \ufb01eld approximation (Attias 1999).) We can then estimate all thesource distributions in parallel by \ufb01tting a standard GMM to E[z\nt]. When the source GMMs are", "445": "12.6. Independent Component Analysis (ICA) 415\nknown, we can compute the marginals pj(zj)very easily, using\npj(zj)=K/summationdisplay\nk=1\u03c0j,kN(zj|\u03bcj,k,\u03c32\nj,k) (12.118)\nGiven the pj\u2019s, we can then use an ICA algorithm to estimate W. Of course, these steps should\nbe interleaved. The details can be found in (Attias 1999).\n12.6.4 Other estimation principles *\nIt is quite common to estimate the parameters of ICA models using methods that seem different\nto maximum likelihood. We will review some of these methods below, because they giveadditional insight into ICA. However, we will also see that these methods in fact are equivalentto maximum likelihood after all. Our presentation is based on (Hyvarinen and Oja 2000).\n12.6.4.1 Maximizing non-Gaussianity\nAn early approach to ICA was to \ufb01nd a matrix Vsuch that the distribution z=Vxis as far\nfrom Gaussian as possible. (There is a related approach in statistics called projection pursuit.)\nOne measure of non-Gaussianity is kurtosis, but this can be sensitive to outliers. Anothermeasure is the negentropy, de\ufb01ned as\nnegentropy( z)/definesH/parenleftbig\nN(\u03bc,\u03c3\n2)/parenrightbig\n\u2212H(z) (12.119)\nwhere\u03bc=E[z]and\u03c32= var[z]. Since the Gaussian is the maximum entropy distribution,\nthis measure is always non-negative and becomes large for distributions that are highly non-Gaussian.\nWe can de\ufb01ne our objective as maximizing\nJ(V)=/summationdisplay\njnegentropy( zj)=/summationdisplay\njH/parenleftbig\nN(\u03bcj,\u03c32\nj)/parenrightbig\n\u2212H(zj) (12.120)\nwherez=Vx.I fw e\ufb01 x Vto be orthogonal, and if we whiten the data, the covariance of z\nwill beIindependently of V, so the \ufb01rst term is a constant. Hence\nJ(V)=/summationdisplay\nj\u2212H(zj)+const=/summationdisplay\njE[logp(zj)]+const (12.121)\nwhich we see is equal (up to a sign change, and irrelevant constants) to the log-likelihood inEquation 12.102.\n12.6.4.2 Minimizing mutual information\nOne measure of dependence of a set of random variables is the multi-information:\nI(z)/definesKL\u239b\n\u239dp(z)||/productdisplay\njp(zj)\u239e\u23a0=/summationdisplay\njH(zj)\u2212H(z) (12.122)", "446": "416 Chapter 12. Latent linear models\nWe would like to minimize this, since we are trying to \ufb01nd independent components. Put\nanother way, we want the best possible factored approximation to the joint distribution.\nNow since z=Vx,w eh a v e\nI(z)=/summationdisplay\njH(zj)\u2212H(Vx) (12.123)\nIf we constrain Vto be orthogonal, we can drop the last term, since then H(Vx)= H(x)\n(since multiplying by Vdoes not change the shape of the distribution), and H(x)is a constant\nwhich is is solely determined by the empirical distribution. Hence we have I(z)=/summationtext\njH(zj).\nMinimizing this is equivalent to maximizing the negentropy, which is equivalent to maximumlikelihood.\n12.6.4.3 Maximizing mutual information (infomax)\nInstead of trying to minimize the mutual information between the components of z,l e tu s\nimagine a neural network where xis the input and y\nj=\u03c6(vT\njx)+/epsilon1is the noisy output, where\n\u03c6is some nonlinear scalar function, and /epsilon1\u223cN(0,1). It seems reasonable to try to maximize\nthe information \ufb02ow through this system, a principle known as infomax. (Bell and Sejnowski\n1995). That is, we want to maximize the mutual information between y(the internal neural\nrepresentation) and x(the observed input signal). We have I(x;y)=H(y)\u2212H(y|x),w h e r e\nthe latter term is constant if we assume the noise has constant variance. One can show that wecan approximate the former term as follows\nH(y)=L/summationdisplay\nj=1E/bracketleftbig\nlog\u03c6/prime(vT\njx)/bracketrightbig\n+log|det(V)| (12.124)\nwhere, as usual, we can drop the last term if Vis orthogonal. If we de\ufb01ne \u03c6(z)to be a cdf,\nthen\u03c6/prime(z)is its pdf, and the above expression is equivalent to the log likelihood. In particular,\nif we use a logistic nonlinearity, \u03c6(z) = sigm( z), then the corresponding pdf is the logistic\ndistribution, and log\u03c6/prime(z) = logcosh( z)(ignoring irrelevant constants). Thus we see that\ninfomax is equivalent to maximum likelihood.\nExercises\nExercise 12.1 Ms t e pf o rF A\nFor the FA model, show that the MLE in the M step for Wis given by Equation 12.23.\nExercise 12.2 MAP estimation for the FA model\nDerive the M step for the FA model using conjugate priors for the parameters.\nExercise 12.3 Heuristic for assessing applicability of PCA\n(Source: (Press 2005, Q9.8).). Let the empirical covariance matrix \u03a3have eigenvalues \u03bb1\u2265\u03bb2\u2265\u00b7\u00b7\u00b7\u2265\n\u03bbd>0. Explain why the variance of the evalues, \u03c32=1\nd/summationtextd\ni=1(\u03bbi\u2212\u03bb)2is a good measure of whether\nor not PCA would be useful for analysing the data (the higher the value of \u03c32the more useful PCA).", "447": "12.6. Independent Component Analysis (ICA) 417\nExercise 12.4 Deriving the second principal component\na. Let\nJ(v2,z2)=1\nnn/summationdisplay\ni=1(xi\u2212zi1v1\u2212zi2v2)T(xi\u2212zi1v1\u2212zi2v2) (12.125)\nShow that\u2202J\n\u2202z2=0yieldszi2=vT\n2xi.\nb. Show that the value of v2that minimizes\n\u02dcJ(v2)=\u2212vT\n2Cv2+\u03bb2(vT\n2v2\u22121)+\u03bb12(vT\n2v1\u22120) (12.126)\nis given by the eigenvector of Cwith the second largest eigenvalue. Hint: recall that Cv1=\u03bb1v1and\n\u2202xTAx\n\u2202x=(A+AT)x.\nExercise 12.5 Deriving the residual error for PCA\na. Prove that\n||xi\u2212K/summationdisplay\nj=1zijvj||2=xT\nixi\u2212K/summationdisplay\nj=1vT\njxixTivj (12.127)\nHint: \ufb01rst consider the case K=2. Use the fact that vT\njvj=1andvT\njvk=0fork/negationslash=j. Also,\nrecallzij=xT\nivj.\nb. Now show that\nJK/defines1\nnn/summationdisplay\ni=1/parenleftBigg\nxT\nixi\u2212K/summationdisplay\nj=1vT\njxixTivj/parenrightBigg\n=1\nnn/summationdisplay\ni=1xTixi\u2212K/summationdisplay\nj=1\u03bbj (12.128)\nHint: recall vT\njCvj=\u03bbjvT\njvj=\u03bbj.\nc. IfK=dthere is no truncation, so Jd=0. Use this to show that the error from only using K<d\nterms is given by\nJK=d/summationdisplay\nj=K+1\u03bbj (12.129)\nHint: partition the sum/summationtextd\nj=1\u03bbjinto/summationtextKj=1\u03bbjand/summationtextdj=K+1\u03bbj.\nExercise 12.6 Derivation of Fisher\u2019s linear discriminant\nShow that the maximum of J(w)=wTSBw\nwTSWwis given by SBw=\u03bbSWw\nwhere\u03bb=wTSBw\nwTSWw.Hint: recall that the derivative of a ratio of two scalars is given byd\ndxf(x)\ng(x)=f/primeg\u2212fg/prime\ng2,\nwheref/prime=d\ndxf(x)andg/prime=d\ndxg(x). Also, recall thatd\ndxxTAx=(A+AT)x.\nExercise 12.7 PCA via successive de\ufb02ation\nLetv1,v2,...,v kbe the \ufb01rst keigenvectors with largest eigenvalues of C=1\nnXTX, i.e., the principal\nbasis vectors. These satisfy\nvT\njvk=/braceleftbigg0ifj/negationslash=k\n1ifj=k(12.130)\nWe will construct a method for \ufb01nding the vjsequentially.", "448": "418 Chapter 12. Latent linear models\nAs we showed in class, v1is the \ufb01rst principal eigenvector of C, and satis\ufb01es Cv1=\u03bb1v1. Now de\ufb01ne\n\u02dcxias the orthogonal projection of xionto the space orthogonal to v1:\n\u02dcxi=P\u22a5v1xi=(I\u2212v1vT\n1)xi (12.131)\nDe\ufb01ne\u02dcX=[\u02dcx1;...;\u02dcxn]as thede\ufb02ated matrix of rankd\u22121, which is obtained by removing from the d\ndimensional data the component that lies in the direction of the \ufb01rst principal direction:\n\u02dcX=(I\u2212v1vT\n1)TX=(I\u2212v1vT\n1)X (12.132)\na. Using the facts that XTXv1=n\u03bb1v1(and hence vT\n1XTX=n\u03bb1vT\n1) andvT\n1v1=1, show that\nthe covariance of the de\ufb02ated matrix is given by\n\u02dcC/defines1\nn\u02dcXT\u02dcX=1\nnXTX\u2212\u03bb1v1vT\n1 (12.133)\nb. Letube the principal eigenvector of \u02dcC. Explain why u=v2. (You may assume uis unit norm.)\nc. Suppose we have a simple method for \ufb01nding the leading eigenvector and eigenvalue of a pd matrix,\ndenoted by [\u03bb,u]=f(C). Write some pseudo code for \ufb01nding the \ufb01rst Kprincipal basis vectors of\nXthat only uses the special ffunction and simple vector arithmetic, i.e., your code should not use\nS V Do rt h eeig function. Hint: this should be a simple iterative routine that takes 2\u20133 lines to write.\nThe input is C,Kand the function f, the output should be vjand\u03bbjforj=1:K. Do not worry\nabout being syntactically correct.\nExercise 12.8 Latent semantic indexing\n(Source: de Freitas.). In this exercise, we study a technique called latent semantic indexing , which applies\nSVD to a document by term matrix, to create a low-dimensional embedding of the data that is designed to\ncapture semantic similarity of words.\nThe \ufb01le lsiDocuments.pdf contains 9 documents on various topics. A list of all the 460 unique\nwords/terms that occur in these documents is in lsiWords.txt . A document by term matrix is in\nlsiMatrix.txt .\na. LetXbe the transpose of lsiMatrix , so each column represents a document. Compute the SVD of X\nand make an approximation to it \u02c6Xusing the \ufb01rst 2 singular values/ vectors. Plot the low dimensional\nrepresentation of the 9 documents in 2D. You should get something like Figure 12.24.\nb. Consider \ufb01nding documents that are about alien abductions. If If you look at lsiWords.txt , there\nare 3 versions of this word, term 23 (\u201cabducted\u201d), term 24 (\u201cabduction\u201d) and term 25 (\u201cabductions\u201d).\nSuppose we want to \ufb01nd documents containing the word \u201cabducted\u201d. Documents 2 and 3 contain it,but document 1 does not. However, document 1 is clearly related to this topic. Thus LSI should also\ufb01nd document 1. Create a test document qcontaining the one word \u201cabducted\u201d, and project it into\nthe 2D subspace to make \u02c6q. Now compute the cosine similarity between \u02c6qand the low dimensional\nrepresentation of all the documents. What are the top 3 closest matches?\nExercise 12.9 Imputation in a FA model\nDerive an expression for p(x\nh|xv,\u03b8)for a FA model.\nExercise 12.10 Efficiently evaluating the PPCA density\nDerive an expression for p(x|\u02c6W,\u02c6\u03c32)for the PPCA model based on plugging in the MLEs and using the\nmatrix inversion lemma.", "449": "12.6. Independent Component Analysis (ICA) 419\n\u22120.45 \u22120.4 \u22120.35 \u22120.3 \u22120.25 \u22120.2\u22120.8\u22120.6\u22120.4\u22120.200.20.40.6\n12\n34\n5678\n9\nFigure 12.24 Projection of 9 documents into 2 dimensions. Figure generated by lsiCode.\nExercise 12.11 PPCA vs FA\n(Source: Exercise 14.15 of (Hastie et al. 2009), due to Hinton.). Generate 200observations from the following\nmodel, where zi\u223cN(0,I):xi1=zi1,xi2=zi1+0.001zi2,xi3=1 0zi3. Fit a FA and PCA model\nwith 1 latent factor. Hence show that the corresponding weight vector waligns with the maximal variance\ndirection (dimension 3) in the PCA case, but with the maximal correlation direction (dimensions 1+2) in the\ncase of FA.", "450": "", "451": "13 Sparse linear models\n13.1 Introduction\nWe introduced the topic of feature selection in Section 3.5.4, where we discussed methods for\n\ufb01nding input variables which had high mutual information with the output. The trouble withthis approach is that it is based on a myopic strategy that only looks at one variable at a time.This can fail if there are interaction effects. For example, if y=xor(x\n1,x2), then neither x1nor\nx2on its own can predict the response, but together they perfectly predict the response. For a\nreal-world example of this, consider genetic association studies: sometimes two genes on theirown may be harmless, but when present together they cause a recessive disease (Balding 2006).\nIn this chapter, we focus on selecting sets of variables at a time using a model-based approach.\nIf the model is a generalized linear model, of the form p(y|x)=p(y|f(w\nTx))for some link\nfunctionf, then we can perform feature selection by encouraging the weight vector wto be\nsparse, i.e., to have lots of zeros. This approach turns out to offer signi\ufb01cant computationaladvantages, as we will see below.\nHere are some applications where feature selection/ sparsity is useful:\n\u2022 In many problems, we have many more dimensions Dthan training cases N. The cor-\nresponding design matrix is short and fat, rather than tall and skinny. This is called thesmallN, largeDproblem. This is becoming increasingly prevalent as we develop more\nhigh throughput measurement devices, For example, with gene microarrays, it is commonto measure the expression levels of D\u223c10,000genes, but to only get N\u223c100such\nexamples. (It is perhaps a sign of the times that even our data seems to be getting fatter...)We may want to \ufb01nd the smallest set of features that can accurately predict the response(e.g., growth rate of the cell) in order to prevent over\ufb01tting, to reduce the cost of building adiagnostic device, or to help with scienti\ufb01c insight into the problem.\n\u2022 In Chapter 14, we will use basis functions centered on the training examples, so \u03c6(x)=\n[\u03ba(x,x\n1),...,\u03ba(x,xN)],w h e r e \u03bais a kernel function. The resulting design matrix has size\nN\u00d7N. Feature selection in this context is equivalent to selecting a subset of the training\nexamples, which can help reduce over\ufb01tting and computational cost. This is known as asparse kernel machine.\n\u2022 In signal processing, it is common to represent signals (images, speech, etc.) in terms of\nwavelet basis functions. To save time and space, it is useful to \ufb01nd a sparse representation", "452": "422 Chapter 13. Sparse linear models\nof the signals, in terms of a small number of such basis functions. This allows us to estimate\nsignals from a small number of measurements, as well as to compress the signal. SeeSection 13.8.3 for more information.\nNote that the topic of feature selection and sparsity is currently one of the most active areas\nof machine learning/ statistics. In this chapter, we only have space to give an overview of themain results.\n13.2 Bayesian variable selection\nA natural way to pose the variable selection problem is as follows. Let \u03b3j=1if feature jis\n\u201crelevant\u201d, and let \u03b3j=0otherwise. Our goal is to compute the posterior over models\np(\u03b3|D)=e\u2212f(\u03b3)\n/summationtext\n\u03b3/primee\u2212f(\u03b3/prime)(13.1)\nwheref(\u03b3)is the cost function:\nf(\u03b3)/defines\u2212[logp(D|\u03b3)+logp(\u03b3)] (13.2)\nFor example, suppose we generate N=2 0samples from a D=1 0dimensional linear\nregression model, yi\u223cN(wTxi,\u03c32), in which K=5elements of ware non-zero. In\nparticular, we use w=( 0.00,\u22121.67,0.13,0.00,0.00,1.19,0.00,\u22120.04,0.33,0.00)and\u03c32=\n1. We enumerate all 210= 1024models and compute p(\u03b3|D)for each one (we give the\nequations for this below). We order the models in Gray code order, which ensures consecutive\nvectors differ by exactly 1 bit (the reasons for this are computational, and are discussed inSection 13.2.3).\nThe resulting set of bit patterns is shown in Figure 13.1(a). The cost of each model, f(\u03b3),i s\nshown in Figure 13.1(b). We see that this objective function is extremely \u201cbumpy\u201d. The resultsare easier to interpret if we compute the posterior distribution over models, p(\u03b3|D). This is\nshown in Figure 13.1(c). The top 8 models are listed below:\nmodel prob members\n4 0.447 2,61 0.241 2, 6,452 0.103 2, 6, 9,60 0.091 2, 3, 6,29 0.041 2, 5,68 0.021 2, 6, 7,36 0.015 2, 5, 6,5 0.010 2, 3,\nThe \u201ctrue\u201d model is {2,3,6,8,9}. However, the coefficients associated with features 3 and 8\nare very small (relative to \u03c3\n2). so these variables are harder to detect. Given enough data, the\nmethod will converge on the true model (assuming the data is generated from a linear model),but for \ufb01nite data sets, there will usually be considerable posterior uncertainty.\nInterpreting the posterior over a large number of models is quite difficult, so we will seek\nvarious summary statistics. A natural one is the posterior mode, or MAP estimate\n\u02c6\u03b3=a r g m a x p(\u03b3|D) = argmin f(\u03b3) (13.3)", "453": "13.2. Bayesian variable selection 423\n100 200 300 400 500 600 700 800 900 10001\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(a)0 200 400 600 800 1000\u2212220\u2212200\u2212180\u2212160\u2212140\u2212120\u2212100\u221280\u221260\u221240log p(model, data)\n(b)\n0 200 400 600 800 100000.010.020.030.040.050.060.070.080.090.1p(model|data)\n(c)1 2 3 4 5 6 7 8 9 1000.10.20.30.40.50.60.70.80.91p(gamma(j)|data\n(d)\nFigure 13.1 (a) All possible bit vectors of length 10 enumerated in Gray code order. (b) Score function for\nall possible models. (c) Posterior over all 1024 models. Vertical scale has been truncated at 0.1 for clarity.\n(d) Marginal inclusion probabilities. Figure generated by linregAllsubsetsGraycodeDemo .\nHowever, the mode is often not representative of the full posterior mass (see Section 5.2.1.3). A\nbetter summary is the median model (Barbieri and Berger 2004; Carvahlo and Lawrence 2007),\ncomputed using\n\u02c6\u03b3={j:p(\u03b3j=1|D)>0.5} (13.4)\nThis requires computing the posterior marginal inclusion probabilities ,p(\u03b3j=1|D). These\nare shown in Figure 13.1(d). We see that the model is con\ufb01dent that variables 2 and 6 are\nincluded; if we lower the decision threshold to 0.1, we would add 3 and 9 as well. However, if\nwe wanted to \u201ccapture\u201d variable 8, we would incur two false positives (5 and 7). This tradeoff\nbetween false positives and false negatives is discussed in more detail in Section 5.7.2.1.\nThe above example illustrates the \u201cgold standard\u201d for variable selection: the problem was\nsufficiently small (only 10 variables) that we were able to compute the full posterior exactly.\nOf course, variable selection is most useful in the cases where the number of dimensions is\nlarge. Since there are 2Dpossible models (bit vectors), it will be impossible to compute the\nfull posterior in general, and even \ufb01nding summaries, such as the MAP estimate or marginal", "454": "424 Chapter 13. Sparse linear models\ninclusion probabilities, will be intractable. We will therefore spend most of this chapter focussing\non algorithmic speedups. But before we do that, we will explain how we computed p(\u03b3|D)in\nthe above example.\n13.2.1 The spike and slab model\nThe posterior is given by\np(\u03b3|D)\u221dp(\u03b3)p(D|\u03b3) (13.5)\nWe \ufb01rst consider the prior, then the likelihood.\nIt is common to use the following prior on the bit vector:\np(\u03b3)=D/productdisplay\nj=1Ber(\u03b3j|\u03c00)=\u03c0||\u03b3||0\n0(1\u2212\u03c00)D\u2212||\u03b3||0(13.6)\nwhere\u03c00is the probability a feature is relevant, and ||\u03b3||0=/summationtextD\nj=1\u03b3jis the/lscript0pseudo-norm,\nthat is, the number of non-zero elements of the vector. For comparison with later models, it is\nuseful to write the log prior as follows:\nlogp(\u03b3|\u03c00)=||\u03b3 ||0log\u03c00+(D\u2212||\u03b3||0)log(1\u2212\u03c00) (13.7)\n=||\u03b3||0(log\u03c00\u2212log(1\u2212\u03c00))+const (13.8)\n=\u2212\u03bb||\u03b3||0+const (13.9)\nwhere\u03bb/defineslog1\u2212\u03c00\n\u03c00controls the sparsity of the model.\nWe can write the likelihood as follows:\np(D|\u03b3)=p(y|X,\u03b3)=/integraldisplay/integraldisplay\np(y|X,w,\u03b3)p(w|\u03b3,\u03c32)p(\u03c32)dwd\u03c32(13.10)\nFor notational simplicity, we have assumed the response is centered, (i.e., y=0), so we can\nignore any offset term \u03bc.\nWe now discuss the prior p(w|\u03b3,\u03c32).I f\u03b3j=0, featurejis irrelevant, so we expect wj=0.\nIf\u03b3j=1, we expect wjto be non-zero. If we standardize the inputs, a reasonable prior is\nN(0,\u03c32\u03c32\nw),w h e r e\u03c32\nwcontrols how big we expect the coefficients associated with the relevant\nvariables to be (which is scaled by the overall noise level \u03c32). We can summarize this prior as\nfollows:\np(wj|\u03c32,\u03b3j)=/braceleftbigg\u03b40(wj) if\u03b3j=0\nN(wj|0,\u03c32\u03c32\nw)if\u03b3j=1(13.11)\nThe \ufb01rst term is a \u201cspike\u201d at the origin. As \u03c32\nw\u2192\u221e, the distribution p(wj|\u03b3j=1 )approaches\na uniform distribution, which can be thought of as a \u201cslab\u201d of constant height. Hence this iscalled the spike and slab model (Mitchell and Beauchamp 1988).\nWe can drop the coefficients w\njfor which wj=0from the model, since they are clamped\nto zero under the prior. Hence Equation 13.10 becomes the following (assuming a Gaussianlikelihood):\np(D|\u03b3)=/integraldisplay/integraldisplay\nN(y|X\n\u03b3w\u03b3,\u03c32IN)N(w\u03b3|0D\u03b3,\u03c32\u03c32\nwID\u03b3)p(\u03c32)dw\u03b3d\u03c32(13.12)", "455": "13.2. Bayesian variable selection 425\nwhereD\u03b3=||\u03b3||0is the number of non-zero elements in \u03b3. In what follows, we will generalize\nthis slightly by de\ufb01ning a prior of the form p(w|\u03b3,\u03c32)=N(w\u03b3|0D\u03b3,\u03c32\u03a3\u03b3)for any positive\nde\ufb01nite matrix \u03a3\u03b3.1\nGiven these priors, we can now compute the marginal likelihood. If the noise variance is\nknown, we can write down the marginal likelihood (using Equation 13.151) as follows:\np(D|\u03b3,\u03c32)=/integraldisplay\nN(y|X\u03b3w\u03b3,\u03c32I)N(w\u03b3|0,\u03c32\u03a3\u03b3)dw\u03b3=N(y|0,C\u03b3) (13.13)\nC\u03b3/defines\u03c32X\u03b3\u03a3\u03b3XT\n\u03b3+\u03c32IN (13.14)\nIf the noise is unknown, we can put a prior on it and integrate it out. It is common to use\np(\u03c32)=I G (\u03c32|a\u03c3,b\u03c3). Some guidelines on setting a,bcan be found in (Kohn et al. 2001). If\nwe usea=b=0, we recover the Jeffrey\u2019s prior, p(\u03c32)\u221d\u03c3\u22122. When we integrate out the noise,\nwe get the following more complicated expression for the marginal likelihood (Brown et al. 1998):\np(D|\u03b3)=/integraldisplay/integraldisplay\np(y|\u03b3,w\u03b3,\u03c32)p(w\u03b3|\u03b3,\u03c32)p(\u03c32)dw\u03b3d\u03c32(13.15)\n\u221d|XT\u03b3X\u03b3+\u03a3\u22121\n\u03b3|\u22121\n2|\u03a3\u03b3|\u22121\n2(2b\u03c3+S(\u03b3))\u2212(2a\u03c3+N\u22121)/2(13.16)\nwhereS(\u03b3)is the RSS:\nS(\u03b3)/definesyTy\u2212yTX\u03b3(XT\u03b3X\u03b3+\u03a3\u22121\n\u03b3)\u22121XT\u03b3y (13.17)\nSee also Exercise 13.4.\nWhen the marginal likelihood cannot be computed in closed form (e.g., if we are using logistic\nregression or a nonlinear model), we can approximate it using BIC, which has the form\nlogp(D|\u03b3)\u2248logp(y|X,\u02c6w\u03b3,\u02c6\u03c32)\u2212||\u03b3||0\n2logN (13.18)\nwhere\u02c6w\u03b3is the ML or MAP estimate based on X\u03b3, and||\u03b3||0is the \u201cdegrees of freedom\u201d of\nthe model (Zou et al. 2007). Adding the log prior, the overall objective becomes\nlogp(\u03b3|D)\u2248logp(y|X,\u02c6w\u03b3,\u02c6\u03c32)\u2212||\u03b3||0\n2logN\u2212\u03bb||\u03b3||0+const (13.19)\nWe see that there are two complexity penalties: one arising from the BIC approximation to\nthe marginal likelihood, and the other arising from the prior on p(\u03b3). Obviously these can be\ncombined into one overall complexity parameter, which we will denote by \u03bb.\n13.2.2 From the Bernoulli-Gaussian model to /lscript0regularization\nAnother model that is sometimes used (e.g., (Kuo and Mallick 1998; Zhou et al. 2009; Soussenet al. 2010)) is the following:\ny\ni|xi,w,\u03b3,\u03c32\u223cN(/summationdisplay\nj\u03b3jwjxij,\u03c32) (13.20)\n\u03b3j\u223cBer(\u03c00) (13.21)\nwj\u223cN(0,\u03c32\nw) (13.22)\n1. It is common to use a g-prior of the form \u03a3\u03b3=g(XT\n\u03b3X\u03b3)\u22121for reasons explained in Section 7.6.3.1 (see also\nExercise 13.4). Various approaches have been proposed for setting g, including cross validation, empirical Bayes (Minka\n2000b; George and Foster 2000), hierarchical Bayes (Liang et al. 2008), etc.", "456": "426 Chapter 13. Sparse linear models\nIn the signal processing literature (e.g., (Soussen et al. 2010)), this is called the Bernoulli-\nGaussian model, although we could also call it the binary mask model, since we can think of\nthe\u03b3jvariables as \u201cmasking out\u201d the weights wj.\nUnlike the spike and slab model, we do not integrate out the \u201cirrelevant\u201d coefficients; they\nalways exist. In addition, the binary mask model has the form \u03b3j\u2192y\u2190wj, whereas the spike\nand slab model has the form \u03b3j\u2192wj\u2192y. In the binary mask model, only the product \u03b3jwj\ncan be identi\ufb01ed from the likelihood.\nOne interesting aspect of this model is that it can be used to derive an objective function that\nis widely used in the (non-Bayesian) subset selection literature. First, note that the joint prior\nhas the form\np(\u03b3,w)\u221dN(w|0,\u03c32\nwI)\u03c0||\u03b3||0\n0(1\u2212\u03c00)D\u2212||\u03b3||0(13.23)\nHence the scaled unnormalized negative log posterior has the form\nf(\u03b3,w)/defines\u22122\u03c32logp(\u03b3,w,y|X)=||y \u2212X(\u03b3.\u2217w)||2\n+\u03c32\n\u03c32w||w||2+\u03bb||\u03b3||0+const (13.24)\nwhere\n\u03bb/defines2\u03c32log(1\u2212\u03c00\n\u03c00) (13.25)\nLet us split winto two subvectors, w\u2212\u03b3andw\u03b3, indexed by the zero and non-zero entries of\n\u03b3respectively. Since X(\u03b3.\u2217w)=X\u03b3w\u03b3, we can just set w\u2212\u03b3=0.\nNow consider the case where \u03c32\nw\u2192\u221e, so we do not regularize the non-zero weights (so\nthere is no complexity penalty coming from the marginal likelihood or its BIC approximation).In this case, the objective becomes\nf(\u03b3,w)=||y\u2212X\n\u03b3w\u03b3||2\n2+\u03bb||\u03b3||0 (13.26)\nThis is similar to the BIC objective above.\nInstead of keeping track of the bit vector \u03b3, we can de\ufb01ne the set of relevant variables to\nbe thesupport, or set of non-zero entries, of w. Then we can rewrite the above equation as\nfollows:\nf(w)=||y\u2212Xw||22+\u03bb||w||0 (13.27)\nThis is called /lscript0regularization. We have converted the discrete optimization problem (over\n\u03b3\u2208{0,1}D) into a continuous one (over w\u2208RD); however, the /lscript0pseudo-norm makes the\nobjective very non smooth, so this is still hard to optimize. We will discuss different solutions\nto this in the rest of this chapter.\n13.2.3 Algorithms\nSince there are 2Dmodels, we cannot explore the full posterior, or \ufb01nd the globally optimal\nmodel. Instead we will have to resort to heuristics of one form or another. All of the methodswe will discuss involve searching through the space of models, and evaluating the cost f(\u03b3)at", "457": "13.2. Bayesian variable selection 427\n{}{1}{ 2}{ 3}{ 4}{1,2}{ 1,3}{ 1,4}{ 2,3}{ 2,4}{ 3,4}{1,2,3}{2,3,4}{1,3,4}{1,2,4}{1,2,3,4}\n(a)0 1 2 3 4 5 6 7 80.40.60.811.21.4\nsubset sizetraining set errorall subsets on prostate cancer\n(b)\nFigure 13.2 (a) A lattice of subsets of {1,2,3,4}. (b) Residual sum of squares versus subset size, on the\nprostate cancer data set. The lower envelope is the best RSS achievable for any set of a given size. Based\non Figure 3.5 of (Hastie et al. 2001). Figure generated by prostateSubsets .\neach point. This requires \ufb01tting the model (i.e., computing argmaxp(D|w)), or evaluating its\nmarginal likelihood (i.e., computing/integraltext\np(D|w)p(w)dw) at each step. This is sometimes called\nthewrapper method, since we \u201cwrap\u201d our search for the best model (or set of good models)\naround a generic model-\ufb01tting procedure.\nIn order to make wrapper methods efficient, it is important that we can quickly evaluate the\nscore function for some new model, \u03b3/prime, given the score of a previous model, \u03b3. This can be\ndone provided we can efficiently update the sufficient statistics needed to compute f(\u03b3). This\nis possible provided \u03b3/primeonly differs from \u03b3in one bit (corresponding to adding or removing\na single variable), and provided f(\u03b3)only depends on the data via X\u03b3. In this case, we can\nuse rank-one matrix updates/ downdates to efficiently compute XT\n\u03b3/primeX\u03b3/primefromXT\n\u03b3X\u03b3. These\nupdates are usually applied to the QR decomposition of X. See e.g., (Miller 2002; Schniter et al.\n2008) for details.\n13.2.3.1 Greedy search\nSuppose we want to \ufb01nd the MAP model. If we use the /lscript0-regularized objective in Equation 13.27,\nwe can exploit properties of least squares to derive various efficient greedy forwards search\nmethods, some of which we summarize below. For further details, see (Miller 2002; Soussenet al. 2010).\n\u2022Single best replacement The simplest method is to use greedy hill climbing, where at each\nstep, we de\ufb01ne the neighborhood of the current model to be all models than can be reached\nby \ufb02ipping a single bit of \u03b3, i.e., for each variable, if it is currently out of the model, we\nconsider adding it, and if it is currently in the model, we consider removing it. In (Soussenet al. 2010), they call this the single best replacement (SBR). Since we are expecting a\nsparse solution, we can start with the empty set, \u03b3=0. We are essentially moving through\nthe lattice of subsets, shown in Figure 13.2(a). We continue adding or removing until noimprovement is possible.\n\u2022Orthogonal least squares If we set \u03bb=0in Equation 13.27, so there is no complexity\npenalty, there will be no reason to perform deletion steps. In this case, the SBR algorithm isequivalent to orthogonal least squares (Chen and Wigger 1995), which in turn is equivalent", "458": "428 Chapter 13. Sparse linear models\nto greedy forwards selection. In this algorithm, we start with the empty set and add the\nbest feature at each step. The error will go down monotonically with ||\u03b3||0, as shown in\nFigure 13.2(b). We can pick the next best feature j\u2217to add to the current set \u03b3tby solving\nj\u2217=a r gm i n\nj/negationslash\u2208\u03b3tmin\nw||y\u2212(X\u03b3t\u222aj)w||2(13.28)\nWe then update the active set by setting \u03b3(t+1)=\u03b3(t)\u222a{j\u2217}. To choose the next feature to\nadd at step t, we need to solve D\u2212Dtleast squares problems at step t,w h e r eDt=|\u03b3t|is\nthe cardinality of the current active set. Having chosen the best feature to add, we need to\nsolve an additional least squares problem to compute wt+1).\n\u2022Orthogonal matching pursuits Orthogonal least squares is somewhat expensive. A simpli-\n\ufb01cation is to \u201cfreeze\u201d the current weights at their current value, and then to pick the nextfeature to add by solving\nj\n\u2217=a r gm i n\nj/negationslash\u2208\u03b3tmin\n\u03b2||y\u2212Xwt\u2212\u03b2x:,j||2(13.29)\nThis inner optimization is easy to solve: we simply set \u03b2=xT\n:,jrt/||x:,j||2,w h e r ert=\ny\u2212Xwtis the current residual vector. If the columns are unit norm, we have\nj\u2217=a r gm a x xT\n:,jrt (13.30)\nso we are just looking for the column that is most correlated with the current residual. We\nthen update the active set, and compute the new least squares estimate wt+1usingX\u03b3t+1.\nThis method is called orthogonal matching pursuits orOMP(Mallat et al. 1994). This only\nrequires one least squares calculation per iteration and so is faster than orthogonal leastsquares, but is not quite as accurate (Blumensath and Davies 2007).\n\u2022Matching pursuits An even more aggressive approximation is to just greedily add the feature\nthat is most correlated with the current residual. This is called matching pursuits (Mallat\nand Zhang 1993). This is also equivalent to a method known as least squares boosting(Section 16.4.6).\n\u2022Backwards selection Backwards selection starts with all variables in the model (the so-\ncalledsaturated model), and then deletes the worst one at each step. This is equivalent\nto performing a greedy search from the top of the lattice downwards. This can give betterresults than a bottom-up search, since the decision about whether to keep a variable ornot is made in the context of all the other variables that might depende on it. However,this method is typically infeasible for large problems, since the saturated model will be tooexpensive to \ufb01t.\n\u2022FoBaTheforwards-backwards algorithm of (Zhang 2008) is similar to the single best\nreplacement algorithm presented above, except it uses an OMP-like approximation whenchoosing the next move to make. A similar \u201cdual-pass\u201d algorithm was described in (Moghad-dam et al. 2008).\n\u2022Bayesian Matching pursuit The algorithm of (Schniter et al. 2008) is similiar to OMP except\nit uses a Bayesian marginal likelihood scoring criterion (under a spike and slab model) insteadof a least squares objective. In addition, it uses a form of beam search to explore multiplepaths through the lattice at once.", "459": "13.3./lscript1regularization: basics 429\n13.2.3.2 Stochastic search\nIf we want to approximate the posterior, rather than just computing a mode (e.g. because we\nwant to compute marginal inclusion probabilities), one option is to use MCMC. The standardapproach is to use Metropolis Hastings, where the proposal distribution just \ufb02ips single bits.This enables us to efficiently compute p(\u03b3\n/prime|D)givenp(\u03b3|D). The probability of a state (bit\ncon\ufb01guration) is estimated by counting how many times the random walk visits this state. See(O\u2019Hara and Sillanpaa 2009) for a review of such methods, and (Bottolo and Richardson 2010)for a very recent method based on evolutionary MCMC.\nHowever, in a discrete state space, MCMC is needlessly inefficient, since we can compute the\n(unnormalized) probability of a state directly using p(\u03b3,D)=e x p ( \u2212f(\u03b3)); thus there is no\nneed to ever revisit a state. A much more efficient alternative is to use some kind of stochasticsearch algorithm, to generate a set Sof high scoring models, and then to make the following\napproximation\np(\u03b3|D)\u2248e\n\u2212f(\u03b3)\n/summationtext\n\u03b3/prime\u2208Se\u2212f(\u03b3/prime)(13.31)\nSee (Heaton and Scott 2009) for a review of recent methods of this kind.\n13.2.3.3 EM and variational inference *\nIt is tempting to apply EM to the spike and slab model, which has the form \u03b3j\u2192wj\u2192y.W e\ncan compute p(\u03b3j=1|wj)in the E step, and optimize win the M step. However, this will not\nwork, because when we compute p(\u03b3j=1|wj), we are comparing a delta-function, \u03b40(wj), with\na Gaussian pdf, N(wj|0,\u03c32\nw). We can replace the delta function with a narrow Gaussian, and\nthen the E step amounts to classifying wjunder the two possible Gaussian models. However,\nthis is likely to suffer from severe local minima.\nAn alternative is to apply EM to the Bernoulli-Gaussian model, which has the form \u03b3j\u2192y\u2190\nwj. In this case, the posterior p(\u03b3|D,w)is intractable to compute because all the bits become\ncorrelated due to explaining away. However, it is possible to derive a mean \ufb01eld approximationof the form/producttext\njq(\u03b3j)q(wj)(Huang et al. 2007; Rattray et al. 2009).\n13.3/lscript1regularization: basics\nWhen we have many variables, it is computationally difficult to \ufb01nd the posterior mode ofp(\u03b3|D). And although greedy algorithms often work well (see e.g., (Zhang 2008) for a theoretical\nanalysis), they can of course get stuck in local optima.\nPart of the problem is due to the fact that the \u03b3\njvariables are discrete, \u03b3j\u2208{0,1}.I n\nthe optimization community, it is common to relax hard constraints of this form by replacingdiscrete variables with continuous variables. We can do this by replacing the spike-and-slab styleprior, that assigns \ufb01nite probability mass to the event that w\nj=0, to continuous priors that\n\u201cencourage\u201d wj=0by putting a lot of probability density near the origin, such as a zero-mean\nLaplace distribution. This was \ufb01rst introduced in Section 7.4 in the context of robust linearregression. There we exploited the fact that the Laplace has heavy tails. Here we exploit the fact", "460": "430 Chapter 13. Sparse linear models\nFigure 13.3 Illustration of /lscript1(left) vs/lscript2(right) regularization of a least squares problem. Based on Figure\n3.12 of (Hastie et al. 2001).\nthat it has a spike near \u03bc=0. More precisely, consider a prior of the form\np(w|\u03bb)=D/productdisplay\nj=1Lap(wj|0,1/\u03bb)\u221dD/productdisplay\nj=1e\u2212\u03bb|wj|(13.32)\nWe will use a uniform prior on the offset term, p(w0)\u221d1. Let us perform MAP estimation with\nthis prior. The penalized negative log likelihood has the form\nf(w)=\u2212logp(D|w)\u2212logp(w|\u03bb) = NLL( w)+\u03bb||w||1 (13.33)\nwhere||w||1=/summationtextD\nj=1|wj|is the/lscript1norm ofw. For suitably large \u03bb, the estimate \u02c6wwill be\nsparse, for reasons we explain below. Indeed, this can be thought of as a convex approximation\nto the non-convex /lscript0objective\nargmin\nwNLL(w)+\u03bb||w||0 (13.34)\nIn the case of linear regression, the /lscript1objective becomes\nf(w)=N/summationdisplay\ni=1\u22121\n2\u03c32(yi\u2212(w0+wTxi))2+\u03bb||w||1 (13.35)\n= RSS( w)+\u03bb/prime||w||1 (13.36)\nwhere\u03bb/prime=2\u03bb\u03c32. This method is known as basis pursuit denoising orBPDN(Chen et al. 1998).\nThe reason for this term will become clear later. In general, the technique of putting a zero-mean\nLaplace prior on the parameters and performing MAP estimation is called /lscript1regularization .\nIt can be combined with any convex or non-convex NLL term. Many different algorithms have\nbeen devised for solving such problems, some of which we review in Section 13.4.\n13.3.1 Why does /lscript1regularization yield sparse solutions?\nWe now explain why /lscript1regularization results in sparse solutions, whereas /lscript2regularization does\nnot. We focus on the case of linear regression, although similar arguments hold for logistic\nregression and other GLMs.", "461": "13.3./lscript1regularization: basics 431\nThe objective is the following non-smooth objective function:\nmin\nwRSS(w)+\u03bb||w||1 (13.37)\nWe can rewrite this as a constrained but smooth objective (a quadratic function with linear\nconstraints):\nmin\nwRSS(w)s.t.||w||1\u2264B (13.38)\nwhereBis an upper bound on the /lscript1-norm of the weights: a small (tight) bound Bcorresponds\nto a large penalty \u03bb, and vice versa.2Equation 13.38 is known as lasso, which stands for \u201cleast\nabsolute shrinkage and selection operator\u201d (Tibshirani 1996). We will see why it has this namelater.\nSimilarly, we can write ridge regression\nmin\nwRSS(w)+\u03bb||w||2\n2 (13.39)\nor as a bound constrained form:\nmin\nwRSS(w)s.t.||w||22\u2264B (13.40)\nIn Figure 13.3, we plot the contours of the RSS objective function, as well as the contours of\nthe/lscript2and/lscript1constraint surfaces. From the theory of constrained optimization, we know that\nthe optimal solution occurs at the point where the lowest level set of the objective function\nintersects the constraint surface (assuming the constraint is active). It should be geometricallyclear that as we relax the constraint B,w e\u201c g r o w \u201dt h e/lscript\n1\u201cball\u201d until it meets the objective; the\ncorners of the ball are more likely to intersect the ellipse than one of the sides, especially in highdimensions, because the corners \u201cstick out\u201d more. The corners correspond to sparse solutions,which lie on the coordinate axes. By contrast, when we grow the /lscript\n2ball, it can intersect the\nobjective at any point; there are no \u201ccorners\u201d, so there is no preference for sparsity.\nTo see this another away, notice that, with ridge regression, the prior cost of a sparse solution,\nsuch asw=( 1,0), is the same as the cost of a dense solution, such as w=( 1/\u221a\n2,1/\u221a\n2),\nas long as they have the same /lscript2norm:\n||(1,0)||2=||(1/\u221a\n2,1/\u221a\n2||2=1 (13.41)\nHowever, for lasso, setting w=( 1,0)is cheaper than setting w=( 1/\u221a\n2,1/\u221a\n2), since\n||(1,0)||1=1<||(1/\u221a\n2,1/\u221a\n2||1=\u221a\n2 (13.42)\nThe most rigorous way to see that /lscript1regularization results in sparse solutions is to examine\nconditions that hold at the optimum. We do this in Section 13.3.2.\n13.3.2 Optimality conditions for lasso\nThe lasso objective has the form\nf(\u03b8) = RSS( \u03b8)+\u03bb||w||1 (13.43)\n2. Equation 13.38 is an example of a quadratic program orQP, since we have a quadratic objective subject to linear\ninequality constraints. Its Lagrangian is given by Equation 13.37.", "462": "432 Chapter 13. Sparse linear models\nX0Xc(x \u2212 x0)f(x) \u2212 f(x0)c\u02bc\nc\nFigure 13.4 Illustration of some sub-derivatives of a function at point x0. Based on a \ufb01gure at http:\n//en.wikipedia .org/wiki/Subderivative . Figure generated by subgradientPlot .\nUnfortunately, the ||w||1term is not differentiable whenever wj=0. This is an example of a\nnon-smooth optimization problem.\nTo handle non-smooth functions, we need to extend the notion of a derivative. We de\ufb01ne a\nsubderivative orsubgradient of a (convex) function f:I\u2192Rat a point \u03b80to be a scalar g\nsuch that\nf(\u03b8)\u2212f(\u03b80)\u2265g(\u03b8\u2212\u03b80)\u2200\u03b8\u2208I (13.44)\nwhereIis some interval containing \u03b80. See Figure 13.4 for an illustration.3We de\ufb01ne the setof\nsubderivatives as the interval [a,b]whereaandbare the one-sided limits\na= lim\n\u03b8\u2192\u03b8\u2212\n0f(\u03b8)\u2212f(\u03b80)\n\u03b8\u2212\u03b80,b= lim\n\u03b8\u2192\u03b8+\n0f(\u03b8)\u2212f(\u03b80)\n\u03b8\u2212\u03b80(13.46)\nThe set[a,b]of all subderivatives is called the subdifferential of the function fat\u03b80and\nis denoted \u2202f(\u03b8)|\u03b80. For example, in the case of the absolute value function f(\u03b8)=|\u03b8|, the\nsubderivative is given by\n\u2202f(\u03b8)=\u23a7\n\u23a8\n\u23a9{\u22121}if\u03b8<0\n[\u22121,1]if\u03b8=0\n{+1}if\u03b8>0(13.47)\nIf the function is everywhere differentiable, then \u2202f(\u03b8)={df(\u03b8)\nd\u03b8}. By analogy to the standard\ncalculus result, one can show that the point \u02c6\u03b8is a local minimum of fiff0\u2208\u2202f(\u03b8)|\u02c6\u03b8.\n3. In general, for a vector valued function, we say that gis a subgradient of fat\u03b80if for all vectors \u03b8,\nf(\u03b8)\u2212f(\u03b80)\u2265(\u03b8\u2212\u03b80)Tg (13.45)\nsogis a linear lower bound to the function at \u03b80.", "463": "13.3./lscript1regularization: basics 433\nFN\n(a)FN\n(b)\nFigure 13.5 Left: soft thresholding. The \ufb02at region is the interval [\u2212\u03bb,+\u03bb]. Right: hard thresholding.\nLet us apply these concepts to the lasso problem. Let us initially ignore the non-smooth\npenalty term. One can show (Exercise 13.1) that\n\u2202\n\u2202wjRSS(w)=a jwj\u2212cj (13.48)\naj=2n/summationdisplay\ni=1x2\nij (13.49)\ncj=2n/summationdisplay\ni=1xij(yi\u2212wT\n\u2212jxi,\u2212j) (13.50)\nwherew\u2212jiswwithout component j, and similarly for xi,\u2212j. We see that cjis (proportional\nto) the correlation between the j\u2019th feature x:,jand the residual due to the other features,\nr\u2212j=y\u2212X:,\u2212jw\u2212j. Hence the magnitude of cjis an indication of how relevant feature jis\nfor predicting y(relative to the other features and the current parameters).\nAdding in the penalty term, we \ufb01nd that the subderivative is given by\n\u2202wjf(w)=(ajwj\u2212cj)+\u03bb\u2202 wj||w||1 (13.51)\n=\u23a7\n\u23a8\n\u23a9{ajwj\u2212cj\u2212\u03bb}ifwj<0\n[\u2212cj\u2212\u03bb,\u2212cj+\u03bb]ifwj=0\n{ajwj\u2212cj+\u03bb}ifwj>0(13.52)\nWe can write this in a more compact fashion as follows:\nXT(Xw\u2212y)j\u2208\u23a7\n\u23a8\n\u23a9{\u2212\u03bb}ifwj<0\n[\u2212\u03bb,\u03bb]ifwj=0\n{\u03bb}ifwj>0(13.53)\nDepending on the value of cj, the solution to \u2202wjf(w)=0can occur at 3 different values\nofwj, as follows:", "464": "434 Chapter 13. Sparse linear models\n1. Ifcj<\u2212\u03bb, so the feature is strongly negatively correlated with the residual, then the\nsubgradient is zero at \u02c6wj=cj+\u03bb\naj<0.\n2. Ifcj\u2208[\u2212\u03bb,\u03bb], so the feature is only weakly correlated with the residual, then the subgradient\nis zero at \u02c6wj=0.\n3. Ifcj>\u03bb, so the feature is strongly positively correlated with the residual, then the subgra-\ndient is zero at \u02c6wj=cj\u2212\u03bb\naj>0.\nIn summary, we have\n\u02c6wj(cj)=\u23a7\n\u23a8\n\u23a9(cj+\u03bb)/ajifcj<\u2212\u03bb\n0ifcj\u2208[\u2212\u03bb,\u03bb]\n(cj\u2212\u03bb)/ajifcj>\u03bb(13.54)\nWe can write this as follows:\n\u02c6wj=soft(cj\naj;\u03bb\naj) (13.55)\nwhere\nsoft(a;\u03b4)/definessign(a)(|a|\u2212\u03b4)+(13.56)\nandx+= max(x,0)is the positive part of x. This is called soft thresholding. This is\nillustrated in Figure 13.5(a), where we plot \u02c6wjvscj. The dotted line is the line wj=cj/aj\ncorresponding to the least squares \ufb01t. The solid line, which represents the regularized estimate\n\u02c6wj(cj), shifts the dotted line down (or up) by \u03bb, except when \u2212\u03bb\u2264cj\u2264\u03bb, in which case it\nsetswj=0.\nBy contrast, in Figure 13.5(b), we illustrate hard thresholding. This sets values of wjto\n0i f\u2212\u03bb\u2264cj\u2264\u03bb, but it does not shrink the values of wjoutside of this interval. The\nslope of the soft thresholding line does not coincide with the diagonal, which means that even\nlarge coefficients are shrunk towards zero; consequently lasso is a biased estimator. This isundesirable, since if the likelihood indicates (via c\nj) that the coefficient wjshould be large, we\ndo not want to shrink it. We will discuss this issue in more detail in Section 13.6.2.\nNow we \ufb01nally can understand why Tibshirani invented the term \u201classo\u201d in (Tibshirani 1996):\nit stands for \u201cleast absolute selection and shrinkage operator\u201d, since it selects a subset of thevariables, and shrinks all the coefficients by penalizing the absolute values. If \u03bb=0, we get the\nOLS solution (of minimal /lscript\n1norm). If \u03bb\u2265\u03bbmax,w eg e t\u02c6w=0,w h e r e\n\u03bbmax=||XTy||\u221e=m a x\nj|yTx:,j| (13.57)\nThis value is computed using the fact that 0is optimal if (XTy)j\u2208[\u2212\u03bb,\u03bb]for allj. In general,\nthe maximum penalty for an /lscript1regularized objective is\n\u03bbmax=m a x\nj|\u2207jNLL(0)| (13.58)", "465": "13.3./lscript1regularization: basics 435\n13.3.3 Comparison of least squares, lasso, ridge and subset selection\nWe can gain further insight into /lscript1regularization by comparing it to least squares, and /lscript2and\n/lscript0regularized least squares. For simplicity, assume all the features of Xare orthonormal, so\nXTX=I. In this case, the RSS is given by\nRSS(w)=||y \u2212Xw||2=yTy+wTXTXw\u22122wTXTy (13.59)\n= const+/summationdisplay\nkw2\nk\u22122/summationdisplay\nk/summationdisplay\niwkxikyi (13.60)\nso we see this factorizes into a sum of terms, one per dimension. Hence we can write down the\nMAP and ML estimates analytically, as follows:\n\u2022MLEThe OLS solution is given by\n\u02c6wOLS\nk=xT\n:ky (13.61)\nwherex:kis thek\u2019th column of X. This follows trivially from Equation 13.60. We see\nthat\u02c6wOLS\nkis just the orthogonal projection of feature konto the response vector (see\nSection 7.3.2).\n\u2022RidgeOne can show that the ridge estimate is given by\n\u02c6wridge\nk=\u02c6wOLS\nk\n1+\u03bb(13.62)\n\u2022LassoFrom Equation 13.55, and using the fact that ak=2and\u02c6wOLS\nk=ck/2,w eh a v e\n\u02c6wlasso\nk=s i g n ( \u02c6 wOLS\nk)/parenleftbigg\n|\u02c6wOLS\nk|\u2212\u03bb\n2/parenrightbigg\n+(13.63)\nThis corresponds to soft thresholding, shown in Figure 13.5(a).\n\u2022Subset selection If we pick the best Kfeatures using subset selection, the parameter\nestimate is as follows\n\u02c6wSS\nk=/braceleftbigg\n\u02c6wOLS\nkif rank(|wOLS\nk|)\u2264K\n0 otherwise(13.64)\nwhere rank refers to the location in the sorted list of weight magnitudes. This corresponds\nto hard thresholding, shown in Figure 13.5(b).\nFigure 13.6(a) plots the MSE vs \u03bbfor lasso for a degree 14 polynomial, and Figure 13.6(b) plots\nthe MSE vs polynomial order. We see that lasso gives similar results to the subset selectionmethod.\nAs another example, consider a data set concerning prostate cancer. We have D=8features\nandN=6 7training cases; the goal is to predict the log prostate-speci\ufb01c antigen levels (see\n(Hastie et al. 2009, p4) for more biological details). Table 13.1 shows that lasso gives betterprediction accuracy (at least on this particular data set) than least squares, ridge, and bestsubset regression. (In each case, the strength of the regularizer was chosen by cross validation.)Lasso also gives rise to a sparse solution. Of course, for other problems, ridge may give betterpredictive accuracy. In practice, a combination of lasso and ridge, known as the elastic net,often performs best, since it provides a good combination of sparsity and regularization (seeSection 13.5.3).", "466": "436 Chapter 13. Sparse linear models\n103.249 10 1 0.5 0.1 0.01 0.0001 005101520253035\nlambdamsetrain\ntest\n(a)0 2 4 6 8 10 12 14 1605101520253035\ndegreemseperformance of MLE\n  \ntrain\ntest\n(b)\nFigure 13.6 (a) MSE vs \u03bbfor lasso for a degree 14 polynomial. Note that \u03bbdecreases as we move to\nthe right. Figure generated by linregPolyLassoDemo . (b) MSE versus polynomial degree. Note that the\nmodel order increases as we move to the right. See Figure 1.18 for a plot of some of these polynomial\nregression models. Figure generated by linregPolyVsDegree .\nTerm LS Best Subset Ridge Lasso\nIntercept 2.452 2.481 2.479 2.480\nlcavol 0.716 0.651 0.656 0.653\nlweight 0.293 0.380 0.300 0.297\nage -0.143 -0.000 -0.129 -0.119\nlbph 0.212 -0.000 0.208 0.200\nsvi 0.310 -0.000 0.301 0.289\nlcp -0.289 -0.000 -0.260 -0.236\ngleason -0.021 -0.000 -0.019 0.000\npgg45 0.277 0.178 0.256 0.226\nTest Error 0.586 0.572 0.580 0.564\nTable 13.1 Results of different methods on the prostate cancer data, which has 8 features and 67 training\ncases. Methods are: LS = least squares, Subset = best subset regression, Ridge, Lasso. Rows representthe coefficients; we see that subset regression and lasso give sparse solutions. Bottom row is the meansquared error on the test set (30 cases). Based on Table 3.3. of (Hastie et al. 2009). Figure generated byprostateComparison .\n13.3.4 Regularization path\nAs we increase \u03bb, the solution vector \u02c6w(\u03bb)will tend to get sparser, although not necessarily\nmonotonically. We can plot the values \u02c6wj(\u03bb)vs\u03bbfor each feature j; this is known as the\nregularization path.\nThis is illustrated for ridge regression in Figure 13.7(a), where we plot \u02c6wj(\u03bb)as the regularizer\n\u03bbdecreases. We see that when \u03bb=\u221e, all the coefficients are zero. But for any \ufb01nite value of\n\u03bb, all coefficients are non-zero; furthermore, they increase in magnitude as \u03bbis decreased.\nIn Figure 13.7(b), we plot the analogous result for lasso. As we move to the right, the upper\nbound on the /lscript1penalty,B, increases. When B=0, all the coefficients are zero. As we increase", "467": "13.3./lscript1regularization: basics 437\n0 5 10 15 20 25 30\u22120.2\u22120.100.10.20.30.40.50.6\n  \nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\n(a)0 5 10 15 20 25\u22120.2\u22120.100.10.20.30.40.50.60.7\n  \nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\n(b)\nFigure 13.7 (a) Pro\ufb01les of ridge coefficients for the prostate cancer example vs bound on /lscript2norm ofw,\nso smallt(large\u03bb) is on the left. The vertical line is the value chosen by 5-fold CV using the 1SE rule.\nBased on Figure 3.8 of (Hastie et al. 2009). Figure generated by ridgePathProstate . (b) Pro\ufb01les of lasso\ncoefficients for the prostate cancer example vs bound on /lscript1norm ofw, so small t(large\u03bb) is on the left.\nBased on Figure 3.10 of (Hastie et al. 2009). Figure generated by lassoPathProstate .\n0 0.5 1 1.5 2\u22120.2\u22120.100.10.20.30.40.50.60.7\n\u03c4  \nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\n(a)1 2 3 4 5 6 7 8 9\u22120.2\u22120.100.10.20.30.40.50.60.7\nlars step  \nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\n(b)\nFigure 13.8 Illustration of piecewise linearity of regularization path for lasso on the prostate cancer\nexample. (a) We plot \u02c6wj(B)vsBfor the critical values of B. (b) We plot vs steps of the LARS algorithm.\nFigure generated by lassoPathProstate .\nB, the coefficients gradually \u201cturn on\u201d. But for any value between 0 and Bmax=||\u02c6wOLS||1,\nthe solution is sparse.4\nRemarkably, it can be shown that the solution path is a piecewise linear function of B(Efron\net al. 2004). That is, there are a set of critical values of Bwhere the active set of non-zero\ncoefficients changes. For values of Bbetween these critical values, each non-zero coefficient\nincreases or decreases in a linear fashion. This is illustrated in Figure 13.8(a). Furthermore,\none can solve for these critical values analytically. This is the basis of the LARSalgorithm\n(Efron et al. 2004), which stands for \u201cleast angle regression and shrinkage\u201d (see Section 13.4.2for details). Remarkably, LARS can compute the entire regularization path for roughly the same\n4. It is common to plot the solution versus the shrinkage factor, de\ufb01ned as s(B)=B/B max, rather than against B.\nThis merely affects the scale of the horizontal axis, not the shape of the curves.", "468": "438 Chapter 13. Sparse linear models\n0 1000 2000 3000 4000\u2212101Original (D = 4096, number of nonzeros = 160)\n0 1000 2000 3000 4000\u2212101L1 reconstruction (K0 = 1024, lambda = 0.0516, MSE = 0.0027)\n0 1000 2000 3000 4000\u2212101Debiased (MSE = 3.26e\u2212005)\n0 1000 2000 3000 4000\u22120.500.5Minimum norm solution (MSE = 0.0292)\nFigure 13.9 Example of recovering a sparse signal using lasso. See text for details. Based on Figure 1 of\n(Figueiredo et al. 2007). Figure generated by sparseSensingDemo , written by Mario Figueiredo.\ncomputational cost as a single least squares \ufb01t (namely O(min(ND2,DN2)).\nIn Figure 13.8(b), we plot the coefficients computed at each critical value of B. Now the\npiecewise linearity is more evident. Below we display the actual coefficient values at each step\nalong the regularization path (the last line is the least squares solution):\nListing 13.1 Output of lassoPathProstate\n00000000\n0.4279 0 0 0 0 0 0 0\n0.5015 0.0735 0 0 0 0 0 0\n0.5610 0.1878 0 0 0.0930 0 0 0\n0.5622 0.1890 0 0.0036 0.0963 0 0 0\n0.5797 0.2456 0 0.1435 0.2003 0 0 0.0901\n0.5864 0.2572 -0.0321 0.1639 0.2082 0 0 0.1066\n0.6994 0.2910 -0.1337 0.2062 0.3003 -0.2565 0 0.2452\n0.7164 0.2926 -0.1425 0.2120 0.3096 -0.2890 -0.0209 0.2773\nBy changing Bfrom 0 to Bmax, we can go from a solution in which all the weights are zero\nto a solution in which all weights are non-zero. Unfortunately, not all subset sizes are achievableusing lasso. One can show that, if D>N, the optimal solution can have at most Nvariables in\nit, before reaching the complete set corresponding to the OLS solution of minimal /lscript\n1norm. In\nSection 13.5.3, we will see that by using an /lscript2regularizer as well as an /lscript1regularizer (a method\nknown as the elastic net), we can achieve sparse solutions which contain more variables thantraining cases. This lets us explore model sizes between NandD.", "469": "13.3./lscript1regularization: basics 439\n13.3.5 Model selection\nIt is tempting to use /lscript1regularization to estimate the set of relevant variables. In some cases,\nwe can recover the true sparsity pattern of w\u2217, the parameter vector that generated the data.\nA method that can recover the true model in the N\u2192\u221elimit is called model selection\nconsistent. The details on which methods enjoy this property, and when, are beyond the scope\nof this book; see e.g., (Buhlmann and van de Geer 2011) for details.\nInstead of going into a theoretical discussion, we will just show a small example. We \ufb01rst\ngenerate a sparse signal w\u2217of sizeD= 4096, consisting of 160 randomly placed \u00b11spikes.\nNext we generate a random design matrix Xof sizeN\u00d7D,w h e r eN= 1024. Finally we\ngenerate a noisy observation y=Xw\u2217+/epsilon1,w h e r e/epsilon1i\u223cN(0,0.012). We then estimate wfrom\nyandX.\nThe original w\u2217is shown in the \ufb01rst row of Figure 13.9. The second row is the /lscript1estimate\n\u02c6wL1using\u03bb=0.1\u03bbmax. We see that this has \u201cspikes\u201d in the right places, but they are too\nsmall. The third row is the least squares estimate of the coefficients which are estimated to benon-zero based on supp(\u02c6w\nL1). This is called debiasing, and is necessary because lasso shrinks\nthe relevant coefficients as well as the irrelevant ones. The last row is the least squares estimatefor all the coefficients jointly, ignoring sparsity. We see that the (debiased) sparse estimateis an excellent estimate of the original signal. By contrast, least squares without the sparsityassumption performs very poorly.\nOf course, to perform model selection, we have to pick \u03bb. It is common to use cross validation.\nHowever, it is important to note that cross validation is picking a value of \u03bbthat results in good\npredictive accuracy. This is not usually the same value as the one that is likely to recover the\u201ctrue\u201d model. To see why, recall that /lscript\n1regularization performs selection andshrinkage, that is,\nthe chosen coefficients are brought closer to 0. In order to prevent relevant coefficients frombeing shrunk in this way, cross validation will tend to pick a value of \u03bbthat is not too large. Of\ncourse, this will result in a less sparse model which contains irrelevant variables (false positives).Indeed, it was proved in (Meinshausen and Buhlmann 2006) that the prediction-optimal valueof\u03bbdoes not result in model selection consistency. In Section 13.6.2, we will discuss some\nadaptive mechanisms for automatically tuning \u03bbon a per-dimension basis that does result in\nmodel selection consistency.\nA downside of using /lscript\n1regularization to select variables is that it can give quite different\nresults if the data is perturbed slightly. The Bayesian approach, which estimates posteriormarginal inclusion probabilities, p(\u03b3\nj=1|D), is much more robust. A frequentist solution to\nthis is to use bootstrap resampling (see Section 6.2.1), and to rerun the estimator on differentversions of the data. By computing how often each variable is selected across different trials,we can approximate the posterior inclusion probabilities. This method is known as stability\nselection (Meinshausen and B\u00c3ijhlmann 2010).\nWe can threshold the stability selection (bootstrap) inclusion probabilities at some level, say\n90%, and thus derive a sparse estimator. This is known as bootstrap lasso orbolasso(Bach\n2008). It will include a variable if it occurs in at least 90% of sets returned by lasso (for a \ufb01xed\u03bb). This process of intersecting the sets is a way of eliminating the false positives that vanillalasso produces. The theoretical results in (Bach 2008) prove that bolasso is model selectionconsistent under a wider range of conditions than vanilla lasso.\nAs an illustration, we reproduced the experiments in (Bach 2008). In particular, we created", "470": "440 Chapter 13. Sparse linear models\nlasso on sign inconsistent data\n\u2212log( \u03bb)variable index\n0 5 10 152\n4\n6\n8\n10\n12\n14\n16\n00.10.20.30.40.50.60.70.80.91\n(a)\nbolasso on sign inconsistent data\n128 bootstraps\n\u2212log( \u03bb)variable index\n0 5 10 152\n4\n6\n8\n10\n12\n14\n16\n00.10.20.30.40.50.60.70.80.91\n(b)0 5 10 1500.51lasso vs bolasso on sign inconsistent data\nnbootstraps = [0,2,4,8,16,32,64,128,256]\n\u2212log( \u03bb)P(correct support)lasso\nbolasso\n(c)\nFigure 13.10 (a) Probability of selection of each variable (white = large probabilities, black = small proba-\nbilities) vs. regularization parameter for Lasso. As we move from left to right, we decrease the amount of\nregularization, and therefore select more variables. (b) Same as (a) but for bolasso. (c) Probability of correct\nsign estimation vs. regularization parameter. Bolasso (red, dashed) and Lasso (black, plain): The number\nof bootstrap replications is in {2,4,8,16,32,64,128,256}. Based on Figures 1-3 of (Bach 2008). Figure\ngenerated by bolassoDemo .\n256 datasets of size N= 1000withD=1 6variables, of which 8are relevant. See (Bach 2008)\nfor more detail on the experimental setup. For dataset n, variable j, and sparsity level k, de\ufb01ne\nS(j,k,n)=I(\u02c6wj(\u03bbk,Dn)/negationslash=0 ). Now de\ufb01ne P(j,k)be the average of S(j,k,n)over the 256\ndatasets. In Figure 13.10(a-b), we plot Pvs\u2212log(\u03bb)for lasso and bolasso. We see that for\nbolasso, there is a large range of \u03bbwhere the true variables are selected, but this is not the\ncase for lasso. This is emphasized in Figure 13.10(c), where we plot the empirical probability that\nthe correct set of variables is recovered, for lasso and for bolasso with an increasing number of\nbootstrap samples. Of course, using more samples takes longer. In practice, 32 bootstraps seems\nto be a good compromise between speed and accuracy.\nWith bolasso, there is the usual issue of picking \u03bb. Obviously we could use cross validation,\nbut plots such as Figure 13.10(b) suggest another heuristic: shuffle the rows to create a large\nblack block, and then pick \u03bbto be in the middle of this region. Of course, operationalizing this\nintuition may be tricky, and will require various ad-hoc thresholds (it is reminiscent of the \u201c\ufb01nd\nthe knee in the curve\u201d heuristic discussed in Section 11.5.2 when discussing how to pick Kfor\nmixture models). A Bayesian approach provides a more principled method for selecting \u03bb.\n13.3.6 Bayesian inference for linear models with Laplace priors\nWe have been focusing on MAP estimation in sparse linear models. It is also possible to perform\nBayesian inference (see e.g., (Park and Casella 2008; Seeger 2008)). However, the posterior mean\nand median, as well as samples from the posterior, are not sparse; only the mode is sparse. This\nis another example of the phenomenon discussed in Section 5.2.1, where we said that the MAP\nestimate is often untypical of the bulk of the posterior.\nAnother argument in favor of using the posterior mean comes from Equation 5.108, which\nshowed that that plugging in the posterior mean, rather than the posterior mode, is the optimal\nthing to do if we want to minimize squared prediction error. (Schniter et al. 2008) shows\nexperimentally, and (Elad and Yavnch 2009) shows theoretically, that using the posterior mean\nwith a spike-and-slab prior results in better prediction accuracy than using the posterior mode\nwith a Laplace prior, albeit at slightly higher computational cost.", "471": "13.4./lscript1regularization: algorithms 441\n13.4/lscript1regularization: algorithms\nIn this section, we give a brief review of some algorithms that can be used to solve /lscript1regularized\nestimation problems. We focus on the lasso case, where we have a quadratic loss. However,\nmost of the algorithms can be extended to more general settings, such as logistic regression (see(Yaun et al. 2010) for a comprehensive review of /lscript\n1regularized logistic regression). Note that this\narea of machine learning is advancing very rapidly, so the methods below may not be state ofthe art by the time you read this chapter. (See (Schmidt et al. 2009; Yaun et al. 2010; Yang et al.2010) for some recent surveys.)\n13.4.1 Coordinate descent\nSometimes it is hard to optimize all the variables simultaneously, but it easy to optimize themone by one. In particular, we can solve for the j\u2019th coefficient with all the others held \ufb01xed:\nw\n\u2217\nj=a r g m i n\nzf(w+zej)\u2212f(w) (13.65)\nwhereejis thej\u2019th unit vector. We can either cycle through the coordinates in a deterministic\nfashion, or we can sample them at random, or we can choose to update the coordinate forwhich the gradient is steepest.\nThe coordinate descent method is particularly appealing if each one-dimensional optimization\nproblem can be solved analytically For example, the shooting algorithm (Fu 1998; Wu and Lange\n2008) for lasso uses Equation 13.54 to compute the optimal value of w\njgiven all the other\ncoefficients. See Algorithm 7 for the pseudo code (and LassoShooting for some Matlab code).\nSee (Yaun et al. 2010) for some extensions of this method to the logistic regression case. The\nresulting algorithm was the fastest method in their experimental comparison, which concerneddocument classi\ufb01cation with large sparse feature vectors (representing bags of words). Othertypes of data (e.g., dense features and/or regression problems) might call for different algorithms.\nAlgorithm 13.1: Coordinate descent for lasso (aka shooting algorithm)\n1Initializew=(XTX+\u03bbI)\u22121XTy;\n2repeat\n3forj=1,...,Ddo\n4 aj=2/summationtextn\ni=1x2\nij;\n5 cj=2/summationtextn\ni=1xij(yi\u2212wTxi+wjxij);\n6 wj=soft(cj\naj,\u03bb\naj);\n7until converged ;\n13.4.2 LARS and other homotopy methods\nThe problem with coordinate descent is that it only updates one variable at a time, so can be\nslow to converge. Active set methods update many variables at a time. Unfortunately, they are", "472": "442 Chapter 13. Sparse linear models\nmore complicated, because of the need to identify which variables are constrained to be zero,\nand which are free to be updated.\nActive set methods typically only add or remove a few variables at a time, so they can take a\nlong if they are started far from the solution. But they are ideally suited for generating a set ofsolutions for different values of \u03bb, starting with the empty set, i.e., for generating regularization\npath. These algorithms exploit the fact that one can quickly compute \u02c6w(\u03bb\nk)from\u02c6w(\u03bbk\u22121)\nif\u03bbk\u2248\u03bbk\u22121; this is known as warm starting. In fact, even if we only want the solution for\na single value of \u03bb, call it \u03bb\u2217, it can sometimes be computationally more efficient to compute\na set of solutions, from \u03bbmaxdown to\u03bb\u2217, using warm-starting; this is called a continuation\nmethodorhomotopy method. This is often much faster than directly \u201ccold-starting\u201d at \u03bb\u2217; this\nis particularly true if \u03bb\u2217is small.\nPerhaps the most well-known example of a homotopy method in machine learning is the\nLARSalgorithm, which stands for \u201cleast angle regression and shrinkage\u201d (Efron et al. 2004) (a\nsimilar algorithm was independently invented in (Osborne et al. 2000b,a)). This can compute\n\u02c6w(\u03bb)for all possible values of \u03bbin an efficient manner.\nLARS works as follows. It starts with a large value of \u03bb, such that only the variable that is most\ncorrelated with the response vector yis chosen. Then \u03bbis decreased until a second variable\nis found which has the same correlation (in terms of magnitude) with the current residual asthe \ufb01rst variable, where the residual at step kis de\ufb01ned as r\nk=y\u2212X:,Fkwk,w h e r eFkis\nthe current active set (c.f., Equation 13.50). Remarkably, one can solve for this new value of\n\u03bbanalytically, by using a geometric argument (hence the term \u201cleast angle\u201d). This allows the\nalgorithm to quickly \u201cjump\u201d to the next point on the regularization path where the active setchanges. This repeats until all the variables are added.\nIt is necessary to allow variables to be removed from the active set if we want the sequence of\nsolutions to correspond to the regularization path of lasso. If we disallow variable removal, weget a slightly different algorithm called LAR, which tends to be faster. In particular, LAR costs\nthe same as a single ordinary least squares \ufb01t, namely O(NDmin(N,D )), which is O(ND\n2)\nifN>D, andO(N2D)ifD>N. LAR is very similar to greedy forward selection, and a\nmethod known as least squares boosting (see Section 16.4.6).\nThere have been many attempts to extend the LARS algorithm to compute the full regulariza-\ntion path for /lscript1regularized GLMs, such as logistic regression. In general, one cannot analytically\nsolve for the critical values of \u03bb. Instead, the standard approach is to start at \u03bbmax, and then\nslowly decrease \u03bb, tracking the solution as we go; this is called a continuation method or\nhomotopy method. These methods exploit the fact that we can quickly compute \u02c6w(\u03bbk)from\n\u02c6w(\u03bbk\u22121)if\u03bbk\u2248\u03bbk\u22121; this is known as warm starting. Even if we don\u2019t want the full path,\nthis method is often much faster than directly \u201ccold-starting\u201d at the desired value of \u03bb(this is\nparticularly true if \u03bbis small).\nThe method described in (Friedman et al. 2010) combines coordinate descent with this warm-\nstarting strategy, and computes the full regularization path for any /lscript1regularized GLM. This has\nbeen implemented in the glmnet package, which is bundled with PMTK.\n13.4.3 Proximal and gradient projection methods\nIn this section, we consider some methods that are suitable for very large scale problems, wherehomotopy methods made be too slow. These methods will also be easy to extend to other kinds", "473": "13.4./lscript1regularization: algorithms 443\nof regularizers, beyond /lscript1, as we will see later. Our presentation in this section is based on\n(Vandenberghe 2011; Yang et al. 2010).\nConsider a convex objective of the form\nf(\u03b8)=L(\u03b8)+R(\u03b8) (13.66)\nwhereL(\u03b8)(representing the loss) is convex and differentiable, and R(\u03b8)(representing the\nregularizer) is convex but not necessarily differentiable. For example, L(\u03b8) = RSS(\u03b8 )and\nR(\u03b8)=\u03bb||\u03b8||1corresponds to the BPDN problem. As another example, the lasso problem can\nbe formulated as follows: L(\u03b8) = RSS( \u03b8)andR(\u03b8)=IC(\u03b8),w h e r eC={\u03b8:||\u03b8||1\u2264B},\nandIC(\u03b8)is the indicator function of a convex set C, de\ufb01ned as\nIC(\u03b8)/defines/braceleftbigg0\u03b8\u2208C\n+\u221eotherwise(13.67)\nIn some cases, it is easy to optimize functions of the form in Equation 13.66. For example,\nsupposeL(\u03b8) = RSS(\u03b8 ), and the design matrix is simply X=I. Then the obective becomes\nf(\u03b8)=R(\u03b8)+1\n2||\u03b8\u2212y|2\n2. The minimizer of this is given by proxR(y), which is the proximal\noperator for the convex function R, de\ufb01ned by\nproxR(y) = argmin\nz/parenleftbigg\nR(z)+1\n2||z\u2212y||2\n2/parenrightbigg\n(13.68)\nIntuitively, we are returning a point that minimizes Rbut which is also close (proximal) to y.\nIn general, we will use this operator inside an iterative optimizer, in which case we want to stay\nclose to the previous iterate. In this case, we use\nproxR(\u03b8k) = argmin\nz/parenleftbigg\nR(z)+1\n2||z\u2212\u03b8k||2\n2/parenrightbigg\n(13.69)\nThe key issues are: how do we efficiently compute the proximal operator for different regu-\nlarizersR, and how do we extend this technique to more general loss functions L? We discuss\nthese issues below.\n13.4.3.1 Proximal operators\nIfR(\u03b8)=\u03bb||\u03b8||1, the proximal operator is given by componentwise soft-thresholding:\nproxR(\u03b8)=s o f t (\u03b8,\u03bb) (13.70)\nas we showed in Section 13.3.2. If R(\u03b8)=\u03bb||\u03b8||0, the proximal operator is given by componen-\ntwise hard-thresholding:\nproxR(\u03b8)=h a r d ( \u03b8,\u221a\n2\u03bb) (13.71)\nwherehard(u,a) /definesuI(|u|>a).\nIfR(\u03b8)=IC(\u03b8), the proximal operator is given by the projection onto the set C:\nproxR(\u03b8) = argmin\nz\u2208C||z\u2212\u03b8||22=p r ojC(\u03b8) (13.72)", "474": "444 Chapter 13. Sparse linear models\nFigure 13.11 Illustration of projected gradient descent. The step along the negative gradient, to \u03b8k\u2212gk,\ntakes us outside the feasible set. If we project that point onto the closest point in the set we get\n\u03b8k+1= proj\u0398(\u03b8k\u2212gk). We can then derive the implicit update direction using dk=\u03b8k+1\u2212\u03b8k. Used\nwith kind permission of Mark Schmidt.\nFor some convex sets, it is easy to compute the projection operator. For example, to project\nonto the rectangular set de\ufb01ned by the box constraints C={\u03b8:/lscriptj\u2264\u03b8j\u2264uj}we can use\nprojC(\u03b8)j=\u23a7\n\u23a8\n\u23a9/lscriptj\u03b8j\u2264/lscriptj\n\u03b8j/lscriptj\u2264\u03b8j\u2264uj\nuj\u03b8j\u2265uj(13.73)\nTo project onto the Euclidean ball C={\u03b8:||\u03b8||2\u22641}we can use\nprojC(\u03b8)=/braceleftbigg\u03b8\n||\u03b8||2||\u03b8||2>1\n\u03b8||\u03b8||2\u22641(13.74)\nTo project onto the 1-norm ball C={\u03b8:||\u03b8||1\u22641}we can use\nprojC(\u03b8)=s o f t (\u03b8,\u03bb) (13.75)\nwhere\u03bb=0if||\u03b8||1\u22641, and otherwise \u03bbis the solution to the equation\nD/summationdisplay\nj=1max(|\u03b8j|\u2212\u03bb,0) = 1 (13.76)\nWe can implement the whole procedure in O(D)time, as explained in (Duchi et al. 2008).\nWe will see an application of these different projection methods in Section 13.5.1.2.\n13.4.3.2 Proximal gradient method\nWe now discuss how to use the proximal operator inside of a gradient descent routine. The\nbasic idea is to minimize a simple quadratic approximation to the loss function, centered on the", "475": "13.4./lscript1regularization: algorithms 445\n\u03b8k:\n\u03b8k+1=a r g m i n\nzR(z)+L(\u03b8k)+gT\nk(z\u2212\u03b8k)+1\n2tk||z\u2212\u03b8k||2\n2 (13.77)\nwheregk=\u2207L(\u03b8k)is the gradient of the loss, tkis a constant discussed below, and the last\nterm arises from a simple approximation to the Hessian of the loss of the form \u22072L(\u03b8k)\u22481\ntkI.\nDropping terms that are independent of z, and multiplying by tk, we can rewrite the above\nexpression in terms of a proximal operator as follows:\n\u03b8k+1=a r g m i n\nz/bracketleftbigg\ntkR(z)+1\n2||z\u2212uk||22/bracketrightbigg\n=p r o xtkR(uk) (13.78)\nuk=\u03b8k\u2212tkgk (13.79)\ngk=\u2207L(\u03b8k) (13.80)\nIfR(\u03b8)=0, this is equivalent to gradient descent. If R(\u03b8)=IC(\u03b8), the method is equivalent\ntoprojected gradient descent, sketched in Figure 13.11. If R(\u03b8)=\u03bb||\u03b8 ||1, the method is\nknown as iterative soft thresholding.\nThere are several ways to pick tk, or equivalently, \u03b1k=1/tk. Given that \u03b1kIis an approxi-\nmation to the Hessian \u22072L, we require that\n\u03b1k(\u03b8k\u2212\u03b8k\u22121)\u2248gk\u2212gk\u22121 (13.81)\nin the least squares sense. Hence\n\u03b1k=a r g m i n\n\u03b1||\u03b1(\u03b8k\u2212\u03b8k\u22121)\u2212(gk\u2212gk\u22121)||22=(\u03b8k\u2212\u03b8k\u22121)T(gk\u2212gk\u22121)\n(\u03b8k\u2212\u03b8k\u22121)T(\u03b8k\u2212\u03b8k\u22121)(13.82)\nThis is known as the Barzilai-Borwein (BB) orspectral stepsize (Barzilai and Borwein 1988;\nFletcher 2005; Raydan 1997). This stepsize can be used with any gradient method, whether\nproximal or not. It does not lead to monotonic decrease of the objective, but it is much fasterthan standard line search techniques. (To ensure convergence, we require that the objectivedecrease \u201con average\u201d, where the average is computed over a sliding window of size M+1.)\nWhen we combine the BB stepsize with the iterative soft thresholding technique (for R(\u03b8)=\n\u03bb||\u03b8||\n1), plus a continuation method that gradually reduces \u03bb, we get a fast method for the\nBPDN problem known as the SpaRSA algorithm, which stands for \u201csparse reconstruction byseparable approximation\u201d (Wright et al. 2009). However, we will call it the iterative shrinkage andthresholding algorithm. See Algorithm 12 for some pseudocode, and SpaRSAfor some Matlab\ncode. See also Exercise 13.11 for a related approach based on projected gradient descent.\n13.4.3.3 Nesterov\u2019s method\nA faster version of proximal gradient descent can be obtained by epxanding the quadraticapproximationaroundapointotherthanthemostrecentparametervalue. Inparticular, considerperforming updates of the form\n\u03b8\nk+1=p r o xtkR(\u03c6k\u2212tkgk) (13.83)\ngk=\u2207L(\u03c6k) (13.84)\n\u03c6k=\u03b8k+k\u22121\nk+2(\u03b8k\u2212\u03b8k\u22121) (13.85)", "476": "446 Chapter 13. Sparse linear models\nAlgorithm 13.2: Iterative Shrinkage-Thresholding Algorithm (ISTA)\n1Input:X\u2208RN\u00d7D,y\u2208RN, parameters \u03bb\u22650,M\u22651,0<s<1;\n2Initialize\u03b80=0,\u03b1=1,r=y,\u03bb0=\u221e;\n3repeat\n4\u03bbt= max(s ||XTr||\u221e,\u03bb)// Adapt the regularizer ;\n5repeat\n6 g=\u2207L(\u03b8);\n7 u=\u03b8\u22121\n\u03b1g;\n8 \u03b8=s o f t (u,\u03bbt\n\u03b1);\n9 Update\u03b1using BB stepsize in Equation 13.82 ;\n10untilf(\u03b8)increased too much within the past Msteps;\n11r=y\u2212X\u03b8// Update residual ;\n12until\u03bbt=\u03bb;\n\u03c32\nNyi\nxiDwj\u03c4j\u03b3\nFigure 13.12 Representing lasso using a Gaussian scale mixture prior.\nThis is known as Nesterov\u2019s method (Nesterov 2004; Tseng 2008). As before, there are a variety\nof ways of setting tk; typically one uses line search.\nWhen this method is combined with the iterative soft thresholding technique (for R(\u03b8)=\n\u03bb||\u03b8||1), plus a continuation method that gradually reduces \u03bb, we get a fast method for the\nBPDN problem known as the fast iterative shrinkage thesholding algorithm orFISTA(Beck\nand Teboulle 2009).", "477": "13.4./lscript1regularization: algorithms 447\n13.4.4 EM for lasso\nIn this section, we show how to solve the lasso problem using lasso. At \ufb01rst sight, this might\nseem odd, since there are no hidden variables. The key insight is that we can represent theLaplace distribution as a Gaussian scale mixture (GSM) (Andrews and Mallows 1974; West 1987)\nas follows:\nLap(w\nj|0,1/\u03b3)=\u03b3\n2e\u2212\u03b3|wj|=/integraldisplay\nN(wj|0,\u03c42\nj)Ga(\u03c42\nj|1,\u03b32\n2)d\u03c42\nj (13.86)\nThus the Laplace is a GSM where the mixing distibution on the variances is the exponential\ndistribution, Expon(\u03c42\nj|\u03b32\n2=G a (\u03c42\nj|1,\u03b32\n2). Using this decomposition, we can represent the\nlasso model as shown in Figure 13.12. The corresponding joint distribution has the form\np(y,w,\u03c4,\u03c32|X)=N (y|Xw,\u03c32IN)N(w|0,D\u03c4)\nIG(\u03c32|a\u03c3,b\u03c3)\u23a1\n\u23a3/productdisplay\njGa(\u03c42\nj|1,\u03b32/2)\u23a4\u23a6 (13.87)\nwhereD\n\u03c4=d i a g (\u03c42\nj), and where we have assumed for notational simplicity that Xis stan-\ndardized and that yis centered (so we can ignore the offset term \u03bc). Expanding out, we\nget\np(y,w,\u03c4,\u03c32|X)\u221d/parenleftbig\n\u03c32/parenrightbig\u2212N/2exp/parenleftbigg\n\u22121\n2\u03c32||y\u2212Xw||2\n2/parenrightbigg\n|D\u03c4|\u22121\n2\nexp/parenleftbigg\n\u22121\n2wTD\u03c4w/parenrightbigg\n(\u03c32)\u2212(a\u03c3+1)\nexp(\u2212b\u03c3/\u03c32)/productdisplay\njexp(\u2212\u03b32\n2\u03c42\nj) (13.88)\nBelow we describe how to apply the EM algorithm to the model in Figure 13.12.5In brief, in\nthe E step we infer \u03c42\njand\u03c32, and in the M step we estimate w. The resulting estimate \u02c6wis\nthe same as the lasso estimator. This approach was \ufb01rst proposed in (Figueiredo 2003) (see also\n(Griffin and Brown 2007; Caron and Doucet 2008; Ding and Harrison 2010) for some extensions).\n13.4.4.1 Why EM?\nBefore going into the details of EM, it is worthwhile asking why we are presenting this approachat all, given that there are a variety of other (often much faster) algorithms that directly solve the/lscript\n1MAP estimation problem (see linregFitL1Test for an empirical comparison). The reason\nis that the latent variable perspective brings several advantages, such as the following:\n\u2022 It provides an easy way to derive an algorithm to \ufb01nd /lscript1-regularized parameter estimates for\na variety of other models, such as robust linear regression (Exercise 11.12) or probit regression\n(Exercise 13.9).\n5. To ensure the posterior is unimodal, one can follow (Park and Casella 2008) and slightly modify the model by\nmaking the prior variance for the weights depend on the observation noise: p(wj|\u03c42\nj,\u03c32)=N(wj|0,\u03c32\u03c42\nj). The EM\nalgorithm is easy to modify.", "478": "448 Chapter 13. Sparse linear models\n\u2022 It suggests trying other priors on the variances besides Ga(\u03c42\nj|1,\u03b32/2). We will consider\nvarious extensions below.\n\u2022 It makes it clear how we can compute the full posterior, p(w|D), rather than just a MAP\nestimate. This technique is known as the Bayesian lasso (Park and Casella 2008; Hans 2009).\n13.4.4.2 The objective function\nFrom Equation 13.88, the complete data penalized log likelihood is as follows (dropping terms\nthat do not depend on w)\n/lscriptc(w)=\u22121\n2\u03c32||y\u2212Xw||2\n2\u22121\n2wT\u039bw+const (13.89)\nwhere\u039b=d i a g (1\n\u03c42\nj)is the precision matrix for w.\n13.4.4.3 The E step\nThe key is to compute E/bracketleftBig\n1\n\u03c42\nj|wj/bracketrightBig\n. We can derive this directly (see Exercise 13.8). Alternatively,\nwe can derive the full posterior, which is given by the following (Park and Casella 2008):\np(1/\u03c42\nj|w,D)=InverseGaussian/parenleftBigg/radicalBigg\n\u03b32\nw2\nj,\u03b32/parenrightBigg\n(13.90)\n(Note that the inverse Gaussian distribution is also known as the Wald distribution.) Hence\nE/bracketleftBigg\n1\n\u03c42\nj|wj/bracketrightBigg\n=\u03b3\n|wj|(13.91)\nLet\u039b=d i a g ( E/bracketleftbig\n1/\u03c42\n1/bracketrightbig\n,...,E/bracketleftbig\n1/\u03c42\nD/bracketrightbig\n)denote the result of this E step.\nWe also need to infer \u03c32. It is easy to show that that the posterior is\np(\u03c32|D,w)=I G (a\u03c3+(N)/2,b\u03c3+1\n2(y\u2212X\u02c6w)T(y\u2212X\u02c6w)) = IG(a N,bN)(13.92)\nHence\nE/bracketleftbig\n1/\u03c32/bracketrightbig\n=aN\nbN/defines\u03c9 (13.93)\n13.4.4.4 The M step\nThe M step consists of computing\n\u02c6w=a r g m a x\nw\u22121\n2\u03c9||y\u2212Xw||22\u22121\n2wT\u039bw (13.94)\nThis is just MAP estimation under a Gaussian prior:\n\u02c6w=(\u03c32\u039b+XTX)\u22121XTy (13.95)", "479": "13.5./lscript1regularization: extensions 449\nHowever, since we expect many wj=0, we will have \u03c42\nj=0for many j, making inverting \u039b\nnumerically unstable. Fortunately, we can use the SVD of X, given by X=UDVT, as follows:\n\u02c6w=\u03a8V(VT\u03a8V+1\n\u03c9D\u22122)\u22121D\u22121UTy (13.96)\nwhere\n\u03a8=\u039b\u22121=d i a g (1\nE/bracketleftbig\n1/\u03c42\nj/bracketrightbig) = diag(|wj|\n\u03c0/prime(wj)) (13.97)\n13.4.4.5 Caveat\nSince the lasso objective is convex, this method should always \ufb01nd the global optimum. Unfor-\ntunately, this sometimes does not happen, for numerical reasons. In particular, suppose that inthe true solution, w\n\u2217\nj/negationslash=0. Further, suppose that we set \u02c6wj=0in an M step. In the following E\nstep we infer that \u03c42\nj=0, so then we set \u02c6wj=0again; thus we can never \u201cundo\u201d our mistake.\nFortunately, in practice, this situation seems to be rare. See (Hunter and Li 2005) for furtherdiscussion.\n13.5/lscript1regularization: extensions\nIn this section, we discuss various extensions of \u201cvanilla\u201d /lscript1regularization.\n13.5.1 Group Lasso\nIn standard /lscript1regularization, we assume that there is a 1:1 correspondence between parameters\nand variables, so that if \u02c6wj=0, we interpret this to mean that variable jis excluded. But\nin more complex models, there may be many parameters associated with a given variable. Inparticular, we may have a vector of weights for each input, w\nj. Here are some examples:\n\u2022Multinomial logistic regression Each feature is associated with Cdifferent weights, one\nper class.\n\u2022Linear regression with categorical inputs Each scalar input is one-hot encoded into a\nvector of length C.\n\u2022Multi-task learning In multi-task learning, we have multiple related prediction problems.\nFor example, we might have Cseparate regression or binary classi\ufb01cation problems. Thus\neach feature is associated with Cdifferent weights. We may want to use a feature for all of\nthe tasks or none of the tasks, and thus select weights at the group level (Obozinski et al.2007).\nIf we use an /lscript\n1regularizer of the form ||w||=/summationtext\nj/summationtext\nc|wjc|, we may end up with with some\nelements of wj,:being zero and some not. To prevent this kind of situation, we partition the\nparameter vector into Ggroups. We now minimize the following objective\nJ(w) = NLL( w)+G/summationdisplay\ng=1\u03bbg||wg||2 (13.98)", "480": "450 Chapter 13. Sparse linear models\nwhere\n||wg||2=/radicalBigg/summationdisplay\nj\u2208gw2\nj (13.99)\nis the 2-norm of the group weight vector. If the NLL is least squares, this method is called\ngroup lasso (Yuan and Lin 2006).\nWe often use a larger penalty for larger groups, by setting \u03bbg=\u03bb/radicalbig\ndg,w h e r edgis the\nnumber of elements in group g. For example, if we have groups {1,2}and{3,4,5}, the\nobjective becomes\nJ(w) = NLL( w)+\u03bb/bracketleftbigg\u221a\n2/radicalBig\n(w2\n1+w2\n2|)+\u221a\n3/radicalBig\n(w2\n3+w2\n4+w2\n5)/bracketrightbigg\n(13.100)\nNote that if we had used the square of the 2-norms, the model would become equivalent toridge regression, since\nG/summationdisplay\ng=1||wg||2\n2=/summationdisplay\ng/summationdisplay\nj\u2208gw2\nj=||w||22(13.101)\nBy using the square root, we are penalizing the radius of a ball containing the group\u2019s weight\nvector: the only way for the radius to be small is if all elements are small. Thus the square rootresults in group sparsity.\nA variant of this technique replaces the 2-norm with the in\ufb01nity-norm (Turlach et al. 2005;\nZhao et al. 2005):\n||w\ng||\u221e=m a x\nj\u2208g|wj| (13.102)\nIt is clear that this will also result in group sparsity.\nAn illustration of the difference is shown in Figures 13.13 and 13.14. In both cases, we have a\ntrue signal wof sizeD=212= 4096, divided into 64 groups each of size 64. We randomly\nchoose 8 groups of wand assign them non-zero values. In the \ufb01rst example, the values are\ndrawn from a N(0,1). In the second example, the values are all set to 1. We then pick a random\ndesign matrix Xof sizeN\u00d7D,w h e r eN=210= 1024. Finally, we generate y=Xw+/epsilon1,\nwhere/epsilon1\u223cN(0,10\u22124IN). Given this data, we estimate the support of wusing/lscript1or group /lscript1,\nand then estimate the non-zero values using least squares. We see that group lasso does a muchbetter job than vanilla lasso, since it respects the known group structure.\n6We also see that the\n/lscript\u221enorm has a tendency to make all the elements within a block to have similar magnitude.\nThis is appropriate in the second example, but not the \ufb01rst. (The value of \u03bbwas the same in all\nexamples, and was chosen by hand.)\n13.5.1.1 GSM interpretation of group lasso\nGroup lasso is equivalent to MAP estimation using the following prior\np(w|\u03b3,\u03c32)\u221dexp/parenleftBigg\n\u2212\u03b3\n\u03c3G/summationdisplay\ng=1||wg||2/parenrightBigg\n(13.103)\n6. The slight non-zero \u201cnoise\u201d in the /lscript\u221egroup lasso results is presumably due to numerical errors.", "481": "13.5./lscript1regularization: extensions 451\n0 500 1000 1500 2000 2500 3000 3500 4000\u2212202Original (D = 4096, number groups = 64, active groups = 8)\n0 500 1000 1500 2000 2500 3000 3500 4000\u2212202Standard L1 (debiased 1, tau = 0.385, MSE = 0.06929)\n(a)\n0 500 1000 1500 2000 2500 3000 3500 4000\u2212202Block\u2212L2 (debiased 1, tau = 0.385, MSE = 0.000351)\n0 500 1000 1500 2000 2500 3000 3500 4000\u2212202Block\u2212Linf (debiased 1, tau = 0.385, MSE = 0.053)\n(b)\nFigure 13.13 Illustration of group lasso where the original signal is piecewise Gaussian. Top left: original\nsignal. Bottom left:: vanilla lasso estimate. Top right: group lasso estimate using a /lscript2norm on the blocks.\nBottom right: group lasso estimate using an /lscript\u221enorm on the blocks. Based on Figures 3-4 of (Wright et al.\n2009). Figure generated by groupLassoDemo , based on code by Mario Figueiredo.\nNow one can show (Exercise 13.10) that this prior can be written as a GSM, as follows:\nwg|\u03c32,\u03c42\ng\u223cN(0,\u03c32\u03c42\ngIdg) (13.104)\n\u03c42\ng|\u03b3\u223cGa(dg+1\n2,\u03b3\n2) (13.105)\nwheredgis the size of group g. So we see that there is one variance term per group, each\nof which comes from a Gamma prior, whose shape parameter depends on the group size, and\nwhose rate parameter is controlled by \u03b3. Figure 13.15 gives an example, where we have 2 groups,\none of size 2 and one of size 3.\nThis picture also makes it clearer why there should be a grouping effect. Suppose w1,1is\nsmall; then \u03c42\n1will be estimated to be small, which will force w1,2to be small. Converseley,\nsupposew1,1is large; then \u03c42\n1will be estimated to be large, which will allow w1,2to be become\nlarge as well.", "482": "452 Chapter 13. Sparse linear models\n0 500 1000 1500 2000 2500 3000 3500 400000.51Original (D = 4096, number groups = 64, active groups = 8)\n0 500 1000 1500 2000 2500 3000 3500 400000.51Standard L1 (debiased 1, tau = 0.356, MSE = 0.1206)\n(a)\n0 500 1000 1500 2000 2500 3000 3500 400000.51Block\u2212L2 (debiased 1, tau = 0.356, MSE = 0.000342)\n0 500 1000 1500 2000 2500 3000 3500 400000.51Block\u2212Linf (debiased 1, tau = 0.356, MSE = 0.000425)\n(b)\nFigure 13.14 Same as Figure 13.13, except the original signal is piecewise constant.\n\u03c32yi\nxiw11w12 w21w22w23\u03c41 \u03c42\u03b3\nFigure 13.15 Graphical model for group lasso with 2 groups, the \ufb01rst has size G1=2, the second has\nsizeG2=3.", "483": "13.5./lscript1regularization: extensions 453\n13.5.1.2 Algorithms for group lasso\nThere are a variety of algorithms for group lasso. Here we brie\ufb02y mention two. The \ufb01rst\napproach is based on proximal gradient descent, discussed in Section 13.4.3. Since the regularizeris separable, R(w)=/summationtext\ng||wg||p, the proximal operator decomposes into Gseparate operators\nof the form\nproxR(b) = argmin\nz\u2208RDg||z\u2212b||2\n2+\u03bb||z||p (13.106)\nwhereb=\u03b8kg\u2212tkgkg.I fp=2, one can show (Combettes and Wajs 2005) that this can be\nimplemented as follows\nproxR(b)=b\u2212proj\u03bbC(b) (13.107)\nwhereC={z:||z||2\u22641}is the/lscript2ball. Using Equation 13.74, if ||b||2<\u03bb,w eh a v e\nproxR(b)=b\u2212b=0 (13.108)\notherwise we have\nproxR(b)=b\u2212\u03bbb\n||b||2=b||b||2\u2212\u03bb\n||b||2(13.109)\nWe can combine these into a vectorial soft-threshold function as follows (Wright et al. 2009):\nproxR(b)=bmax(||b||2\u2212\u03bb,0)\nmax(||b||2\u2212\u03bb,0)+\u03bb(13.110)\nIfp=\u221e, we use C={z:||z||1\u22641}, which is the /lscript1ball. We can project onto this in O(dg)\ntime using an algorithm described in (Duchi et al. 2008).\nAnother approach is to modify the EM algorithm. The method is almost the same as for\nvanilla lasso. If we de\ufb01ne \u03c42\nj=\u03c42\ng(j),w h e r eg(j)is the group to which dimension jbelongs,\nwe can use the same full conditionals for \u03c32andwas before. The only changes are as follows:\n\u2022 We must modify the full conditional for the weight precisions, which are estimated based on\na shared set of weights:\n1\n\u03c42g|\u03b3,w,\u03c32,y,X\u223cInverseGaussian(/radicalBigg\n\u03b32\u03c32\n||wg||2\n2,\u03b32) (13.111)\nwhere||wg||22=/summationtext\nj\u2208gw2\njg. For the E step, we can use\nE/bracketleftbigg1\n\u03c42g/bracketrightbigg\n=\u03b3\u03c3\n||wg||2(13.112)\n\u2022 We must modify the full conditional for the tuning parameter, which is now only estimated\nbased on Gvalues of \u03c42\ng:\np(\u03b32|\u03c4)=G a (a\u03b3+G/2,b \u03b3+1\n2G/summationdisplay\ng\u03c42\ng) (13.113)", "484": "454 Chapter 13. Sparse linear models\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf \u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n0 100 200 300 400\u22121.5 \u22121.0 \u22120.5 0.0 0.5\nIndexCGH\n(a) (b) (c)\nFigure 13.16 (a) Example of the fused lasso. The vertical axis represents array CGH (chromosomal genome\nhybridization) intensity, and the horizontal axis represents location along a genome. Source: Figure 1 of\n(Hoe\ufb02ing 2010). (b) Noisy image. (c) Fused lasso estimate using 2d lattice prior. Source: Figure 2 of(Hoe\ufb02ing 2010). Used with kind permission of Holger Hoe\ufb02ing.\n13.5.2 Fused lasso\nIn some problem settings (e.g., functional data analysis), we want neighboring coefficients to be\nsimilar to each other, in addition to being sparse. An example is given in Figure 13.16(a), wherewe want to \ufb01t a signal that is mostly \u201coff\u201d, but in addition has the property that neighboringlocations are typically similar in value. We can model this by using a prior of the form\np(w|\u03c3\n2)\u221dexp\u239b\n\u239d\u2212\u03bb1\n\u03c3D/summationdisplay\nj=1|wj|\u2212\u03bb2\n\u03c3D\u22121/summationdisplay\nj=1|wj+1\u2212wj|\u239e\u23a0 (13.114)\nThis is known as the fused lasso penalty. In the context of functional data analysis, we often\nuseX=I, so there is one coefficient for each location in the signal (see Section 4.4.2.3). In this\ncase, the overall objective has the form\nJ(w,\u03bb\n1,\u03bb2)=N/summationdisplay\ni=1(yi\u2212wi)2+\u03bb1N/summationdisplay\ni=1|wi|+\u03bb2N\u22121/summationdisplay\ni=1|wi+1\u2212wi| (13.115)\nThis is a sparse version of Equation 4.148.\nIt is possible to generalize this idea beyond chains, and to consider other graph structures,\nusing a penalty of the form\nJ(w,\u03bb1,\u03bb2)=/summationdisplay\ns\u2208V(ys\u2212ws)2+\u03bb1/summationdisplay\ns\u2208V|ws|+\u03bb2/summationdisplay\n(s,t)\u2208E|ws\u2212wt| (13.116)\nThis is called graph-guided fused lasso (see e.g., (Chen et al. 2010)). The graph might come\nfrom some prior knowledge, e.g., from a database of known biological pathways. Another\nexample is shown in Figure 13.16(b-c), where the graph structure is a 2d lattice.", "485": "13.5./lscript1regularization: extensions 455\n13.5.2.1 GSM interpretation of fused lasso\nOne can show (Kyung et al. 2010) that the fused lasso model is equivalent to the following\nhierarchical model\nw|\u03c32,\u03c4,\u03c9\u223cN(0,\u03c32\u03a3(\u03c4,\u03c9)) (13.117)\n\u03c42\nj|\u03b31\u223cExpon(\u03b32\n1\n2),j=1:D (13.118)\n\u03c92\nj|\u03b32\u223cExpon(\u03b32\n2\n2),j=1:D\u22121 (13.119)\nwhere\u03a3=\u03a9\u22121, and\u03a9is a tridiagonal precision matrix with\nmain diagonal ={1\n\u03c42\nj+1\n\u03c92\nj\u22121+1\n\u03c92\nj} (13.120)\noff diagonal ={\u22121\n\u03c92\nj} (13.121)\nwhere we have de\ufb01ned \u03c9\u22122\n0=\u03c9\u22122\nD=0. This is very similar to the model in Section 4.4.2.3,\nwhere we used a chain-structured Gaussian Markov random \ufb01eld as the prior, with \ufb01xed vari-ance. Here we just let the variance be random. In the case of graph-guided lasso, the structureof the graph is re\ufb02ected in the zero pattern of the Gaussian precision matrix (see Section 19.4.4).\n13.5.2.2 Algorithms for fused lasso\nIt is possible to generalize the EM algorithm to \ufb01t the fused lasso model, by exploiting theMarkov structure of the Gaussian prior for efficiency. Direct solvers (which don\u2019t use the latentvariable trick) can also be derived (see e.g., (Hoe\ufb02ing 2010)). However, this model is undeniablymore expensive to \ufb01t than the other variants we have considered.\n13.5.3 Elastic net (ridge and lasso combined)\nAlthough lasso has proved to be effective as a variable selection technique, it has severalproblems (Zou and Hastie 2005), such as the following:\n\u2022 If there is a group of variables that are highly correlated (e.g., genes that are in the same\npathway), then the lasso tends to select only one of them, chosen rather arbitrarily. (This\nis evident from the LARS algorithm: once one member of the group has been chosen, theremaining members of the group will not be very correlated with the new residual and hencewill not be chosen.) It is usually better to select all the relevant variables in a group. If weknow the grouping structure, we can use group lasso, but often we don\u2019t know the groupingstructure.\n\u2022 In the D>Ncase, lasso can select at most Nvariables before it saturates.\n\u2022I fN>D, but the variables are correlated, it has been empirically observed that the\nprediction performance of ridge is better than that of lasso.", "486": "456 Chapter 13. Sparse linear models\nZou and Hastie (Zou and Hastie 2005) proposed an approach called the elastic net, which is\na hybrid between lasso and ridge regression, which solves all of these problems. It is apparently\ncalled the \u201celastic net\u201d because it is \u201clike a stretchable \ufb01shing net that retains \u2019all the big \ufb01sh\u201d\u2019(Zou and Hastie 2005).\n13.5.3.1 Vanilla version\nThe vanilla version of the model de\ufb01nes the following objective function:\nJ(w,\u03bb\n1,\u03bb2)=||y\u2212Xw||2+\u03bb2||w||2\n2+\u03bb1||w||1 (13.122)\nNotice that this penalty function is strictly convex (assuming \u03bb2>0) so there is a unique global\nminimum, even if Xis not full rank.\nIt can be shown (Zou and Hastie 2005) that any strictly convex penalty on wwill exhibit\nagrouping effect, which means that the regression coefficients of highly correlated variables\ntend to be equal (up to a change of sign if they are negatively correlated). For example, if two\nfeatures are equal, so X:j=X:k, one can show that their estimates are also equal, \u02c6wj=\u02c6wk.\nBy contrast, with lasso, we may have that \u02c6wj=0and\u02c6wk/negationslash=0or vice versa.\n13.5.3.2 Algorithms for vanilla elastic net\nIt is simple to show (Exercise 13.5) that the elastic net problem can be reduced to a lasso problemon modi\ufb01ed data. In particular, de\ufb01ne\n\u02dcX=c/parenleftbiggX\u221a\n\u03bb2ID/parenrightbigg\n,\u02dcy=/parenleftbiggy\n0D\u00d71/parenrightbigg\n(13.123)\nwherec=( 1+\u03bb2)\u22121\n2. Then we solve\n\u02dcw=a r gm i n\n\u02dcw||\u02dcy\u2212\u02dcX\u02dcw||2+c\u03bb1||\u02dcw||1 (13.124)\nand setw=c\u02dcw.\nWe can use LARS to solve this subproblem; this is known as the LARS-EN algorithm. If we\nstop the algorithm after mvariables have been included, the cost is O(m3+Dm2). Note that\nwe can use m=Dif we wish, since \u02dcXhas rank D. This is in contrast to lasso, which cannot\nselect more than Nvariables (before jumping to the OLS solution) if N<D.\nWhen using LARS-EN (or other /lscript1solvers), one typically uses cross-validation to select \u03bb1and\n\u03bb2.\n13.5.3.3 Improved version\nUnfortunately it turns out that the \u201cvanilla\u201d elastic net does not produce functions that predictvery accurately, unless it is very close to either pure ridge or pure lasso. Intuitively the reasonis that it performs shrinkage twice: once due to the /lscript\n2penalty and again due to the /lscript1penalty.\nThe solution is simple: undo the /lscript2shrinkage by scaling up the estimates from the vanilla\nversion. In other words, if w\u2217is the solution of Equation 13.124, then a better estimate is\n\u02c6w=/radicalbig\n1+\u03bb2\u02dcw (13.125)", "487": "13.6. Non-convex regularizers 457\nWe will call this a corrected estimate.\nOne can show that the corrected estimates are given by\n\u02c6w=a r gm i n\nwwT/parenleftbiggXTX+\u03bb2I\n1+\u03bb2/parenrightbigg\nw\u22122yTXw+\u03bb1||w||1 (13.126)\nNow\nXTX+\u03bb2I\n1+\u03bb2=( 1\u2212\u03c1)\u02c6\u03a3+\u03c1I (13.127)\nwhere\u03c1=\u03bb2/(1 +\u03bb2). So the the elastic net is like lasso but where we use a version of\n\u02c6\u03a3that is shrunk towards I. (See Section 4.2.6 for more discussion of regularized estimates of\ncovariance matrices.)\n13.5.3.4 GSM interpretation of elastic net\nThe implicit prior being used by the elastic net obviously has the form\np(w|\u03c32)\u221dexp\u239b\n\u239d\u2212\u03b31\n\u03c3D/summationdisplay\nj=1|wj|\u2212\u03b32\n2\u03c32D/summationdisplay\nj=1w2\nj\u239e\u23a0 (13.128)\nwhich is just a product of Gaussian and Laplace distributions.\nThis can be written as a hierarchical prior as follows (Kyung et al. 2010; Chen et al. 2011):\nw\nj|\u03c32,\u03c42\nj\u223cN(0,\u03c32(\u03c4\u22122\nj+\u03b32)\u22121) (13.129)\n\u03c42\nj|\u03b31\u223cExpon(\u03b32\n1\n2) (13.130)\nClearly if \u03b32=0, this reduces to the regular lasso.\nIt is possible to perform MAP estimation in this model using EM, or Bayesian inference using\nMCMC (Kyung et al. 2010) or variational Bayes (Chen et al. 2011).\n13.6 Non-convex regularizers\nAlthough the Laplace prior results in a convex optimization problem, from a statistical point\nof view this prior is not ideal. There are two main problems with it. First, it does not putenough probability mass near 0, so it does not sufficiently suppress noise. Second, it doesnot put enough probability mass on large values, so it causes shrinkage of relevant coefficients,corresponding to \u201csignal\u201d. (This can be seen in Figure 13.5(a): we see that /lscript\n1estimates of large\ncoefficients are signi\ufb01cantly smaller than their ML estimates, a phenomenon known as bias.)\nBoth problems can be solved by going to more \ufb02exible kinds of priors which have a larger\nspike at 0 and heavier tails. Even though we cannot \ufb01nd the global optimum anymore, thesenon-convex methods often outperform /lscript\n1regularization, both in terms of predictive accuracy\nand in detecting relevant variables (Fan and Li 2001; Schniter et al. 2008). We give some examplesbelow.", "488": "458 Chapter 13. Sparse linear models\n13.6.1 Bridge regression\nA natural generalization of /lscript1regularization, known as bridge regression (Frank and Friedman\n1993), has the form\n\u02c6w= NLL(w)+\u03bb/summationdisplay\nj|wj|b(13.131)\nforb\u22650. This corresponds to MAP estimation using a exponential power distribution given\nby\nExpPower( w|\u03bc,a,b) /definesb\n2a\u0393(1+1/b)exp/parenleftBigg\n\u2212|x\u2212\u03bc|\nab/parenrightBigg\n(13.132)\nIfb=2, we get the Gaussian distribution (with a=\u03c3\u221a\n2), corresonding to ridge regression; if\nwe setb=1, we get the Laplace distribution, corresponding to lasso; if we set b=0,w eg e t\n/lscript0regression, which is equivalent to best subset selection. Unfortunately, the objective is not\nconvex for b<1, and is not sparsity promoting for b>1. So the /lscript1norm is the tightest convex\napproximation to the /lscript0norm.\nThe effect of changing bis illustrated in Figure 13.17, where we plot the prior for b=2,b=1\nandb=0.4; we assume p(w)=p(w1)p(w2). We also plot the posterior after seeing a single\nobservation, (x,y), which imposes a single linear constraint of the form, y=wTx, with a\ncertain tolerance controlled by the observation noise (compare to Figure 7.11). We see see that\nthe mode of the Laplace is on the vertical axis, corresponding to w1=0. By contrast, there are\ntwo modes when using b=0.4, corresponding to two different sparse solutions. When using\nthe Gaussian, the MAP estimate is not sparse (the mode does not lie on either of the coordinateaxes).\n13.6.2 Hierarchical adaptive lasso\nRecall that one of the principal problems with lasso is that it results in biased estimates.This is because it needs to use a large value of \u03bbto \u201csquash\u201d the irrelevant parameters, but\nthis then over-penalizes the relevant parameters. It would be better if we could associate adifferent penalty parameter with each parameter. Of course, it is completely infeasible to tuneDparameters by cross validation, but this poses no problem to the Bayesian: we simply make\neach\u03c4\n2\njhave its own private tuning parameter, \u03b3j, which are now treated as random variables\ncoming from the conjugate prior \u03b3j\u223cIG(a,b). The full model is as follows:\n\u03b3j\u223cIG(a,b) (13.133)\n\u03c42\nj|\u03b3j\u223cGa(1,\u03b32\nj/2) (13.134)\nwj|\u03c42\nj\u223cN(0,\u03c42\nj) (13.135)\nSee Figure 13.18(a). This has been called the hierarchical adaptive lasso (HAL) (Lee et al. 2010)\n(see also (Lee et al. 2011; Cevher 2009; Armagan et al. 2011)). We can integrate out \u03c42\nj, which\ninduces a Lap(w j|0,1/\u03b3j)distribution on wjas before. The result is that p(wj)is now a\nscaled mixture of Laplacians. It turns out that we can \ufb01t this model (i.e., compute a local\nposterior mode) using EM, as we explain below. The resulting estimate, \u02c6wHAL, often works", "489": "13.6. Non-convex regularizers 459\nFigure 13.17 Top: plot of log priorfor three different distributions with unit variance: Gaussian, Laplace\nand exponential power. Bottom: plot of log posteriorafter observing a single observation, corresponding\nto a single linear constraint. The precision of this observation is shown by the diagonal lines in the top\n\ufb01gure. In the case of the Gaussian prior, the posterior is unimodal and symmetric. In the case of theLaplace prior, the posterior is unimodal and asymmetric (skewed). In the case of the exponential prior, theposterior is bimodal. Based on Figure 1 of (Seeger 2008). Figure generated by sparsePostPlot , written\nby Florian Steinke.\nmuch better than the estimate returned by lasso, \u02c6wL1, in the sense that it is more likely to\ncontain zeros in the right places (model selection consistency) and more likely to result in good\npredictions (prediction consistency) (Lee et al. 2010). We give an explanation for this behavior inSection 13.6.2.2.\n13.6.2.1 EM for HAL\nSince the inverse Gamma is conjugate to the Laplace, we \ufb01nd that the E step for \u03b3\njis given by\np(\u03b3j|wj)=I G (a+1,b+|wj|) (13.136)\nThe E step for \u03c32is the same as for vanilla lasso.\nThe prior for whas the following form:\np(w|\u03b3)=/productdisplay\nj1\n2\u03b3jexp(\u2212|w j|/\u03b3j) (13.137)\nHence the M step must optimize\n\u02c6w(t+1)=a r g m a x\nwlogN(y|Xw,\u03c32)\u2212/summationdisplay\nj|wj|E[1/\u03b3j] (13.138)", "490": "460 Chapter 13. Sparse linear models\na\u03c3\nb\u03c3\u03c32\nyi\nxiwj\u03c42\nj\u03b3jab\nND\n(a)\u22121 \u22120.8\u22120.6\u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8 1\u22121\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.81HAL\n  \na=1, b=0.01\na=1, b=0.10\na=1, b=1.00\n(b)\nFigure 13.18 (a) DGM for hierarchical adaptive lasso. (b) Contours of Hierarchical adpative Laplace. Based\non Figure 1 of (Lee et al. 2010). Figure generated by normalGammaPenaltyPlotDemo .\nThe expectation is given by\nE[1/\u03b3j]=a+1\nb+|w(t)\nj|/definess(t)\nj (13.139)\nThus the M step becomes a weighted lasso problem:\n\u02c6w(t+1)= argmin\nw||y\u2212Xw||2\n2+/summationdisplay\njs(t)\nj|wj| (13.140)\nThis is easily solved using standard methods (e.g., LARS). Note that if the coefficient was esti-\nmated to be large in the previous iteration (so w(t)\njis large), then the scaling factor s(t)\njwill\nbe small, so large coefficients are not penalized heavily. Conversely, small coefficients doget\npenalized heavily. This is the way that the algorithm adapts the penalization strength of each\ncoefficient. The result is an estimate that is often much sparser than returned by lasso, but alsoless biased.\nN o t et h a ti fw es e ta =b=0, and we only perform 1 iteration of EM, we get a method that\nis closely related to the adaptive lasso of (Zou 2006; Zou and Li 2008). This EM algorithm is\nalso closely related to some iteratively reweighted /lscript\n1methods proposed in the signal processing\ncommunity (Chartrand and Yin 2008; Candes et al. 2008).\n13.6.2.2 Understanding the behavior of HAL\nWe can get a better understanding of HAL by integrating out \u03b3jto get the following marginal\ndistribution,\np(wj|a,b)=a\n2b/parenleftbigg|wj|\nb+1/parenrightbigg\u2212(a+1)\n(13.141)", "491": "13.6. Non-convex regularizers 461\n\u221210 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8 10\u221210\u22128\u22126\u22124\u221220246810Lasso\nwMLEwMAP\n(a)\u221210 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8 10\u221210\u22128\u22126\u22124\u221220246810\nwMLEwMAPHAL\n  \nb = 0.010, a=1\nb = 0.100, a=1\nb = 1.000, a=1\n(b)\nFigure 13.19 Thresholding behavior of two penalty functions (negative log priors). (a) Laplace.\n(b) Hierarchical adaptive Laplace. Based on Figure 2 of (Lee et al. 2010). Figure generated by\nnormalGammaThresholdPlotDemo .\nThis is an instance of the generalized t distribution (McDonald and Newey 1988) (in (Cevher\n2009; Armagan et al. 2011), this is called the double Pareto distribution) de\ufb01ned as\nGT(w|\u03bc,a,c,q)/definesq\n2ca1/qB(1/q,a)/parenleftbigg\n1+|w\u2212\u03bc|q\nacq/parenrightbigg\u2212(a+1/q)\n(13.142)\nwherecis the scale parameter (which controls the degree of sparsity), and ais related to the\ndegrees of freedom. When q=2andc=\u221a\n2we recover the standard t distribution; when\na\u2192\u221e, we recover the exponential power distribution; and when q=1anda=\u221ewe\nget the Laplace distribution. In the context of the current model, we see that p(wj|a,b)=\nGT(wj|0,a,b/a, 1).\nThe resulting penalty term has the form\n\u03c0\u03bb(wj)/defines\u2212logp(wj)=(a+1)log(1+|wj|\nb)+const (13.143)\nwhere\u03bb=(a,b)are the tuning parameters. We plot this penalty in 2d (i.e., we plot \u03c0\u03bb(w1)+\n\u03c0\u03bb(w2)) in Figure 13.18(b) for various values of b. Compared to the diamond-shaped Laplace\npenalty, shown in Figure 13.3(a), we see that the HAL penalty looks more like a \u201cstar \ufb01sh\u201d: it\nputs much more density along the \u201cspines\u201d, thus enforcing sparsity more aggressively. Note thatthis penalty is clearly not convex.\nWe can gain further understanding into the behavior of this penalty function by considering\napplying it to the problem of linear regression with an orthogonal design matrix. In this case,", "492": "462 Chapter 13. Sparse linear models\np(\u03c42\nj) p(\u03b3j)p(wj) Ref\nGa(1,\u03b32\n2)Fixed Lap(0,1/\u03b3)(Andrews and Mallows 1974; West 1987)\nGa(1,\u03b32\n2)I G ( a,b)G T ( 0 ,a,b/a,1)(Lee et al. 2010, 2011; Cevher 2009; Armagan et al. 2011)\nGa(1,\u03b32\n2)G a ( a,b)NEG(a,b) (Griffin and Brown 2007, 2010; Chen et al. 2011)\nGa(\u03b4,\u03b32\n2)Fixed NG (\u03b4,\u03b3) (Griffin and Brown 2007, 2010)\nGa(\u03c42\nj|0,0)-N J (wj) (Figueiredo 2003)\nIG(\u03b4\n2,\u03b4\u03b32\n2)Fixed T(0,\u03b4,\u03b3) (Andrews and Mallows 1974; West 1987)\nC+(0,\u03b3)C+(0,b)horseshoe( b)(Carvahlo et al. 2010)\nTable 13.2 Some scale mixtures of Gaussians. Abbreviations: C+= half-recti\ufb01ed Cauchy; Ga= Gamma\n(shape and rate parameterization); GT = generalized t; IG= inverse Gamma; NEG = Normal-Exponential-\nGamma; NG = Normal-Gamma; NJ = Normal-Jeffreys. The horseshoe distribution is the name we give\nto the distribution induced on wjby the prior described in (Carvahlo et al. 2010); this has no simple\nanalytic form. The de\ufb01nitions of the NEG and NG densities are a bit complicated, but can be found in thereferences. The other distributions are de\ufb01ned in the text.\none can show that the objective becomes\nJ(w)=1\n2||y\u2212Xw||2\n2+D/summationdisplay\nj=1\u03c0\u03bb(|wj|) (13.144)\n=1\n2||y\u2212\u02c6y||2+1\n2D/summationdisplay\nj=1(\u02c6wmle\nj\u2212wj)2+D/summationdisplay\nj=1\u03c0\u03bb(|wj|) (13.145)\nwhere\u02c6wmle=XTyis the MLE and \u02c6y=X\u02c6wmle. Thus we can compute the MAP estimate\none dimension at a time by solving the following 1d optimization problem:\n\u02c6wj=a r g m i n\nwj1\n2(\u02c6wmle\nj\u2212wj)2+\u03c0\u03bb(wj) (13.146)\nIn Figure 13.19(a) we plot the lasso estimate, \u02c6wL1, vs the ML estimate, \u02c6wmle. We see that the\n/lscript1estimator has the usual soft-thresholding behavior seen earlier in Figure 13.5(a). However,\nthis behavior is undesirable since the large magnitude coefficients are also shrunk towards 0,\nwhereas we would like them to be equal to their unshrunken ML estimates.\nIn Figure 13.19(b) we plot the HAL estimate, \u02c6wHAL, vs the ML estimate \u02c6wmle. We see that\nthis approximates the more desirable hard thresholding behavior seen earlier in Figure 13.5(b)much more closely.\n13.6.3 Other hierarchical priors\nMany other hierarchical sparsity-promoting priors have been proposed; see Table 13.2 for a briefsummary. In some cases, we can analytically derive the form of the marginal prior for w\nj.\nGenerally speaking, this prior is not concave.\nA particularly interesting prior is the improper Normal-Jeffreys prior, which has been used\nin (Figueiredo 2003). This puts a non-informative Jeffreys prior on the variance, Ga(\u03c42\nj|0,0)\u221d", "493": "13.7. Automatic relevance determination (ARD)/sparse Bayesian learning (SBL) 463\n1/\u03c42\nj; the resulting marginal has the form p(wj)=NJ(wj)\u221d1/|wj|. This gives rise to a\nthresholding rule that looks very similar to HAL in Figure 13.19(b), which in turn is very similar\nto hard thresholding. However, this prior has no free parameters, which is both a good thing(nothing to tune) and a bad thing (no ability to adapt the level of sparsity).\n13.7 Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)\nAll the methods we have considered so far (except for the spike-and-slab methods in Sec-tion 13.2.1) have used a factorial prior of the form p(w)=/producttext\njp(wj). We have seen how these\npriors can be represented in terms of Gaussian scale mixtures of the form wj\u223cN(0,\u03c42\nj),w h e r e\n\u03c42\njhas one of the priors listed in Table 13.2. Using these latent variances, we can represent the\nmodel in the form \u03c42\nj\u2192wj\u2192y\u2190X. We can then use EM to perform MAP estimation,\nw h e r ei nt h eEs t e pw ei n f e rp( \u03c42\nj|wj), and in the M step we estimate wfromy,Xand\u03c4.\nThis M step either involves a closed-form weighted /lscript2optimization (in the case of Gaussian\nscale mixtures), or a weighted /lscript1optimization (in the case of Laplacian scale mixtures). We also\ndiscussed how to perform Bayesian inference in such models, rather than just computing MAPestimates.\nIn this section, we discuss an alternative approach based on type II ML estimation (empirical\nBayes), whereby we integrate out wand maximize the marginal likelihood wrt \u03c4. This EB\nprocedure can be implemented via EM, or via a reweighted /lscript\n1scheme, as we will explain below.\nHaving estimated the variances, we plug them in to compute the posterior mean of the weights,E[w|\u02c6\u03c4,D]; rather surprisingly (in view of the Gaussian prior), the result is an (approximately)\nsparse estimate, for reasons we explain below.\nIn the context of neural networks, this this method is called called automatic relevance\ndetermination orARD(MacKay 1995b; Neal 1996): see Section 16.5.7.5. In the context of the\nlinear models we are considering in this chapter, this method is called sparse Bayesian learning\norSBL(Tipping 2001). Combining ARD/SBL with basis function expansion in a linear model\ngives rise to a technique called the relevance vector machine (RVM), which we will discuss inSection 14.3.2.\n13.7.1 ARD for linear regression\nWe will explain the procedure in the context of linear regression; ARD for GLMs requires the useof the Laplace (or some other) approximation. case can be It is conventional, when discussingARD / SBL, to denote the weight precisions by \u03b1\nj=1/\u03c42\nj, and the measurement precision\nby\u03b2=1/\u03c32(do not confuse this with the use of \u03b2in statistics to represent the regression\ncoefficients!). In particular, we will assume the following model:\np(y|x,w,\u03b2)=N (y|wTx,1/\u03b2) (13.147)\np(w)=N (w|0,A\u22121) (13.148)", "494": "464 Chapter 13. Sparse linear models\nwhereA=d i a g (\u03b1). The marginal likelihood can be computed analytically as follows:\np(y|X,\u03b1,\u03b2)=/integraldisplay\nN(y|Xw,\u03b2IN)N(w|0,A)dw (13.149)\n=N(y|0,\u03b2IN+XA\u22121XT) (13.150)\n=( 2\u03c0)\u2212N/2|C\u03b1|\u22121\n2exp(\u22121\n2yTC\u22121\n\u03b1y) (13.151)\nwhere\nC\u03b1/defines\u03b2\u22121IN+XA\u22121XT(13.152)\nCompare this to the marginal likelihood in Equation 13.13 in the spike and slab model; modulo\nthe\u03b2=1/\u03c32factor missing from the second term, the equations are the same, except we have\nreplaced the binary \u03b3j\u2208{0,1}with continuous \u03b1j\u2208R+. In log form, the objective becomes\n/lscript(\u03b1,\u03b2)/defines\u22121\n2logp(y|X,\u03b1,\u03b2)=l o g|C\u03b1|+yTC\u22121\n\u03b1y (13.153)\nTo regularize the problem, we may put a conjugate prior on each precision, \u03b1j\u223cGa(a,b)\nand\u03b2\u223cGa(c,d). The modi\ufb01ed objective becomes\n/lscript(\u03b1,\u03b2)/defines\u22121\n2logp(y|X,\u03b1,\u03b2)+/summationdisplay\njlogGa(\u03b1 j|a,b)+logGa( \u03b2|c,d) (13.154)\n=l o g|C\u03b1|+yTC\u22121\n\u03b1y+/summationdisplay\nj(alog\u03b1j\u2212b\u03b1j)+clog\u03b2\u2212d\u03b2 (13.155)\nThis is useful when performing Bayesian inference for \u03b1and\u03b2(Bishop and Tipping 2000).\nHowever, when performing (type II) point estimation, we will use the improper prior a=b=\nc=d=0, which results in maximal sparsity.\nBelow we describe how to optimize /lscript(\u03b1,\u03b2)wrt the precision terms \u03b1and\u03b2.7This is a\nproxy for \ufb01nding the most probable model setting of \u03b3in the spike and slab model, which in\nturn is closely related to /lscript0regularization. In particular, it can be shown (Wipf et al. 2010) that\nthe objective in Equation 13.153 has many fewer local optima than the /lscript0objective, and hence\nis much easier to optimize.\nOnce we have estimated \u03b1and\u03b2, we can compute the posterior over the parameters using\np(w|D,\u02c6\u03b1,\u02c6\u03b2)=N(\u03bc,\u03a3) (13.156)\n\u03a3\u22121=\u02c6\u03b2XTX+A (13.157)\n\u03bc=\u02c6\u03b2\u03a3XTy (13.158)\nThe fact that we compute a posterior over w, while simultaneously encouraging sparsity, is why\nthe method is called \u201csparse Bayesian learning\u201d. Nevertheless, since there are many ways to besparse and Bayesian, we will use the \u201cARD\u201d term instead, even in the linear model context. (Inaddition, SBL is only \u201cbeing Bayesian\u201d about the values of the coefficients, rather than re\ufb02ectinguncertainty about the set of relevant variables, which is typically of more interest.)\n7. An alternative approach to optimizing \u03b2is to put a Gamma prior on \u03b2and to integrate it out to get a Student\nposterior for w(Buntine and Weigend 1991). However, it turns out that this results in a less accurate estimate for\n\u03b1(MacKay 1999). In addition, working with Gaussians is easier than working with the Student distribution, and the\nGaussian case generalizes more easily to other cases such as logistic regression.", "495": "13.7. Automatic relevance determination (ARD)/sparse Bayesian learning (SBL) 465\nx\ny\n(a)Cy\n(b)\nFigure 13.20 Illustration of why ARD results in sparsity. The vector of inputs xdoes not point towards\nthe vector of outputs y, so the feature should be removed. (a) For \ufb01nite \u03b1, the probability density is spread\nin directions away from y. (b) When \u03b1=\u221e, the probability density at yis maximized. Based on Figure\n8 of (Tipping 2001).\n13.7.2 Whence sparsity?\nIf\u02c6\u03b1j\u22480, we \ufb01nd \u02c6wj\u2248\u02c6wmle\nj, since the Gaussian prior shrinking wjtowards 0 has zero\nprecision. However, if we \ufb01nd that \u02c6\u03b1j\u2248\u221e, then the prior is very con\ufb01dent that wj=0, and\nhence that feature jis \u201cirrelevant\u201d. Hence the posterior mean will have \u02c6wj\u22480. Thus irrelevant\nfeatures automatically have their weights \u201cturned off\u201d or \u201cpruned out\u201d.\nWe now give an intuitive argument, based on (Tipping 2001), about why ML-II should encour-\nage\u03b1j\u2192\u221efor irrelevant features. Consider a 1d linear regression with 2 training examples,\nsoX=x=(x1,x2), andy=(y1,y2). We can plot xandyas vectors in the plane, as\nshown in Figure 13.20. Suppose the feature is irrelevant for predicting the response, so xpoints\nin a nearly orthogonal direction to y. Let us see what happens to the marginal likelihood as we\nchange\u03b1. The marginal likelihood is given by p(y|x,\u03b1,\u03b2)=N(y|0,C),w h e r e\nC=1\n\u03b2I+1\n\u03b1xxT(13.159)\nIf\u03b1is \ufb01nite, the posterior will be elongated along the direction of x, as in Figure 13.20(a).\nHowever, if \u03b1=\u221e, we \ufb01nd C=1\n\u03b2I,s oCis spherical, as in Figure 13.20(b). If |C|is held\nconstant, the latter assigns higher probability density to the observed response vector y, so this\nis the preferred solution. In other words, the marginal likelihood \u201cpunishes\u201d solutions where \u03b1j\nis small but X:,jis irrelevant, since these waste probability mass. It is more parsimonious (from\nthe point of view of Bayesian Occam\u2019s razor) to eliminate redundant dimensions.\n13.7.3 Connection to MAP estimation\nARD seems quite different from the MAP estimation methods we have been considering earlier\nin this chapter. In particular, in ARD, we are not integrating out \u03b1and optimizing w, but vice", "496": "466 Chapter 13. Sparse linear models\nversa. Because the parameters wjbecome correlated in the posterior (due to explaining away),\nwhen we estimate \u03b1jwe are borrowing information from all the features, not just feature j.\nConsequently, the effective prior p(w|\u02c6\u03b1)isnon-factorial, and furthermore it depends on the\ndataD(and\u03c32). However, in (Wipf and Nagarajan 2007), it was shown that ARD can be viewed\nas the following MAP estimation problem:\n\u02c6wARD=a r g m i n\nw\u03b2||y\u2212Xw||2\n2+gARD(w) (13.160)\ngARD(w)/definesmin\n\u03b1\u22650/summationdisplay\nj\u03b1jw2\nj+log|C\u03b1| (13.161)\nThe proof, which is based on convex analysis, is a little complicated and hence is omitted.\nFurthermore, (Wipf and Nagarajan 2007; Wipf et al. 2010) prove that MAP estimation with\nnon-factorial priors is strictly better than MAP estimation with any possible factorial prior in\nthe following sense: the non-factorial objective always has fewer local minima than factorialobjectives, while still satisfying the property that the global optimum of the non-factorial objec-tive corresponds to the global optimum of the /lscript\n0objective \u2014 a property that /lscript1regularization,\nwhich has no local minima, does not enjoy.\n13.7.4 Algorithms for ARD *\nIn this section, we review several different algorithms for implementing ARD.\n13.7.4.1 EM algorithm\nThe easiest way to implement SBL/ARD is to use EM. The expected complete data log likelihoodis given by\nQ(\u03b1,\u03b2)=E/bracketleftbig\nlogN(y|Xw,\u03c3\n2I)+logN(w|0,A\u22121)/bracketrightbig\n(13.162)\n=1\n2E\u23a1\n\u23a3Nlog\u03b2\u2212\u03b2||y\u2212Xw||2+/summationdisplay\njlog\u03b1j\u2212tr(AwwT)\u23a4\u23a6+const(13.163)\n=1\n2Nlog\u03b2\u2212\u03b2\n2/parenleftbig\n||y\u2212X\u03bc||2+tr(XTX\u03a3)/parenrightbig\n+1\n2/summationdisplay\njlog\u03b1j\u22121\n2tr[A(\u03bc\u03bcT+\u03a3)]+const (13.164)\nwhere\u03bcand\u03a3are computed in the E step using Equation 13.158.\nSuppose we put a Ga(a,b) prior on\u03b1jand aGa(c,d)prior on\u03b2. The penalized objective\nbecomes\nQ/prime(\u03b1,\u03b2)=Q(\u03b1,\u03b2)+/summationdisplay\nj(alog\u03b1j\u2212b\u03b1j)+clog\u03b2\u2212d\u03b2 (13.165)\nSettingdQ/prime\nd\u03b1j=0we get the following M step:\n\u03b1j=1+2a\nE/bracketleftbig\nw2\nj/bracketrightbig\n+2b=1+2a\nm2\nj+\u03a3jj+2b(13.166)", "497": "13.7. Automatic relevance determination (ARD)/sparse Bayesian learning (SBL) 467\nIf\u03b1j=\u03b1, anda=b=0, the update becomes\n\u03b1=D\nE[wTw]=D\n\u03bcT\u03bc+tr(\u03a3)(13.167)\nThe update for \u03b2is given by\n\u03b2\u22121\nnew=||y\u2212X\u03bc||2+\u03b2\u22121/summationtext\nj(1\u2212\u03b1j\u03a3jj)+2d\nN+2c(13.168)\n(Deriving this is Exercise 13.2.)\n13.7.4.2 Fixed-point algorithm\nA faster and more direct approach is to directly optimize the objective in Equation 13.155. One\ncan show (Exercise 13.3) that the equationsd/lscript\nd\u03b1j=0andd/lscript\nd\u03b2=0lead to the following \ufb01xed\npoint updates:\n\u03b1j\u2190\u03b3j+2a\nm2\nj+2b(13.169)\n\u03b2\u22121\u2190||y\u2212X\u03bc||2+2d\nN\u2212/summationtext\nj\u03b3j+2c(13.170)\n\u03b3j/defines1\u2212\u03b1j\u03a3jj (13.171)\nThe quantity \u03b3jis a measure of how well-determined wjis by the data (MacKay 1992). Hence\n\u03b3=/summationtext\nj\u03b3jis the effective degrees of freedom of the model. See Section 7.5.3 for further\ndiscussion.\nSince\u03b1and\u03b2both depend on \u03bcand\u03a3(which can be computed using Equation 13.158 or the\nLaplace approximation), we need to re-estimate these equations until convergence. (Convergence\nproperties of this algorithm have been studied in (Wipf and Nagarajan 2007).) At convergence,the results are formally identical to those obtained by EM, but since the objective is non-convex,the results can depend on the initial values.\n13.7.4.3 Iteratively reweighted /lscript\n1algorithm\nAnother approach to solving the ARD problem is based on the view that it is a MAP estimationproblem. Although the log prior g(w)is rather complex in form, it can be shown to be a\nnon-decreasing, concave function of |w\nj|. This means that it can be solved by an iteratively\nreweighted /lscript1problem of the form\nwt+1=a r gm i n\nwNLL(w)+/summationdisplay\nj\u03bb(t)\nj|wj| (13.172)\nIn (Wipf and Nagarajan 2010), the following procedure for setting the penalty terms is suggested\n(based on a convex bound to the penalty function). We initialize with \u03bb(0)\nj=1, and then at", "498": "468 Chapter 13. Sparse linear models\niterationt+1, compute \u03bb(t+1)\njby iterating the following equation a few times:8\n\u03bbj\u2190/bracketleftbigg\nX:,j/parenleftBig\n\u03c32I+Xdiag(1/\u03bb j)diag(|w(t+1)\nj|)/parenrightBig\u22121\nXT)\u22121X:,j/bracketrightbigg1\n2\n(13.173)\nWe see that the new penalty \u03bbjdepends on allthe old weights. This is quite different from the\nadaptive lasso method of Section 13.6.2.\nTo understand this difference, consider the noiseless case where \u03c32=0, and assume D/greatermuchN.\nIn this case, there are/parenleftbiggD\nN/parenrightbigg\nsolutions which perfectly reconstruct the data, Xw=y, and which\nhave sparsity ||w||0=N; these are called basic feasible solutions or BFS. What we want are\nsolutions that satsify Xw=ybut which are much sparser than this. Suppose the method has\nfound a BFS. We do not want to increase the penalty on a weight just because it is small (as\nin adaptive lasso), since that will just reinforce our current local optimum. Instead, we want toincrease the penalty on a weight if it is small and if we have ||w\n(t+1)||<N. The covariance\nterm(Xdiag(1/\u03bbj)diag(|w(t+1)\nj|))\u22121has this effect: if wis a BFS, this matrix will be full rank,\nso the penalty will not increase much, but if wis sparser than N, the matrix will not be full\nrank, so the penalties associated with zero-valued coefficients will increase, thus reinforcing thissolution (Wipf and Nagarajan 2010).\n13.7.5 ARD for logistic regression\nNow consider binary logistic regression, p(y|x,w)=B e r ( y|sigm(wTx)), using the same\nGaussian prior, p(w)=N(w|0,A\u22121). We can no longer use EM to estimate \u03b1, since the\nGaussian prior is not conjugate to the logistic likelihood, so the E step cannot be done exactly.One approach is to use a variational approximation to the E step, as discussed in Section 21.8.1.1.A simpler approach is to use a Laplace approximation (see Section 8.4.1) in the E step. We canthen use this approximation inside the same EM procedure as before, except we no longer needto update \u03b2. Note, however, that this is not guaranteed to converge.\nAn alternative is to use the techniques from Section 13.7.4.3. In this case, we can use exact\nmethods to compute the inner weighted /lscript\n1regularized logistic regression problem, and no\napproximations are required.\n13.8 Sparse coding *\nSo far, we have been concentrating on sparse priors for supervised learning. In this section, wediscuss how to use them for unsupervised learning.\nIn Section 12.6, we discussed ICA, which is like PCA except it uses a non-Gaussian prior\nfor the latent factors z\ni. If we make the non-Gaussian prior be sparsity promoting, such as a\nLaplace distribution, we will be approximating each observed vector xias a sparse combination\nof basis vectors (columns of W); note that the sparsity pattern (controlled by zi) changes from\ndata case to data case. If we relax the constraint that Wis orthogonal, we get a method called\n8. The algorithm in (Wipf and Nagarajan 2007) is equivalent to a single iteration of Equation 13.173. However, since the\nequation is cheap to compute (only O(ND||w(t+1)||0)time), it is worth iterating a few times before solving the more\nexpensive /lscript1problem.", "499": "13.8. Sparse coding * 469\nMethod p(zi) p(W)Worthogonal\nPCA Gauss - yes\nFA Gauss - no\nICA Non-Gauss - yes\nSparse coding Laplace - no\nSparse PCA Gauss Laplace maybeSparse MF Laplace Laplace no\nTable 13.3 Summary of various latent factor models. A dash \u201c-\u201d in the p(W)column means we are\nperforming ML parameter estimation rather than MAP parameter estimation. Summary of abbreviations:\nPCA = principal components analysis; FA = factor analysis; ICA = independent components analysis; MF =matrix factorization.\nsparse coding. In this context, we call the factor loading matrix Wadictionary; each column\nis referred to as an atom.9In view of the sparse representation, it is common for L>D,i n\nwhich case we call the representation overcomplete.\nIn sparse coding, the dictionary can be \ufb01xed or learned. If it is \ufb01xed, it is common to use a\nwavelet or DCT basis, since many natural signals can be well approximated by a small number\nof such basis functions. However, it is also possible to learn the dictionary, by maximizing thelikelihood\nlogp(D|W)=N/summationdisplay\ni=1log/integraldisplay\nziN(xi|Wzi,\u03c32I)p(zi)dzi (13.174)\nWe discuss ways to optimize this below, and then we present several interesting applications.\nDo not confuse sparse coding with sparse PCA (see e.g., (Witten et al. 2009; Journee et al.\n2010)): this puts a sparsity promoting prior on the regression weights W, whereas in sparse\ncoding, we put a sparsity promoting prior on the latent factors zi. Of course, the two techniques\ncan be combined; we call the result sparse matrix factorization, although this term is non-\nstandard. See Table 13.3 for a summary of our terminology.\n13.8.1 Learning a sparse coding dictionary\nSince Equation 13.174 is a hard objective to maximize, it is common to make the followingapproximation:\nlogp(D|W)\u2248N/summationdisplay\ni=1max\nzi/bracketleftbig\nlogN(xi|Wzi,\u03c32I)+logp(zi)/bracketrightbig\n(13.175)\nIfp(zi)is Laplace, we can rewrite the NLL as\nNLL(W,Z)=N/summationdisplay\ni=11\n2||xi\u2212Wzi||2\n2+\u03bb||zi||1 (13.176)\n9. It is common to denote the dictionary by D, and to denote the latent factors by \u03b1i. However, we will stick with the\nWandzinotation.", "500": "470 Chapter 13. Sparse linear models\nTo prevent Wfrom becoming arbitrarily large, it is common to constrain the /lscript2norm of its\ncolumns to be less than or equal to 1. Let us denote this constraint set by\nC={W\u2208RD\u00d7Ls.t.wT\njwj\u22641} (13.177)\nThen we want to solve minW\u2208C,Z\u2208RN\u00d7LNLL(W,Z). F o ra\ufb01 x e d zi, the optimization over\nWis a simple least squares problem. And for a \ufb01xed dictionary W, the optimization problem\noverZis identical to the lasso problem, for which many fast algorithms exist. This suggests\nan obvious iterative optimization scheme, in which we alternate between optimizing WandZ.\n(Mumford 1994) called this kind of approach an analysis-synthesis loop, where estimating the\nbasisWis the analysis phase, and estimating the coefficients Zis the synthesis phase. In cases\nwhere this is too slow, more sophisticated algorithms can be used, see e.g., (Mairal et al. 2010).\nA variety of other models result in an optimization problem that is similar to Equation 13.176.\nFor example, non-negative matrix factorization orNMF(Paatero and Tapper 1994; Lee and\nSeung 2001) requires solving an objective of the form\nmin\nW\u2208C,Z\u2208RL\u00d7N1\n2N/summationdisplay\ni=1||xi\u2212Wzi||2\n2s.t.W\u22650,zi\u22650 (13.178)\n(Note that this has no hyper-parameters to tune.) The intuition behind this constraint is that the\nlearned dictionary may be more interpretable if it is a positive sum of positive \u201cparts\u201d, ratherthan a sparse sum of atoms that may be positive or negative. Of course, we can combine NMFwith a sparsity promoting prior on the latent factors. This is called non-negative sparse coding\n(Hoyer 2004).\nAlternatively, we can drop the positivity constraint, but impose a sparsity constraint on both\nthe factors z\niand the dictionary W. We call this sparse matrix factorization . To ensure strict\nconvexity, we can use an elastic net type penalty on the weights (Mairal et al. 2010) resulting in\nmin\nW,Z1\n2N/summationdisplay\ni=1||xi\u2212Wzi||2\n2+\u03bb||zi||1s.t.||wj||22+\u03b3||wj||1\u22641 (13.179)\nThere are several related objectives one can write down. For example, we can replace the lasso\nNLL with group lasso or fused lasso (Witten et al. 2009).\nWe can also use other sparsity-promoting priors besides the Laplace. For example, (Zhou et al.\n2009) propose a model in which the latent factors ziare made sparse using the binary mask\nmodel of Section 13.2.2. Each bit of the mask can be generated from a Bernoulli distributionwith parameter \u03c0, which can be drawn from a beta distribution. Alternatively, we can use a\nnon-parametric prior, such as the beta process. This allows the model to use dictionaries ofunbounded size, rather than having to specify Lin advance. One can perform Bayesian inference\nin this model using e.g., Gibbs sampling or variational Bayes. One \ufb01nds that the effective sizeof the dictionary goes down as the noise level goes up, due to the Bayesian Occam\u2019s razor. Thiscan prevent over\ufb01tting. See (Zhou et al. 2009) for details.\n13.8.2 Results of dictionary learning from image patches\nOne reason that sparse coding has generated so much interest recently is because it explains aninteresting phenomenon in neuroscience. In particular, the dictionary that is learned by applying", "501": "13.8. Sparse coding * 471\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 13.21 Illustration of the \ufb01lters learned by various methods when applied to natural image patches.\n(Each patch is \ufb01rst centered and normalized to unit norm.) (a) ICA. Figure generated by icaBasisDemo ,\nkindly provided by Aapo Hyvarinen. (b) sparse coding. (c) PCA. (d) non-negative matrix factorization. (e)\nsparse PCA with low sparsity on weight matrix. (f) sparse PCA with high sparsity on weight matrix. Figure\ngenerated by sparseDictDemo , written by Julien Mairal.", "502": "472 Chapter 13. Sparse linear models\nsparse coding to patches of natural images consists of basis vectors that look like the \ufb01lters that\nare found in simple cells in the primary visual cortex of the mammalian brain (Olshausen andField 1996). In particular, the \ufb01lters look like bar and edge detectors, as shown in Figure 13.21(b).(In this example, the parameter \u03bbwas chosen so that the number of active basis functions\n(non-zero components of z\ni) is about 10.) Interestingly, using ICA gives visually similar results,\nas shown in Figure 13.21(a). By contrast, applying PCA to the same data results in sinusoidalgratings, as shown in Figure 13.21(c); these do not look like cortical cell response patterns.\n10It\nhas therefore been conjectured that parts of the cortex may be performing sparse coding of thesensory input; the resulting latent representation is then further processed by higher levels ofthe brain.\nFigure 13.21(d) shows the result of using NMF, and Figure 13.21(e-f) show the results of sparse\nPCA, as we increase the sparsity of the basis vectors.\n13.8.3 Compressed sensing\nAlthough it is interesting to look at the dictionaries learned by sparse coding, it is not necessarilyvery useful. However, there are some practical applications of sparse coding, which we discussbelow.\nImagine that, instead of observing the data x\u2208R\nD, we observe a low-dimensional projection\nof it,y=Rx+/epsilon1wherey\u2208RM,Ris aM\u00d7Dmatrix,M/lessmuchD, and/epsilon1is a noise term\n(usually Gaussian). We assume Ris a known sensing matrix, corresponding to different linear\nprojections of x. For example, consider an MRI scanner: each beam direction corresponds to a\nvector, encoded as a row in R. Figure 13.22 illustrates the modeling assumptions.\nOur goal is to infer p(x|y,R). How can we hope to recover all of xif we do not measure\nall ofx? The answer is: we can use Bayesian inference with an appropriate prior, that exploits\nthe fact that natural signals can be expressed as a weighted combination of a small number ofsuitably chosen basis functions. That is, we assume x=Wz,w h e r ezhas a sparse prior, and\nWis suitable dictionary. This is called compressed sensing orcompressive sensing (Candes\net al. 2006; Baruniak 2007; Candes and Wakin 2008; Bruckstein et al. 2009).\nFor CS to work, it is important to represent the signal in the right basis, otherwise it will\nnot be sparse. In traditional CS applications, the dictionary is \ufb01xed to be a standard form,such as wavelets. However, one can get much better performance by learning a domain-speci\ufb01cdictionary using sparse coding (Zhou et al. 2009). As for the sensing matrix R, it is often chosen\nto be a random matrix, for reasons explained in (Candes and Wakin 2008). However, one canget better performance by adapting the projection matrix to the dictionary (Seeger and Nickish2008; Chang et al. 2009).\n13.8.4 Image inpainting and denoising\nSuppose we have an image which is corrupted in some way, e.g., by having text or scratchessparsely superimposed on top of it, as in Figure 13.23. We might want to estimate the underlying\n10. ThereasonPCAdiscoverssinusoidalgratingpatternsisbecauseitistryingtomodelthecovarianceofthedata, which,\nin the case of image patches, is translation invariant. This means cov[I(x,y),I(x/prime,y/prime)] =f/bracketleftbig\n(x\u2212x/prime)2+(y\u2212y/prime)2/bracketrightbig\nfor some function f,w h e r eI(x,y)is the image intensity at location (x,y). One can show (Hyvarinen et al. 2009, p125)\nthat the eigenvectors of a matrix of this kind are always sinusoids of different phases, i.e., PCA discovers a Fourier basis.", "503": "13.8. Sparse coding * 473\ny Rx Wz \u03bb\nFigure 13.22 Schematic DGM for compressed sensing. We observe a low dimensional measurement y\ngenerated by passing xthrough a measurement matrix R, and possibly subject to observation noise with\nvariance\u03c32. We assume that xhas a sparse decomposition in terms of the dictionary Wand the latent\nvariables z. the parameter \u03bbcontrolls the sparsity level.\n(a)\n (b)\nFigure 13.23 An example of image inpainting using sparse coding. Left: original image. Right: recon-\nstruction. Source: Figure 13 of (Mairal et al. 2008). Used with kind permission of Julien Mairal.\n\u201cclean\u201d image. This is called image inpainting . One can use similar techniques for image\ndenoising .\nWe can model this as a special kind of compressed sensing problem. The basic idea is as\nfollows. We partition the image into overlapping patches, yi, and concatenate them to form y.\nWe de\ufb01ne Rso that the i\u2019th row selects out patch i. Now de\ufb01ne Vto be the visible (uncorrupted)\ncomponents of y, andHto be the hidden components. To perform image inpainting, we just\ncomputep(yH|yV,\u03b8),w h e r e\u03b8are the model parameters, which specify the dictionary Wand\nthe sparsity level \u03bbofz. We can either learn a dictionary offline from a database of images, or\nwe can learn a dictionary just for this image, based on the non-corrupted patches.\nFigure 13.23 shows this technique in action. The dictionary (of size 256 atoms) was learned\nfrom7\u00d7106undamaged 12\u00d712color patches in the 12 mega-pixel image.\nAn alternative approach is to use a graphical model (e.g., the \ufb01elds of experts model (S.", "504": "474 Chapter 13. Sparse linear models\nand Black 2009)) which directly encodes correlations between neighboring image patches, rather\nthan using a latent variable model. Unfortunately such models tend to be computationally moreexpensive.\nExercises\nExercise 13.1 Partial derivative of the RSS\nDe\ufb01ne\nRSS(w)=||Xw\u2212y||2\n2 (13.180)\na. Show that\n\u2202\n\u2202wkRSS(w)=a kwk\u2212ck (13.181)\nak=2n/summationdisplay\ni=1x2ik=2||x:,k||2(13.182)\nck=2n/summationdisplay\ni=1xik(yi\u2212wT\n\u2212kxi,\u2212k)=2xT:,krk (13.183)\nwherew\u2212k=wwithout component k,xi,\u2212kisxiwithout component k, andrk=y\u2212wT\n\u2212kx:,\u2212k\nis the residual due to using all the features except feature k. Hint: Partition the weights into those\ninvolving kand those not involving k.\nb. Show that if\u2202\n\u2202wkRSS(w)=0, then\n\u02c6wk=xT\n:,krk\n||x:,k||2(13.184)\nHence when we sequentially add features, the optimal weight for feature kis computed by computing\northogonally projecting x:,konto the current residual.\nExercise 13.2 Derivation of M step for EB for linear regression\nDerive Equations 13.166 and 13.168. Hint: the following identity should be useful\n\u03a3XTX=\u03a3XTX+\u03b2\u22121\u03a3A\u2212\u03b2\u22121\u03a3A (13.185)\n=\u03a3(XTX\u03b2+A)\u03b2\u22121\u2212\u03b2\u22121\u03a3A (13.186)\n=(A+\u03b2XTX)\u22121(XTX\u03b2+A)\u03b2\u22121\u2212\u03b2\u22121\u03a3A (13.187)\n=(I\u2212A\u03a3)\u03b2\u22121(13.188)\nExercise 13.3 Derivation of \ufb01xed point updates for EB for linear regression\nDerive Equations 13.169 and 13.170. Hint: The easiest way to derive this result is to rewrite logp(D|\u03b1,\u03b2)\nas in Equation 8.54. This is exactly equivalent, since in the case of a Gaussian prior and likelihood, the\nposterior is also Gaussian, so the Laplace \u201capproximation\u201d is exact. In this case, we get\nlogp(D|\u03b1,\u03b2)=N\n2log\u03b2\u2212\u03b2\n2||y\u2212Xw||2\n+1\n2/summationdisplay\njlog\u03b1j\u22121\n2mTAm+1\n2log|\u03a3|\u2212D\n2log(2\u03c0) (13.189)\nThe rest is straightforward algebra.", "505": "13.8. Sparse coding * 475\nExercise 13.4 Marginal likelihood for linear regression\nSuppose we use a g-prior of the form \u03a3\u03b3=g(XT\n\u03b3X\u03b3)\u22121. Show that Equation 13.16 simpli\ufb01es to\np(D|\u03b3)\u221d(1+g)\u2212D\u03b3/2(2b\u03c3+S(\u03b3))\u2212(2a\u03c3+N\u22121)/2(13.190)\nS(\u03b3)=yTy\u2212g\n1+gyTX\u03b3(XT\n\u03b3X\u03b3)\u22121XT\u03b3y (13.191)\nExercise 13.5 Reducing elastic net to lasso\nDe\ufb01ne\nJ1(w)=|y\u2212Xw|2+\u03bb2|w|2+\u03bb1|w|1 (13.192)\nand\nJ2(w)=|\u02dcy\u2212\u02dcX\u02dcw|2+c\u03bb1|w|1 (13.193)\nwherec=( 1+\u03bb2)\u22121\n2and\n\u02dcX=c/parenleftbiggX\u221a\n\u03bb2Id/parenrightbigg\n,\u02dcy=/parenleftbiggy\n0d\u00d71/parenrightbigg\n(13.194)\nShow\nargminJ1(w)=c(argmin J2(w)) (13.195)\ni.e.\nJ1(cw)=J2(w) (13.196)\nand hence that one can solve an elastic net problem using a lasso solver on modi\ufb01ed data.\nExercise 13.6 Shrinkage in linear regression\n(Source: Jaakkola.) Consider performing linear regression with an orthonormal design matrix, so ||x:,k||2\n2=\n1for each column (feature) k, andxT\n:,kx:,j=0, so we can estimate each parameter wkseparately.\nFigure 13.24 plots \u02c6wkvsck=2yTx:,k, the correlation of feature kwith the response, for 3 different\nesimation methods: ordinary least squares (OLS), ridge regression with parameter \u03bb2, and lasso with\nparameter \u03bb1.\na. Unfortunately we forgot to label the plots. Which method does the solid (1), dotted (2) and dashed (3)\nline correspond to? Hint: see Section 13.3.3.\nb. What is the value of \u03bb1?\nc. What is the value of \u03bb2?\nExercise 13.7 Prior for the Bernoulli rate parameter in the spike and slab model\nConsider the model in Section 13.2.1. Suppose we put a prior on the sparsity rates, \u03c0j\u223cBeta(\u03b1 1,\u03b12).\nDerive an expression for p(\u03b3|\u03b1)after integrating out the \u03c0j\u2019s. Discuss some advantages and disadvantages\nof this approach compared to assuming \u03c0j=\u03c00for \ufb01xed\u03c00.", "506": "476 Chapter 13. Sparse linear models\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2\u22121\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.81\nckwk1\n2\n3\nFigure 13.24 Plot of\u02c6wkvs amount of correlation ckfor three different estimators.\nExercise 13.8 Deriving E step for GSM prior\nShow that\nE/bracketleftbigg1\n\u03c42\nj|wj/bracketrightbigg\n=\u03c0/prime(wj)\n|wj|(13.197)\nwhere\u03c0(wj)=\u2212logp(wj)andp(wj)=intN(wj|0,\u03c42\nj)p(\u03c42\nj)d\u03c42\nj. Hint 1:\n1\n\u03c42\njN(wj|0,\u03c42\nj)\u221d1\n\u03c42\njexp(\u2212w2\nj\n2\u03c42\nj) (13.198)\n=\u22121\n|wj|\u22122wj\n2\u03c42\njexp(\u2212w2\nj\n2\u03c42\nj) (13.199)\n=\u22121\n|wj|d\nd|wj|N(wj|0,\u03c42\nj) (13.200)\nHint 2:\nd\nd|wj|p(wj)=1\np(wj)d\nd|wj|logp(wj) (13.201)\nExercise 13.9 EM for sparse probit regression with Laplace prior\nDerive an EM algorithm for \ufb01tting a binary probit classi\ufb01er (Section 9.4) using a Laplace prior on the\nweights. (If you get stuck, see (Figueiredo 2003; Ding and Harrison 2010).)\nExercise 13.10 GSM representation of group lasso\nConsider the prior \u03c42\nj\u223cGa(\u03b4,\u03c12/2), ignoring the grouping issue for now. The marginal distribution\ninduced on the weights by a Gamma mixing distribution is called the normal Gamma distribution and is", "507": "13.8. Sparse coding * 477\ngiven by\nNG(w j|\u03b4,\u03c1)=/integraldisplay\nN(wj|0,\u03c42\nj)Ga(\u03c42\nj|\u03b4,\u03c12/2)d\u03c42\nj (13.202)\n=1\nZ|wj|\u03b4\u22121/2K\u03b4\u22121\n2(\u03c1|wj|) (13.203)\n1/Z=\u03c1\u03b4+1\n2\u221a\u03c02\u03b4\u22121/2\u03c1(\u03b4)(13.204)\nwhereK\u03b1(x)is the modi\ufb01ed Bessel function of the second kind (the besselk function in Matlab).\nNow suppose we have the following prior on the variances\np(\u03c32\n1:D)=G/productdisplay\ng=1p(\u03c321:d\ng),p(\u03c321:d\ng)=/productdisplay\nj\u2208gGa(\u03c42\nj|\u03b4g,\u03c12/2) (13.205)\nThe corresponding marginal for each group of weights has the form\np(wg)\u221d|ug|\u03b4g\u2212dg/2K\u03b4g\u2212dg/2(\u03c1ug) (13.206)\nwhere\nug/defines/radicalBigg/summationdisplay\nj\u2208gw2\ng,j=||wg||2 (13.207)\nNow suppose \u03b4g=(dg+1)/2,s o\u03b4g\u2212dg/2=1\n2. Conveniently, we have K1\n2(z)=/radicalbig\u03c0\n2zexp(\u2212z). Show\nthat the resulting MAP estimate is equivalent to group lasso.\nExercise 13.11 Projected gradient descent for /lscript1regularized least squares\nConsider the BPDN problem argmin\u03b8RSS(\u03b8)+\u03bb||\u03b8||1. By using the split variable trick introducted in\nSection 7.4 (i.e., by de\ufb01ning \u03b8=[\u03b8+,\u03b8\u2212]), rewrite this as a quadratic program with a simple bound\nconstraint. Then sketch how to use projected gradient descent to solve this problem. (If you get stuck,\nconsult (Figueiredo et al. 2007).)\nExercise 13.12 Subderivative of the hinge loss function\nLetf(x)=( 1\u2212x)+be the hinge loss function, where (z)+=m a x ( 0 ,z). What are \u2202f(0),\u2202f(1), and\n\u2202f(2)?\nExercise 13.13 Lower bounds to convex functions\nLetfbe a convex function. Explain how to \ufb01nd a global affine lower bound to fat an arbitrary point\nx\u2208dom(f).", "508": "", "509": "14 Kernels\n14.1 Introduction\nSo far in this book, we have been assuming that each object that we wish to classify or cluster\nor process in anyway can be represented as a \ufb01xed-size feature vector, typically of the formx\ni\u2208RD. However, for certain kinds of objects, it is not clear how to best represent them\nas \ufb01xed-sized feature vectors. For example, how do we represent a text document or proteinsequence, which can be of variable length? or a molecular structure, which has complex 3dgeometry? or an evolutionary tree, which has variable size and shape?\nOne approach to such problems is to de\ufb01ne a generative model for the data, and use the\ninferred latent representation and/or the parameters of the model as features, and then to plugthese features in to standard methods. For example, in Chapter 28, we discuss deep learning,which is essentially an unsupervised way to learn good feature representations.\nAnother approach is to assume that we have some way of measuring the similarity between\nobjects, that doesn\u2019t require preprocessing them into feature vector format. For example, whencomparing strings, we can compute the edit distance between them. Let \u03ba(x,x\n/prime)\u22650be some\nmeasure of similarity between objects x,x/prime\u2208X,w h e r eXis some abstract space; we will call \u03ba\nakernel function. Note that the word \u201ckernel\u201d has several meanings; we will discuss a different\ninterpretation in Section 14.7.1.\nIn this chapter, we will discuss several kinds of kernel functions. We then describe some\nalgorithms that can be written purely in terms of kernel function computations. Such methodscan be used when we don\u2019t have access to (or choose not to look at) the \u201cinside\u201d of the objectsxthat we are processing.\n14.2 Kernel functions\nWe de\ufb01ne a kernel function to be a real-valued function of two arguments, \u03ba(x,x/prime)\u2208R,f o r\nx,x/prime\u2208X. Typically the function is symmetric (i.e., \u03ba(x,x/prime)=\u03ba(x/prime,x)), and non-negative (i.e.,\n\u03ba(x,x/prime)\u22650), so it can be interpreted as a measure of similarity, but this is not required. We\ngive several examples below.", "510": "480 Chapter 14. Kernels\n14.2.1 RBF kernels\nThesquared exponential kernel (SE kernel) or Gaussian kernel is de\ufb01ned by\n\u03ba(x,x/prime)=e x p/parenleftbigg\n\u22121\n2(x\u2212x/prime)T\u03a3\u22121(x\u2212x/prime)/parenrightbigg\n(14.1)\nIf\u03a3is diagonal, this can be written as\n\u03ba(x,x/prime)=e x p\u239b\n\u239d\u22121\n2D/summationdisplay\nj=11\n\u03c32\nj(xj\u2212x/prime\nj)2\u239e\n\u23a0 (14.2)\nWe can interpret the \u03c3jas de\ufb01ning the characteristic length scale of dimension j.I f\u03c3j=\u221e,\nthe corresponding dimension is ignored; hence this is known as the ARD kernel.I f \u03a3is\nspherical, we get the isotropic kernel\n\u03ba(x,x/prime)=e x p/parenleftbigg\n\u2212||x\u2212x/prime||2\n2\u03c32/parenrightbigg\n(14.3)\nHere\u03c32is known as the bandwidth. Equation 14.3 is an example of a a radial basis function\norRBFkernel, since it is only a function of ||x\u2212x/prime||.\n14.2.2 Kernels for comparing documents\nWhen performing document classi\ufb01cation or retrieval, it is useful to have a way of comparing\ntwo documents, xiandxi/prime. If we use a bag of words representation, where xijis the number\nof times words joccurs in document i, we can use the cosine similarity, which is de\ufb01ned by\n\u03ba(xi,xi/prime)=xT\nixi/prime\n||xi||2||xi/prime||2(14.4)\nThis quantity measures the cosine of the angle between xiandxi/primewhen interpreted as vectors.\nSincexiis a count vector (and hence non-negative), the cosine similarity is between 0 and 1,\nwhere 0 means the vectors are orthogonal and therefore have no words in common.\nUnfortunately, this simple method does not work very well, for two main reasons. First, if xi\nhas any word in common with xi/prime, it is deemed similar, even though some popular words, such\nas \u201cthe\u201d or \u201cand\u201d occur in many documents, and are therefore not discriminative. (These are\nknown as stop words.) Second, if a discriminative word occurs many times in a document, the\nsimilarity is arti\ufb01cially boosted, even though word usage tends to be bursty, meaning that oncea word is used in a document it is very likely to be used again (see Section 3.5.5).\nFortunately, we can signi\ufb01cantly improve performance using some simple preprocessing. The\nidea is to replace the word count vector with a new feature vector called the TF-IDFrepresenta-\ntion, which stands for \u201cterm frequency inverse document frequency\u201d. We de\ufb01ne this as follows.First, the term frequency is de\ufb01ned as a log-transform of the count:\ntf(x\nij)/defineslog(1+xij) (14.5)\nThis reduces the impact of words that occur many times within one document. Second, theinverse document frequency is de\ufb01ned as\nidf(j)/defineslogN\n1+/summationtextN\ni=1I(xij>0)(14.6)", "511": "14.2. Kernel functions 481\nwhereNis the total number of documents, and the denominator counts how many documents\ncontain term j. Finally, we de\ufb01ne\ntf-idf(xi)/defines[tf(xij)\u00d7idf(j)]V\nj=1 (14.7)\n(There are several other ways to de\ufb01ne the tf and idf terms, see (Manning et al. 2008) for details.)\nWe then use this inside the cosine similarity measure. That is, our new kernel has the form\n\u03ba(xi,xi/prime)=\u03c6(xi)T\u03c6(xi/prime)\n||\u03c6(xi)||2||\u03c6(xi/prime)||2(14.8)\nwhere\u03c6(x)=tf-idf(x). This gives good results for information retrieval (Manning et al. 2008).\nA probabilistic interpretation of the tf-idf kernel is given in (Elkan 2005).\n14.2.3 Mercer (positive de\ufb01nite) kernels\nSome methods that we will study require that the kernel function satisfy the requirement thattheGram matrix, de\ufb01ned by\nK=\u239b\n\u239c\u239d\u03ba(x\n1,x1)\u00b7\u00b7\u00b7\u03ba(x1,xN)\n...\n\u03ba(xN,x1)\u00b7\u00b7\u00b7\u03ba(xN,xN)\u239e\n\u239f\u23a0 (14.9)\nbe positive de\ufb01nite for any set of inputs {xi}N\ni=1. We call such a kernel a Mercer kernel,o r\npositive de\ufb01nite kernel. It can be shown (Schoelkopf and Smola 2002) that the Gaussian kernel\nis a Mercer kernel as is the cosine similarity kernel (Sahami and Heilman 2006).\nThe importance of Mercer kernels is the following result, known as Mercer\u2019s theorem. If the\nGram matrix is positive de\ufb01nite, we can compute an eigenvector decomposition of it as follows\nK=UT\u039bU (14.10)\nwhere\u039bis a diagonal matrix of eigenvalues \u03bbi>0. Now consider an element of K:\nkij=(\u039b1\n2U:,i)T(\u039b1\n2U:j) (14.11)\nLet us de\ufb01ne \u03c6(xi)=\u039b1\n2U:i. Then we can write\nkij=\u03c6(xi)T\u03c6(xj) (14.12)\nThus we see that the entries in the kernel matrix can be computed by performing an innerproduct of some feature vectors that are implicitly de\ufb01ned by the eigenvectors U. In general, if\nthe kernel is Mercer, then there exists a function \u03c6mapping x\u2208XtoR\nDsuch that\n\u03ba(x,x/prime)=\u03c6(x)T\u03c6(x/prime) (14.13)\nwhere\u03c6depends on the eigen functions of\u03ba(soDis a potentially in\ufb01nite dimensional space).\nFor example, consider the (non-stationary) polynomial kernel \u03ba(x,x/prime)=(\u03b3xTx/prime+r)M,\nwherer>0. One can show that the corresponding feature vector \u03c6(x)will contain all terms\nup to degree M. For example, if M=2,\u03b3=r=1andx,x/prime\u2208R2,w eh a v e\n(1+xTx/prime)2=( 1 + x1x/prime\n1+x2x/prime2)2(14.14)\n=1 + 2 x1x/prime1+2x2x/prime2+(x1x1)2+(x2x/prime2)2+2x1x/prime1x2x/prime2(14.15)", "512": "482 Chapter 14. Kernels\nThis can be written as \u03c6(x)T\u03c6(x/prime),w h e r e\n\u03c6(x)=[ 1,\u221a\n2x1,\u221a\n2x2,x2\n1,x22,\u221a\n2x1x2]T(14.16)\nSo using this kernel is equivalent to working in a 6 dimensional feature space. In the case of\na Gaussian kernel, the feature map lives in an in\ufb01nite dimensional space. In such a case, it isclearly infeasible to explicitly represent the feature vectors.\nAn example of a kernel that is not a Mercer kernel is the so-called sigmoid kernel, de\ufb01ned\nby\n\u03ba(x,x\n/prime)=t a n h ( \u03b3xTx/prime+r) (14.17)\n(Note that this uses the tanhfunction even though it is called a sigmoid kernel.) This kernel\nwas inspired by the multi-layer perceptron (see Section 16.5), but there is no real reason to useit. (For a true \u201cneural net kernel\u201d, which is positive de\ufb01nite, see Section 15.4.5.)\nIn general, establishing that a kernel is a Mercer kernel is difficult, and requires techniques\nfrom functional analysis. However, one can show that it is possible to build up new Mercerkernels from simpler ones using a set of standard rules. For example, if \u03ba\n1and\u03ba2are both\nMercer, so is \u03ba(x,x/prime)=\u03ba1(x,x/prime)+\u03ba2(x,x/prime). See e.g., (Schoelkopf and Smola 2002) for details.\n14.2.4 Linear kernels\nDeriving the feature vector implied by a kernel is in general quite difficult, and only possible ifthe kernel is Mercer. However, deriving a kernel from a feature vector is easy: we just use\n\u03ba(x,x\n/prime)=\u03c6(x)T\u03c6(x/prime)=/angbracketleft\u03c6(x),\u03c6(x/prime)/angbracketright (14.18)\nIf\u03c6(x)=x,w eg e tt h elinear kernel, de\ufb01ned by\n\u03ba(x,x/prime)=xTx/prime(14.19)\nThis is useful if the original data is already high dimensional, and if the original features areindividually informative, e.g., a bag of words representation where the vocabulary size is large,or the expression level of many genes. In such a case, the decision boundary is likely to berepresentable as a linear combination of the original features, so it is not necessary to work insome other feature space.\nOf course, not all high dimensional problems are linearly separable. For example, images are\nhigh dimensional, but individual pixels are not very informative, so image classi\ufb01cation typicallyrequires non-linear kernels (see e.g., Section 14.2.7).\n14.2.5 Matern kernels\nTheMatern kernel, which is commonly used in Gaussian process regression (see Section 15.2),\nhas the following form\n\u03ba(r)=21\u2212\u03bd\n\u0393(\u03bd)/parenleftBigg\u221a\n2\u03bdr\n/lscript/parenrightBigg\u03bd\nK\u03bd/parenleftBigg\u221a\n2\u03bdr\n/lscript/parenrightBigg\n(14.20)", "513": "14.2. Kernel functions 483\nwherer=||x\u2212x/prime||,\u03bd>0,/lscript>0, and K\u03bdis a modi\ufb01ed Bessel function. As \u03bd\u2192\u221e, this\napproaches the SE kernel. If \u03bd=1\n2, the kernel simpli\ufb01es to\n\u03ba(r)=e x p (\u2212r//lscript) (14.21)\nIfD=1, and we use this kernel to de\ufb01ne a Gaussian process (see Chapter 15), we get the\nOrnstein-Uhlenbeck process, which describes the velocity of a particle undergoing Brownian\nmotion (the corresponding function is continuous but not differentiable, and hence is very\u201cjagged\u201d).\n14.2.6 String kernels\nThe real power of kernels arises when the inputs are structured objects. As an example, we nowdescribe one way of comparing two variable length strings using a string kernel. We follow the\npresentation of (Rasmussen and Williams 2006, p100) and (Hastie et al. 2009, p668).\nConsider two strings x, andx\n/primeof lengths D,D\u2019, each de\ufb01ned over the alphabet A.\nFor example, consider two amino acid sequences, de\ufb01ned over the 20 letter alphabet A=\n{A,R,N,D,C,E,Q,G,H,I,L,K,M,F,P,S,T,W,Y,V }.L e txbe the following sequence\nof length 110\nIPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTLESQTVQGGTV\nERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDY LQEFLGVMNTEWI\nand letx/primebe the following sequence of length 153\nPHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAER LQENLQAYRTFHVLLA\nRLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEADGMLFEKKLWGLKV LQE\nLSQWTVRSIHDLRFISSHQTGIP\nThese strings have the substring LQEin common. We can de\ufb01ne the similarity of two strings\nto be the number of substrings they have in common.\nMore formally and more generally, let us say that sis a substring of xif we can write x=usv\nfor some (possibly empty) strings u,sandv. Now let \u03c6s(x)denote the number of times that\nsubstring sappears in string x. We de\ufb01ne the kernel between two strings xandx/primeas\n\u03ba(x,x/prime)=/summationdisplay\ns\u2208A\u2217ws\u03c6s(x)\u03c6s(x/prime) (14.22)\nwherews\u22650andA\u2217is the set of all strings (of any length) from the alphabet A(this is known\nas the Kleene star operator). This is a Mercer kernel, and be computed in O(|x|+|x/prime|)time\n(for certain settings of the weights {ws}) using suffix trees (Leslie et al. 2003; Vishwanathan and\nSmola 2003; Shawe-Taylor and Cristianini 2004).\nThere are various cases of interest. If we set ws=0for|s|>1we get a bag-of-characters\nkernel. This de\ufb01nes \u03c6(x)to be the number of times each character in Aoccurs in x.I fw e\nrequiresto be bordered by white-space, we get a bag-of-words kernel, where \u03c6(x)counts how\nmany times each possible word occurs. Note that this is a very sparse vector, since most words", "514": "484 Chapter 14. Kernels\noptimal partial \nmatching\nmatching\nFigure 14.1 Illustration of a pyramid match kernel computed from two images. Used with kind permission\nof Kristen Grauman.\nwill not be present. If we only consider strings of a \ufb01xed length k, we get the k-spectrum\nkernel. This has been used to classify proteins into SCOP superfamilies (Leslie et al. 2003). For\nexample if k=3,w eh a v e \u03c6LQE(x)=1and\u03c6LQE(x/prime)=2for the two strings above.\nVarious extensions are possible. For example, we can allow character mismatches (Leslie et al.\n2003). And we can generalize string kernels to compare trees, as described in (Collins and Duffy\n2002). This is useful for classifying (or ranking) parse trees, evolutionary trees, etc.\n14.2.7 Pyramid match kernels\nIn computer vision, it is common to create a bag-of-words representation of an image by\ncomputing a feature vector (often using SIFT (Lowe 1999)) from a variety of points in the image,\ncommonly chosen by an interest point detector. The feature vectors at the chosen places are\nthen vector-quantized to create a bag of discrete symbols.\nOne way to compare two variable-sized bags of this kind is to use a pyramid match kernel\n(Grauman and Darrell 2007). The basic idea is illustrated in Figure 14.1. Each feature set is\nmapped to a multi-resolution histogram. These are then compared using weighted histogram\nintersection. It turns out that this provides a good approximation to the similarity measure one\nwould obtain by performing an optimal bipartite match at the \ufb01nest spatial resolution, and then\nsumming up pairwise similarities between matched points. However, the histogram method is\nfaster and is more robust to missing and unequal numbers of points. This is a Mercer kernel.", "515": "14.2. Kernel functions 485\n14.2.8 Kernels derived from probabilistic generative models\nSuppose we have a probabilistic generative model of feature vectors, p(x|\u03b8). Then there are\nseveral ways we can use this model to de\ufb01ne kernel functions, and thereby make the model\nsuitable for discriminative tasks. We sketch two approaches below.\n14.2.8.1 Probability product kernels\nOne approach is to de\ufb01ne a kernel as follows:\n\u03ba(xi,xj)=/integraldisplay\np(x|xi)\u03c1p(x|xj)\u03c1dx (14.23)\nwhere\u03c1>0, and p(x|xi)is often approximated by p(x|\u02c6\u03b8(xi)),w h e r e \u02c6\u03b8(xi)is a parameter\nestimate computed using a single data vector. This is called a probability product kernel\n(Jebara et al. 2004).\nAlthough it seems strange to \ufb01t a model to a single data point, it is important to bear in\nmind that the \ufb01tted model is only being used to see how similar two objects are. In particular,if we \ufb01t the model to x\niand then the model thinks xjis likely, this means that xiandxjare\nsimilar. For example, suppose p(x|\u03b8)=N(\u03bc,\u03c32I),w h e r e\u03c32is \ufb01xed. If \u03c1=1, and we use\n\u02c6\u03bc(xi)=xiand\u02c6\u03bc(xj)=xj, we \ufb01nd (Jebara et al. 2004, p825) that\n\u03ba(xi,xj)=1\n(4\u03c0\u03c32)D/2exp/parenleftbigg\n\u22121\n4\u03c32||xi\u2212xj||2/parenrightbigg\n(14.24)\nwhich is (up to a constant factor) the RBF kernel.\nIt turns out that one can compute Equation 14.23 for a variety of generative models, including\nones with latent variables, such as HMMs. This provides one way to de\ufb01ne kernels on variablelength sequences. Furthermore, this technique works even if the sequences are of real-valuedvectors, unlike the string kernel in Section 14.2.6. See (Jebara et al. 2004) for further details.\n14.2.8.2 Fisher kernels\nA more efficient way to use generative models to de\ufb01ne kernels is to use a Fisher kernel\n(Jaakkola and Haussler 1998) which is de\ufb01ned as follows:\n\u03ba(x,x\n/prime)=g(x)TF\u22121g(x/prime) (14.25)\nwheregis the gradient of the log likelihood, or score vector, evaluated at the MLE \u02c6\u03b8\ng(x)/defines\u2207\u03b8logp(x|\u03b8)/vextendsingle/vextendsingle\u02c6\u03b8(14.26)\nandFis the Fisher information matrix, which is essentially the Hessian:\nF=\u2207\u2207logp(x|\u03b8)/vextendsingle/vextendsingle\n\u02c6\u03b8(14.27)\nNote that \u02c6\u03b8is a function of all the data, so the similarity of xandx/primeis computed in the context\nof all the data as well. Also, note that we only have to \ufb01t one model.\nThe intuition behind the Fisher kernel is the following: let g(x)be the direction (in parameter\nspace) in which xwould like the parameters to move (from \u02c6\u03b8) so as to maximize its own", "516": "486 Chapter 14. Kernels\n(a)0 1 2 3 4 5 6\u2212101234567poly10\n(b)0 1 2 3 4 5 6\u2212101234567rbf prototypes\n(c)\nFigure 14.2 (a) xor truth table. (b) Fitting a linear logistic regression classi\ufb01er using degree 10 polynomial\nexpansion. (c) Same model, but using an RBF kernel with centroids speci\ufb01ed by the 4 black crosses. Figure\ngenerated by logregXorDemo .\nlikelihood; call this the directional gradient. Then we say that two vectors xandx/primeare similar\nif their directional gradients are similar wrt the the geometry encoded by the curvature of the\nlikelihood function (see Section 7.5.3).\nInterestingly, it was shown in (Saunders et al. 2003) that the string kernel of Section 14.2.6\nis equivalent to the Fisher kernel derived from an L\u2019th order Markov chain (see Section 17.2).\nAlso, it was shown in (Elkan 2005) that a kernel de\ufb01ned by the inner product of TF-IDF vectors\n(Section 14.2.2) is approximately equal to the Fisher kernel for a certain generative model of text\nbased on the compound Dirichlet multinomial model (Section 3.5.5).\n14.3 Using kernels inside GLMs\nIn this section, we discuss one simple way to use kernels for classi\ufb01cation and regression. We\nwill see other approaches later.\n14.3.1 Kernel machines\nWe de\ufb01ne a kernel machine to be a GLM where the input feature vector has the form\n\u03c6(x)=[\u03ba(x,\u03bc1),...,\u03ba(x,\u03bcK)] (14.28)\nwhere\u03bck\u2208Xa r eas e to f Kcentroids .I f\u03bais an RBF kernel, this is called an RBF network .\nWe discuss ways to choose the \u03bckparameters below. We will call Equation 14.28 a kernelised\nfeature vector . Note that in this approach, the kernel need not be a Mercer kernel.\nWe can use the kernelized feature vector for logistic regression by de\ufb01ning p(y|x,\u03b8)=\nBer(wT\u03c6(x)). This provides a simple way to de\ufb01ne a non-linear decision boundary. As an\nexample, consider the data coming from the exclusive or orxorfunction. This is a binary-\nvalued function of two binary inputs. Its truth table is shown in Figure 14.2(a). In Figure 14.2(b),\nwe have show some data labeled by the xor function, but we have jitteredthe points to make\nthe picture clearer.1We see we cannot separate the data even using a degree 10 polynomial.\n1. Jittering is a common visualization trick in statistics, wherein points in a plot/display that would otherwise land on\ntop of each other are dispersed with uniform additive noise.", "517": "14.3. Using kernels inside GLMs 487\n0 5 10 15 20\u22121001020\n0 5 10 15 2000.20.40.60.8\n2 4 6 8 105\n10\n15\n20\n0 5 10 15 20\u22121001020\n0 5 10 15 2000.010.020.030.04\n2 4 6 8 105\n10\n15\n20\n0 5 10 15 20\u22121001020\n0 5 10 15 207.27.47.67.88x 10\u22123\n2 4 6 8 105\n10\n15\n20\nFigure 14.3 RBF basis in 1d. Left column: \ufb01tted function. Middle column: basis functions evaluated on\na grid. Right column: design matrix. Top to bottom we show different bandwidths: \u03c4=0.1,\u03c4=0.5,\n\u03c4=5 0. Figure generated by linregRbfDemo .\nHowever, using an RBF kernel and just 4 prototypes easily solves the problem as shown in\nFigure 14.2(c).\nWe can also use the kernelized feature vector inside a linear regression model by de\ufb01ning\np(y|x,\u03b8)=N(wT\u03c6(x),\u03c32). For example, Figure 14.3 shows a 1d data set \ufb01t with K=1 0\nuniformly spaced RBF prototypes, but with the bandwidth ranging from small to large. Small\nvalues lead to very wiggly functions, since the predicted function value will only be non-zero for\npointsxthat are close to one of the prototypes \u03bck. If the bandwidth is very large, the design\nmatrix reduces to a constant matrix of 1\u2019s, since each point is equally close to every prototype;\nhence the corresponding function is just a straight line.\n14.3.2 L1VMs, RVMs, and other sparse vector machines\nThe main issue with kernel machines is: how do we choose the centroids \u03bck? If the input is\nlow-dimensional Euclidean space, we can uniformly tile the space occupied by the data with\nprototypes, as we did in Figure 14.2(c). However, this approach breaks down in higher numbers\nof dimensions because of the curse of dimensionality. If \u03bck\u2208RD, we can try to perform\nnumerical optimization of these parameters (see e.g., (Haykin 1998)), or we can use MCMC\ninference, (see e.g., (Andrieu et al. 2001; Kohn et al. 2001)), but the resulting objective function\n/ posterior is highly multimodal. Furthermore, these techniques is hard to extend to structured\ninput spaces, where kernels are most useful.\nAnother approach is to \ufb01nd clusters in the data and then to assign one prototype per cluster", "518": "488 Chapter 14. Kernels\ncenter (many clustering algorithms just need a similarity metric as input). However, the regions\nof space that have high density are not necessarily the ones where the prototypes are mostuseful for representing the output, that is, clustering is an unsupervised task that may not yielda representation that is useful for prediction. Furthermore, there is the need to pick the numberof clusters.\nA simpler approach is to make each example x\nibe a prototype, so we get\n\u03c6(x)=[\u03ba(x,x1),...,\u03ba(x,xN)] (14.29)\nNow we see D=N, so we have as many parameters as data points. However, we can use any\nof the sparsity-promoting priors for wdiscussed in Chapter 13 to efficiently select a subset of\nthe training exemplars. We call this a sparse vector machine.\nThe most natural choice is to use /lscript1regularization (Krishnapuram et al. 2005). (Note that in\nthe multi-class case, it is necessary to use group lasso, since each exemplar is associated with C\nweights, one per class.) We call this L1VM, which stands for \u201c/lscript 1-regularized vector machine\u201d. By\nanalogy, we de\ufb01ne the use of an /lscript2regularizer to be a L2VMor \u201c/lscript2-regularized vector machine\u201d;\nthis of course will not be sparse.\nWe can get even greater sparsity by using ARD/SBL, resulting in a method called the rele-\nvance vector machine orRVM(Tipping 2001). One can \ufb01t this model using generic ARD/SBL\nalgorithms, although in practice the most common method is the greedy algorithm in (Tippingand Faul 2003) (this is the algorithm implemented in Mike Tipping\u2019s code, which is bundled withPMTK).\nAnother very popular approach to creating a sparse kernel machine is to use a support\nvector machine orSVM. This will be discussed in detail in Section 14.5. Rather than using a\nsparsity-promoting prior, it essentially modi\ufb01es the likelihood term, which is rather unnaturalfrom a Bayesian point of view. Nevertheless, the effect is similar, as we will see.\nIn Figure 14.4, we compare L2VM, L1VM, RVM and an SVM using the same RBF kernel on a\nbinary classi\ufb01cation problem in 2d. For simplicity, \u03bbwas chosen by hand for L2VM and L1VM;\nfor RVMs, the parameters are estimated using empirical Bayes; and for the SVM, we use CV topickC=1/\u03bb, since SVM performance is very sensitive to this parameter (see Section 14.5.3).\nWe see that all the methods give similar performance. However, RVM is the sparsest (and hencefastest at test time), then L1VM, and then SVM. RVM is also the fastest to train, since CV for anSVM is slow. (This is despite the fact that the RVM code is in Matlab and the SVM code is inC.) This result is fairly typical.\nIn Figure 14.5, we compare L2VM, L1VM, RVM and an SVM using an RBF kernel on a 1d\nregression problem. Again, we see that predictions are quite similar, but RVM is the sparsest,then L2VM, then SVM. This is further illustrated in Figure 14.6.\n14.4 The kernel trick\nRather than de\ufb01ning our feature vector in terms of kernels, \u03c6(x)=[\u03ba(x,x1),...,\u03ba(x,xN)],\nwe can instead work with the original feature vectors x, but modify the algorithm so that it\nreplaces all inner products of the form /angbracketleftx,x/prime/angbracketrightwith a call to the kernel function, \u03ba(x,x/prime). This\nis called the kernel trick. It turns out that many algorithms can be kernelized in this way. We\ngive some examples below. Note that we require that the kernel be a Mercer kernel for this trickto work.", "519": "14.4. The kernel trick 489\n\u22122 \u22121 0 1 2 3\u22123\u22122\u221210123logregL2, nerr=174\n(a)\u22122 \u22121 0 1 2 3\u22123\u22122\u221210123logregL1, nerr=169\n(b)\n\u22122 \u22121 0 1 2 3\u22123\u22122\u221210123RVM, nerr=173\n(c)\u22122 \u22121 0 1 2 3\u22123\u22122\u221210123SVM, nerr=173\n(d)\nFigure 14.4 Example of non-linear binary classi\ufb01cation using an RBF kernel with bandwidth \u03c3=0.3. (a)\nL2VM with \u03bb=5. (b) L1VM with \u03bb=1. (c) RVM. (d) SVM with C=1/\u03bbchosen by cross validation.\nBlack circles denote the support vectors. Figure generated by kernelBinaryClassifDemo .\n14.4.1 Kernelized nearest neighbor classi\ufb01cation\nRecall that in a 1NN classi\ufb01er (Section 1.4.2), we just need to compute the Euclidean distance of\na test vector to all the training points, \ufb01nd the closest one, and look up its label. This can bekernelized by observing that\n||x\ni\u2212xi/prime||2\n2=/angbracketleftxi,xi/angbracketright+/angbracketleftxi/prime,xi/prime/angbracketright\u22122/angbracketleftxi,xi/prime/angbracketright (14.30)\nThis allows us to apply the nearest neighbor classi\ufb01er to structured data objects.\n14.4.2 Kernelized K-medoids clustering\nK-means clustering (Section 11.4.2.5) uses Euclidean distance to measure dissimilarity, which is\nnot always appropriate for structured objects. We now describe how to develop a kernelized", "520": "490 Chapter 14. Kernels\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2\u22120.4\u22120.200.20.40.60.811.2linregL2\n(a)\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2\u22120.4\u22120.200.20.40.60.811.2linregL1\n(b)\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2\u22120.4\u22120.200.20.40.60.811.2RVM\n(c)\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2\u22120.4\u22120.200.20.40.60.811.2SVM\n(d)\nFigure 14.5 Example of kernel based regression on the noisy sinc function using an RBF kernel with\nbandwidth \u03c3=0.3. (a) L2VM with \u03bb=0.5. (b) L1VM with \u03bb=0.5. (c) RVM. (d) SVM regression with\nC=1/\u03bbchosen by cross validation, and /epsilon1=0.1(the default for SVMlight). Red circles denote the\nretained training exemplars. Figure generated by kernelRegrDemo .\nversion of the algorithm.\nThe \ufb01rst step is to replace the K-means algorithm with the K-medoids algorothm. This is\nsimilar to K-means, but instead of representing each cluster\u2019s centroid by the mean of all data\nvectors assigned to this cluster, we make each centroid be one of the data vectors themselves.Thus we always deal with integer indexes, rather than data objects. We assign objects to theirclosest centroids as before. When we update the centroids, we look at each object that belongsto the cluster, and measure the sum of its distances to all the others in the same cluster; wethen pick the one which has the smallest such sum:\nm\nk=a r g m i n\ni:zi=k/summationdisplay\ni/prime:zi/prime=kd(i,i/prime) (14.31)", "521": "14.4. The kernel trick 491\n0 20 40 60 80 100\u22120.06\u22120.04\u22120.0200.020.040.06weights for linregL2\n(a)0 20 40 60 80 100\u22120.06\u22120.04\u22120.0200.020.040.060.080.10.12weights for linregL1\n(b)\n0 20 40 60 80 100\u22120.200.20.40.60.81weights for RVM\n(c)0 20 40 60 80 100\u22121.5\u22121\u22120.500.511.5weights for SVM\n(d)\nFigure 14.6 Coefficient vectors of length N= 100for the models in Figure 14.6. Figure generated by\nkernelRegrDemo .\nwhere\nd(i,i/prime)/defines||xi\u2212xi/prime||2\n2 (14.32)\nThis takes O(n2\nk)work per cluster, whereas K-means takes O(nkD)to update each cluster. The\npseudo-code is given in Algorithm 5. This method can be modi\ufb01ed to derive a classi\ufb01er, by\ncomputing the nearest medoid for each class. This is known as nearest medoid classi\ufb01cation\n(Hastie et al. 2009, p671).\nThis algorithm can be kernelized by using Equation 14.30 to replace the distance computation,\nd(i,i/prime).", "522": "492 Chapter 14. Kernels\nAlgorithm 14.1: K-medoids algorithm\n1initialize m1:Kas a random subset of size Kfrom{1,...,N };\n2repeat\n3zi=a r g m i nkd(i,mk)fori=1:N;\n4mk\u2190argmini:zi=k/summationtext\ni/prime:zi/prime=kd(i,i/prime)fork=1:K;\n5until converged ;\n14.4.3 Kernelized ridge regression\nApplying the kernel trick to distance-based methods was straightforward. It is not so obvious\nhow to apply it to parametric models such as ridge regression. However, it can be done, as wenow explain. This will serve as a good \u201cwarm up\u201d for studying SVMs.\n14.4.3.1 The primal problem\nLetx\u2208R\nDbe some feature vector, and Xbe the corresponding N\u00d7Ddesign matrix. We\nwant to minimize\nJ(w)=(y\u2212Xw)T(y\u2212Xw)+\u03bb||w||2(14.33)\nThe optimal solution is given by\nw=(XTX+\u03bbID)\u22121XTy=(/summationdisplay\nixixT\ni+\u03bbID)\u22121XTy (14.34)\n14.4.3.2 The dual problem\nEquation 14.34 is not yet in the form of inner products. However, using the matrix inversion\nlemma (Equation 4.107) we rewrite the ridge estimate as follows\nw=XT(XXT+\u03bbIN)\u22121y (14.35)\nwhich takes O(N3+N2D)time to compute. This can be advantageous if Dis large. Further-\nmore, we see that we can partially kernelize this, by replacing XXTwith the Gram matrix K.\nBut what about the leading XTterm?\nLet us de\ufb01ne the following dual variables:\n\u03b1/defines(K+\u03bbIN)\u22121y (14.36)\nThen we can rewrite the primal variables as follows\nw=XT\u03b1=N/summationdisplay\ni=1\u03b1ixi (14.37)\nThis tells us that the solution vector is just a linear sum of the Ntraining vectors. When we\nplug this in at test time to compute the predictive mean, we get\n\u02c6f(x)=wTx=N/summationdisplay\ni=1\u03b1ixT\nix=N/summationdisplay\ni=1\u03b1i\u03ba(x,xi) (14.38)", "523": "14.4. The kernel trick 493\n\u22121 0 1\u22120.500.511.5Eigenvalue=22.558\n\u22121 0 1\u22120.500.511.5Eigenvalue=20.936\n\u22121 0 1\u22120.500.511.5Eigenvalue=4.648\n\u22121 0 1\u22120.500.511.5Eigenvalue=3.988\n\u22121 0 1\u22120.500.511.5Eigenvalue=3.372\n\u22121 0 1\u22120.500.511.5Eigenvalue=2.956\n\u22121 0 1\u22120.500.511.5Eigenvalue=2.760\n\u22121 0 1\u22120.500.511.5Eigenvalue=2.211\nFigure 14.7 Visualization of the \ufb01rst 8 kernel principal component basis functions derived from some 2d\ndata. We use an RBF kernel with \u03c32=0.1. Figure generated by kpcaScholkopf , written by Bernhard\nScholkopf.\nSo we have succesfully kernelized ridge regression by changing from primal to dual variables.\nThis technique can be applied to many other linear models, such as logistic regression.\n14.4.3.3 Computational cost\nThe cost of computing the dual variables \u03b1isO(N3), whereas the cost of computing the primal\nvariables wisO(D3). Hence the kernel method can be useful in high dimensional settings,\neven if we only use a linear kernel (c.f., the SVD trick in Equation 7.44). However, predictionusing the dual variables takes O(ND)time, while prediction using the primal variables only\ntakesO(D)time. We can speedup prediction by making \u03b1sparse, as we discuss in Section 14.5.\n14.4.4 Kernel PCA\nIn Section 12.2, we saw how we could compute a low-dimensional linear embedding of somedata using PCA. This required \ufb01nding the eigenvectors of the sample covariance matrix S=", "524": "494 Chapter 14. Kernels\n1\nN/summationtextN\ni=1xixT\ni=( 1/N)XTX. However, we can also compute PCA by \ufb01nding the eigenvectors\nof the inner product matrix XXT, as we show below. This will allow us to produce a nonlinear\nembedding, using the kernel trick, a method known as kernel PCA (Schoelkopf et al. 1998).\nFirst, letUbe an orthogonal matrix containing the eigenvectors of XXTwith corresponding\neigenvalues in \u039b. By de\ufb01nition we have (XXT)U=U\u039b. Pre-multiplying by XTgives\n(XTX)(XTU)=(XTU)\u039b (14.39)\nfrom which we see that the eigenvectors of XTX(and hence of S)a r eV=XTU, with eigen-\nvalues given by \u039bas before. However, these eigenvectors are not normalized, since ||vj||2=\nuTjXXTuj=\u03bbjuTjuj=\u03bbj. So the normalized eigenvectors are given by Vpca=XTU\u039b\u22121\n2.\nThis is a useful trick for regular PCA if D>N, sinceXTXhas sizeD\u00d7D, whereas XXT\nhas sizeN\u00d7N. It will also allow us to use the kernel trick, as we now show.\nNow letK=XXTbe the Gram matrix. Recall from Mercer\u2019s theorem that the use of a kernel\nimplies some underlying feature space, so we are implicitly replacing xiwith\u03c6(xi)=\u03c6i.L e t\n\u03a6be the corresponding (notional) design matrix, and S\u03c6=1\nN/summationtext\ni\u03c6i\u03c6T\nibe the corresponding\n(notional) covariance matrix in feature space. The eigenvectors are given by Vkpca=\u03a6TU\u039b\u22121\n2,\nwhereUand\u039bcontain the eigenvectors and eigenvalues of K. Of course, we can\u2019t actually\ncompute Vkpca, since\u03c6iis potentially in\ufb01nite dimensional. However, we can compute the\nprojection of a test vector x\u2217onto the feature space as follows:\n\u03c6T\u2217Vkpca=\u03c6T\u2217\u03a6U\u039b\u22121\n2=kT\n\u2217U\u039b\u22121\n2 (14.40)\nwherek\u2217=[\u03ba(x\u2217,x1),...,\u03ba(x\u2217,xN)].\nThere is one \ufb01nal detail to worry about. So far, we have assumed the projected data has\nzero mean, which is not the case in general. We cannot simply subtract off the mean in\nfeature space. However, there is a trick we can use. De\ufb01ne the centered feature vector as\n\u02dc\u03c6i=\u03c6(xi)\u22121\nN/summationtextN\nj=1\u03c6(xj). The Gram matrix of the centered feature vectors is given by\n\u02dcKij=\u02dc\u03c6T\ni\u02dc\u03c6j (14.41)\n=\u03c6T\ni\u03c6j\u22121\nNN/summationdisplay\nk=1\u03c6Ti\u03c6k\u22121\nNN/summationdisplay\nk=1\u03c6Tj\u03c6k+1\nN2N/summationdisplay\nk=1M/summationdisplay\nl=1\u03c6Tk\u03c6l (14.42)\n=\u03ba(xi,xj)\u22121\nNN/summationdisplay\nk=1\u03ba(xi,xk)\u22121\nNN/summationdisplay\nk=1\u03ba(xj,xk)+1\nN2N/summationdisplay\nk=1M/summationdisplay\nl=1\u03ba(xk,xl)(14.43)\nThis can be expressed in matrix notation as follows:\n\u02dcK=HKH (14.44)\nwhereH/definesI\u22121\nN1N1T\nN. is thecentering matrix. We can convert all this algebra into the\npseudocode shown in Algorithm 9.\nWhereas linear PCA is limited to using L\u2264Dcomponents, in kPCA, we can use up to N\ncomponents, since the rank of \u03a6isN\u00d7D\u2217,w h e r eD\u2217is the (potentially in\ufb01nite) dimensionality\nof embedded feature vectors. Figure 14.7 gives an example of the method applied to some\nD=2dimensional data using an RBF kernel. We project points in the unit grid onto the \ufb01rst", "525": "14.4. The kernel trick 495\nAlgorithm 14.2: Kernel PCA\n1Input:Kof sizeN\u00d7N,K\u2217of sizeN\u2217\u00d7N, num. latent dimensions L;\n2O=1N1T\nN/N;\n3\u02dcK=K\u2212OK\u2212KO+OKO;\n4[U,\u039b]=eig( \u02dcK);\n5fori=1:Ndo\n6vi=ui/\u221a\u03bbi\n7O\u2217=1N\u22171TN/N;\n8\u02dcK\u2217=K\u2217\u2212O\u2217K\u2217\u2212K\u2217O\u2217+O\u2217K\u2217O\u2217;\n9Z=\u02dcK\u2217V(:,1:L)\n\u22120.6 \u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8\u22120.8\u22120.6\u22120.4\u22120.200.20.40.6pca\n(a)\u22120.8 \u22120.6 \u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8\u22120.8\u22120.6\u22120.4\u22120.200.20.40.6kpca\n(b)\nFigure 14.8 2d visualization of some 2d data. (a) PCA projection. (b) Kernel PCA projection. Figure\ngenerated by kpcaDemo2 , based on code by L.J.P. van der Maaten.\n8 components and visualize the corresponding surfaces using a contour plot. We see that the\n\ufb01rst two component separate the three clusters, and following components split the clusters.\nAlthough the features learned by kPCA can be useful for classi\ufb01cation (Schoelkopf et al. 1998),\nthey are not necessarily so useful for data visualization. For example, Figure 14.8 shows theprojection of the data from Figure 14.7 onto the \ufb01rst 2 principal bases computed using PCA andkPCA. Obviously PCA perfectly represents the data. kPCA represents each cluster by a differentline.\nOf course, there is no need to project 2d data back into 2d. So let us consider a different\ndata set. We will use a 12 dimensional data set representing the three known phases of \ufb02owin an oil pipeline. (This data, which is widely used to compare data visualization methods, issynthetic, and comes from (Bishop and James 1993).) We project this into 2d using PCA andkPCA (with an RBF kernel). The results are shown in Figure 14.9. If we perform nearest neighborclassi\ufb01cation in the low-dimensional space, kPCA makes 13 errors and PCA makes 20 (Lawrence", "526": "496 Chapter 14. Kernels\n\u22122 \u22121 0 1 2\u221232.5\u221221.5\u221210.500.5\n(a)0 0.05 0.1 0.15 0.20.050.1.150.2.250.3\n(b)\nFigure 14.9 2d representation of 12 dimensional oil \ufb02ow data. The different colors/symbols represent the\n3 phases of oil \ufb02ow. (a) PCA. (b) Kernel PCA with Gaussian kernel. Compare to Figure 15.10(b). From Figure\n1 of (Lawrence 2005). Used with kind permission of Neil Lawrence.\n2005). Nevertheless, the kPCA projection is rather unnatural. In Section 15.5, we will discuss\nhow to make kernelized versions of probabilistic PCA.\nNote that there is a close connection between kernel PCA and a technique known as mul-\ntidimensional scaling or MDS. This methods \ufb01nds a low-dimensional embedding such thatEuclidean distance in the embedding space approximates the original dissimilarity matrix. Seee.g., (Williams 2002) for details.\n14.5 Support vector machines (SVMs)\nIn Section 14.3.2, we saw one way to derive a sparse kernel machine, namely by using a GLMwith kernel basis functions, plus a sparsity-promoting prior such as /lscript\n1or ARD. An alternative\napproach is to change the objective function from negative log likelihood to some other lossfunction, as we discussed in Section 6.5.5. In particular, consider the /lscript\n2regularized empirical\nrisk function\nJ(w,\u03bb)=N/summationdisplay\ni=1L(yi,\u02c6yi)+\u03bb||w||2(14.45)\nwhere\u02c6yi=wTxi+w0. (So far this is in the original feature space; we introduce kernels in a\nmoment.) If Lis quadratic loss, this is equivalent to ridge regression, and if Lis the log-loss\nde\ufb01ned in Equation 6.73, this is equivalent to logistic regression.\nIn the ridge regression case, we know that the solution to this has the form \u02c6w=(XTX+\n\u03bbI)\u22121XTy, and plug-in predictions take the form \u02c6w0+\u02c6wTx. As we saw in Section 14.4.3,\nwe can rewrite these equations in a way that only involves inner products of the form xTx/prime,\nwhich we can replace by calls to a kernel function, \u03ba(x,x/prime). This is kernelized, but not sparse.\nHowever, if we replace the quadratic/ log-loss with some other loss function, to be explainedbelow, we can ensure that the solution is sparse, so that predictions only depend on a subsetof the training data, known as support vectors. This combination of the kernel trick plus a\nmodi\ufb01ed loss function is known as a support vector machine orSVM. This technique was", "527": "14.5. Support vector machines (SVMs) 497\n\u22123 \u22122 \u22121 0 1 2 3\u22120.500.511.522.533.544.55\n  \nL2\n\u03b5\u2212insensitive\nhuber\n(a)xy(x)\ny\u2212/epsilon1yy+/epsilon1\n\u03be\u2217>0\u03be>0\n(b)\nFigure 14.10 (a) Illustration of /lscript2, Huber and /epsilon1-insensitive loss functions, where /epsilon1=1.5. Figure generated\nbyhuberLossDemo . (b) Illustration of the /epsilon1-tube used in SVM regression. Points above the tube have\n\u03bei>0and\u03be\u2217\ni=0. Points below the tube have \u03bei=0and\u03be\u2217\ni>0. Points inside the tube have\n\u03bei=\u03be\u2217\ni=0. Based on Figure 7.7 of (Bishop 2006a).\noriginally designed for binary classi\ufb01cation, but can be extended to regression and multi-class\nclassi\ufb01cation as we explain below.\nNote that SVMs are very unnatural from a probabilistic point of view. First, they encode\nsparsity in the loss function rather than the prior. Second, they encode kernels by using analgorithmic trick, rather than being an explicit part of the model. Finally, SVMs do not result inprobabilistic outputs, which causes various difficulties, especially in the multi-class classi\ufb01cationsetting (see Section 14.5.2.4 for details).\nIt is possible to obtain sparse, probabilistic, multi-class kernel-based classi\ufb01ers, which work as\nwell or better than SVMs, using techniques such as the L1VM or RVM, discussed in Section 14.3.2.However, we include a discussion of SVMs, despite their non-probabilistic nature, for two mainreasons. First, they are very popular and widely used, so all students of machine learning shouldknow about them. Second, they have some computational advantages over probabilistic methodsin the structured output case; see Section 19.7.\n14.5.1 SVMs for regression\nThe problem with kernelized ridge regression is that the solution vector wdepends on all the\ntraining inputs. We now seek a method to produce a sparse estimate.\nVapnik (Vapnik et al. 1997) proposed a variant of the Huber loss function (Section 7.4) called\ntheepsilon insensitive loss function, de\ufb01ned by\nL/epsilon1(y,\u02c6y)/defines/braceleftbigg0if|y\u2212\u02c6y|</epsilon1\n|y\u2212\u02c6y|\u2212/epsilon1otherwise(14.46)\nThis means that any point lying inside an /epsilon1-tubearound the prediction is not penalized, as in\nFigure 14.10.\nThe corresponding objective function is usually written in the following form\nJ=CN/summationdisplay\ni=1L/epsilon1(yi,\u02c6yi)+1\n2||w||2(14.47)", "528": "498 Chapter 14. Kernels\nwhere\u02c6yi=f(xi)=wTxi+w0andC=1/\u03bbis a regularization constant. This objective is\nconvex and unconstrained, but not differentiable, because of the absolute value function in the\nloss term. As in Section 13.4, where we discussed the lasso problem, there are several possiblealgorithms we could use. One popular approach is to formulate the problem as a constrainedoptimization problem. In particular, we introduce slack variables to represent the degree to\nwhich each point lies outside the tube:\ny\ni\u2264f(xi)+/epsilon1+\u03be+\ni (14.48)\nyi\u2265f(xi)\u2212/epsilon1\u2212\u03be\u2212\ni (14.49)\nGiven this, we can rewrite the objective as follows:\nJ=CN/summationdisplay\ni=1(\u03be+\ni+\u03be\u2212\ni)+1\n2||w||2(14.50)\nThis is a quadratic function of w, and must be minimized subject to the linear constraints\nin Equations 14.48-14.49, as well as the positivity constraints \u03be+\ni\u22650and\u03be\u2212\ni\u22650. This is a\nstandard quadratic program in 2N+D+1variables.\nOne can show (see e.g., (Schoelkopf and Smola 2002)) that the optimal solution has the form\n\u02c6w=/summationdisplay\ni\u03b1ixi (14.51)\nwhere\u03b1i\u22650. Furthermore, it turns out that the \u03b1vector is sparse, because we don\u2019t care\nabout errors which are smaller than /epsilon1. Thexifor which \u03b1i>0are called the support vectors;\nthse are points for which the errors lie on or outside the /epsilon1tube.\nOnce the model is trained, we can then make predictions using\n\u02c6y(x)=\u02c6w0+\u02c6wTx (14.52)\nPlugging in the de\ufb01nition of \u02c6wwe get\n\u02c6y(x)=\u02c6w0+/summationdisplay\ni\u03b1ixT\nix (14.53)\nFinally, we can replace xT\nixwith\u03ba(xi,x)to get a kernelized solution:\n\u02c6y(x)=\u02c6w0+/summationdisplay\ni\u03b1i\u03ba(xi,x) (14.54)\n14.5.2 SVMs for classi\ufb01cation\nWe now discuss how to apply SVMs to classi\ufb01cation. We \ufb01rst focus on the binary case, and\nthen discuss the multi-class case in Section 14.5.2.4.\n14.5.2.1 Hinge loss\nIn Section 6.5.5, we showed that the negative log likelihood of a logistic regression model,\nLnll(y,\u03b7)=\u2212logp(y|x,w)=l o g ( 1+ e\u2212y\u03b7) (14.55)", "529": "14.5. Support vector machines (SVMs) 499\nwas a convex upper bound on the 0-1 risk of a binary classi\ufb01er, where \u03b7=f(x)=wTx+w0\nis the log odds ratio, and we have assumed the labels are y\u2208{1,\u22121}rather than {0,1}.I n\nthis section, we replace the NLL loss with the hinge loss, de\ufb01ned as\nLhinge(y,\u03b7)=m a x ( 0 ,1\u2212y\u03b7)=( 1 \u2212y\u03b7)+ (14.56)\nHere\u03b7=f(x)is our \u201ccon\ufb01dence\u201d in choosing label y=1; however, it need not have any\nprobabilistic semantics. See Figure 6.7 for a plot. We see that the function looks like a door\nhinge, hence its name. The overall objective has the form\nmin\nw,w01\n2||w||2+CN/summationdisplay\ni=1(1\u2212yif(xi))+ (14.57)\nOnce again, this is non-differentiable, because of the max term. However, by introducing slackvariables \u03be\ni, one can show that this is equivalent to solving\nmin\nw,w0,\u03be1\n2||w||2+CN/summationdisplay\ni=1\u03beis.t.\u03bei\u22650,yi(xT\niw+w0)\u22651\u2212\u03bei,i=1:N (14.58)\nThis is a quadratic program in N+D+1variables, subjet to O(N)constraints. We\ncan eliminate the primal variables w,w0and\u03bei, and just solve the Ndual variables, which\ncorrespond to the Lagrange multipliers for the constraints. Standard solvers take O(N3)time.\nHowever, specialized algorithms, which avoid the use of generic QP solvers, have been developed\nfor this problem, such as the sequential minimal optimization orSMOalgorithm (Platt 1998).\nIn practice this can take O(N2). However, even this can be too slow if Nis large. In such\nsettings, it is common to use linear SVMs, which take O(N)time to train (Joachims 2006; Bottou\net al. 2007).\nOne can show that the solution has the form\n\u02c6w=/summationdisplay\ni\u03b1ixi (14.59)\nwhere\u03b1i=\u03bbiyiand where \u03b1is sparse (because of the hinge loss). The xifor which \u03b1i>0are\ncalled support vectors; these are points which are either incorrectly classi\ufb01ed, or are classi\ufb01edcorrectly but are on or inside the margin (we disuss margins below). See Figure 14.12(b) for anillustration.\nAt test time, prediction is done using\n\u02c6y(x)=s g n (f(x)) = sgn/parenleftbig\n\u02c6w\n0+\u02c6wTx/parenrightbig\n(14.60)\nUsing Equation 14.59 and the kernel trick we have\n\u02c6y(x)=s g n/parenleftBigg\n\u02c6w0+N/summationdisplay\ni=1\u03b1i\u03ba(xi,x)/parenrightBigg\n(14.61)\nThis takes O(sD) time to compute, where s\u2264Nis the number of support vectors. This\ndepends on the sparsity level, and hence on the regularizer C.", "530": "500 Chapter 14. Kernels\nFigure 14.11 Illustration of the large margin principle. Left: a separating hyper-plane with large margin.\nRight: a separating hyper-plane with small margin.\nR1\nR0y=0y>0\ny<0\nwx\nr=f(x)\n/bardblw/bardbl\nx\u22a5\n\u2212w0\n/bardblw/bardbl\n(a)y=0\ny=1y=\u22121\n\u03be> 1\n\u03be< 1\n\u03be=0\u03be=0\n(b)\nFigure 14.12 (a) Illustration of the geometry of a linear decision boundary in 2d. A point xis classi\ufb01ed\nas belonging in decision region R1iff(x)>0, otherwise it belongs in decision region R2;h e r ef(x)\nis known as a discriminant function . The decision boundary is the set of points such that f(x)=0.\nwis a vector which is perpendicular to the decision boundary. The term w0controls the distance of\nthe decision boundary from the origin. The signed distance of xfrom its orthogonal projection onto the\ndecision boundary, x\u22a5, is given by f(x)/||w||. Based on Figure 4.1 of (Bishop 2006a). (b) Illustration of\nthe soft margin principle. Points with circles around them are support vectors. We also indicate the value\nof the corresponding slack variables. Based on Figure 7.3 of (Bishop 2006a).", "531": "14.5. Support vector machines (SVMs) 501\n14.5.2.2 The large margin principle\nIn this section, we derive Equation 14.58 form a completely different perspective. Recall that our\ngoal is to derive a discriminant function f(x)which will be linear in the feature space implied\nby the choice of kernel. Consider a point xin this induced space. Referring to Figure 14.12(a),\nwe see that\nx=x\u22a5+rw\n||w||(14.62)\nwhereris the distance of xfrom the decision boundary whose normal vector is w, andx\u22a5is\nthe orthogonal projection of xonto this boundary. Hence\nf(x)=wTx+w0=(wTx\u22a5+w0)+rwTw\n||w||(14.63)\nNowf(x\u22a5)=0so0=wTx\u22a5+w0. Hencef(x)=rwTw\u221a\nwTw, andr=f(x)\n||w||.\nWe would like to make this distance r=f(x)/||w||as large as possible, for reasons illustrated\nin Figure 14.11. In particular, there might be many lines that perfectly separate the training data(especially if we work in a high dimensional feature space), but intuitively, the best one to pickis the one that maximizes the margin, i.e., the perpendicular distance to the closest point. Inaddition, we want to ensure each point is on the correct side of the boundary, hence we wantf(x\ni)yi>0. So our objective becomes\nmax\nw,w0N\nmin\ni=1yi(wTxi+w0)\n||w||(14.64)\nNote that by rescaling the parameters using w\u2192kwandw0\u2192kw0, we do not change the\ndistance of any point to the boundary, since the kfactor cancels out when we divide by ||w||.\nTherefore let us de\ufb01ne the scale factor such that yifi=1for the point that is closest to the\ndecision boundary. We therefore want to optimize\nmin\nw,w01\n2||w||2s.t.yi(wTxi+w0)\u22651,i=1:N (14.65)\n(The fact of1\n2is added for convenience and doesn\u2019t affect the optimal parameters.) The\nconstraint says that we want all points to be on the correct side of the decision boundary witha margin of at least 1. For this reason, we say that an SVM is an example of a large margin\nclassi\ufb01er.\nIf the data is not linearly separable (even after using the kernel trick), there will be no feasible\nsolution in which y\nifi\u22651for alli. We therefore introduce slack variables \u03bei\u22650such that\n\u03bei=0if the point is on or inside the correct margin boundary, and \u03bei=|yi\u2212fi|otherwise. If\n0<\u03bei\u22641the point lies inside the margin, but on the correct side of the decision boundary.\nIf\u03bei>1, the point lies on the wrong side of the decision boundary. See Figure 14.12(b).\nWe replace the hard constraints that yifi\u22650with thesoft margin constraints thatyifi\u2265\n1\u2212\u03bei. The new objective becomes\nmin\nw,w0,\u03be1\n2||w||2+CN/summationdisplay\ni=1\u03beis.t.\u03bei\u22650,yi(xT\niw+w0)\u22651\u2212\u03bei (14.66)", "532": "502 Chapter 14. Kernels\n0.4 0.6 0.8 1 1.2\u22126\u22124\u221220246Correct log\u2212odds\nRVM y(x)\nSVM y(x)\nFigure 14.13 Log-odds vs xfor 3 different methods. Based on Figure 10 of (Tipping 2001). Used with kind\npermission of Mike Tipping.\nwhich is the same as Equation 14.58. Since \u03bei>1means point iis misclassi\ufb01ed, we can\ninterpret/summationtext\ni\u03beias an upper bound on the number of misclassi\ufb01ed points.\nThe parameter Cis a regularization parameter that controls the number of errors we are\nwilling to tolerate on the training set. It is common to de\ufb01ne this using C=1/(\u03bdN),w h e r e\n0<\u03bd\u22641controls the fraction of misclassi\ufb01ed points that we allow during the training phase.\nThis is called a \u03bd-SVM classi\ufb01er. This is usually set using cross-validation (see Section 14.5.3).\n14.5.2.3 Probabilistic output\nAn SVM classi\ufb01er produces a hard-labeling, \u02c6y(x) = sign(f (x)). However, we often want a\nmeasure of con\ufb01dence in our prediction. One heuristic approach is to interpret f(x)as the\nlog-odds ratio, logp(y=1|x)\np(y=0|x). We can then convert the output of an SVM to a probability using\np(y=1|x,\u03b8)=\u03c3(af(x)+b) (14.67)\nwherea,bcan be estimated by maximum likelihood on a separate validation set. (Using the\ntraining set to estimate aandbleads to severe over\ufb01tting.) This technique was \ufb01rst proposed in\n(Platt 2000).\nHowever, the resulting probabilities are not particularly well calibrated, since there is nothing\nin the SVM training procedure that justi\ufb01es interpreting f(x)as a log-odds ratio. To illustrate\nthis, consider an example from (Tipping 2001). Suppose we have 1d data where p(x|y=0 )=\nUnif(0,1)andp(x|y=1 )=U n i f( 0 .5,1.5). Since the class-conditional distributions overlap in\nthe middle, the log-odds of class 1 over class 0 should be zero in [0.5,1.0], and in\ufb01nite outside\nthis region. We sampled 1000 points from the model, and then \ufb01t an RVM and an SVM with\na Gaussian kenel of width 0.1. Both models can perfectly capture the decision boundary, andachieve a generalizaton error of 25%, which is Bayes optimal in this problem. The probabilisticoutput from the RVM is a good approximation to the true log-odds, but this is not the case forthe SVM, as shown in Figure 14.13.", "533": "14.5. Support vector machines (SVMs) 503\n?\nR1 R2\nR3 C1\nNotC1C2\nNotC2\n(a)?R1\nR2R3C1 C3\nC2\nC3C1\nC2\n(b)\nFigure 14.14 (a) The one-versus-rest approach. The green region is predicted to be both class 1 and class\n2. (b) The one-versus-one approach. The label of the green region is ambiguous. Based on Figure 4.2 of\n(Bishop 2006a).\n14.5.2.4 SVMs for multi-class classi\ufb01cation\nIn Section 8.3.7, we saw how we could \u201cupgrade\u201d a binary logistic regression model to the multi-\nclass case, by replacing the sigmoid function with the softmax, and the Bernoulli distributionwith the multinomial. Upgrading an SVM to the multi-class case is not so easy, since the outputsare not on a calibrated scale and hence are hard to compare to each other.\nThe obvious approach is to use a one-versus-the-rest approach (also called one-vs-all), in\nwhich we train Cbinary classi\ufb01ers, f\nc(x), where the data from class cis treated as positive,\nand the data from all the other classes is treated as negative. However, this can result in regionsof input space which are ambiguously labeled, as shown in Figure 14.14(a).\nA common alternative is to pick \u02c6y(x) = argmax\ncfc(x). However, this technique may\nnot work either, since there is no guarantee that the different fcfunctions have comparable\nmagnitudes. In addition, each binary subproblem is likely to suffer from the class imbalance\nproblem. To see this, suppose we have 10 equally represented classes. When training f1,w e\nwill have 10% positive examples and 90% negative examples, which can hurt performance. It ispossible to devise ways to train all Cclassi\ufb01ers simultaneously (Weston and Watkins 1999), but\nthe resulting method takes O(C\n2N2)time, instead of the usual O(CN2)time.\nAnother approach is to use the one-versus-one or OVO approach, also called all pairs,i n\nwhich we train C(C\u22121)/2classi\ufb01ers to discriminate all pairs fc,c/prime. We then classify a point into\nthe class which has the highest number of votes. However, this can also result in ambiguities,as shown in Figure 14.14(b). Also, it takes O(C\n2N2)time to train and O(C2Nsv)to test each\ndata point, where Nsvis the number of support vectors.2See also (Allwein et al. 2000) for an\napproach based on error-correcting output codes.\nIt is worth remembering that all of these difficulties, and the plethora of heuristics that have\nbeen proposed to \ufb01x them, fundamentally arise because SVMs do not model uncertainty usingprobabilities, so their output scores are not comparable across classes.\n2. We can reduce the test time by structuring the classes into a DAG (directed acyclic graph), and performing O(C)\npairwise comparisons (Platt et al. 2000). However, the O(C2)factor in the training time is unavoidable.", "534": "504 Chapter 14. Kernels\n10\u22122100102104\n10\u221211001010.10.20.30.40.5\nC \u03b3cv error\n(a)10\u2212210\u221211001011021031040.20.250.30.35\nCcv error\u03b3 = 5.0\n(b)\nFigure 14.15 (a) A cross validation estimate of the 0-1 error for an SVM classi\ufb01er with RBF kernel with\ndifferent precisions \u03b3=1/(2\u03c32)and different regularizer \u03bb=1/C, applied to a synthetic data set drawn\nfrom a mixture of 2 Gaussians. (b) A slice through this surface for \u03b3=5The red dotted line is the Bayes\noptimal error, computed using Bayes rule applied to the model used to generate the data. Based on Figure\n12.6 of (Hastie et al. 2009). Figure generated by svmCgammaDemo .\n14.5.3 Choosing C\nSVMs for both classi\ufb01cation and regression require that you specify the kernel function and the\nparameter C. Typically Cis chosen by cross-validation. Note, however, that Cinteracts quite\nstrongly with the kernel parameters. For example, suppose we are using an RBF kernel withprecision \u03b3=\n1\n2\u03c32.I f\u03b3=5, corresponding to narrow kernels, we need heavy regularization,\nand hence small C(so\u03bb=1/Cis big). If \u03b3=1, a larger value of Cshould be used. So we\nsee that\u03b3andCare tightly coupled. This is illustrated in Figure 14.15, which shows the CV\nestimate of the 0-1 risk as a function of Cand\u03b3.\nThe authors of libsvm recommend (Hsu et al. 2009) using CV over a 2d grid with values C\u2208\n{2\u22125,2\u22123,...,215}and\u03b3\u2208{2\u221215,2\u221213,...,23}. In addition, it is important to standardize\nthe data \ufb01rst, for a spherical Gaussian kernel to make sense.\nTo choose Cefficiently, one can develop a path following algorithm in the spirit of lars\n(Section 13.3.4). The basic idea is to start with \u03bblarge, so that the margin 1/||w(\u03bb)||is wide,\nand hence all points are inside of it and have \u03b1i=1. By slowly decreasing \u03bb, a small set of\npoints will move from inside the margin to outside, and their \u03b1ivalues will change from 1 to 0,\nas they cease to be support vectors. When \u03bbis maximal, the function is completely smoothed,\nand no support vectors remain. See (Hastie et al. 2004) for the details.\n14.5.4 Summary of key points\nSummarizing the above discussion, we recognize that SVM classi\ufb01ers involve three key ingre-dients: the kernel trick, sparsity, and the large margin principle. The kernel trick is necessaryto prevent under\ufb01tting, i.e., to ensure that the feature vector is sufficiently rich that a linearclassi\ufb01er can separate the data. (Recall from Section 14.2.3 that any Mercer kernel can be viewedas implicitly de\ufb01ning a potentially high dimensional feature vector.) If the original features arealready high dimensional (as in many gene expression and text classi\ufb01cation problems), it suf-\ufb01ces to use a linear kernel, \u03ba(x,x\n/prime)=xTx/prime, which is equivalent to working with the original\nfeatures.", "535": "14.6. Comparison of discriminative kernel methods 505\nMethod Opt. wOpt. kernel Sparse Prob. Multiclass Non-Mercer Section\nL2VM Convex EB No Yes Yes Yes 14.3.2\nL1VM Convex CV Yes Yes Yes Yes 14.3.2RVM Not convex EB Yes Yes Yes Yes 14.3.2SVM Convex CV Yes No Indirectly No 14.5GP N/A EB No Yes Yes No 15\nTable 14.1 Comparison of various kernel based classi\ufb01ers. EB = empirical Bayes, CV = cross validation.\nSee text for details.\nThe sparsity and large margin principles are necessary to prevent over\ufb01tting, i.e., to ensure\nthat we do not use all the basis functions. These two ideas are closely related to each other,and both arise (in this case) from the use of the hinge loss function. However, there are othermethods of achieving sparsity (such as /lscript\n1), and also other methods of maximizing the margin\n(such as boosting). A deeper discussion of this point takes us outside of the scope of this book.See e.g., (Hastie et al. 2009) for more information.\n14.5.5 A probabilistic interpretation of SVMs\nIn Section 14.3, we saw how to use kernels inside GLMs to derive probabilistic classi\ufb01ers, such asthe L1VM and RVM. And in Section 15.3, we will discuss Gaussian process classi\ufb01ers, which alsouse kernels. However, all of these approaches use a logistic or probit likelihood, as opposed tothe hinge loss used by SVMs. It is natural to wonder if one can interpret the SVM more directlyas a probabilistic model. To do so, we must interpret Cg(m)as a negative log likelihood, where\ng(m)=( 1 \u2212m)\n+,w h e r em=yf(x)is the margin. Hence p(y=1|f)=e x p ( \u2212Cg(f))\nandp(y=\u22121|f)=e x p ( \u2212Cg(\u2212f)). By summing over both values of y, we require that\nexp(\u2212Cg(f))+exp(\u2212Cg (\u2212f))be a constant independent of f. But it turns out this is not\npossible for any C>0(Sollich 2002).\nHowever, if we are willing to relax the sum-to-one condition, and work with a pseudo-\nlikelihood, we canderive a probabilistic interpretation of the hinge loss (Polson and Scott 2011).\nIn particular, one can show that\nexp(\u22122(1\u2212yixT\niw)+)=/integraldisplay\u221e\n01\u221a2\u03c0\u03bbiexp/parenleftbigg\n\u22121\n2(1+\u03bbi\u2212yixT\niw)2\n\u03bbi/parenrightbigg\nd\u03bbi (14.68)\nThus the exponential of the negative hinge loss can be represented as a Gaussian scale mixture.\nThis allows one to \ufb01t an SVM using EM or Gibbs sampling, where \u03bbiare the latent variables. This\nin turn opens the door to Bayesian methods for setting the hyper-parameters for the prior onw. See (Polson and Scott 2011) for details. (See also (Franc et al. 2011) for a different probabilistic\ninterpretation of SVMs.)\n14.6 Comparison of discriminative kernel methods\nWe have mentioned several different methods for classi\ufb01cation and regression based on kernels,which we summarize in Table 14.1. (GP stands for \u201cGaussian process\u201d, which we discuss inChapter 15.) The columns have the following meaning:", "536": "506 Chapter 14. Kernels\n\u2022 Optimize w: a key question is whether the objective J(w)=\u2212logp(D|w)\u2212logp(w)\nis convex or not. L2VM, L1VM and SVMs have convex objectives. RVMs do not. GPs are\nBayesian methods that do not perform parameter estimation.\n\u2022 Optimize kernel: all the methods require that one \u201ctune\u201d the kernel parameters, such as the\nbandwidth of the RBF kernel, as well as the level of regularization. For methods based onGaussians, including L2VM, RVMs and GPs, we can use efficient gradient based optimizers tomaximize the marginal likelihood. For SVMs, and L1VM, we must use cross validation, whichis slower (see Section 14.5.3).\n\u2022 Sparse: L1VM, RVMs and SVMs are sparse kernel methods, in that they only use a subset of\nthe training examples. GPs and L2VM are not sparse: they use all the training examples. Theprinciple advantage of sparsity is that prediction at test time is usually faster. In addition,one can sometimes get improved accuracy.\n\u2022 Probabilistic: All the methods except for SVMs produce probabilistic output of the form\np(y|x). SVMs produce a \u201ccon\ufb01dence\u201d value that can be converted to a probability, but such\nprobabilities are usually very poorly calibrated (see Section 14.5.2.3).\n\u2022 Multiclass: All the methods except for SVMs naturally work in the multiclass setting, by using\na multinoulli output instead of Bernoulli. The SVM can be made into a multiclass classi\ufb01er,but there are various difficulties with this approach, as discussed in Section 14.5.2.4.\n\u2022 Mercer kernel: SVMs and GPs require that the kernel is positive de\ufb01nite; the other techniques\ndo not.\nApart from these differences, there is the natural question: which method works best? In\na small experiment\n3, we found that all of these methods had similar accuracy when averaged\nover a range of problems, provided they have the same kernel, and provided the regularizationconstants are chosen appropriately.\nGiven that the statistical performance is roughly the same, what about the computational\nperformance? GPs and L2VM are generally the slowest, taking O(N\n3)time, since they don\u2019t\nexploit sparsity (although various speedups are possible, see Section 15.6). SVMs also takeO(N\n3)time to train (unless we use a linear kernel, in which case we only need O(N)time\n(Joachims 2006)). However, the need to use cross validation can make SVMs slower than RVMs.L1VM should be faster than an RVM, since an RVM requires multiple rounds of /lscript\n1minimization\n(see Section 13.7.4.3). However, in practice it is common to use a greedy method to train RVMs,which is faster than /lscript\n1minimization. This is re\ufb02ected in our empirical results.\nThe conclusion of all this is as follows: if speed matters, use an RVM, but if well-calibrated\nprobabilistic output matters (e.g., for active learning or control problems), use a GP. The onlycircumstances under which using an SVM seems sensible is the structured output case, wherelikelihood-based methods can be slow. (We attribute the enormous popularity of SVMs notto their superiority, but to ignorance of the alternatives, and also to the lack of high qualitysoftware implementing the alternatives.)\nSection 16.7.1 gives a more extensive experimental comparison of supervised learning methods,\nincluding SVMs and various non kernel methods.\n3. See http://pmtk3 .googlecode .com/svn/trunk/docs/tutorial/html/tutKernelClassif .html.", "537": "14.7. Kernels for building generative models 507\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.500.10.20.30.40.50.60.70.80.9\n  \nBoxcar\nEpanechnikov\nTricube\nGaussian\nFigure 14.16 A comparison of some popular smoothing kernels. The boxcar kernel has compact support\nbut is not smooth. The Epanechnikov kernel has compact support but is not differentiable at its boundary.\nThe tri-cube has compact support and two continuous derivatives at the boundary of its support. TheGaussian is differentiable, but does not have compact support. Based on Figure 6.2 of (Hastie et al. 2009).Figure generated by smoothingKernelPlot .\n14.7 Kernels for building generative models\nThere is a different kind of kernel known as a smoothing kernel which can be used to create\nnon-parametric density estimates. This can be used for unsupervised density estimation, p(x),\nas well as for creating generative models for classi\ufb01cation and regression by making models ofthe form p(y,x).\n14.7.1 Smoothing kernels\nAsmoothing kernel is a function of one argument which satis\ufb01es the following properties:/integraldisplay\n\u03ba(x)dx=1,/integraldisplay\nx\u03ba(x)dx=0,/integraldisplay\nx2\u03ba(x)dx >0 (14.69)\nA simple example is the Gaussian kernel,\n\u03ba(x)/defines1\n(2\u03c0)1\n2e\u2212x2/2(14.70)\nWe can control the width of the kernel by introducing a bandwidth parameter h:\n\u03bah(x)/defines1\nh\u03ba(x\nh) (14.71)\nWe can generalize to vector valued inputs by de\ufb01ning an RBF kernel:\n\u03bah(x)=\u03bah(||x||) (14.72)\nIn the case of the Gaussian kernel, this becomes\n\u03bah(x)=1\nhD(2\u03c0)D/2D/productdisplay\nj=1exp(\u22121\n2h2x2\nj) (14.73)", "538": "508 Chapter 14. Kernels\nAlthough Gaussian kernels are popular, they have unbounded support. An alternative kernel,\nwith compact support, is the Epanechnikov kernel, de\ufb01ned by\n\u03ba(x)/defines3\n4(1\u2212x2)I(|x|\u22641) (14.74)\nThis is plotted in Figure 14.16. Compact support can be useful for efficiency reasons, since one\ncan use fast nearest neighbor methods to evaluate the density.\nUnfortunately, the Epanechnikov kernel is not differentiable at the boundary of its support.\nAn alterative is the tri-cube kernel, de\ufb01ned as follows:\n\u03ba(x)/defines70\n81(1\u2212|x|3)3I(|x|\u22641) (14.75)\nThis has compact support and has two continuous derivatives at the boundary of its support.See Figure 14.16.\nTheboxcar kernel is simply the uniform distribution:\n\u03ba(x)/definesI(|x|\u22641) (14.76)\nWe will use this kernel below.\n14.7.2 Kernel density estimation (KDE)\nRecall the Gaussian mixture model from Section 11.2.1. This is a parametric density estimator fordata in R\nD. However, it requires specifying the number Kand locations \u03bckof the clusters. An\nalternative to estimating the \u03bckis to allocate one cluster center per data point, so \u03bci=xi.I n\nthis case, the model becomes\np(x|D)=1\nNN/summationdisplay\ni=1N(x|xi,\u03c32I) (14.77)\nWe can generalize the approach by writing\n\u02c6p(x)=1\nNN/summationdisplay\ni=1\u03bah(x\u2212xi) (14.78)\nThis is called a Parzen window density estimator,o r kernel density estimator (KDE), and\nis a simple non-parametric density model. The advantage over a parametric model is that nomodel \ufb01tting is required (except for tuning the bandwidth, usually done by cross-validation). andthere is no need to pick K. The disadvantage is that the model takes a lot of memory to store,\nand a lot of time to evaluate. It is also of no use for clustering tasks.\nFigure 14.17 illustrates KDE in 1d for two kinds of kernel. On the top, we use a boxcar kernel,\n\u03ba(x)=I(\u22121\u2264z\u22641). The result is equivalent to a histogram estimate of the density, since\nwe just count how many data points land within an interval of size haroundx\ni. On the bottom,\nwe use a Gaussian kernel, which results in a smoother \ufb01t.\nThe usual way to pick his to minimize an estimate (such as cross validation) of the frequentist\nrisk (see e.g., (Bowman and Azzalini 1997)). In Section 25.2, we discuss a Bayesian approach tonon-parametric density estimation, based on Dirichlet process mixture models, which allows us", "539": "14.7. Kernels for building generative models 509\n\u22125 0 5 1000.050.10.150.20.250.30.35unif, h=1.000\n(a)\u22125 0 5 1000.020.040.060.080.10.120.14unif, h=2.000\n(b)\n\u22125 0 5 1000.020.040.060.080.10.120.140.16gauss, h=1.000\n(c)\u22125 0 5 1000.010.020.030.040.050.06gauss, h=2.000\n(d)\nFigure 14.17 A nonparametric (Parzen) density estimator in 1D estimated from 6 data points, denoted\nby x. Top row: uniform kernel. Bottom row: Gaussian kernel. Rows represent increasingly large band-\nwidth parameters. Based on http://en .wikipedia .org/wiki/Kernel_density_estimation . Figure\ngenerated by parzenWindowDemo2 .\nto inferh. DP mixtures can also be more efficient than KDE, since they do not need to store\nall the data. See also Section 15.2.4 where we discuss an empirical Bayes approach to estimating\nkernel parameters in a Gaussian process model for classi\ufb01cation/ regression.\n14.7.3 From KDE to KNN\nWe can use KDE to de\ufb01ne the class conditional densities in a generative classi\ufb01er. This turnsout to provide an alternative derivation of the nearest neighbors classi\ufb01er, which we introducedin Section 1.4.2. To show this, we follow the presentation of (Bishop 2006a, p125). In kdewith a boxcar kernel, we \ufb01xed the bandwidth and count how many data points fall within thehyper-cube centered on a datapoint. Suppose that, instead of \ufb01xing the bandwidth h, we instead", "540": "510 Chapter 14. Kernels\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2\u22120.4\u22120.200.20.40.60.811.2Gaussian kernel regression\n  \ntrue\ndata\nestimate\nFigure 14.18 An example of kernel regression in 1d using a Gaussian kernel. Figure generated by\nkernelRegressionDemo , based on code by Yi Cao.\nallow the bandwidth or volume to be different for each data point. Speci\ufb01cally, we will \u201cgrow\u201d\na volume around xuntil we encounter Kdata points, regardless of their class label. Let the\nresulting volume have size V(x)(this was previously hD), and let there be Nc(x)examples\nfrom class cin this volume. Then we can estimate the class conditional density as follows:\np(x|y=c,D)=Nc(x)\nNcV(x)(14.79)\nwhereNcis the total number of examples in class cin the whole data set. The class prior can\nbe estimated by\np(y=c|D)=Nc\nN(14.80)\nHence the class posterior is given by\np(y=c|x,D)=Nc(x)\nNcV(x)Nc\nN/summationtext\nc/primeNc/prime(x)\nNc/primeV(x)Nc/prime\nN=Nc(x)/summationtext\nc/primeNc/prime(x)=Nc(x)\nK(14.81)\nwhere we used the fact that/summationtext\ncNc(x)=K, since we choose a total of Kpoints (regardless of\nclass) around every point. This is equivalent to Equation 1.2, since Nc(x)=/summationtext\ni\u2208NK(x,D)I(yi=\nc).\n14.7.4 Kernel regression\nIn Section 14.7.2, we discussed the use of kernel density estimation or KDE for unsupervisedlearning. We can also use KDE for regression. The goal is to compute the conditional expectation\nf(x)=E[y|x]=/integraldisplay\nyp(y|x)dy=/integraltext\nyp(x,y)dy\n/integraltext\np(x,y)dy(14.82)", "541": "14.7. Kernels for building generative models 511\nWe can use KDE to approximate the joint density p(x,y)as follows:\np(x,y)\u22481\nNN/summationdisplay\ni=1\u03bah(x\u2212xi)\u03bah(y\u2212yi) (14.83)\nHence\nf(x)=1\nN/summationtextN\ni=1\u03bah(x\u2212xi)/integraltext\ny\u03bah(y\u2212yi)dy\n1\nN/summationtextNi=1\u03bah(x\u2212xi)/integraltext\n\u03bah(y\u2212yi)dy(14.84)\n=/summationtextNi=1\u03bah(x\u2212xi)yi/summationtextNi=1\u03bah(x\u2212xi)(14.85)\nTo derive this result, we used two properties of smoothing kernels. First, that they integrate to\none, i.e.,/integraltext\n\u03bah(y\u2212yi)dy=1. And second, the fact that/integraltext\ny\u03bah(y\u2212yi)dy=yi. This follows by\nde\ufb01ningx=y\u2212yiand using the zero mean property of smoothing kernels:\n/integraldisplay\n(x+yi)\u03bah(x)dx=/integraldisplay\nx\u03bah(x)dx+yi/integraldisplay\n\u03bah(x)dx=0+yi=yi (14.86)\nWe can rewrite the above result as follows:\nf(x)=N/summationdisplay\ni=1wi(x)yi (14.87)\nwi(x)/defines\u03bah(x\u2212xi)/summationtextN\ni/prime=1\u03bah(x\u2212xi/prime)(14.88)\nWe see that the prediction is just a weighted sum of the outputs at the training points, where\nthe weights depend on how similar xis to the stored training points. This method is called\nkernel regression, kernel smoothing, or the Nadaraya-Watson model. See Figure 14.18 for an\nexample, where we use a Gaussian kernel.\nNote that this method only has one free parameter, namely h. One can show (Bowman and\nAzzalini 1997) that for 1d data, if the true density is Gaussian and we are using Gaussian kernels,the optimal bandwidth his given by\nh=/parenleftbigg4\n3N/parenrightbigg1/5\n\u02c6\u03c3 (14.89)\nWe can compute a robust approximation to the standard deviation by \ufb01rst computing the mean\nabsolute deviation\nMAD=median(|x\u2212median(x)|) (14.90)\nand then using\n\u02c6\u03c3=1.4826MAD=1\n0.6745MAD (14.91)\nThe code used to produce Figure 14.18 estimated hxandhyseparately, and then set h=/radicalbig\nhxhy.", "542": "512 Chapter 14. Kernels\nAlthough these heuristics seem to work well, their derivation rests on some rather dubious\nassumptions (such as Gaussianity of the true density). Furthermore, these heuristics are limited\nto tuning just a single parameter. In Section 15.2.4 we discuss an empirical Bayes approach toestimating multiple kernel parameters in a Gaussian process model for classi\ufb01cation/ regression,which can handle many tuning parameters, and which is based on much more transparentprinciples (maximizing the marginal likelihood).\n14.7.5 Locally weighted regression\nIf we de\ufb01ne \u03bah(x\u2212xi)=\u03ba(x,xi), we can rewrite the prediction made by kernel regression as\nfollows\n\u02c6f(x\u2217)=N/summationdisplay\ni=1yi\u03ba(x\u2217,xi)/summationtextN\ni/prime=1\u03ba(x\u2217,xi/prime)(14.92)\nNote that \u03ba(x,xi)need not be a smoothing kernel. If it is not, we no longer need the\nnormalization term, so we can just write\n\u02c6f(x\u2217)=N/summationdisplay\ni=1yi\u03ba(x\u2217,xi) (14.93)\nThis model is essentially \ufb01tting a constant function locally. We can improve on this by \ufb01tting a\nlinear regression model for each point x\u2217by solving\nmin\n\u03b2(x\u2217)N/summationdisplay\ni=1\u03ba(x\u2217,xi)[yi\u2212\u03b2(x\u2217)T\u03c6(xi)]2(14.94)\nwhere\u03c6(x)=[ 1,x]. This is called locally weighted regression. An example of such a method\nisLOESS,a k aLOWESS, which stands for \u201clocally-weighted scatterplot smoothing\u201d (Cleveland\nand Devlin 1988). See also (Edakunni et al. 2010) for a Bayesian version of this model.\nWe can compute the paramters \u03b2(x\u2217)for each test case by solving the following weighted\nleast squares problem:\n\u03b2(x\u2217)=(\u03a6TD(x\u2217)\u03a6)\u22121\u03a6TD(x\u2217)y (14.95)\nwhere\u03a6is anN\u00d7(D+1 )design matrix and D=d i a g (\u03ba(x\u2217,xi)). The corresponding\nprediction has the form\n\u02c6f(x\u2217)=\u03c6(x\u2217)T\u03b2(x\u2217)=(\u03a6TD(x\u2217)\u03a6)\u22121\u03a6TD(x\u2217)y=N/summationdisplay\ni=1wi(x\u2217)yi (14.96)\nThe term wi(x\u2217), which combines the local smoothing kernel with the effect of linear regression,\nis called the equivalent kernel. See also Section 15.4.2.\nExercises\nExercise 14.1 Fitting an SVM classi\ufb01er by hand\n(Source: Jaakkola.) Consider a dataset with 2 points in 1d: (x1=0,y1=\u22121)and(x2=\u221a\n2,y2=1 ).\nConsider mapping each point to 3d using the feature vector \u03c6(x)=[ 1,\u221a\n2x,x2]T. (This is equivalent to", "543": "14.7. Kernels for building generative models 513\nusing a second order polynomial kernel.) The max margin classi\ufb01er has the form\nmin||w||2s.t. (14.97)\ny1(wT\u03c6(x1)+w0)\u22651 (14.98)\ny2(wT\u03c6(x2)+w0)\u22651 (14.99)\na. Write down a vector that is parallel to the optimal vector w. Hint: recall from Figure 7.8 (12Apr10\nversion) that wis perpendicular to the decision boundary between the two points in the 3d feature\nspace.\nb. What is the value of the margin that is achieved by this w? Hint: recall that the margin is the distance\nfrom each support vector to the decision boundary. Hint 2: think about the geometry of 2 points in\nspace, with a line separating one from the other.\nc. Solve for w, using the fact the margin is equal to 1/||w||.\nd. Solve for w0using your value for wand Equations 14.97 to 14.99. Hint: the points will be on the\ndecision boundary, so the inequalities will be tight.\ne. Write down the form of the discriminant function f(x)=w0+wT\u03c6(x)as an explicit function of x.\nExercise 14.2 Linear separability\n(Source: Koller..) Consider \ufb01tting an SVM with C>0to a dataset that is linearly separable. Is the resulting\ndecision boundary guaranteed to separate the classes?", "544": "", "545": "15 Gaussian processes\n15.1 Introduction\nIn supervised learning, we observe some inputs xiand some outputs yi. We assume that\nyi=f(xi), for some unknown function f, possibly corrupted by noise. The optimal approach\nis to infer a distribution over functions given the data, p(f|X,y), and then to use this to make\npredictions given new inputs, i.e., to compute\np(y\u2217|x\u2217,X,y)=/integraldisplay\np(y\u2217|f,x\u2217)p(f|X,y)df (15.1)\nUp until now, we have focussed on parametric representations for the function f, so that\ninstead of inferring p(f|D), we infer p(\u03b8|D). In this chapter, we discuss a way to perform\nBayesian inference over functions themselves.\nOur approach will be based on Gaussian processes orGPs. A GP de\ufb01nes a prior over\nfunctions, which can be converted into a posterior over functions once we have seen some data.\nAlthough it might seem difficult to represent a distribution over a function, it turns out that weonly need to be able to de\ufb01ne a distribution over the function\u2019s values at a \ufb01nite, but arbitrary,set of points, say x\n1,...,xN. A GP assumes that p(f(x1),...,f(xN))is jointly Gaussian, with\nsome mean \u03bc(x)and covariance \u03a3(x)given by\u03a3ij=\u03ba(xi,xj),w h e r e\u03bais a positive de\ufb01nite\nkernel function (see Section 14.2 information on kernels). The key idea is that if xiandxjare\ndeemed by the kernel to be similar, then we expect the output of the function at those pointsto be similar, too. See Figure 15.1 for an illustration.\nIt turns out that, in the regression setting, all these computations can be done in closed form,\ninO(N\n3)time. (We discuss faster approximations in Section 15.6.) In the classi\ufb01cation setting,\nwe must use approximations, such as the Gaussian approximation, since the posterior is nolonger exactly Gaussian.\nGPs can be thought of as a Bayesian alternative to the kernel methods we discussed in Chap-\nter 14, including L1VM, RVM and SVM. Although those methods are sparser and therefore faster,they do not give well-calibrated probabilistic outputs (see Section 15.4.4 for further discussion).Having properly tuned probabilistic output is important in certain applications, such as onlinetracking for vision and robotics (Ko and Fox 2009), reinforcement learning and optimal control(Engel et al. 2005; Deisenroth et al. 2009), global optimization of non-convex functions (Mockuset al. 1996; Lizotte 2008; Brochu et al. 2009), experiment design (Santner et al. 2003), etc.", "546": "516 Chapter 15. Gaussian processes\nx1 x2 x\u22c6f1 f2 f\u22c6y1 y2 y\u22c6\nFigure 15.1 A Gaussian process for 2 training points and 1 testing point, represented as a mixed directed\nand undirected graphical model representing p(y,f|x)=N(f|0,K(x))/producttext\nip(yi|fi). The hidden nodes\nfi=f(xi)represent the value of the function at each of the data points. These hidden nodes are fully\ninterconnected by undirected edges, forming a Gaussian graphical model; the edge strengths represent the\ncovariance terms \u03a3ij=\u03ba(xi,xj). If the test point x\u2217is similar to the training points x1andx2, then\nthe predicted output y\u2217will be similar to y1andy2.\nOur presentation is closely based on (Rasmussen and Williams 2006), which should be con-\nsulted for futher details. See also (Diggle and Ribeiro 2007), which discusses the related approach\nknown as kriging, which is widely used in the spatial statistics literature.\n15.2 GPs for regression\nIn this section, we discuss GPs for regression. Let the prior on the regression function be a GP,denoted by\nf(x)\u223cGP(m(x),\u03ba(x,x\n/prime)) (15.2)\nwherem(x)is the mean function and \u03ba(x,x/prime)is the kernel or covariance function, i.e.,\nm(x)=E [f(x)] (15.3)\n\u03ba(x,x/prime)=E/bracketleftbig\n(f(x)\u2212m(x))(f(x/prime)\u2212m(x/prime))T/bracketrightbig\n(15.4)\nWe obviously require that \u03ba()be a positive de\ufb01nite kernel. For any \ufb01nite set of points, this\nprocess de\ufb01nes a joint Gaussian:\np(f|X)=N(f|\u03bc,K) (15.5)\nwhereKij=\u03ba(xi,xj)and\u03bc=(m(x1),...,m(xN)).\nNote that it is common to use a mean function of m(x)=0, since the GP is \ufb02exible enough\nto model the mean arbitrarily well, as we will see below. However, in Section 15.2.6 we willconsider parametric models for the mean function, so the GP just has to model the residualerrors. This semi-parametric approach combines the interpretability of parametric models withthe accuracy of non-parametric models.", "547": "15.2. GPs for regression 517\n\u22125 0 5\u22122\u22121.5\u22121\u22120.500.511.52\n(a)\u22125 0 5\u22122\u22121.5\u22121\u22120.500.511.522.5\n(b)\nFigure 15.2 Left: some functions sampled from a GP prior with SE kernel. Right: some samples from a GP\nposterior, after conditioning on 5 noise-free observations. The shaded area represents E[f(x)]\u00b12std(f(x).\nBased on Figure 2.2 of (Rasmussen and Williams 2006). Figure generated by gprDemoNoiseFree .\n15.2.1 Predictions using noise-free observations\nSuppose we observe a training set D={(xi,fi),i=1:N},w h e r efi=f(xi)is the noise-free\nobservation of the function evaluated at xi. Given a test set X\u2217of sizeN\u2217\u00d7D, we want to\npredict the function outputs f\u2217.\nIf we ask the GP to predict f(x)for a value of xthat it has already seen, we want the GP to\nreturn the answer f(x)with no uncertainty. In other words, it should act as an interpolator\nof the training data. This will only happen if we assume the observations are noiseless. We will\nconsider the case of noisy observations below.\nNow we return to the prediction problem. By de\ufb01nition of the GP, the joint distribution has\nthe following form\n/parenleftbiggf\nf\u2217/parenrightbigg\n\u223cN/parenleftbigg/parenleftbigg\u03bc\n\u03bc\u2217/parenrightbigg\n,/parenleftbiggKK \u2217\nKT\n\u2217K\u2217\u2217/parenrightbigg/parenrightbigg\n(15.6)\nwhereK=\u03ba(X,X)isN\u00d7N,K\u2217=\u03ba(X,X\u2217)isN\u00d7N\u2217, andK\u2217\u2217=\u03ba(X\u2217,X\u2217)isN\u2217\u00d7N\u2217.\nBy the standard rules for conditioning Gaussians (Section 4.3), the posterior has the following\nform\np(f\u2217|X\u2217,X,f)=N (f\u2217|\u03bc\u2217,\u03a3\u2217) (15.7)\n\u03bc\u2217=\u03bc(X\u2217)+KT\n\u2217K\u22121(f\u2212\u03bc(X)) (15.8)\n\u03a3\u2217=K\u2217\u2217\u2212KT\u2217K\u22121K\u2217 (15.9)\nThis process is illustrated in Figure 15.2. On the left we show sample samples from the prior,\np(f|X), where we use a squared exponential kernel, aka Gaussian kernel or RBF kernel. In\n1d, this is given by\n\u03ba(x,x/prime)=\u03c32\nfexp(\u22121\n2/lscript2(x\u2212x/prime)2) (15.10)\nHere/lscriptcontrols the horizontal length scale over which the function varies, and \u03c32\nfcontrols the\nvertical variation. (We discuss how to estimate such kernel parameters below.) On the right we", "548": "518 Chapter 15. Gaussian processes\nshow samples from the posterior, p(f\u2217|X\u2217,X,f). We see that the model perfectly interpolates\nthe training data, and that the predictive uncertainty increases as we move further away from\nthe observed data.\nOne application of noise-free GP regression is as a computationally cheap proxy for the\nbehavior of a complex simulator, such as a weather forecasting program. (If the simulator isstochastic, we can de\ufb01ne fto be its mean output; note that there is still no observation noise.)\nOne can then estimate the effect of changing simulator parameters by examining their effecton the GP\u2019s predictions, rather than having to run the simulator many times, which may beprohibitively slow. This strategy is known as DACE, which stands for design and analysis ofcomputer experiments (Santner et al. 2003).\n15.2.2 Predictions using noisy observations\nNow let us consider the case where what we observe is a noisy version of the underlyingfunction, y=f(x)+/epsilon1,w h e r e/epsilon1\u223cN(0,\u03c3\n2\ny). In this case, the model is not required to\ninterpolate the data, but it must come \u201cclose\u201d to the observed data. The covariance of theobserved noisy responses is\ncov[y\np,yq]=\u03ba(xp,xq)+\u03c32\ny\u03b4pq (15.11)\nwhere\u03b4pq=I(p=q). In other words\ncov[y|X]=K+\u03c32\nyIN/definesKy (15.12)\nThe second matrix is diagonal because we assumed the noise terms were independently addedto each observation.\nThe joint density of the observed data and the latent, noise-free function on the test points\nis given by\n/parenleftbiggy\nf\n\u2217/parenrightbigg\n\u223cN/parenleftbigg\n0,/parenleftbiggKyK\u2217\nKT\n\u2217K\u2217\u2217/parenrightbigg/parenrightbigg\n(15.13)\nwhere we are assuming the mean is zero, for notational simplicity. Hence the posterior predictive\ndensity is\np(f\u2217|X\u2217,X,y)=N (f\u2217|\u03bc\u2217,\u03a3\u2217) (15.14)\n\u03bc\u2217=KT\n\u2217K\u22121\nyy (15.15)\n\u03a3\u2217=K\u2217\u2217\u2212KT\u2217K\u22121\nyK\u2217 (15.16)\nIn the case of a single test input, this simpli\ufb01es as follows\np(f\u2217|x\u2217,X,y)=N (f\u2217|kT\u2217K\u22121\nyy,k\u2217\u2217\u2212kT\u2217K\u22121\nyk\u2217) (15.17)\nwherek\u2217=[\u03ba(x\u2217,x1),...,\u03ba(x\u2217,xN)]andk\u2217\u2217=\u03ba(x\u2217,x\u2217). Another way to write the\nposterior mean is as follows:\nf\u2217=kT\u2217K\u22121\nyy=N/summationdisplay\ni=1\u03b1i\u03ba(xi,x\u2217) (15.18)\nwhere\u03b1=K\u22121\nyy. We will revisit this expression later.", "549": "15.2. GPs for regression 519\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22123\u22122\u221210123\n(a)\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22123\u22122\u221210123\n(b)\n\u22128 \u22126 \u22124 \u22122 0 2 4 6 8\u22123\u22122\u221210123\n(c)\nFigure 15.3 Some 1d GPs with SE kernels but different hyper-parameters \ufb01t to 20 noisy observations. The\nkernel has the form in Equation 15.19. The hyper-parameters (/lscript,\u03c3f,\u03c3y)are as follows: (a) (1,1,0.1) (b) (0.3,\n0.1.08, 0.00005), (c) (3.0, 1.16, 0.89). Based on Figure 2.5 of (Rasmussen and Williams 2006). Figure generated\nbygprDemoChangeHparams , written by Carl Rasmussen.\n15.2.3 Effect of the kernel parameters\nThe predictive performance of GPs depends exclusively on the suitability of the chosen kernel.\nSuppose we choose the following squared-exponential (SE) kernel for the noisy observations\n\u03bay(xp,xq)=\u03c32\nfexp(\u22121\n2/lscript2(xp\u2212xq)2)+\u03c32\ny\u03b4pq (15.19)\nHere/lscriptis the horizontal scale over which the function changes, \u03c32\nfcontrols the vertical scale of\nthe function, and \u03c32\nyis the noise variance. Figure 15.3 illustrates the effects of changing these\nparameters. We sampled 20 noisy data points from the SE kernel using (/lscript,\u03c3f,\u03c3y)=( 1,1,0.1),\nand then made predictions various parameters, conditional on the data. In Figure 15.3(a), we use(/lscript,\u03c3\nf,\u03c3y)=( 1,1,0.1), and the result is a good \ufb01t. In Figure 15.3(b), we reduce the length scale\nto/lscript=0.3(the other parameters were optimized by maximum (marginal) likelihood, a technique\nwe discuss below); now the function looks more \u201cwiggly\u201d. Also, the uncertainty goes up faster,since the effective distance from the training points increases more rapidly. In Figure 15.3(c), weincrease the length scale to /lscript=3; now the function looks smoother.", "550": "520 Chapter 15. Gaussian processes\n\u2212202\n\u2212202\u22122\u22121012\ninput x1 input x2output y\n(a)\u2212202\n\u2212202\u22122\u22121012\ninput x1 input x2output y\n(b)\n\u2212202\n\u2212202\u22122\u22121012\ninput x1 input x2output y\n(c)\nFigure 15.4 Some 2d functions sampled from a GP with an SE kernel but different hyper-parameters. The\nkernel has the form in Equation 15.20 where (a) M=I, (b)M= diag(1 ,3)\u22122, (c)M=( 1,\u22121;\u22121,1)+\ndiag(6,6)\u22122. Based on Figure 5.1 of (Rasmussen and Williams 2006). Figure generated by gprDemoArd ,\nwritten by Carl Rasmussen.\nWe can extend the SE kernel to multiple dimensions as follows:\n\u03bay(xp,xq)=\u03c32\nfexp(\u22121\n2(xp\u2212xq)TM(xp\u2212xq))+\u03c32\ny\u03b4pq (15.20)\nWe can de\ufb01ne the matrix Min several ways. The simplest is to use an isotropic matrix,\nM1=/lscript\u22122I. See Figure 15.4(a) for an example. We can also endow each dimension with its\nown characteristic length scale, M2=d i a g (/lscript)\u22122. If any of these length scales become large,\nthe corresponding feature dimension is deemed \u201cirrelevant\u201d, just as in ARD (Section 13.7). In\nFigure 15.4(b), we use M=M2with/lscript=( 1,3), so the function changes faster along the x1\ndirection than the x2direction.\nWe can also create a matrix of the form M3=\u039b\u039bT+diag(/lscript)\u22122,w h e r e\u039bis aD\u00d7Kmatrix,\nwhereK<D. (Rasmussen and Williams 2006, p107) calls this the factor analysis distance\nfunction, by analogy to the fact that factor analysis (Section 12.1) approximates a covariancematrix as a low rank matrix plus a diagonal matrix. The columns of \u039bcorrespond to relevant\ndirections in input space. In Figure 15.4(c), we use /lscript= (6;6)and\u039b=( 1 ;\u22121), so the function\nchanges mostly rapidly in the direction which is perpendicular to (1,1).", "551": "15.2. GPs for regression 521\n15.2.4 Estimating the kernel parameters\nTo estimate the kernel parameters, we could use exhaustive search over a discrete grid of values,\nwith validation loss as an objective, but this can be quite slow. (This is the approach used totune kernels used by SVMs.) Here we consider an empirical Bayes approach, which will allow usto use continuous optimization methods, which are much faster. In particular, we will maximizethe marginal likelihood\n1\np(y|X)=/integraldisplay\np(y|f,X)p(f|X)df (15.21)\nSincep(f|X)=N(f|0,K), andp(y|f)=/producttext\niN(yi|fi,\u03c32\ny), the marginal likelihood is given by\nlogp(y|X)=l o g N(y|0,Ky)=\u22121\n2yK\u22121\nyy\u22121\n2log|Ky|\u2212N\n2log(2\u03c0) (15.22)\nThe \ufb01rst term is a data \ufb01t term, the second term is a model complexity term, and the third termis just a constant. To understand the tradeoff between the \ufb01rst two terms, consider a SE kernelin 1D, as we vary the length scale /lscriptand hold \u03c3\n2\ny\ufb01xed. Let J(/lscript)=\u2212logp(y|X,/lscript). For short\nlength scales, the \ufb01t will be good, so yTK\u22121\nyywill be small. However, the model complexity\nwill be high: Kwill be almost diagonal (as in Figure 14.3, top right), since most points will not\nbe considered \u201cnear\u201d any others, so the log|Ky|will be large. For long length scales, the \ufb01t will\nbe poor but the model complexity will be low: Kwill be almost all 1\u2019s (as in Figure 14.3, bottom\nright), so log|Ky|will be small.\nWe now discuss how to maximize the marginal likelhiood. Let the kernel parameters (also\ncalled hyper-parameters) be denoted by \u03b8. One can show that\n\u2202\n\u2202\u03b8jlogp(y|X)=1\n2yTK\u22121\ny\u2202Ky\n\u2202\u03b8jK\u22121\nyy\u22121\n2tr(K\u22121\ny\u2202Ky\n\u2202\u03b8j) (15.23)\n=1\n2tr/parenleftbigg\n(\u03b1\u03b1T\u2212K\u22121\ny)\u2202Ky\n\u2202\u03b8j/parenrightbigg\n(15.24)\nwhere\u03b1=K\u22121\nyy. It takes O(N3)time to compute K\u22121\ny, and then O(N2)time per hyper-\nparameter to compute the gradient.\nThe form of\u2202Ky\n\u2202\u03b8jdepends on the form of the kernel, and which parameter we are taking\nderivatives with respect to. Often we have constraints on the hyper-parameters, such as \u03c32\ny\u22650.\nIn this case, we can de\ufb01ne \u03b8= log(\u03c32\ny), and then use the chain rule.\nGiven an expression for the log marginal likelihood and its derivative, we can estimate the\nkernel parameters using any standard gradient-based optimizer. However, since the objective isnot convex, local minima can be a problem, as we illustrate below.\n15.2.4.1 Example\nConsiderFigure15.5. WeusetheSEkernelinEquation15.19with \u03c3\n2\nf=1, andplot logp(y|X,/lscript,\u03c32\ny)\n(whereXandyare the 7 data points shown in panels b and c) as we vary /lscriptand\u03c32\ny. The two\n1. The reason it is called the marginal likelihood, rather than just likelihood, is because we have marginalized out the\nlatent Gaussian vector f. This moves us up one level of the Bayesian hierarchy, and reduces the chances of over\ufb01tting\n(the number of kernel parameters is usually fairly small compared to a standard parametric model).", "552": "522 Chapter 15. Gaussian processes\n10010110\u22121100\ncharacteristic len gthscalenoise standard deviation\n(a)\u22125 0 5\u22122\u22121012\ninput, xoutput, y\n(b)\n\u22125 0 5\u22122\u22121012\ninput, xoutput, y\n(c)\nFigure 15.5 Illustration of local minima in the marginal likelihood surface. (a) We plot the log marginal\nlikelihood vs \u03c32\nyand/lscript, for \ufb01xed \u03c32\nf=1, using the 7 data points shown in panels b and c. (b) The function\ncorresponding to the lower left local minimum, (/lscript,\u03c32\nn)\u2248(1,0.2). This is quite \u201cwiggly\u201d and has low\nnoise. (c) The function corresponding to the top right local minimum, (/lscript,\u03c32\nn)\u2248(10,0.8). This is quite\nsmooth and has high noise. The data was generated using (/lscript,\u03c32\nn)=( 1,0.1). Source: Figure 5.5 of\n(Rasmussen and Williams 2006). Figure generated by gprDemoMarglik , written by Carl Rasmussen.\nlocal optima are indicated by +. The bottom left optimum corresponds to a low-noise, short-\nlength scale solution (shown in panel b). The top right optimum corresponds to a high-noise,long-length scale solution (shown in panel c). With only 7 data points, there is not enoughevidence to con\ufb01dently decide which is more reasonable, although the more complex model(panel b) has a marginal likelihood that is about 60% higher than the simpler model (panel c).With more data, the MAP estimate should come to dominate.\nFigure 15.5 illustrates some other interesting (and typical) features. The region where \u03c3\n2\ny\u22481\n(top of panel a) corresponds to the case where the noise is very high; in this regime, the marginallikelihood is insensitive to the length scale (indicated by the horizontal contours), since all thedata is explained as noise. The region where /lscript\u22480.5(left hand side of panel a) corresponds to\nthe case where the length scale is very short; in this regime, the marginal likelihood is insensitiveto the noise level, since the data is perfectly interpolated. Neither of these regions would bechosen by a good optimizer.", "553": "15.2. GPs for regression 523\nlog(length\u2212scale)log(magn itude)\nz1z2\n2.8 3 3.2 3.4\u22123\u22122.5\u22122\u22121.5\u22121\u22120.5\n(a)log(length\u2212scale)log(magn itude)\n2.8 3 3.2 3.4\u22123\u22122.5\u22122\u22121.5\u22121\u22120.5\n(b)\nlog(length\u2212scale)log(magn itude)\n2.8 3 3.2 3.4\u22123\u22122.5\u22122\u22121.5\u22121\u22120.5\n(c)\nFigure 15.6 Three different approximations to the posterior over hyper-parameters: grid-based, Monte\nCarlo, and central composite design. Source: Figure 3.2 of (Vanhatalo 2010). Used with kind permission\nof Jarno Vanhatalo.\n15.2.4.2 Bayesian inference for the hyper-parameters\nAn alternative to computing a point estimate of the hyper-parameters is to compute their poste-\nrior. Let\u03b8represent all the kernel parameters, as well as \u03c32\ny. If the dimensionality of \u03b8is small,\nwe can compute a discrete grid of possible values, centered on the MAP estimate \u02c6\u03b8(computed\nas above). We can then approximate the posterior over the latent variables using\np(f|D)\u221dS/summationdisplay\ns=1p(f|D,\u03b8s)p(\u03b8s|D)\u03b4s (15.25)\nwhere\u03b4sdenotes the weight for grid point s.\nIn higher dimensions, a regular grid suffers from the curse of dimensionality. An obvious\nalternative is Monte Carlo, but this can be slow. Another approach is to use a form of quasi-Monte Carlo, whereby we place grid points at the mode, and at a distance \u00b11sd from the mode\nalong each dimension, for a total of 2|\u03b8|+1points. This is called a central composite design\n(Rue et al. 2009). (This is also used in the unscented Kalman \ufb01lter, see Section 18.5.2.) To makethis Gaussian-like approximation more reasonable, we often log-transform the hyper-parameters.See Figure 15.6 for an illustration.", "554": "524 Chapter 15. Gaussian processes\n15.2.4.3 Multiple kernel learning\nA quite different approach to optimizing kernel parameters known as multiple kernel learning.\nThe idea is to de\ufb01ne the kernel as a weighted sum of base kernels, \u03ba(x,x/prime)=/summationtext\njwj\u03baj(x,x/prime),\nand then to optimize the weights wjinstead of the kernel parameters themselves. This is\nparticularly useful if we have different kinds of data which we wish to fuse together. See\ne.g., (Rakotomamonjy et al. 2008) for an approach based on risk-minimization and convexoptimization, and (Girolami and Rogers 2005) for an approach based on variational Bayes.\n15.2.5 Computational and numerical issues *\nThe predictive mean is given by f\u2217=kT\n\u2217K\u22121\nyy. For reasons of numerical stability, it is unwise\nto directly invert Ky. A more robust alternative is to compute a Cholesky decomposition,\nKy=LLT. We can then compute the predictive mean and variance, and the log marginal\nlikelihood, as shown in the pseudo-code in Algorithm 6 (based on (Rasmussen and Williams\n2006, p19)). It takes O(N3)time to compute the Cholesky decomposition, and O(N2)time to\nsolve for\u03b1=K\u22121\nyy=L\u2212TL\u22121y. We can then compute the mean using kT\n\u2217\u03b1inO(N)time\nand the variance using k\u2217\u2217\u2212kT\u2217L\u2212TL\u22121k\u2217inO(N2)time for each test case.\nAn alternative to Cholesky decomposition is to solve the linear system Ky\u03b1=yusing\nconjugate gradients (CG). If we terminate this algorithm after kiterations, it takes O(kN2)time.\nIf we run for k=N, it gives the exact solution in O(N3)time. Another approach is to\napproximate the matrix-vector multiplies needed by CG using the fast Gauss transform. (Yang\net al. 2005); however, this doesn\u2019t scale to high-dimensional inputs. See also Section 15.6 for adiscussion of other speedup techniques.\nAlgorithm 15.1: GP regression\n1L=cholesky( K+\u03c32\nyI);\n2\u03b1=LT\\(L\\y);\n3E[f\u2217]=kT\n\u2217\u03b1;\n4v=L\\k\u2217;\n5var[f\u2217]=\u03ba(x\u2217,x\u2217)\u2212vTv;\n6logp(y|X)=\u22121\n2yT\u03b1\u2212/summationtext\nilogLii\u2212N\n2log(2\u03c0)\n15.2.6 Semi-parametric GPs *\nSometimes it is useful to use a linear model for the mean of the process, as follows:\nf(x)=\u03b2T\u03c6(x)+r(x) (15.26)\nwherer(x)\u223cGP(0,\u03ba(x,x/prime))models the residuals. This combines a parametric and a non-\nparametric model, and is known as a semi-parametric model.\nIf we assume \u03b2\u223cN(b,B), we can integrate these parameters out to get a new GP (O\u2019Hagan\n1978):\nf(x)\u223cGP/parenleftbig\n\u03c6(x)Tb,\u03ba(x,x/prime)+\u03c6(x)TB\u03c6(x/prime)/parenrightbig\n(15.27)", "555": "15.3. GPs meet GLMs 525\nlogp(yi|fi)\u2202\n\u2202filogp(yi|fi)\u22022\n\u2202f2\nilogp(yi|fi)\nlogsigm(y ifi)ti\u2212\u03c0i \u2212\u03c0i(1\u2212\u03c0i)\nlog\u03a6(y ifi)yi\u03c6(fi)\n\u03a6(yifi)\u2212\u03c62\ni\n\u03a6(yifi)2\u2212yifi\u03c6(fi)\n\u03a6(yifi)\nTable 15.1 Likelihood, gradient and Hessian for binary logistic/ probit GP regression. We assume yi\u2208\n{\u22121,+1}and de\ufb01ne ti=(yi+1)/2\u2208{0,1}and\u03c0i= sigm(f i)for logistic regression, and \u03c0i=\u03a6 (fi)\nfor probit regression. Also, \u03c6and\u03a6are the pdf and cdf of N(0,1). From (Rasmussen and Williams 2006,\np43).\nIntegrating out \u03b2, the corresponding predictive distribution for test inputs X\u2217has the following\nform (Rasmussen and Williams 2006, p28):\np(f\u2217|X\u2217,X,y)=N (f\u2217,cov[f\u2217]) (15.28)\nf\u2217=\u03a6T\n\u2217\u03b2+KT\n\u2217K\u22121\ny(y\u2212\u03a6\u03b2) (15.29)\n\u03b2=(\u03a6TK\u22121\ny\u03a6+B\u22121)\u22121(\u03a6K\u22121\nyy+B\u22121b) (15.30)\ncov[f\u2217]=K \u2217\u2217\u2212KT\u2217K\u22121\nyK\u2217+RT(B\u22121+\u03a6K\u22121\ny\u03a6T)\u22121R (15.31)\nR=\u03a6\u2217\u2212\u03a6K\u22121\ny\u03a6\u2217 (15.32)\nThe predictive mean is the output of the linear model plus a correction term due to the GP, and\nthe predictive covariance is the usual GP covariance plus an extra term due to the uncertaintyin\u03b2.\n15.3 GPs meet GLMs\nIn this section, we extend GPs to the GLM setting, focussing on the classi\ufb01cation case. As withBayesian logistic regression, the main difficulty is that the Gaussian prior is not conjugate tothe bernoulli/ multinoulli likelihood. There are several approximations one can adopt: Gaussianapproximation (Section 8.4.3), expectation propagation (Kuss and Rasmussen 2005; Nickisch andRasmussen 2008), variational (Girolami and Rogers 2006; Opper and Archambeau 2009), MCMC(Neal 1997; Christensen et al. 2006), etc. Here we focus on the Gaussian approximation, since itis the simplest and fastest.\n15.3.1 Binary classi\ufb01cation\nIn the binary case, we de\ufb01ne the model as p(yi|xi)=\u03c3(yif(xi)), where, following (Rasmussen\nand Williams 2006), we assume yi\u2208{ \u22121,+1}, and we let \u03c3(z) = sigm(z )(logistic regression)\nor\u03c3(z)=\u03a6 (z)(probit regression). As for GP regression, we assume f\u223cGP(0,\u03ba).\n15.3.1.1 Computing the posterior\nDe\ufb01ne the log of the unnormalized posterior as follows:\n/lscript(f)=l o gp(y|f)+logp(f|X)=l o g p(y|f)\u22121\n2fTK\u22121f\u22121\n2log|K|\u2212N\n2log2\u03c0(15.33)", "556": "526 Chapter 15. Gaussian processes\nLetJ(f)/defines\u2212/lscript(f)be the function we want to minimize. The gradient and Hessian of this are\ngiven by\ng=\u2212\u2207logp(y|f)+K\u22121f (15.34)\nH=\u2212\u2207\u2207logp(y|f)+K\u22121=W+K\u22121(15.35)\nNote that W/defines\u2212\u2207\u2207logp(y|f)is a diagonal matrix because the data are iid (conditional on\nf). Expressions for the gradient and Hessian of the log likelihood for the logit and probit case\nare given in Sections 8.3.1 and 9.4.1, and summarized in Table 15.1.\nWe can use IRLS to \ufb01nd the MAP estimate. The update has the form\nfnew=f\u2212H\u22121g=f+(K\u22121+W)\u22121(\u2207logp(y|f)\u2212K\u22121f) (15.36)\n=(K\u22121+W)\u22121(Wf+\u2207logp(y|f)) (15.37)\nAt convergence, the Gaussian approximation of the posterior takes the following form:\np(f|X,y)\u2248N(\u02c6f,(K\u22121+W)\u22121) (15.38)\n15.3.1.2 Computing the posterior predictive\nWe now compute the posterior predictive. First we predict the latent function at the test case\nx\u2217. For the mean we have\nE[f\u2217|x\u2217,X,y]=/integraldisplay\nE[f\u2217|f,x\u2217,X,y]p(f|X,y)df (15.39)\n=/integraldisplay\nkT\n\u2217K\u22121fp(f|X,y)df (15.40)\n=kT\u2217K\u22121E[f|X,y]\u2248kT\u2217K\u22121\u02c6f (15.41)\nwhere we used Equation 15.8 to get the mean of f\u2217given noise-free f.\nTo compute the predictive variance, we use the rule of iterated variance:\nvar[f\u2217]=E [var[f\u2217|f]]+v ar[ E[f\u2217|f]] (15.42)\nwhere all probabilities are conditioned on x\u2217,X,y. From Equation 15.9 we have\nE[var[f\u2217|f]] =E/bracketleftbig\nk\u2217\u2217\u2212kT\u2217K\u22121k\u2217/bracketrightbig\n=k\u2217\u2217\u2212kT\u2217K\u22121k\u2217 (15.43)\nFrom Equation 15.9 we have\nvar[E[f\u2217|f] ]=v a r/bracketleftbig\nk\u2217K\u22121f/bracketrightbig\n=kT\u2217K\u22121cov[f]K\u22121k\u2217 (15.44)\nCombining these we get\nvar[f\u2217]=k \u2217\u2217\u2212kT\u2217(K\u22121\u2212K\u22121cov[f]K\u22121)k\u2217 (15.45)\nFrom Equation 15.38 we have cov[f]\u2248(K\u22121+W)\u22121. Using the matrix inversion lemma we\nget\nvar[f\u2217]\u2248k\u2217\u2217\u2212kT\u2217K\u22121k\u2217+kT\u2217K\u22121(K\u22121+W)\u22121K\u22121k\u2217 (15.46)\n=k\u2217\u2217\u2212kT\u2217(K+W\u22121)\u22121k\u2217 (15.47)", "557": "15.3. GPs meet GLMs 527\nSo in summary we have\np(f\u2217|x\u2217,X,y)=N(E[f\u2217],var[f\u2217]) (15.48)\nTo convert this in to a predictive distribution for binary responses, we use\n\u03c0\u2217=p(y\u2217=1|x\u2217,X,y)\u2248/integraldisplay\n\u03c3(f\u2217)p(f\u2217|x\u2217,X,y)df\u2217 (15.49)\nThis can be approximated using any of the methods discussed in Section 8.4.4, where we\ndiscussed Bayesian logistic regression. For example, using the probit approximation of Sec-tion 8.4.4.2, we have \u03c0\n\u2217\u2248sigm(\u03ba(v)E[f\u2217]),w h e r ev = var[f \u2217]and\u03ba2(v)=( 1+ \u03c0v/8)\u22121.\n15.3.1.3 Computing the marginal likelihood\nWe need the marginal likelihood in order to optimize the kernel parameters. Using the Laplaceapproximation in Equation 8.54 we have\nlogp(y|X)\u2248/lscript(\u02c6f)\u22121\n2log|H|+const (15.50)\nHence\nlogp(y|X)\u2248logp(y|\u02c6f)\u22121\n2\u02c6fTK\u22121\u02c6f\u22121\n2log|K|\u22121\n2log|K\u22121+W| (15.51)\nComputing the derivatives\u2202logp(y|X,\u03b8)\n\u2202\u03b8jis more complex than in the regression case, since \u02c6f\nandW, as well as K, depend on \u03b8. Details can be found in (Rasmussen and Williams 2006,\np125).\n15.3.1.4 Numerically stable computation *\nTo implement the above equations in a numerically stable way, it is best to avoid inverting K\norW. (Rasmussen and Williams 2006, p45) suggest de\ufb01ning\nB=IN+W1\n2KW1\n2 (15.52)\nwhich has eigenvalues bounded below by 1 (because of the I) and above by 1+N\n4maxijKij\n(becausewii=\u03c0i(1\u2212\u03c0)\u22640.25), and hence can be safely inverted.\nOne can use the matrix inversion lemma to show\n(K\u22121+W)\u22121=K\u2212KW1\n2B\u22121W1\n2K (15.53)\nHence the IRLS update becomes\nfnew=(K\u22121+W)\u22121(Wf+\u2207logp(y|f))/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\nb(15.54)\n=K(I\u2212W1\n2B\u22121W1\n2K)b (15.55)\n=K(b\u2212W1\n2LT\\(L\\(W1\n2Kb)))/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\na(15.56)", "558": "528 Chapter 15. Gaussian processes\nwhereB=LLTis a Cholesky decomposition of B. The \ufb01tting algorithm takes in O(TN3)\ntime and O(N2)space, where Tis the number of Newton iterations.\nAt convergence we have a=K\u22121\u02c6f, so we can evaluate the log marginal likelihood (Equa-\ntion 15.51) using\nlogp(y|X)=l o gp(y|\u02c6f)\u22121\n2aT\u02c6f\u2212/summationdisplay\nilogLii (15.57)\nwhere we exploited the fact that\n|B|=|K||K\u22121+W|=|IN+W1\n2KW1\n2| (15.58)\nWe now compute the predictive distribution. Rather than using E[f\u2217]=kT\n\u2217K\u22121\u02c6f, we exploit\nthe fact that at the mode, \u2207/lscript=0,s o\u02c6f=K(\u2207logp(y|\u02c6f)). Hence we can rewrite the predictive\nmean as follows:2\nE[f\u2217]=kT\n\u2217\u2207logp(y|\u02c6f) (15.59)\nTo compute the predictive variance, we exploit the fact that\n(K+W\u22121)\u22121=W1\n2W\u22121\n2(K+W\u22121)\u22121W\u22121\n2W1\n2=W1\n2B\u22121W1\n2 (15.60)\nto get\nvar[f\u2217]=k\u2217\u2217\u2212kT\u2217W1\n2(LLT)\u22121W1\n2k\u2217=k\u2217\u2217\u2212vTv (15.61)\nwherev=L\\(W1\n2k\u2217). We can then compute \u03c0\u2217.\nThe whole algorithm is summarized in Algorithm 16, based on (Rasmussen and Williams 2006,\np46). Fitting takes O(N3)time, and prediction takes O(N2N\u2217)time, where N\u2217is the number\nof test cases.\n15.3.1.5 Example\nIn Figure 15.7, we show a synthetic binary classi\ufb01cation problem in 2d. We use an SE kernel. On\nthe left, we show predictions using hyper-parameters set by hand; we use a short length scale,hence the very sharp turns in the decision boundary. On the right, we show the predictionsusing the learned hyper-parameters; the model favors a more parsimonious explanation of thedata.\n15.3.2 Multi-class classi\ufb01cation\nIn this section, we consider a model of the form p(yi|xi)=C a t ( yi|S(fi)),w h e r e fi=\n(fi1,...,f iC), and we assume f.c\u223cGP(0,\u03bac). Thus we have one latent function per class,\nwhich are a priori independent, and which may use different kernels. As before, we will usea Gaussian approximation to the posterior. (A similar model, but using the multinomial probitfunction instead of the multinomial logit, is described in (Girolami and Rogers 2006).)\n2. We see that training points that are well-predicted by the model, for which \u2207ilogp(yi|fi)\u22480, do not contribute\nstrongly to the prediction at test points; this is similar to the behavior of support vectors in an SVM (see Section 14.5).", "559": "15.3. GPs meet GLMs 529\nAlgorithm 15.2: GP binary classi\ufb01cation using Gaussian approximation\n1// First compute MAP estimate using IRLS;\n2f=0;\n3repeat\n4W=\u2212\u2207\u2207logp(y|f);\n5B=IN+W1\n2KW1\n2;\n6L=cholesky(B);\n7b=Wf+\u2207logp(y|f);\n8a=b\u2212W1\n2LT\\(L\\(W1\n2Kb));\n9f=Ka;\n10until converged ;\n11logp(y|X)=l o gp(y|f)\u22121\n2aTf\u2212/summationtext\nilogLii;\n12// Now perform prediction ;\n13E[f\u2217]=kT\n\u2217\u2207logp(y|f);\n14v=L\\(W1\n2k\u2217);\n15var[f\u2217]=k\u2217\u2217\u2212vTv;\n16p(y\u2217=1 )=/integraltext\nsigm(z)N(z|E[f\u2217],var[f\u2217])dz;\nSE kernel, l=0.500, \u03c32=10.000\n\u22124 \u22123 \u22122 \u22121 0 1 2 3 4\u22124\u22123\u22122\u2212101234\n0.10.20.30.40.50.60.70.80.9\n(a)SE kernel, l=1.280, \u03c32=14.455\n\u22124 \u22123 \u22122 \u22121 0 1 2 3 4\u22124\u22123\u22122\u2212101234\n0.10.20.30.40.50.60.70.80.9\n(b)\nFigure 15.7 Contours of the posterior predictive probability for the red circle class generated by a GP with\nan SE kernel. Thick black line is the decision boundary if we threshold at a probability of 0.5. (a) Manual\nparameters, short length scale. (b) Learned parameters, long length scale. Figure generated by gpcDemo2d ,\nbased on code by Carl Rasmussen.", "560": "530 Chapter 15. Gaussian processes\n15.3.2.1 Computing the posterior\nThe unnormalized log posterior is given by\n/lscript(f)=\u22121\n2fTK\u22121f+yTf\u2212N/summationdisplay\ni=1log/parenleftBiggC/summationdisplay\nc=1expfic/parenrightBigg\n\u22121\n2log|K|\u2212CN\n2log2\u03c0(15.62)\nwhere\nf=(f11,...,f N1,f12,...,f N2,\u00b7\u00b7\u00b7,f1C,...,f NC)T(15.63)\nandyis a dummy encoding of the yi\u2019s which has the same layout as f. Also,Kis a block\ndiagonal matrix containing Kc,w h e r eKc=[\u03bac(xi,xj)]models the correlation of the c\u2019th\nlatent function.\nThe gradient and Hessian are given by\n\u2207/lscript=\u2212K\u22121f+y\u2212\u03c0 (15.64)\n\u2207\u2207/lscript=\u2212K\u22121\u2212W (15.65)\nwhereW/definesdiag(\u03c0)\u2212\u03a0\u03a0T,w h e r e\u03a0is aCN\u00d7Nmatrix obtained by stacking diag(\u03c0 :c)\nvertically. (Compare these expressions to standard logistic regression in Section 8.3.7.)\nWe can use IRLS to compute the mode. The Newton step has the form\nfnew=(K\u22121+W)\u22121(Wf+y\u2212\u03c0) (15.66)\nNaively implementing this would take O(C3N3)time. However, we can reduce this to O(CN3),\nas shown in (Rasmussen and Williams 2006, p52).\n15.3.2.2 Computing the posterior predictive\nWe can compute the posterior predictive in a manner analogous to Section 15.3.1.2. For the\nmean of the latent response we have\nE[f\u2217c]=kc(x\u2217)TK\u22121\nc\u02c6fc=kc(x\u2217)T(yc\u2212\u02c6\u03c0c) (15.67)\nWe can put this in vector form by writing\nE[f\u2217]=Q\u2217T(y\u2212\u02c6\u03c0) (15.68)\nwhere\nQ\u2217=\u239b\n\u239c\u239dk1(x\u2217)...0\n...\n0...kC(x\u2217)\u239e\n\u239f\u23a0 (15.69)\nUsing a similar argument to Equation 15.47, we can show that the covariance of the latent\nresponse is given by\ncov[f\u2217]=\u03a3 +QT\n\u2217K\u22121(K\u22121+W)\u22121K\u22121Q\u2217 (15.70)\n=d i a g ( k(x\u2217,x\u2217))\u2212QT\u2217(K+W\u22121)\u22121Q\u2217 (15.71)", "561": "15.3. GPs meet GLMs 531\nwhere\u03a3is aC\u00d7Cdiagonal matrix with \u03a3cc=\u03bac(x\u2217,x\u2217)\u2212kT\nc(x\u2217)K\u22121\nckc(x\u2217), and\nk(x\u2217,x\u2217)=[\u03bac(x\u2217,x\u2217)].\nTo compute the posterior predictive for the visible response, we need to use\np(y|x\u2217,X,y)\u2248/integraldisplay\nCat(y|S(f\u2217))N(f\u2217|E[f\u2217],cov[f\u2217])df\u2217 (15.72)\nWe can use any of deterministic approximations to the softmax function discussed in Sec-\ntion 21.8.1.1 to compute this. Alternatively, we can just use Monte Carlo.\n15.3.2.3 Computing the marginal likelihood\nUsing arguments similar to the binary case, we can show that\nlogp(y|X)\u2248\u22121\n2\u02c6fTK\u22121\u02c6f+yT\u02c6f\u2212N/summationdisplay\ni=1log/parenleftBiggC/summationdisplay\nc=1exp\u02c6fic/parenrightBigg\n\u22121\n2log|ICN+W1\n2KW1\n2|(15.73)\nThis can be optimized numerically in the usual way.\n15.3.2.4 Numerical and computational issues\nOne can implement model \ufb01tting in O(TCN3)time and O(CN2)space, where Tis the\nnumber of Newton iterations, using the techniques described in (Rasmussen and Williams 2006,p50). Prediction takes O(CN\n3+CN2N\u2217)time, where N\u2217is the number of test cases.\n15.3.3 GPs for Poisson regression\nIn this section, we illustrate GPs for Poisson regression. An interesting application of this is tospatialdisease mapping. For example, (Vanhatalo et al. 2010) discuss the problem of modeling\nthe relative risk of heart attack in different regions in Finland. The data consists of the heartattacks in Finland from 1996-2000 aggregated into 20km x 20km lattice cells. The model hasthe following form:\ny\ni\u223cPoi(eiri) (15.74)\nwhereeiis the known expected number of deaths (related to the population of cell iand the\noverall death rate), and riis therelative risk of celliwhich we want to infer. Since the\ndata counts are small, we regularize the problem by sharing information with spatial neighbors.Hence we assume f/defineslog(r)\u223cGP(0,\u03ba), where we use a Matern kernel with \u03bd=3/2, and a\nlength scale and magnitude that are estimated from data.\nFigure 15.8 gives an example of the kind of output one can obtain from this method, based\non data from 911 locations. On the left we plot the posterior mean relative risk (RR), and on theright, the posterior variance. We see that the RR is higher in Eastern Finland, which is consistentwith other studies. We also see that the variance in the North is higher, since there are fewerpeople living there.", "562": "532 Chapter 15. Gaussian processes\n0 10 20 300102030405060Posterior mean of the relative risk, FIC\n  \n0.70.80.911.11.21.31.4\n(a)0 10 20 300102030405060Posterior variance of the relative risk, FIC\n  \n0.0050.010.0150.020.0250.030.035\n(b)\nFigure 15.8 We show the relative risk of heart disease in Finland using a Poisson GP. Left: posterior mean.\nRight: posterior variance. Figure generated by gpSpatialDemoLaplace , written by Jarno Vanhatalo.\n15.4 Connection with other methods\nThere are variety of other methods in statistics and machine learning that are closely related to\nGP regression/ classi\ufb01cation. We give a brief review of some of these below.\n15.4.1 Linear models compared to GPs\nConsider Bayesian linear regression for D-dimensional features, where the prior on the weights\nisp(w)=N(0,\u03a3). The posterior predictive distribution is given by the following;\np(f\u2217|x\u2217,X,y)=N(\u03bc,\u03c32) (15.75)\n\u03bc=1\n\u03c32yxT\n\u2217A\u22121XTy (15.76)\n\u03c32=xT\n\u2217A\u22121x\u2217 (15.77)\nwhereA=\u03c3\u22122\nyXTX+\u03a3\u22121. One can show that we can rewrite the above distribution as\nfollows\n\u03bc=xT\n\u2217\u03a3XT(K+\u03c32\nyI)\u22121y (15.78)\n\u03c32=xT\n\u2217\u03a3x\u2217\u2212xT\n\u2217\u03a3XT(K+\u03c32I)\u22121X\u03a3x\u2217 (15.79)\nwhere we have de\ufb01ned K=X\u03a3XT, which is of size N\u00d7N. Since the features only ever\nappear in the form X\u03a3XT,xT\n\u2217\u03a3XTorxT\n\u2217\u03a3x\u2217, we can kernelize the above expression by\nde\ufb01ning\u03ba(x,x/prime)=xT\u03a3x/prime.\nThus we see that Bayesian linear regression is equivalent to a GP with covariance function\n\u03ba(x,x/prime)=xT\u03a3x/prime. Note, however, that this is a degenerate covariance function, since it has at\nmostDnon-zero eigenvalues. Intuitively this re\ufb02ects the fact that the model can only represent\na limited number of functions. This can result in under\ufb01tting, since the model is not \ufb02exible\nenough to capture the data. What is perhaps worse, it can result in overcon\ufb01dence, since the", "563": "15.4. Connection with other methods 533\nmodel\u2019s prior is so impoverished that its posterior will become too concentrated. So not only is\nthe model wrong, it think it\u2019s right!\n15.4.2 Linear smoothers compared to GPs\nAlinear smoother is a regression function which is a linear function of the training outputs:\n\u02c6f(x\u2217)=/summationdisplay\niwi(x\u2217)yi (15.80)\nwherewi(x\u2217)is called the weight function (Silverman 1984). (Do not confuse this with a linear\nmodel, where the output is a linear function of the input vector.)\nThere are a variety of linear smoothers, such as kernel regression (Section 14.7.4), locally\nweighted regression (Section 14.7.5), smoothing splines (Section 15.4.6), and GP regression. To seethat GP regession is a linear smoother, note that the mean of the posterior predictive distributiono faG Pi sg i v e nb y\nf(x\u2217)=kT\n\u2217(K+\u03c32\nyIN)\u22121y=N/summationdisplay\ni=1yiwi(x\u2217) (15.81)\nwherewi(x\u2217)=[ (K+\u03c32\nyIN)\u22121k\u2217]i.\nIn kernel regression, we derive the weight function from a smoothing kernel rather than a\nMercer kernel, so it is clear that the weight function will then have local support. In the case\nof a GP, things are not as clear, since the weight function depends on the inverse of K.F o r\ncertain GP kernel functions, we can analytically derive the form of wi(x); this is known as the\nequivalent kernel (Silverman 1984). One can show that/summationtextN\ni=1wi(x\u2217)=1, although we may\nhavewi(x\u2217)<0, so we are computing a linear combination but not a convex combination of\ntheyi\u2019s. More interestingly, wi(x\u2217)is a local function, even if the original kernel used by the GP\nis not local. Futhermore the effective bandwidth of the equivalent kernel of a GP automatically\ndecreases as the sample size Nincreases, whereas in kernel smoothing, the bandwidth hneeds\nto be set by hand to adapt to N. See e.g., (Rasmussen and Williams 2006, Sec 2.6,Sec 7.1) for\ndetails.\n15.4.2.1 Degrees of freedom of linear smoothers\nIt is clear why this method is called \u201clinear\u201d, but why is it called a \u201csmoother\u201d? This is bestexplained in terms of GPs. Consider the prediction on the training set:\nf=K(K+\u03c32\ny)\u22121y (15.82)\nNow letKhave the eigendecomposition K=/summationtextN\ni=1\u03bbiuiuT\ni. SinceKis real and symmetric\npositive de\ufb01nite, the eigenvalues \u03bbiare real and non-negative, and the eigenvectors uiare\northonormal. Now let y=/summationtextN\ni=1\u03b3iui,w h e r e\u03b3i=uT\niy. Then we can rewrite the above\nequation as follows:\nf=N/summationdisplay\ni=1\u03b3i\u03bbi\n\u03bbi+\u03c32yui (15.83)", "564": "534 Chapter 15. Gaussian processes\nThis is the same as Equation 7.47, except we are working with the eigenvectors of the Gram\nmatrixKinstead of the data matrix X. In any case, the interpretation is similar: if\u03bbi\n\u03bbi+\u03c32\ny/lessmuch1,\nthen the corresponding basis function uiwill not have much in\ufb02uence. Consequently the high-\nfrequency components in yare smoothed out. The effective degrees of freedom of the linear\nsmoother is de\ufb01ned as\ndof/definestr(K(K+\u03c32\nyI)\u22121)=N/summationdisplay\ni=1\u03bbi\n\u03bbi+\u03c32y(15.84)\nThis speci\ufb01es how \u201cwiggly\u201d the curve is.\n15.4.3 SVMs compared to GPs\nWe saw in Section 14.5.2 that the SVM objective for binary classi\ufb01cation is given by Equation 14.57\nJ(w)=1\n2||w||2+CN/summationdisplay\ni=1(1\u2212yifi)+ (15.85)\nWe also know from Equation 14.59 that the optimal solution has the form w=/summationtext\ni\u03b1ixi,\nso||w||2=/summationtext\ni,j\u03b1i\u03b1jxT\nixj. Kernelizing we get ||w||2=\u03b1K\u03b1. From Equation 14.61, and\nabsorbing the \u02c6w0term into one of the kernels, we have f=K\u03b1,s o||w||2=fTK\u22121f. Hence\nthe SVM objective can be rewritten as\nJ(f)=1\n2fTf+CN/summationdisplay\ni=1(1\u2212yifi)+ (15.86)\nCompare this to MAP estimation for GP classi\ufb01er:\nJ(f)=1\n2fTf\u2212N/summationdisplay\ni=1logp(yi|fi) (15.87)\nIt is tempting to think that we can \u201cconvert\u201d an SVM into a GP by \ufb01guring out what likelihood\nwould be equivalent to the hinge loss. However, it turns out there is no such likelihood (Sollich2002), although there is a pseudo-likelihood that matches the SVM (see Section 14.5.5).\nFrom Figure 6.7 we saw that the hinge loss and the logistic loss (as well as the probit loss)\nare quite similar to each other. The main difference is that the hinge loss is strictly 0 for errorslarger than 1. This gives rise to a sparse solution. In Section 14.3.2, we discussed other waysto derive sparse kernel machines. We discuss the connection between these methods and GPsbelow.\n15.4.4 L1VM and RVMs compared to GPs\nSparse kernel machines are just linear models with basis function expansion of the form \u03c6(x)=\n[\u03ba(x,x1),...,\u03ba(x,xN)]. From Section 15.4.1, we know that this is equivalent to a GP with the\nfollowing kernel:\n\u03ba(x,x/prime)=D/summationdisplay\nj=11\n\u03b1j\u03c6j(x)\u03c6j(x/prime) (15.88)", "565": "15.4. Connection with other methods 535\nwherep(w)=N(0,diag(\u03b1\u22121\nj)). This kernel function has two interesting properties. First, it\nis degenerate, meaning it has at most Nnon-zero eigenvalues, so the joint distribution p(f,f\u2217)\nwill be highly constrained. Second, the kernel depends on the training data. This can cause the\nmodel to be overcon\ufb01dent when extrapolating beyond the training data. To see this, considera pointx\n\u2217far outside the convex hull of the data. All the basis functions will have values\nclose to 0, so the prediction will back off to the mean of the GP. More worryingly, the variancewill back off to the noise variance. By contrast, when using a non-degenerate kernel function,the predictive variance increases as we move away from the training data, as desired. See(Rasmussen and Qui\u00f1onero-Candela 2005) for further discussion.\n15.4.5 Neural networks compared to GPs\nIn Section 16.5, we will discuss neural networks, which are a nonlinear generalization of GLMs.In the binary classi\ufb01cation case, a neural network is de\ufb01ned by a logistic regression modelapplied to a logistic regression model:\np(y|x,\u03b8)=B e r/parenleftbig\ny|sigm/parenleftbig\nw\nTsigm(Vx)/parenrightbig/parenrightbig\n(15.89)\nIt turns out there is an interesting connection between neural networks and Gaussian processes,as \ufb01rst pointed out by (Neal 1996).\nTo explain the connection, we follow the presentation of (Rasmussen and Williams 2006, p91).\nConsider a neural network for regression with one hidden layer. This has the form\np(y|x,\u03b8)=N(y|f(x;\u03b8),\u03c3\n2) (15.90)\nwhere\nf(x)=b+H/summationdisplay\nj=1vjg(x;uj) (15.91)\nwherebis the offset of bias term, vjis the output weight from hidden unit jto the response\ny,ujare the inputs weights to unit jfrom the input x, andg()is the hidden unit activation\nfunction. This is typically the sigmoid or tanh function, but can be any smooth function.\nLet us use the following priors on the weights: where b\u223cN(0,\u03c32\nb)v\u223c/producttext\njN(vj|0,\u03c32\nw),\nu\u223c/producttext\njp(uj)for some unspeci\ufb01ed p(uj). Denoting all the weights by \u03b8we have\nE\u03b8[f(x) ]=0 (15.92)\nE\u03b8[f(x)f(x/prime)] =\u03c32\nb+/summationdisplay\nj\u03c32\nvEv[g(x;uj)g(x/prime;uj)] (15.93)\n=\u03c32\nb+H\u03c32\nvEu[g(x;u)g(x/prime;u)] (15.94)\nwhere the last equality follows since the Hhidden units are iid. If we let \u03c32\nvscale as\u03c92/H\n(since more hidden units will increase the input to the \ufb01nal node, so we should scale downthe magnitude of the weights), then the last term becomes \u03c9\n2Eu[g(x;u)g(x/prime;u)]. This is a\nsum over Hiid random variables. Assuming that gis bounded, we can apply the central limit\ntheorem. The result is that as H\u2192\u221e, we get a Gaussian process.", "566": "536 Chapter 15. Gaussian processes\n\u22120.5\n\u22120.50\n00.50.5\n0.950.95\ninput, xinput, x\u02bc\n\u22124 0 4\u2212404\n(a)\u22124 0 4\u2212101\ninput, xoutput, f(x)\u03c3 = 10\n\u03c3 = 3\n\u03c3 = 1\n(b)\nFigure 15.9 (a) Covariance function \u03baNN(x,x/prime)for\u03c30=1 0,\u03c3=1 0. (b) Samples from from a GP with\nthis kernel, using various values of \u03c3. Figure generated by gpnnDemo , written by Chris Williams.\nIf we use as activation / transfer function g(x;u)=erf(u0+/summationtextD\nj=1ujxj), where erf( z)=\n2/\u221a\u03c0/integraltextz\n0e\u2212t2dt, and we choose u\u223cN(0,\u03a3), then (Williams 1998) showed that the covariance\nkernel has the form\n\u03baNN(x,x/prime)=2\n\u03c0sin\u22121/parenleftBigg\n2\u02dcxT\u03a3\u02dcx/prime\n/radicalbig\n(1+2\u02dcxT\u03a3\u02dcx)(1+2(\u02dcx/prime)T\u03a3\u02dcx/prime)/parenrightBigg\n(15.95)\nwhere\u02dcx=( 1,x1,...,x D). This is a true \u201cneural network\u201d kernel, unlike the \u201csigmoid\u201d kernel\n\u03ba(x,x/prime)=t a n h ( a+bxTx/prime), which is not positive de\ufb01nite.\nFigure 15.9(a) illustrates this kernel when D=2and\u03a3= diag(\u03c32\n0,\u03c32). Figure 15.9(b) shows\nsome functions sampled from the corresponding GP. These are equivalent to functions which\nare superpositions of erf( u0+ux)whereu0anduare random. As \u03c32increases, the variance\nofuincreases, so the function varies more quickly. Unlike the RBF kernel, functions sampled\nfrom this kernel do not tend to 0 away from the data, but rather they tend to remain at thesame value they had at the \u201cedge\u201d of the data.\nNow suppose we use an RBF network, which is equivalent to a hidden unit activation function\nof the form g(x;u) = exp( \u2212|x\u2212u|\n2/(2\u03c32\ng)).I fu\u223cN(0,\u03c32\nuI), one can show that the\ncoresponding kernel is equivalent to the RBF or SE kernel.\n15.4.6 Smoothing splines compared to GPs *\nSmoothing splines are a widely used non-parametric method for smoothly interpolating data\n(Green and Silverman 1994). They are are a special case of GPs, as we will see. They are usuallyused when the input is 1 or 2 dimensional.\n15.4.6.1 Univariate splines\nThe basic idea is to \ufb01t a function fby minimizing the discrepancy to the data plus a smoothing\nterm that penalizes functions that are \u201ctoo wiggly\u201d. If we penalize the m\u2019th derivative of the", "567": "15.4. Connection with other methods 537\nfunction, the objective becomes\nJ(f)=N/summationdisplay\ni=1(f(xi)\u2212yi)2+\u03bb/integraldisplay\n(dm\ndxmf(x))2dx (15.96)\nOne can show (Green and Silverman 1994) that the solution is a piecewise polynomial where\nthe polynomials have order 2m\u22121in the interior bins [xi\u22121,xi](denotedI), and order m\u22121\nin the two outermost intervals (\u2212\u221e,x1]and[xN,\u221e):\nf(x)=m\u22121/summationdisplay\nj=0\u03b2jxj+I(x\u2208I)/parenleftBiggN/summationdisplay\ni=1\u03b1i(x\u2212xi)2m\u22121\n+/parenrightBigg\n+I(x/negationslash\u2208I)/parenleftBiggN/summationdisplay\ni=1\u03b1i(x\u2212xi)m\u22121\n+/parenrightBigg\n(15.97)\nFor example, if m=2, we get the (natural) cubic spline\nf(x)=\u03b2 0+\u03b21x+I(x\u2208I)/parenleftBiggN/summationdisplay\ni=1\u03b1i(x\u2212xi)3\n+/parenrightBigg\n+I(x/negationslash\u2208I)/parenleftBiggN/summationdisplay\ni=1\u03b1i(x\u2212xi)+/parenrightBigg\n(15.98)\nwhich is a series of truncated cubic polynomials, whose left hand sides are located at each of the\nNtraining points. (The fact that the model is linear on the edges prevents it from extrapolating\ntoo wildly beyond the range of the data; if we drop this requirement, we get an \u201cunrestricted\u201dspline.)\nWe can clearly \ufb01t this model using ridge regression: \u02c6w=(\u03a6\nT\u03a6+\u03bbIN)\u22121\u03a6Ty, where the\ncolumns of \u03a6are 1,xiand(x\u2212xi)3\n+fori=2:N\u22121and(x\u2212xi)+fori=1ori=N.\nHowever, we can also derive an O(N)time method (Green and Silverman 1994, Sec 2.3.3).\n15.4.6.2 Regression splines\nIn general, we can place the polynomials at a \ufb01xed set of Klocations known as knots, denoted\n\u03bek. The result is called a regression spline. This is a parametric model, which uses basis\nfunction expansion of the following form (where we drop the interior/ exterior distinction for\nsimplicity):\nf(x)=\u03b2 0+\u03b21x+K/summationdisplay\nk=1\u03b1j(x\u2212\u03bek)3\n+ (15.99)\nChoosing the number and locations of the knots is just like choosing the number and values of\nthe support vectors in Section 14.3.2. If we impose an /lscript2regularizer on the regression coefficients\n\u03b1j, the method is known as penalized splines. See Section 9.6.1 for a practical example of\npenalized splines.\n15.4.6.3 The connection with GPs\nOne can show (Rasmussen and Williams 2006, p139) that the cubic spline is the MAP estimateof the following function\nf(x)=\u03b2\n0+\u03b21x+r(x) (15.100)", "568": "538 Chapter 15. Gaussian processes\nwherep(\u03b2j)\u221d1(so that we don\u2019t penalize the zero\u2019th and \ufb01rst derivatives of f), andr(x)\u223c\nGP(0,\u03c32\nf\u03basp(x,x/prime)),w h e r e\n\u03basp(x,x/prime)/defines/integraldisplay1\n0(x\u2212u)+(x/prime\u2212u)+du (15.101)\nNote that the kernel in Equation 15.101 is rather unnatural, and indeed posterior samples from\nthe resulting GP are rather unsmooth. However, the posterior mode/mean is smooth. This shows\nthat regularizers don\u2019t always make good priors.\n15.4.6.4 2d input (thin-plate splines)\nOne can generalize cubic splines to 2d input by de\ufb01ning a regularizer of the following form:\n/integraldisplay/integraldisplay/bracketleftBigg/parenleftbigg\u22022f(x)\n\u2202x2\n1/parenrightbigg2\n+2/parenleftbigg\u22022f(x)\n\u2202x1\u2202x2/parenrightbigg2\n+/parenleftbigg\u22022f(x)\n\u2202x22/parenrightbigg2/bracketrightBigg\ndx1dx2 (15.102)\nOne can show that the solution has the form\nf(x)=\u03b2 0+\u03b2T\n1x+N/summationdisplay\ni=1\u03b1i\u03c6i(x) (15.103)\nwhere\u03c6i(x)=\u03b7(||x\u2212xi||), and\u03b7(z)=z2logz2. This is known as a thin plate spline. This\nis equivalent to MAP estimation with a GP whose kernel is de\ufb01ned in (Williams and Fitzgibbon\n2006).\n15.4.6.5 Higher-dimensional inputs\nIt is hard to analytically solve for the form of the optimal solution when using higher-orderinputs. However, in the parametric regression spline setting, where we forego the regularizer onf, we have more freedom in de\ufb01ning our basis functions. One way to handle multiple inputs is\nto use atensor product basis, de\ufb01ned as the cross product of 1d basis functions. For example,\nfor 2d input, we can de\ufb01ne\nf(x\n1,x2)=\u03b2 0+/summationdisplay\nm\u03b21m(x1\u2212\u03be1m)++/summationdisplay\nm\u03b22m(x2\u2212\u03be2m)+ (15.104)\n+/summationdisplay\nm\u03b212m(x1\u2212\u03be1m)+(x2\u2212\u03be2m)+ (15.105)\nIt is clear that for high-dimensional data, we cannot allow higher-order interactions, because\nthere will be too many parameters to \ufb01t. One approach to this problem is to use a searchprocedure to look for useful interaction terms. This is known as MARS, which stands for\u201cmultivariate adaptive regression splines\u201d. See Section 16.3.3 for details.\n15.4.7 RKHS methods compared to GPs *\nWe can generalize the idea of penalizing derivatives of functions, as used in smoothing splines,to \ufb01t functions with a more general notion of smoothness. Recall from Section 14.2.3 that", "569": "15.4. Connection with other methods 539\nMercer\u2019s theorem says that any positive de\ufb01nite kernel function can be represented in terms of\neigenfunctions:\n\u03ba(x,x/prime)=\u221e/summationdisplay\ni=1\u03bbi\u03c6i(x)\u03c6i(x/prime) (15.106)\nThe\u03c6iform an orthormal basis for a function space:\nHk={f:f(x)=\u221e/summationdisplay\ni=1fi\u03c6i(x),\u221e/summationdisplay\ni=1f2\ni/\u03bbi<\u221e} (15.107)\nNow de\ufb01ne the inner product between two functions f(x)=/summationtext\u221e\ni=1fi\u03c6i(x)andg(x)=/summationtext\u221ei=1gi\u03c6i(x)in this space as follows:\n/angbracketleftf,g/angbracketrightH/defines\u221e/summationdisplay\ni=1figi\n\u03bbi(15.108)\nIn Exercise 15.1, we show that this de\ufb01nition implies that\n/angbracketleft\u03ba(x1,\u00b7),\u03ba(x2,\u00b7)/angbracketrightH=\u03ba(x1,x2) (15.109)\nThis is called the reproducing property, and the space of functions Hkis called a reproducing\nkernel Hilbert space orRKHS.\nNow consider an optimization problem of the form\nJ(f)=1\n2\u03c32yN/summationdisplay\ni=1(yi\u2212f(xi))2+1\n2||f||2\nH (15.110)\nwhere||f||Jis thenorm of a function:\n||f||H=/angbracketleftf,f/angbracketrightH=\u221e/summationdisplay\ni=1f2\ni\n\u03bbi(15.111)\nThe intuition is that functions that are complex wrt the kernel will have large norms, because\nthey will need many eigenfunctions to represent them. We want to pick a simple function thatprovides a good \ufb01t to the data.\nOne can show (see e.g., (Schoelkopf and Smola 2002)) that the solution must have the form\nf(x)=N/summationdisplay\ni=1\u03b1i\u03ba(x,xi) (15.112)\nThis is known as the representer theorem, and holds for other convex loss functions besides\nsquared error.\nWe can solve for the \u03b1by substituting in f(x)=/summationtextN\ni=1\u03b1i\u03ba(x,xi)and using the reproducing\nproperty to get\nJ(\u03b1)=1\n2\u03c32y|y\u2212K\u03b1|2+1\n2\u03b1TK\u03b1 (15.113)", "570": "540 Chapter 15. Gaussian processes\nMinimizing wrt \u03b1we \ufb01nd\n\u02c6\u03b1=(K+\u03c32\nyI)\u22121(15.114)\nand hence\n\u02c6f(x\u2217)=/summationdisplay\ni\u02c6\u03b1i\u03ba(x\u2217,xi)=kT\n\u2217(K+\u03c32\nyI)\u22121y (15.115)\nThis is identical to Equation 15.18, the posterior mean of a GP predictive distribution. Indeed,\nsince the mean and mode of a Gaussian are the same, we can see that linear regresson with anRKHS regularizer is equivalent to MAP estimation with a GP. An analogous statement holds forthe GP logistic regression case, which also uses a convex likelihood / loss function.\n15.5 GP latent variable model\nIn Section 14.4.4, we discussed kernel PCA, which applies the kernel trick to regular PCA. Inthis section, we discuss a different way to combine kernels with probabilistic PCA. The resultingmethod is known as the GP-LVM, which stands for \u201cGaussian process latent variable model\u201d\n(Lawrence 2005).\nTo explain the method, we start with PPCA. Recall from Section 12.2.4 that the PPCA model is\nas follows:\np(z\ni)=N (zi|0,I) (15.116)\np(yi|zi,\u03b8)=N (yi|Wz i,\u03c32I) (15.117)\nWe can \ufb01t this model by maximum likelihood, by integrating out the ziand maximizing W\n(and\u03c32). The objective is given by\np(Y|W,\u03c32)=( 2\u03c0)\u2212DN/ 2|C|\u2212N/2exp/parenleftbigg\n\u22121\n2tr(C\u22121YTY)/parenrightbigg\n(15.118)\nwhere C=WWT+\u03c32I. As we showed in Theorem 12.2.2, the MLE for this can be computed\nin terms of the eigenvectors of YTY.\nNow we consider the dual problem, whereby we maximize Zand integrate out W. We will\nuse a prior of the form p(W)=/producttext\njN(wj|0,I). The corresponding likelihood becomes\np(Y|Z,\u03c32)=D/productdisplay\nd=1N(y:,d|0,ZZT+\u03c32I) (15.119)\n=( 2\u03c0)\u2212DN/ 2|Kz|\u2212D/2exp/parenleftbigg\n\u22121\n2tr(K\u22121\nzYYT)/parenrightbigg\n(15.120)\nwhere Kz=ZZT+\u03c32I. Based on our discussion of the connection between the eigenvalues\nofYYTand of YTYin Section 14.4.4, it should come as no surprise that we can also solve\nthe dual problem using eigenvalue methods (see (Lawrence 2005) for the details).\nIf we use a linear kernel, we recover PCA. But we can also use a more general kernel:\nKz=K+\u03c32I,w h e r e Kis the Gram matrix for Z. The MLE for \u02c6Zwill no longer be available", "571": "15.5. GP latent variable model 541\n0 0.05 0.1 0.15 0.20.050.1.150.2.250.3\n(a)\u22120.8\u22120.6\u22120.4 \u22120.2 0 0.2 0.4 0.60.60.40.200.20.4\n(b)\nFigure 15.10 2d representation of 12 dimensional oil \ufb02ow data. The different colors/symbols represent\nthe 3 phases of oil \ufb02ow. (a) Kernel PCA with Gaussian kernel. (b) GP-LVM with Gaussian kernel. The\nshading represents the precision of the posterior, where lighter pixels have higher precision. From Figure 1\nof (Lawrence 2005). Used with kind permission of Neil Lawrence.\nvia eigenvalue methods; instead we must use gradient-based optimization. The objective is given\nby\n/lscript=\u2212D\n2log|Kz|\u22121\n2tr(K\u22121\nzYYT) (15.121)\nand the gradient is given by\n\u2202/lscript\n\u2202Zij=\u2202/lscript\n\u2202Kz\u2202Kz\n\u2202Zij(15.122)\nwhere\n\u2202/lscript\n\u2202Kz=K\u22121\nzYYTK\u22121\nz\u2212DK\u22121\nz (15.123)\nThe form of\u2202Kz\n\u2202Zijwill of course depend on the kernel used. (For example, with a linear kernel,\nwhereKz=ZZT+\u03c32I,w eh a v e\u2202Kz\n\u2202Z=Z.) We can then pass this gradient to any standard\noptimizer, such as conjugate gradient descent.\nLet us now compare GP-LVM to kernel PCA. In kPCA, we learn a kernelized mapping from\nthe observed space to the latent space, whereas in GP-LVM, we learn a kernelized mapping from\nthe latent space to the observed space. Figure 15.10 illustrates the results of applying kPCA and\nGP-LVM to visualize the 12 dimensional oil \ufb02ow data shown in In Figure 14.9(a). We see that the\nembedding produced by GP-LVM is far better. If we perform nearest neighbor classi\ufb01cation in\nthe latent space, GP-LVM makes 4 errors, while kernel PCA (with the same kernel but separately\noptimized hyper-parameters) makes 13 errors, and regular PCA makes 20 errors.\nGP-LVM inherits the usual advantages of probabilistic generative models, such as the ability\nto handle missing data and data of different types, the ability to use gradient-based methods\n(instead of grid search) to tune the kernel parameters, the ability to handle prior information,", "572": "542 Chapter 15. Gaussian processes\netc. For a discussion of some other probabilistic methods for (spectral) dimensionality reduction,\nsee (Lawrence 2012).\n15.6 Approximation methods for large datasets\nThe principal drawback of GPs is that they take O(N3)time to use. This is because of the\nneed to invert (or compute the Cholesky decomposition of) the N\u00d7Nkernel matrix K.A\nvariety of approximation methods have been devised which take O(M2N)time, where Mis a\nuser-speci\ufb01able parameter. For details, see (Quinonero-Candela et al. 2007).\nExercises\nExercise 15.1 Reproducing property\nProve Equation 15.109.", "573": "16 Adaptive basis function models\n16.1 Introduction\nIn Chapters 14 and 15, we discussed kernel methods, which provide a powerful way to create non-\nlinear models for regression and classi\ufb01cation. The prediction takes the form f(x)=wT\u03c6(x),\nwhere we de\ufb01ne\n\u03c6(x)=[\u03ba(x,\u03bc1),...,\u03ba(x,\u03bcN)] (16.1)\nand where \u03bckare either all the training data or some subset. Models of this form essen-\ntially perform a form of template matching, whereby they compare the input xto the stored\nprototypes \u03bck.\nAlthough this can work well, it relies on having a good kernel function to measure the\nsimilarity between data vectors. Often coming up with a good kernel function is quite difficult.For example, how do we de\ufb01ne the similarity between two images? Pixel-wise comparison ofintensities (which is what a Gaussian kernel corresponds to) does not work well. Although it ispossible (and indeed common) to hand-engineer kernels for speci\ufb01c tasks (see e.g., the pyramidmatch kernel in Section 14.2.7), it would be more interesting if we could learn the kernel.\nIn Section 15.2.4, we discussed a way to learn the parameters of a kernel function, by maxi-\nmizing the marginal likelihood. For example, if we use the ARD kernel,\n\u03ba(x,x\n/prime)=\u03b80exp\u239b\n\u239d\u22121\n2D/summationdisplay\nj=1\u03b8j(xj\u2212x/prime\nj)2\u239e\n\u23a0 (16.2)\nwe can can estimate the \u03b8j, and thus perform a form of nonlinear feature selection. However,\nsuch methods can be computationally expensive. Another approach, known as multiple kernel\nlearning (see e.g., (Rakotomamonjy et al. 2008)) uses a convex combination of base kernels,\u03ba(x,x\n/prime)=/summationtext\njwj\u03baj(x,x/prime), and then estimates the mixing weights wj. But this relies on\nhaving good base kernels (and is also computationally expensive).\nAn alternative approach is to dispense with kernels altogether, and try to learn useful features\n\u03c6(x)directly from the input data. That is, we will create what we call an adaptive basis-\nfunction model (ABM), which is a model of the form\nf(x)=w0+M/summationdisplay\nm=1wm\u03c6m(x) (16.3)", "574": "544 Chapter 16. Adaptive basis function models\nwhere\u03c6m(x)is them\u2019th basis function, which is learned from data. This framework covers all\nof the models we will discuss in this chapter.\nTypically the basis functions are parametric, so we can write \u03c6m(x)=\u03c6(x;vm),w h e r evm\nare the parameters of the basis function itself. We will use \u03b8=(w0,w1:M,{vm}M\nm=1)to\ndenote the entire parameter set. The resulting model is not linear-in-the-parameters anymore,\nso we will only be able to compute a locally optimal MLE or MAP estimate of \u03b8. Nevertheless,\nsuch models often signi\ufb01cantly outperform linear models, as we will see.\n16.2 Classi\ufb01cation and regression trees (CART)\nClassi\ufb01cation and regression trees orCARTmodels, also called decision trees (not to be\nconfused with the decision trees used in decision theory) are de\ufb01ned by recursively partitioningthe input space, and de\ufb01ning a local model in each resulting region of input space. This can berepresented by a tree, with one leaf per region, as we explain below.\n16.2.1 Basics\nTo explain the CART approach, consider the tree in Figure 16.1(a). The \ufb01rst node asks if x1is\nless than some threshold t1. If yes, we then ask if x2is less than some other threshold t2.I f\nyes, we are in the bottom left quadrant of space, R1. If no, we ask if x1is less than t3. And\nso on. The result of these axis parallel splits is to partition 2d space into 5 regions, as shown\nin Figure 16.1(b). We can now associate a mean response with each of these regions, resulting inthe piecewise constant surface shown in Figure 16.1(c).\nWe can write the model in the following form\nf(x)=E[y|x]=M/summationdisplay\nm=1wmI(x\u2208Rm)=M/summationdisplay\nm=1wm\u03c6(x;vm) (16.4)\nwhereRmis them\u2019th region, wmis the mean response in this region, and vmencodes the\nchoice of variable to split on, and the threshold value, on the path from the root to the m\u2019th leaf.\nThis makes it clear that a CART model is just a an adaptive basis-function model, where thebasis functions de\ufb01ne the regions, and the weights specify the response value in each region.We discuss how to \ufb01nd these basis functions below.\nWe can generalize this to the classi\ufb01cation setting by storing the distribution over class labels\nin each leaf, instead of the mean response. This is illustrated in Figure 16.2. This model canbe used to classify the data in Figure 1.1. For example, we \ufb01rst check the color of the object.If it is blue, we follow the left branch and end up in a leaf labeled \u201c4,0\u201d, which means wehave 4 positive examples and 0 negative examples which match this criterion. Hence we predictp(y=1|x)=4/4ifxis blue. If it is red, we then check the shape: if it is an ellipse, we\nend up in a leaf labeled \u201c1,1\u201d, so we predict p(y=1|x)=1/2. If it is red but not an ellipse,\nwe predict p(y=1|x)=0/2; If it is some other colour, we check the size: if less than 10,\nwe predict p(y=1|x)=4/4, otherwise p(y=1|x)=0/5. These probabilities are just the\nempirical fraction of positive examples that satisfy each conjunction of feature values, whichde\ufb01nes a path from the root to a leaf.", "575": "16.2. Classi\ufb01cation and regression trees (CART) 545\nX1\u2264t1\nX2\u2264t2\nR1X1\u2264t3\nR4 R5X2\u2264t4\nR2 R3\n(a)0246810\n02468102345678910\n(b)\nFigure 16.1 A simple regression tree on two inputs. Based on Figure 9.2 of (Hastie et al. 2009). Figure\ngenerated by regtreeSurfaceDemo .\n1,1 0,2shape4,0color\nsize<10\n4,0 0,5ellipse otherblueredother\nyes no\nFigure 16.2 A simple decision tree for the data in Figure 1.1. A leaf labeled as (n1,n0)means that\nthere are n1positive examples that match this path, and n0negative examples. In this tree, most of\nthe leaves are \u201cpure\u201d, meaning they only have examples of one class or the other; the only exception is\nleaf representing red ellipses, which has a label distribution of (1,1). We could distinguish positive from\nnegative red ellipses by adding a further test based on size. However, it is not always desirable to constructtrees that perfectly model the training data, due to over\ufb01tting.\n16.2.2 Growing a tree\nFinding the optimal partitioning of the data is NP-complete (Hya\ufb01l and Rivest 1976), so it is\ncommon to use the greedy procedure shown in Algorithm 6 to compute a locally optimal MLE.This method is used by CART, (Breiman et al. 1984) C4.5(Quinlan 1993), and ID3(Quinlan 1986),\nwhich are three popular implementations of the method. (See dtfitfor a simple Matlab\nimplementation.)\nThe split function chooses the best feature, and the best value for that feature, as follows:\n(j\n\u2217,t\u2217) = arg min\nj\u2208{1,...,D }min\nt\u2208Tjcost({xi,yi:xij\u2264t})+cost( {xi,yi:xij>t}) (16.5)", "576": "546 Chapter 16. Adaptive basis function models\nAlgorithm 16.1: Recursive procedure to grow a classi\ufb01cation/ regression tree\n1function \ufb01tTree(node, D, depth) ;\n2node.prediction = mean(y i:i\u2208D) // or class label distribution ;\n3(j\u2217,t\u2217,DL,DR)=split(D);\n4ifnot worthSplitting(depth, cost, DL,DR)then\n5return node\n6else\n7node.test = \u03bbx.xj\u2217<t\u2217// anonymous function;\n8node.left = \ufb01tTree(node, DL, depth+1);\n9node.right = \ufb01tTree(node, DR, depth+1);\n10return node;\nwhere the cost function for a given dataset will be de\ufb01ned below. For notational simplicity, we\nhave assumed all inputs are real-valued or ordinal, so it makes sense to compare a feature xij\nto a numeric value t. The set of possible thresholds Tjfor feature jcan be obtained by sorting\nthe unique values of xij. For example, if feature 1 has the values {4.5,\u221212,72,\u221212}, then we\nsetT1={\u221212,4.5,72}. In the case of categorical inputs, the most common approach is to\nconsider splits of the form xij=ckandxij/negationslash=ck, for each possible class label ck. Although\nwe could allow for multi-way splits (resulting in non-binary trees), this would result in data\nfragmentation, meaning too little data might \u201cfall\u201d into each subtree, resulting in over\ufb01tting.\nThe function that checks if a node is worth splitting can use several stopping heuristics, such\nas the following:\n\u2022 is the reduction in cost too small? Typically we de\ufb01ne the gain of using a feature to be a\nnormalized measure of the reduction in cost:\n\u0394/definescost(D)\u2212/parenleftbigg|DL|\n|D|cost(DL)+|DR|\n|D|cost(DR)/parenrightbigg\n(16.6)\n\u2022 has the tree exceeded the maximum desired depth?\n\u2022 is the distribution of the response in either DLorDRsufficiently homogeneous (e.g., all\nlabels are the same, so the distribution is pure)?\n\u2022 is the number of examples in either DLorDRtoo small?\nAll that remains is to specify the cost measure used to evaluate the quality of a proposed\nsplit. This depends on whether our goal is regression or classi\ufb01cation. We discuss both cases\nbelow.\n16.2.2.1 Regression cost\nIn the regression setting, we de\ufb01ne the cost as follows:\ncost(D)=/summationdisplay\ni\u2208D(yi\u2212y)2(16.7)", "577": "16.2. Classi\ufb01cation and regression trees (CART) 547\nwherey=1\n|D|/summationtext\ni\u2208Dyiis the mean of the response variable in the speci\ufb01ed set of data.\nAlternatively, we can \ufb01t a linear regression model for each leaf, using as inputs the features that\nwere chosen on the path from the root, and then measure the residual error.\n16.2.2.2 Classi\ufb01cation cost\nIn the classi\ufb01cation setting, there are several ways to measure the quality of a split. First, we\ufb01t a multinoulli model to the data in the leaf satisfying the test X\nj<tby estimating the\nclass-conditional probabilities as follows:\n\u02c6\u03c0c=1\n|D|/summationdisplay\ni\u2208DI(yi=c) (16.8)\nwhereDis the data in the leaf. Given this, there are several common error measures for\nevaluating a proposed partition:\n\u2022Misclassi\ufb01cation rate. We de\ufb01ne the most probable class label as \u02c6yc=a r g m a xc\u02c6\u03c0c. The\ncorresponding error rate is then\n1\n|D|/summationdisplay\ni\u2208DI(yi/negationslash=\u02c6y)=1\u2212\u02c6\u03c0\u02c6y (16.9)\n\u2022Entropy,o rdeviance:\nH(\u02c6\u03c0)=\u2212C/summationdisplay\nc=1\u02c6\u03c0clog\u02c6\u03c0c (16.10)\nNote that minimizing the entropy is equivalent to maximizing the information gain (Quinlan\n1986) between test Xj<tand the class label Y, de\ufb01ned by\ninfoGain( Xj<t ,Y)/definesH(Y)\u2212H(Y|Xj<t) (16.11)\n=/parenleftBigg\n\u2212/summationdisplay\ncp(y=c)logp(y=c)/parenrightBigg\n(16.12)\n+/parenleftBigg/summationdisplay\ncp(y=c|Xj<t)logp(c|Xj<t)/parenrightBigg\n(16.13)\nsince\u02c6\u03c0cis an MLE for the distribution p(c|Xj<t).1\n1. IfXjis categorical, and we use tests of the form Xj=k, then taking expectations over values of Xjgives\nthe mutual information between XjandY:E[infoGain(X j,Y)] =/summationtext\nkp(Xj=k)infoGain(X j=k,Y)=\nH(Y)\u2212H(Y|Xj)=I(Y;Xj).", "578": "548 Chapter 16. Adaptive basis function models\n0 0.2 0.4 0.6 0.8 100.050.10.150.20.250.30.350.40.450.5\n  \nError rate\nGini\nEntropy\nFigure 16.3 Node impurity measures for binary classi\ufb01cation. The horizontal axis corresponds to p, the\nprobability of class 1. The entropy measure has been rescaled to pass through (0.5,0.5). Based on Figure\n9.3 of (Hastie et al. 2009). Figure generated by giniDemo .\n\u2022Gini index\nC/summationdisplay\nc=1\u02c6\u03c0c(1\u2212\u02c6\u03c0c)=/summationdisplay\nc\u02c6\u03c0c\u2212/summationdisplay\nc\u02c6\u03c02\nc=1\u2212/summationdisplay\nc\u02c6\u03c02\nc (16.14)\nThis is the expected error rate. To see this, note that \u02c6\u03c0cis the probability a random entry in\nthe leaf belongs to class c, and(1\u2212\u02c6\u03c0cis the probability it would be misclassi\ufb01ed.\nIn the two-class case, where p=\u03c0m(1), the misclassi\ufb01cation rate is 1\u2212max(p,1\u2212p), the\nentropy is H2(p), and the Gini index is 2p(1\u2212p). These are plotted in Figure 16.3. We see\nthat the cross-entropy and Gini measures are very similar, and are more sensitive to changes in\nclass probability than is the misclassi\ufb01cation rate. For example, consider a two-class problemwith 400 cases in each class. Suppose one split created the nodes (300,100) and (100,300), whilethe other created the nodes (200,400) and (200,0). Both splits produce a misclassi\ufb01cation rate of0.25. However, the latter seems preferable, since one of the nodes is pure, i.e., it only contains\none class. The cross-entropy and Gini measures will favor this latter choice.\n16.2.2.3 Example\nAs an example, consider two of the four features from the 3-class iris dataset, shown in Fig-ure 16.4(a). The resulting tree is shown in Figure 16.5(a), and the decision boundaries are shownin Figure 16.4(b). We see that the tree is quite complex, as are the resulting decision boundaries.In Figure 16.5(b), we show that the CV estimate of the error is much higher than the training seterror, indicating over\ufb01tting. Below we discuss how to perform a tree-pruning stage to simplifythe tree.", "579": "16.2. Classi\ufb01cation and regression trees (CART) 549\n4 4.5 5 5.5 6 6.5 7 7.5 822.533.544.5\nSepal lengthSepal width\n  \nsetosa\nversicolor\nvirginica\n(a)4 4.5 5 5.5 6 6.5 7 7.5 822.533.544.5\nxyunpruned decision tree\n  \nversicolor\nsetosa\nvirginica\n(b)\nFigure 16.4 (a) Iris data. We only show the \ufb01rst two features, sepal length and sepal width, and ignore\npetal length and petal width. (b) Decision boundaries induced by the decision tree in Figure 16.5(a).\nversicolor setosa\nsetosa virginica\nversicolor versicolor\nversicolor versicolor\nversicolor virginica virginica\nvirginica versicolor\nversicolor virginica\nvirginica versicolor versicolor virginicaSL < 5.45   \nW < 2.8   SL < 6.15   \nSW < 3.45   SL < 7.05   \nSL < 5.75   SW < 2.4   \nSW < 3.1   SL < 6.95   \nSW < 2.95   SW < 3.15   \nSL < 6.55   \nSW < 2.95   SL < 6.65   \nSL < 6.45   SW < 2.65   \nSW < 2.85   SW < 2.9     SL >= 5.45\n  SW >= 2.8   SL >= 6.15\n  SW >= 3.45   SL >= 7.05\n  SL >= 5.75   SW >= 2.4\n  SW >= 3.1   SL >= 6.95\n  SW >= 2.95   SW >= 3.15\n  SL >= 6.55\n  SW >= 2.95   SL >= 6.65\n  SL >= 6.45   SW >= 2.65\n  SW >= 2.85   SW >= 2.9\n(a)0 5 10 15 200.10.20.30.40.50.60.70.8\nNumber of terminal nodesCost (misclassification error)\n  \nCross\u2212validation\nTraining set\nMin + 1 std. err.\nBest choice\n(b)\nFigure 16.5 (a) Unpruned decision tree for Iris data. (b) Plot of misclassi\ufb01cation error rate vs depth of\ntree. Figure generated by dtreeDemoIris .\n16.2.3 Pruning a tree\nTo prevent over\ufb01tting, we can stop growing the tree if the decrease in the error is not sufficient\nto justify the extra complexity of adding an extra subtree. However, this tends to be too myopic.For example, on the xor data in Figure 14.2(c), it would might never make any splits, since eachfeature on its own has little predictive power.\nThe standard approach is therefore to grow a \u201cfull\u201d tree, and then to perform pruning. This\ncan be done using a scheme that prunes the branches giving the least increase in the error. See(Breiman et al. 1984) for details.\nTo determine how far to prune back, we can evaluate the cross-validated error on each such\nsubtree, and then pick the tree whose CV error is within 1 standard error of the minimum. Thisis illustrated in Figure 16.4(b). The point with the minimum CV error corresponds to the simpletree in Figure 16.6(a).", "580": "550 Chapter 16. Adaptive basis function models\nversicolor setosa virginica\nversicolor setosaSL < 5.45   \nSW < 2.8   SL < 6.15   \nSW < 3.45     SL >= 5.45\n  SW >= 2.8   SL >= 6.15\n  SW >= 3.45\n(a)4 4.5 5 5.5 6 6.5 7 7.5 822.533.544.5\nxypruned decision tree\n  \nversicolor\nsetosa\nvirginica\n(b)\nFigure 16.6 Pruned decision tree for Iris data. Figure generated by dtreeDemoIris .\n16.2.4 Pros and cons of trees\nCART models are popular for several reasons: they are easy to interpret2, they can easily handle\nmixed discrete and continuous inputs, they are insensitive to monotone transformations of the\ninputs (because the split points are based on ranking the data points), they perform automaticvariable selection, they are relatively robust to outliers, they scale well to large data sets, andthey can be modi\ufb01ed to handle missing inputs.\n3\nHowever, CART models also have some disadvantages. The primary one is that they do\nnot predict very accurately compared to other kinds of model. This is in part due to thegreedy nature of the tree construction algorithm. A related problem is that trees are unstable:\nsmall changes to the input data can have large effects on the structure of the tree, due to thehierarchical nature of the tree-growing process, causing errors at the top to affect the rest of thetree. In frequentist terminology, we say that trees are high variance estimators. We discuss asolution to this below.\n16.2.5 Random forests\nOne way to reduce the variance of an estimate is to average together many estimates. Forexample, we can train Mdifferent trees on different subsets of the data, chosen randomly with\n2. We can postprocess the tree to derive a series of logical rulessuch as \u201cIf x1<5.45then ...\u201d (Quinlan 1990).\n3. The standard heuristic for handling missing inputs in decision trees is to look for a series of \u201dbackup\u201d variables,\nwhich can induce a similar partition to the chosen variable at any given split; these can be used in case the chosen\nvariable is unobserved at test time. These are called surrogate splits. This method \ufb01nds highly correlated features,\nand can be thought of as learning a local joint model of the input. This has the advantage over a generative modelof not modeling the entire joint distribution of inputs, but it has the disadvantage of being entirely ad hoc. A simpler\napproach, applicable to categorical variables, is to code \u201cmissing\u201d as a new value, and then to treat the data as fully\nobserved.", "581": "16.2. Classi\ufb01cation and regression trees (CART) 551\nreplacement, and then compute the ensemble\nf(x)=M/summationdisplay\nm=11\nMfm(x) (16.15)\nwherefmis them\u2019th tree. This technique is called bagging(Breiman 1996), which stands for\n\u201cbootstrap aggregating\u201d.\nUnfortunately, simply re-running the same learning algorithm on different subsets of the data\ncan result in highly correlated predictors, which limits the amount of variance reduction that is\npossible. The technique known as random forests (Breiman 2001a) tries to decorrelate the base\nlearners by learning trees based on a randomly chosen subset of input variables, as well as arandomly chosen subset of data cases. Such models often have very good predictive accuracy(Caruana and Niculescu-Mizil 2006), and have been widely used in many applications (e.g., forbody pose recognition using Microsoft\u2019s popular kinect sensor (Shotton et al. 2011)).\nBagging is a frequentist concept. It is also possible to adopt a Bayesian approach to learning\ntrees. Inparticular, (Chipmanetal.1998;Denisonetal.1998;Wuetal.2007)performapproximateinference over the space of trees (structure and parameters) using MCMC. This reduces thevariance of the predictions. We can also perform Bayesian inference over the space of ensemblesof trees, which tends to work much better. This is known as Bayesian adaptive regression\ntreesorBART(Chipman et al. 2010). Note that the cost of these sampling-based Bayesian\nmethods is comparable to the sampling-based random forest method. That is, both approachesare farily slow to train, but produce high quality classi\ufb01ers.\nUnfortunately, methods that use multiple trees (whether derived from a Bayesian or frequen-\ntist standpoint) lose their nice interpretability properties. Fortunately, various post-processingmeasures can be applied, as discussed in Section 16.8.\n16.2.6 CART compared to hierarchical mixture of experts *\nAn interesting alternative to a decision tree is known as the hierarchical mixture of experts.Figure 11.7(b) gives an illustration where we have two levels of experts. This can be thought ofas a probabilistic decision tree of depth 2, since we recursively partition the space, and applya different expert to each partition. Hastie et al. (Hastie et al. 2009, p331) write that \u201cTheHME approach is a promising competitor to CART trees\u201d. Some of the advantages include thefollowing:\n\u2022 The model can partition the input space using any set of nested linear decision boundaries.\nBy contrast, standard decision trees are constrained to use axis-parallel splits.\n\u2022 The model makes predictions by averaging over all experts. By contrast, in a standard\ndecision tree, predictions are made only based on the model in the corresponding leaf. Since\nleaves often contain few training examples, this can result in over\ufb01tting.\n\u2022 Fitting an HME involves solving a smooth continuous optimization problem (usually using\nEM), which is likely to be less prone to local optima than the standard greedy discreteoptimization methods used to \ufb01t decision trees. For similar reasons, it is computationallyeasier to \u201cbe Bayesian\u201d about the parameters of an HME (see e.g., (Peng et al. 1996; Bishop", "582": "552 Chapter 16. Adaptive basis function models\nand Svens\u00e9n 2003)) than about the structure and parameters of a decision tree (see e.g., (Wu\net al. 2007)).\n16.3 Generalized additive models\nA simple way to create a nonlinear model with multiple inputs is to use a generalized additive\nmodel(Hastie and Tibshirani 1990), which is a model of the form\nf(x)=\u03b1+f1(x1)+\u00b7\u00b7\u00b7+fD(xD) (16.16)\nHere each fjcan be modeled by some scatterplot smoother, and f(x)can be mapped to p(y|x)\nusing a link function, as in a GLM (hence the term generalized additive model).\nIf we use regression splines (or some other \ufb01xed basis function expansion approach) for the\nfj, then each fj(xj)can be written as \u03b2T\nj\u03c6j(xj), so the whole model can be written as\nf(x)=\u03b2T\u03c6(x),w h e r e\u03c6(x)=[ 1,\u03c61(x1),...,\u03c6D(xD)]. However, it is more common to use\nsmoothing splines (Section 15.4.6) for the fj. In this case, the objective (in the regression setting)\nbecomes\nJ(\u03b1,f1,...,f D)=N/summationdisplay\ni=1\u239b\n\u239dyi\u2212\u03b1\u2212D/summationdisplay\nj=1fj(xij)\u239e\u23a02\n+D/summationdisplay\nj=1\u03bbj/integraldisplay\nf/prime/prime\nj(tj)2dtj (16.17)\nwhere\u03bbjis the strength of the regularizer for fj.\n16.3.1 Back\ufb01tting\nWe now discuss how to \ufb01t the model using MLE. The constant \u03b1is not uniquely identi\ufb01able,\nsince we can always add or subtract constants to any of the fjfunctions. The convention is to\nassume/summationtextN\ni=1fj(xij)=0for allj. In this case, the MLE for \u03b1is just\u02c6\u03b1=1\nN/summationtextNi=1yi.\nTo \ufb01t the rest of the model, we can center the responses (by subtracting \u02c6\u03b1), and then\niteratively update each fjin turn, using as a target vector the residuals obtained by omitting\ntermfj:\n\u02c6fj:=smoother( {yi\u2212/summationdisplay\nk/negationslash=j\u02c6fk(xik)}N\ni=1) (16.18)\nWe should then ensure the output is zero mean using\n\u02c6fj:=\u02c6fj\u22121\nNN/summationdisplay\ni=1\u02c6fj(xij) (16.19)\nThis is called the back\ufb01tting algorithm (Hastie and Tibshirani 1990). If Xhas full column rank,\nthen the above objective is convex (since each smoothing spline is a linear operator, as shown\nin Section 15.4.2), so this procedure is guaranteed to converge to the global optimum.\nIn the GLM case, we need to modify the method somewhat. The basic idea is to replace the\nweighted least squares step of IRLS (see Section 8.3.4) with a weighted back\ufb01tting algorithm. Inthe logistic regression case, each response has weight s\ni=\u03bci(1\u2212\u03bci)associated with it, where\n\u03bci=s i g m (\u02c6\u03b1+/summationtextD\nj=1\u02c6fj(xij)).)", "583": "16.3. Generalized additive models 553\n16.3.2 Computational efficiency\nEach call to the smoother takes O(N)time, so the total cost is O(NDT),w h e r eTis the\nnumber of iterations. If we have high-dimensional inputs, \ufb01tting a GAM is expensive. One\napproach is to combine it with a sparsity penalty, see e.g., the SpAM(sparse additive model)\napproach of (Ravikumar et al. 2009). Alternatively, we can use a greedy approach, such asboosting (see Section 16.4.6)\n16.3.3 Multivariate adaptive regression splines (MARS)\nWe can extend GAMs by allowing for interaction effects. In general, we can create an ANOVAdecomposition:\nf(x)=\u03b2\n0+D/summationdisplay\nj=1fj(xj)+/summationdisplay\nj,kfjk(xj,xk)+/summationdisplay\nj,k,lfjkl(xj,xk,xl)+\u00b7\u00b7\u00b7 (16.20)\nOf course, we cannot allow for too many higher-order interactions, because there will be toomany parameters to \ufb01t.\nIt is common to use greedy search to decide which variables to add. The multivariate\nadaptive regression splines orMARSalgorithm is one example of this (Hastie et al. 2009,\nSec9.4). It \ufb01ts models of the form in Equation 16.20, where it uses a tensor product basis ofregression splines to represent the multidimensional regression functions. For example, for 2dinput, we might use\nf(x\n1,x2)=\u03b2 0+/summationdisplay\nm\u03b21m(x1\u2212t1m)+\n+/summationdisplay\nm\u03b22m(t2m\u2212x2)++/summationdisplay\nm\u03b212m(x1\u2212t1m)+(t2m\u2212x2)+ (16.21)\nTo create such a function, we start with a set of candidate basis functions of the form\nC={(xj\u2212t)+,(t\u2212xj)+:t\u2208{x1j,...,x Nj},j=1,...,D} (16.22)\nThese are 1d linear splines where the knots are at all the observed values for that variable. Weconsider splines sloping up in both directions; this is called a re\ufb02ecting pair. See Figure 16.7(a).\nLetMrepresent the current set of basis functions. We initialize by using M={1}.W e\nconsider creating a new basis function pair by multplying an h\nm\u2208Mwith one of the re\ufb02ecting\npairs inC. For example, we might initially get\nf(x)=2 5\u22124(x1\u22125)++20(5\u2212x1)+ (16.23)\nobtained by multiplying h0(x)=1with a re\ufb02ecting pair involving x1with knot t=5. This\npair is added to M. See Figure 16.7(b). At the next step, we might create a model such as\nf(x)== 2 \u22122(x1\u22125)++3(5\u2212x1)+\n\u2212(x2\u221210)+\u00d7(5\u2212x1)+\u22121.2(10\u2212x2)+\u00d7(5\u2212x1)+ (16.24)\nobtained by multiplying (5\u2212x1)+fromMby the new re\ufb02ecting pair (x2\u221210)+and(10\u2212x2)+.\nThis new function is shown in Figure 16.7(c).", "584": "554 Chapter 16. Adaptive basis function models\n0 5 10 15 20051015\n(a)0 5 10 15 20\u221240\u221220020406080100120\n(b)05101520\n05101520\u221260\u221240\u221220020\n(c)\nFigure 16.7 (a) Linear spline function with a knot at 5. Solid blue: (x\u22125)+. Dotted red: (5\u2212x)+. (b) A\nMARS model in 1d given by Equation 16.23. (c) A simple MARS model in 2d given by Equation 16.24. Figure\ngenerated by marsDemo .\nWe proceed in this way until the model becomes very large. (We may impose an upper\nbound on the order of interactions.) Then we prune backwards, at each step eliminating the\nbasis function that causes the smallest increase in the residual error, until the CV error stopsimproving.\nThe whole procedure is closely related to CART. To see this, suppose we replace the piecewise\nlinear basis functions by step functions I(x\nj>t)andI(xj<t). Multiplying by a pair of\nre\ufb02ected step functions is equivalent to splitting a node. Now suppose we impose the constraintthat once a variable is involved in a multiplication by a candidate term, that variable getsreplaced by the interaction, so the original variable is no longer available. This ensures that avariable can not be split more than once, thus guaranteeing that the resulting model can berepresented as a tree. In this case, the MARS growing strategy is the same as the CART growingstrategy.\n16.4 Boosting\nBoosting (Schapire and Freund 2012) is a greedy algorithm for \ufb01tting adaptive basis-function\nmodels of the form in Equation 16.3, where the \u03c6mare generated by an algorithm called a weak\nlearneror abase learner. The algorithm works by applying the weak learner sequentially to\nweighted versions of the data, where more weight is given to examples that were misclassi\ufb01edby earlier rounds.\nThis weak learner can be any classi\ufb01cation or regression algorithm, but it is common to use a\nCART model. In 1998, the late Leo Breiman called boosting, where the weak learner is a shallowdecision tree, the \u201cbest off-the-shelf classi\ufb01er in the world\u201d (Hastie et al. 2009, p340). Thisis supported by an extensive empirical comparison of 10 different classi\ufb01ers in (Caruana andNiculescu-Mizil 2006), who showed that boosted decision trees were the best both in terms ofmisclassi\ufb01cation error and in terms of producing well-calibrated probabilities, as judged by ROCcurves. (The second best method was random forests, invented by Breiman; see Section 16.2.5.)By contrast, single decision trees performed very poorly.\nBoosting was originally derived in the computational learning theory literature (Schapire 1990;\nFreund and Schapire 1996), where the focus is binary classi\ufb01cation. In these papers, it wasproved that one could boost the performance (on the training set) of any weak learner arbitrarily", "585": "16.4. Boosting 555\n0 20 40 60 80 100 120 14000.020.040.060.080.10.120.140.16\ntrain\ntest\nFigure 16.8 Performance of adaboost using a decision stump as a weak learner on the data in Figure 16.10.\nTraining (solid blue) and test (dotted red) error vs number of iterations. Figure generated by boostingDemo ,\nwritten by Richard Stapenhurst.\nhigh, provided the weak learner could always perform slightly better than chance. For example,\nin Figure 16.8, we plot the training and test error for boosted decision stumps on a 2d datasetshown in Figure 16.10. We see that the training set error rapidly goes to near zero. What is moresurprising is that the test set error continues to decline even after the training set error hasreached zero (although the test set error will eventually go up). Thus boosting is very resistantto over\ufb01tting. (Boosted decision stumps form the basis of a very successful face detector (Violaand Jones 2001), which was used to generate the results in Figure 1.6, and which is used in manydigital cameras.)\nIn view of its stunning empirical success, statisticians started to become interested in this\nmethod. Breiman (Breiman 1998) showed that boosting can be interpreted as a form of gradient\ndescent in function space. This view was then extended in (Friedman et al. 2000), who showedhow boosting could be extended to handle a variety of loss functions, including for regression,robust regression, Poisson regression, etc. In this section, we shall present this statistical inter-pretation of boosting, drawing on the reviews in (Buhlmann and Hothorn 2007) and (Hastie et al.2009, ch10), which should be consulted for further details.\n16.4.1 Forward stagewise additive modeling\nThe goal of boosting is to solve the following optimization problem:\nmin\nfN/summationdisplay\ni=1L(yi,f(xi)) (16.25)\nandL(y,\u02c6y)is some loss function, and fis assumed to be an ABM model as in Equation 16.3.\nCommon choices for the loss function are listed in Table 16.1.\nIf we use squared error loss, the optimal estimate is given by\nf\u2217(x) = argmin\nf(x)=Ey|x/bracketleftbig\n(Y\u2212f(x))2/bracketrightbig\n=E[Y|x] (16.26)", "586": "556 Chapter 16. Adaptive basis function models\nName Loss Derivative f\u2217Algorithm\nSquared error1\n2(yi\u2212f(xi))2yi\u2212f(xi) E[y|xi] L2Boosting\nAbsolute error |yi\u2212f(xi)|sgn(yi\u2212f(xi))median(y|xi)Gradient boosting\nExponential loss exp(\u2212\u02dcyif(xi))\u2212\u02dcyiexp(\u2212\u02dcyif(xi))1\n2log\u03c0i\n1\u2212\u03c0iAdaBoost\nLogloss log(1+e\u2212\u02dcyifi)yi\u2212\u03c0i1\n2log\u03c0i\n1\u2212\u03c0iLogitBoost\nTable 16.1 Some commonly used loss functions, their gradients, their population minimizers f\u2217, and\nsome algorithms to minimize the loss. For binary classi\ufb01cation problems, we assume \u02dcyi\u2208{ \u22121,+1},\nyi\u2208{0,1}and\u03c0i= sigm(2f (xi)). For regression problems, we assume yi\u2208R. Adapted from (Hastie\net al. 2009, p360) and (Buhlmann and Hothorn 2007, p483).\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 201234567\ny\u2212 floss\n  \n0\u22121\nlogloss\nexp\nFigure 16.9 Illustration of various loss functions for binary classi\ufb01cation. The horizontal axis is the\nmarginy\u03b7, the vertical axis is the loss. The log loss uses log base 2. Figure generated by hingeLossPlot .\nas we showed in Section 5.7.1.3. Of course, this cannot be computed in practice since it requires\nknowing the true conditional distribution p(y|x). Hence this is sometimes called the population\nminimizer, where the expectation is interpreted in a frequentist sense. Below we will see thatboosting will try to approximate this conditional expectation.\nFor binary classi\ufb01cation, the obvious loss is 0-1 loss, but this is not differentiable. Instead\nit is common to use logloss, which is a convex upper bound on 0-1 loss, as we showed inSection 6.5.5. In this case, one can show that the optimal estimate is given by\nf\n\u2217(x)=1\n2logp(\u02dcy=1|x)\np(\u02dcy=\u22121|x)(16.27)\nwhere\u02dcy\u2208{ \u22121,+1}. One can generalize this framework to the multiclass case, but we will not\ndiscuss that here.\nAn alternative convex upper bound is exponential loss, de\ufb01ned by\nL(\u02dcy,f)=e x p (\u2212\u02dcyf) (16.28)\nSee Figure 16.9 for a plot. This will have some computational advantages over the logloss,to be discussed below. It turns out that the optimal estimate for this loss is also f\n\u2217(x)=", "587": "16.4. Boosting 557\n1\n2logp(\u02dcy=1|x)\np(\u02dcy=\u22121|x). To see this, we can just set the derivative of the expected loss (for each x)t o\nzero:\n\u2202\n\u2202f(x)E/bracketleftBig\ne\u2212\u02dcyf(x)|x/bracketrightBig\n=\u2202\n\u2202f(x)[p(\u02dcy=1|x)e\u2212f(x)+p(\u02dcy=\u22121|x)ef(x)] (16.29)\n=\u2212p(\u02dcy=1|x)e\u2212f(x)+p(\u02dcy=\u22121|x)ef(x)(16.30)\n=0\u21d2p(\u02dcy=1|x)\np(\u02dcy=1\u2212|x)=e2f(x)(16.31)\nSo in both cases, we can see that boosting should try to approximate (half) the log-odds ratio.\nSince \ufb01nding the optimal fis hard, we shall tackle it sequentially. We initialise by de\ufb01ning\nf0(x)=a r gm i n\n\u03b3N/summationdisplay\ni=1L(yi,f(xi;\u03b3)) (16.32)\nFor example, if we use squared error, we can set f0(x)=y, and if we use log-loss or exponential\nloss , we can set f0(x)=1\n2log\u02c6\u03c0\n1\u2212\u02c6\u03c0,w h e r e\u02c6\u03c0=1\nN/summationtextN\ni=1I(yi=1 ). We could also use a more\npowerful model for our baseline, such as a GLM.\nThen at iteration m, we compute\n(\u03b2m,\u03b3m) = argmin\n\u03b2,\u03b3N/summationdisplay\ni=1L(yi,fm\u22121(xi)+\u03b2\u03c6(xi;\u03b3)) (16.33)\nand then we set\nfm(x)=fm\u22121(x)+\u03b2m\u03c6(x;\u03b3m) (16.34)\nThe key point is that we do not go back and adjust earlier parameters. This is why the method\nis calledforward stagewise additive modeling.\nWe continue this for a \ufb01xed number of iterations M. In factMis the main tuning parameter\nof the method. Often we pick it by monitoring the performance on a separate validation set, andthen stopping once performance starts to decrease; this is called early stopping. Alternatively,\nwe can use model selection criteria such as AIC or BIC (see e.g., (Buhlmann and Hothorn 2007)for details).\nIn practice, better (test set) performance can be obtained by performing \u201cpartial updates\u201d of\nthe form\nf\nm(x)=fm\u22121(x)+\u03bd\u03b2 m\u03c6(x;\u03b3m) (16.35)\nHere0<\u03bd\u22641is a step-size parameter. In practice it is common to use a small value such as\n\u03bd=0.1. This is called shrinkage.\nBelow we discuss how to solve the suproblem in Equation 16.33. This will depend on the\nform of loss function. However, it is independent of the form of weak learner.\n16.4.2 L2boosting\nSuppose we used squared error loss. Then at step mthe loss has the form\nL(yi,fm\u22121(xi)+\u03b2\u03c6(xi;\u03b3)) = (rim\u2212\u03c6(xi;\u03b3))2(16.36)", "588": "558 Chapter 16. Adaptive basis function models\n(a)\n (b)\n (c)\nFigure 16.10 Example of adaboost using a decision stump as a weak learner. The degree of blackness\nrepresents the con\ufb01dence in the red class. The degree of whiteness represents the con\ufb01dence in the blue\nclass. The size of the datapoints represents their weight. Decision boundary is in yellow. (a) After 1\nround. (b) After 3 rounds. (c) After 120 rounds. Figure generated by boostingDemo , written by Richard\nStapenhurst.\nwhererim/definesyi\u2212fm\u22121(xi)is the current residual, and we have set \u03b2=1without loss of\ngenerality. Hence we can \ufb01nd the new basis function by using the weak learner to predict rm.\nThis is called L2boosting ,o rleast squares boosting (Buhlmann and Yu 2003). In Section 16.4.6,\nwe will see that this method, with a suitable choice of weak learner, can be made to give the\nsame results as LARS, which can be used to perform variable selection (see Section 13.4.2).\n16.4.3 AdaBoost\nConsider a binary classi\ufb01cation problem with exponential loss. At step mwe have to minimize\nLm(\u03c6)=N/summationdisplay\ni=1exp[\u2212\u02dcyi(fm\u22121(xi)+\u03b2\u03c6(xi))] =N/summationdisplay\ni=1wi,mexp(\u2212\u03b2\u02dcyi\u03c6(xi)) (16.37)\nwherewi,m/definesexp(\u2212\u02dcyifm\u22121(xi))is a weight applied to datacase i, and\u02dcyi\u2208{ \u22121,+1}.W e\ncan rewrite this objective as follows:\nLm=e\u2212\u03b2/summationdisplay\n\u02dcyi=\u03c6(xi)wi,m+e\u03b2/summationdisplay\n\u02dcyi/negationslash=\u03c6(xi)wi,m (16.38)\n=(e\u03b2\u2212e\u2212\u03b2)N/summationdisplay\ni=1wi,mI(\u02dcyi/negationslash=\u03c6(xi))+e\u2212\u03b2N/summationdisplay\ni=1wi,m (16.39)\nConsequently the optimal function to add is\n\u03c6m=a r g m i n\n\u03c6wi,mI(\u02dcyi/negationslash=\u03c6(xi)) (16.40)\nThis can be found by applying the weak learner to a weighted version of the dataset, with\nweightswi,m. Subsituting \u03c6mintoLmand solving for \u03b2we \ufb01nd\n\u03b2m=1\n2log1\u2212errm\nerrm(16.41)", "589": "16.4. Boosting 559\nwhere\nerrm=/summationtextN\ni=1wiI(\u02dcyi/negationslash=\u03c6m(xi))\n/summationtextNi=1wi,m(16.42)\nThe overall update becomes\nfm(x)=fm\u22121(x)+\u03b2m\u03c6m(x) (16.43)\nWith this, the weights at the next iteration become\nwi,m+1=wi,me\u2212\u03b2m\u02dcyi\u03c6m(xi)(16.44)\n=wi,me\u03b2m(2I(\u02dcyi/negationslash=\u03c6m(xi))\u22121)(16.45)\n=wi,me2\u03b2mI(\u02dcyi/negationslash=\u03c6m(xi))e\u2212\u03b2m(16.46)\nwhere we exploited the fact that \u2212\u02dcyi\u03c6m(xi)=\u22121if\u02dcyi=\u03c6m(xi)and\u2212\u02dcyi\u03c6m(xi)=+ 1\notherwise. Since e\u2212\u03b2mwill cancel out in the normalization step, we can drop it. The result is\nthe algorithm shown in Algorithm 7, known Adaboost.M1.4\nAn example of this algorithm in action, using decision stumps as the weak learner, is given in\nFigure 16.10. We see that after many iterations, we can \u201ccarve out\u201d a complex decision boundary.\nWhat is rather surprising is that AdaBoost is very slow to over\ufb01t, as is apparent in Figure 16.8.See Section 16.4.8 for a discussion of this point.\nAlgorithm 16.2: Adaboost.M1, for binary classi\ufb01cation with exponential loss\n1wi=1/N;\n2form=1:M do\n3Fit a classi\ufb01er \u03c6m(x)to the training set using weights w;\n4Compute err m=/summationtextN\ni=1wi,mI(\u02dcyi/negationslash=\u03c6m(xi))/summationtextN\ni=1wi,m);\n5Compute \u03b1m= log[(1\u2212errm)/errm];\n6Setwi\u2190wiexp[\u03b1mI(\u02dcyi/negationslash=\u03c6m(xi))];\n7Returnf(x)=s g n/bracketleftBig/summationtextM\nm=1\u03b1m\u03c6m(x)/bracketrightBig\n;\n16.4.4 LogitBoost\nThe trouble with exponential loss is that it puts a lot of weight on misclassi\ufb01ed examples, as\nis apparent from the exponential blowup on the left hand side of Figure 16.9. This makes themethod very sensitive to outliers (mislabeled examples). In addition, e\n\u2212\u02dcyfis not the logarithm\nof any pmf for binary variables \u02dcy\u2208{ \u22121,+1}; consequently we cannot recover probability\nestimates from f(x).\n4. In (Friedman et al. 2000), this is called discrete AdaBoost, since it assumes that the base classi\ufb01er \u03c6mreturns a\nbinary class label. If \u03c6mreturns a probability instead, a modi\ufb01ed algorithm, known as real AdaBoost, can be used. See\n(Friedman et al. 2000) for details.", "590": "560 Chapter 16. Adaptive basis function models\nA natural alternative is to use logloss instead. This only punishes mistakes linearly, as is clear\nfrom Figure 16.9. Furthermore, it means that we will be able to extract probabilities from the\n\ufb01nal learned function, using\np(y=1|x)=ef(x)\ne\u2212f(x)+ef(x)=1\n1+e\u22122f(x)(16.47)\nThe goal is to minimze the expected log-loss, given by\nLm(\u03c6)=N/summationdisplay\ni=1log[1+exp(\u2212 2\u02dcyi(fm\u22121(x)+\u03c6(xi)))] (16.48)\nBy performing a Newton upate on this objective (similar to IRLS), one can derive the algorithmshown in Algorithm 8. This is known as logitBoost (Friedman et al. 2000). It can be generalized\nto the multi-class setting, as explained in (Friedman et al. 2000).\nAlgorithm 16.3: LogitBoost, for binary classi\ufb01cation with log-loss\n1wi=1/N,\u03c0i=1/2;\n2form=1:M do\n3Compute the working response zi=y\u2217\ni\u2212\u03c0i\n\u03c0i(1\u2212\u03c0i);\n4Compute the weights wi=\u03c0i(1\u2212\u03c0i);\n5\u03c6m=a r g m i n\u03c6/summationtextN\ni=1wi(zi\u2212\u03c6(xi))2;\n6Updatef(x)\u2190f(x)+1\n2\u03c6m(x);\n7Compute \u03c0i=1/(1+exp(\u2212 2f(xi)));\n8Returnf(x)=s g n/bracketleftBig/summationtextMm=1\u03c6m(x)/bracketrightBig\n;\n16.4.5 Boosting as functional gradient descent\nRather than deriving new versions of boosting for every different loss function, it is possible to\nderive a generic version, known as gradient boosting (Friedman 2001; Mason et al. 2000). To\nexplain this, imagine minimizing\n\u02c6f=a r g m i n\nfL(f) (16.49)\nwheref=(f(x1),...,f(xN))are the \u201cparameters\u201d. We will solve this stagewise, using gradient\ndescent. At step m,l e tg mbe the gradient of L(f)evaluated at f=fm\u22121:\ngim=/bracketleftbigg\u2202L(yi,f(xi))\n\u2202f(xi)/bracketrightbigg\nf=fm\u22121(16.50)\nGradients of some common loss functions are given in Table 16.1. We then make the update\nfm=fm\u22121\u2212\u03c1mgm (16.51)", "591": "16.4. Boosting 561\nwhere\u03c1mis the step length, chosen by\n\u03c1m=a r g m i n\n\u03c1L(fm\u22121\u2212\u03c1gm) (16.52)\nThis is called functional gradient descent.\nIn its current form, this is not much use, since it only optimizes fat a \ufb01xed set of Npoints,\nso we do not learn a function that can generalize. However, we can modify the algorithm by\n\ufb01tting a weak learner to approximate the negative gradient signal. That is, we use this update\n\u03b3m=a r g m i n\n\u03b3N/summationdisplay\ni=1(\u2212gim\u2212\u03c6(xi;\u03b3))2(16.53)\nThe overall algorithm is summarized in Algorithm 6. (We have omitted the line search step,which is not strictly necessary, as argued in (Buhlmann and Hothorn 2007).)\nAlgorithm 16.4: Gradient boosting\n1Initializef0(x) = argmin\u03b3/summationtextN\ni=1L(yi,\u03c6(xi;\u03b3));\n2form=1:M do\n3Compute the gradient residual using rim=\u2212/bracketleftBig\n\u2202L(yi,f(xi))\n\u2202f(xi)/bracketrightBig\nf(xi)=fm\u22121(xi);\n4Use the weak learner to compute \u03b3mwhich minimizes/summationtextNi=1(rim\u2212\u03c6(xi;\u03b3m))2;\n5Updatefm(x)=fm\u22121(x)+\u03bd\u03c6(x;\u03b3m);\n6Returnf(x)=fM(x)\nIf we apply this algorithm using squared loss, we recover L2Boosting. If we apply this\nalgorithm to log-loss, we get an algorithm known as BinomialBoost (Buhlmann and Hothorn\n2007). The advantage of this over LogitBoost is that it does not need to be able to do weighted\n\ufb01tting: it just applies any black-box regression model to the gradient vector. Also, it is relativelyeasy to extend to the multi-class case (see (Hastie et al. 2009, p387)). We can also apply thisalgorithm to other loss functions, such as the Huber loss (Section 7.4), which is more robust tooutliers than squared error loss.\n16.4.6 Sparse boosting\nSuppose we use as our weak learner the following algorithm: search over all possible variablesj=1:D, and pick the one j(m)that best predicts the residual vector:\nj(m) = argmin\njN/summationdisplay\ni=1(rim\u2212\u02c6\u03b2jmxij)2(16.54)\n\u02c6\u03b2jm=/summationtextN\ni=1xijrim/summationtextNi=1x2\nij(16.55)\n\u03c6m(x)=\u02c6\u03b2j(m),mxj(m) (16.56)", "592": "562 Chapter 16. Adaptive basis function models\nThis method, which is known as sparse boosting (Buhlmann and Yu 2006), is identical to the\nmatching pursuit algorithm discussed in Section 13.2.3.1.\nIt is clear that this will result in a sparse estimate, at least if Mis small. To see this, let us\nrewrite the update as follows:\n\u03b2m:=\u03b2m\u22121+\u03bd(0,...,0,\u02c6\u03b2j(m),m,0,...,0) (16.57)\nwhere the non-zero entry occurs in location j(m). This is known as forward stagewise linear\nregression (Hastie et al. 2009, p608), which becomes equivalent to the LAR algorithm discussed\nin Section 13.4.2 as \u03bd\u21920. Increasing the number of steps min boosting is analogous to\ndecreasing the regularization penalty \u03bb. If we modify boosting to allow some variable deletion\nsteps (Zhao and Yu 2007), we can make it equivalent to the LARS algorithm, which computes\nthe full regularization path for the lasso problem. The same algorithm can be used for sparselogistic regression, by simply modifying the residual to be the appropriate negative gradient.\nNow consider a weak learner that is similar to the above, except it uses a smoothing spline\ninstead of linear regression when mapping from x\njto the residual. The result is a sparse\ngeneralized additive model (see Section 16.3). It can obviously be extended to pick pairs ofvariables at a time. The resulting method often works much better than MARS (Buhlmann andYu 2006).\n16.4.7 Multivariate adaptive regression trees (MART)\nIt is quite common to use CART models as weak learners. It is usually advisable to use a shallowtree, so that the variance is low. Even though the bias will be high (since a shallow tree is likelyto be far from the \u201ctruth\u201d), this will compensated for in subsequent rounds of boosting.\nThe height of the tree is an additional tuning parameter (in addition to M, the number of\nrounds of boosting, and \u03bd, the shrinkage factor). Suppose we restrict to trees with Jleaves.\nIfJ=2, we get a stump, which can only split on a single variable. If J=3, we allow for\ntwo-variable interactions, etc. In general, it is recommended (e.g., in (Hastie et al. 2009, p363)and (Caruana and Niculescu-Mizil 2006)) to use J\u22486.\nIf we combine the gradient boosting algorithm with (shallow) regression trees, we get a model\nknown as MART, which stands for \u201cmultivariate adaptive regression trees\u201d. This actually includes\na slight re\ufb01nement to the basic gradient boosting algorithm: after \ufb01tting a regression tree to theresidual (negative gradient), we re-estimate the parameters at the leaves of the tree to minimizethe loss:\n\u03b3\njm=a r g m i n\n\u03b3/summationdisplay\nxi\u2208RjmL(yi,fm\u22121(xi)+\u03b3) (16.58)\nwhereRjmis the region for leaf jin them\u2019th tree, and \u03b3jmis the corresponding parameter (the\nmean response of yfor regression problems, or the most probable class label for classi\ufb01cation\nproblems).\n16.4.8 Why does boosting work so well?\nWe have seen that boosting works very well, especially for classi\ufb01ers. There are two mainreasons for this. First, it can be seen as a form of /lscript\n1regularization, which is known to help", "593": "16.5. Feedforward neural networks (multilayer perceptrons) 563\nprevent over\ufb01tting by eliminating \u201cirrelevant\u201d features. To see this, imagine pre-computing all\npossible weak-learners, and de\ufb01ning a feature vector of the form \u03c6(x)=[\u03c61(x),...,\u03c6 K(x)].\nWe could use /lscript1regularization to select a subset of these. Alternatively we can use boosting,\nwhere at each step, the weak learner creates a new \u03c6kon the \ufb02y. It is possible to combine\nboosting and /lscript1regularization, to get an algorithm known as L1-Adaboost (Duchi and Singer\n2009). Essentially this method greedily adds the best features (weak learners) using boosting,and then prunes off irrelevant ones using /lscript\n1regularization.\nAnother explanation has to do with the concept of margin, which we introduced in Sec-\ntion 14.5.2.2. (Schapire et al. 1998; Ratsch et al. 2001) proved that AdaBoost maximizes themargin on the training data. (Rosset et al. 2004) generalized this to other loss functions, such aslog-loss.\n16.4.9 A Bayesian view\nSo far, our presentation of boosting has been very frequentist, since it has focussed on greedilyminimizing loss functions. A likelihood interpretation of the algorithm was given in (Neal andMacKay 1998; Meek et al. 2002). The idea is to consider a mixture of experts model of the form\np(y|x,\u03b8)=\nM/summationdisplay\nm=1\u03c0mp(y|x,\u03b3m) (16.59)\nwhere each expert p(y|x,\u03b3m)is like a weak learner. We usually \ufb01t all Mexperts at once\nusing EM, but we can imagine a sequential scheme, whereby we only update the parametersfor one expert at a time. In the E step, the posterior responsibilities will re\ufb02ect how well theexisting experts explain a given data point; if this is a poor \ufb01t, these data points will havemore in\ufb02uence on the next expert that is \ufb01tted. (This view naturally suggest a way to use aboosting-like algorithm for unsupervised learning: we simply sequentially \ufb01t mixture models,instead of mixtures of experts.)\nNotice that this is a rather \u201cbroken\u201d MLE procedure, since it never goes back to update the\nparameters of an old expert. Similarly, if boosting ever wants to change the weight assigned to aweak learner, the only way to do this is to add the weak learner again with a new weight. Thiscan result in unnecessarily large models. By contrast, the BART model (Chipman et al. 2006,2010) uses a Bayesian version of back\ufb01tting to \ufb01t a small sum of weak learners (typically trees).\n16.5 Feedforward neural networks (multilayer perceptrons)\nAfeedforward neural network,a k a multi-layer perceptron (MLP), is a series of logistic\nregression models stacked on top of each other, with the \ufb01nal layer being either another logisticregression or a linear regression model, depending on whether we are solving a classi\ufb01cation orregression problem. For example, if we have two layers, and we are solving a regression problem,the model has the form\np(y|x,\u03b8)=N (y|w\nTz(x),\u03c32) (16.60)\nz(x)=g(Vx)=[g(vT\n1x),...,g(vT\nHx)] (16.61)\nwheregis a non-linear activation ortransfer function (commonly the logistic function),\nz(x)=\u03c6(x,V)is called the hidden layer (a deterministic function of the input), His the", "594": "564 Chapter 16. Adaptive basis function models\nxn1xnixnD\nzn1znjznH\nyn1ynkynC\n......\nvij wjk\nFigure 16.11 A neural network with one hidden layer.\nnumber of hidden units, Vis the weight matrix from the inputs to the hidden nodes, and\nwis the weight vector from the hidden nodes to the output. It is important that gbe non-\nlinear, otherwise the whole model collapses into a large linear regression model of the form\ny=wT(Vx). One can show that an MLP is a universal approximator, meaning it can model\nany suitably smooth function, given enough hidden units, to any desired level of accuracy(Hornik 1991).\nTo handle binary classi\ufb01cation, we pass the output through a sigmoid, as in a GLM:\np(y|x,\u03b8)=B e r (y|sigm(w\nTz(x))) (16.62)\nWe can easily extend the MLP to predict multiple outputs. For example, in the regression case,we have\np(y|x,\u03b8)=N(y|W\u03c6(x,V),\u03c3\n2I) (16.63)\nSee Figure 16.11 for an illustration. If we add mutual inhibition arcs between the output units,\nensuring that only one of them turns on, we can enforce a sum-to-one constraint, which can beused for multi-class classi\ufb01cation. The resulting model has the form\np(y|x,\u03b8)=C a t ( y|S(Wz(x)) (16.64)\n16.5.1 Convolutional neural networks\nThe purpose of the hidden units is to learn non-linear combinations of the original inputs; thisis calledfeature extraction orfeature construction. These hidden features are then passed as\ninput to the \ufb01nal GLM. This approach is particularly useful for problems where the original inputfeatures are not very individually informative. For example, each pixel in an image is not veryinformative; it is the combination of pixels that tells us what objects are present. Conversely, fora task such as document classi\ufb01cation using a bag of words representation, each feature (wordcount) isinformative on its own, so extracting \u201chigher order\u201d features is less important. Not\nsuprisingly, then, much of the work in neural networks has been motivated by visual pattern", "595": "16.5. Feedforward neural networks (multilayer perceptrons) 565\nFigure 16.12 The convolutional neural network from (Simard et al. 2003). Source: http://www .codep\nroject.com/KB/library/NeuralNetRecognition .aspx. Used with kind permission of Mike O\u2019Neill.\nrecognition (e.g., (LeCun et al. 1989)), although they have also been applied to other types of\ndata, including text (e.g., (Collobert and Weston 2008)).\nA form of MLP which is particularly well suited to 1d signals like speech or text, or 2d signals\nlike images, is the convolutional neural network . This is an MLP in which the hidden units\nhave local receptive \ufb01elds (as in the primary visual cortex), and in which the weights are tied\nor shared across the image, in order to reduce the number of parameters. Intuitively, the effect\nof such spatial parameter tying is that any useful features that are \u201cdiscovered\u201d in some portion\nof the image can be re-used everywhere else without having to be independently learned. The\nresulting network then exhibits translation invariance , meaning it can classify patterns no\nmatter where they occur inside the input image.\nFigure 16.12 gives an example of a convolutional network, designed by Simard and colleagues\n(Simard et al. 2003), with 5 layers (4 layers of adjustable parameters) designed to classify 29\u00d729\ngray-scale images of handwritten digits from the MNIST dataset (see Section 1.2.1.3). In layer 1,\nwe have 6 feature maps each of which has size 13\u00d713. Each hidden node in one of these\nfeature maps is computed by convolving the image with a 5\u00d75weight matrix (sometimes called\na kernel), adding a bias, and then passing the result through some form of nonlinearity. There\nare therefore 13\u00d713\u00d76 = 1014 neurons in Layer 1, and (5\u00d75+1)\u00d76 = 156weights. (The\n\"+1\" is for the bias.) If we did not share these parameters, there would be 1014\u00d726 = 26,364\nweights at the \ufb01rst layer. In layer 2, we have 50 feature maps, each of which is obtained by\nconvolving each feature map in layer 1 with a 5\u00d75weight matrix, adding them up, adding a\nbias, and passing through a nonlinearity. There are therefore 5\u00d75\u00d750 = 1250 neurons in\nLayer 2,(5\u00d75+1 )\u00d76\u00d750 = 7800 adjustable weights (one kernel for each pair of feature", "596": "566 Chapter 16. Adaptive basis function models\nmaps in layers 1 and 2), and 1250\u00d726 = 32, 500connections. Layer 3 is fully connected to\nlayer 2, and has 100 neurons and 100\u00d7(1250+1) = 125, 100weights. Finally, layer 4 is also\nfully connected, and has 10 neurons, and 10\u00d7(100 + 1) = 1010 weights. Adding the above\nnumbers, there are a total of 3,215 neurons, 134,066 adjustable weights, and 184,974 connections.\nThis model is usually trained using stochastic gradient descent (see Section 16.5.4 for details).\nA single pass over the data set is called an epoch. When Mike O\u2019Neill did these experiments in\n2006, he found that a single epoch took about 40 minutes (recall that there are 60,000 trainingexamples in MNIST). Since it took about 30 epochs for the error rate to converge, the totaltraining time was about 20 hours.\n5Using this technique, he obtained a misclassi\ufb01cation rate on\nthe 10,000 test cases of about 1.40%.\nTo further reduce the error rate, a standard trick is to expand the training set by including\ndistorted versions of the original data, to encourage the network to be invariant to small changes\nthat don\u2019t affect the identity of the digit. These can be created by applying a random \ufb02ow \ufb01eldto shift pixels around. See Figure 16.13 for some examples. (If we use online training, such asstochastic gradient descent, we can create these distortions on the \ufb02y, rather than having tostore them.) Using this technique, Mike O\u2019Neill obtained a misclassi\ufb01cation rate on the 10,000test cases of about 0.74%, which is close to the current state of the art.\n6\nYann Le Cun and colleagues (LeCun et al. 1998) obtained similar performance using a slightly\nmore complicated architecture shown in Figure 16.14. This model is known as LeNet5, and\nhistorically it came before the model in Figure 16.12. There are two main differences. First,LeNet5 has a subsampling layer between each convolutional layer, which either averages or\ncomputes the max over each small window in the previous layer, in order to reduce the size, andto obtain a small amount of shift invariance. The convolution and sub-sampling combinationwas inspired by Hubel and Wiesel\u2019s model of simple and complex cells in the visual cortex(Hubel and Wiesel 1962), and it continues to be popular in neurally-inspired models of visualobject recognition (Riesenhuber and Poggio 1999). A similar idea \ufb01rst appeared in Fukushima\u2019sneocognitron (Fukushima 1975), though no globally supervised training algorithm was available\nat that time.\nThe second difference between LeNet5 and the Simard architecture is that the \ufb01nal layer is\nactually an RBF network rather than a more standard sigmoidal or softmax layer. This modelgets a test error rate of about 0.95% when trained with no distortions, and 0.8% when trainedwith distortions. Figure 16.15 shows all 82 errors made by the system. Some are genuinelyambiguous, but several are errors that a person would never make. A web-based demo of theLeNet5 can be found at http://yann .lecun.com/exdb/lenet/index .html.\nOf course, classifying isolated digits is of limited applicability: in the real world, people usually\nwrite strings of digits or other letters. This requires both segmentation and classi\ufb01cation. Le Cunand colleagues devised a way to combine convolutional neural networks with a model similarto a conditional random \ufb01eld (described in Section 19.6) to solve this problem. The systemwas eventually deployed by the US postal service. (See (LeCun et al. 1998) for a more detailedaccount of the system, which remains one of the best performing systems for this task.)\n5. Implementation details: Mike used C++ code and a variety of speedup tricks. He was using standard 2006 era\nhardware (an Intel Pentium 4 hyperthreaded processor running at 2.8GHz). See http://www .codeproject .com/KB/\nlibrary/NeuralNetRecognition .aspxfor details.\n6. A list of various methods, along with their misclassi\ufb01cation rates on the MNIST test set, is available from http:\n//yann.lecun.com/exdb/mnist/ . Error rates within 0.1\u20130.2% of each other are not statistically signi\ufb01cantly different.", "597": "16.5. Feedforward neural networks (multilayer perceptrons) 567\n(a)\n (b)\n(c) (d)\n(e) (f)\nFigure 16.13 Several synthetic warpings of a handwritten digit. Based on Figure 5.14 of (Bishop 2006a).\nFigure generated by elasticDistortionsDemo , written by Kevin Swersky.\nINPUT\n32x32\nConvolutions Subsampling ConvolutionsC1: feature maps \n6@28x28\nSubsamplingS2: f. maps\n6@14x14S4: f. maps 16@5x5\nC5: layer\n120C3: f. maps 16@10x10\nF6: layer\n 84\nFull connection\nFull connectionGaussian connectionsOUTPUT\n 10\nFigure 16.14 LeNet5, a convolutional neural net for classifying handwritten digits. Source: Figure 2 from\n(LeCun et al. 1998) . Used with kind permission of Yann LeCun.", "598": "568 Chapter 16. Adaptive basis function models\n4\u2212>6\n3\u2212>5\n8\u2212>2\n2\u2212>1\n5\u2212>3\n4\u2212>8\n2\u2212>8\n3\u2212>5\n6\u2212>5\n7\u2212>3\n9\u2212>4\n8\u2212>0\n7\u2212>8\n5\u2212>3\n8\u2212>7\n0\u2212>6\n3\u2212>7\n2\u2212>7\n8\u2212>3\n9\u2212>4\n8\u2212>2\n5\u2212>3\n4\u2212>8\n3\u2212>9\n6\u2212>0\n9\u2212>8\n4\u2212>9\n6\u2212>1\n9\u2212>4\n9\u2212>1\n9\u2212>4\n2\u2212>0\n6\u2212>1\n3\u2212>5\n3\u2212>2\n9\u2212>5\n6\u2212>0\n6\u2212>0\n6\u2212>0\n6\u2212>8\n4\u2212>6\n7\u2212>3\n9\u2212>4\n4\u2212>6\n2\u2212>7\n9\u2212>7\n4\u2212>3\n9\u2212>4\n9\u2212>4\n9\u2212>4\n8\u2212>7\n4\u2212>2\n8\u2212>4\n3\u2212>5\n8\u2212>4\n6\u2212>5\n8\u2212>5\n3\u2212>8\n3\u2212>8\n9\u2212>8\n1\u2212>5\n9\u2212>8\n6\u2212>3\n0\u2212>2\n6\u2212>5\n9\u2212>5\n0\u2212>7\n1\u2212>6\n4\u2212>9\n2\u2212>1\n2\u2212>8\n8\u2212>5\n4\u2212>9\n7\u2212>2\n7\u2212>2\n6\u2212>5\n9\u2212>7\n6\u2212>1\n5\u2212>6\n5\u2212>0\n4\u2212>9\n2\u2212>8\nFigure 16.15 These are the 82 errors made by LeNet5 on the 10,000 test cases of MNIST. Below each\nimage is a label of the form correct-label \u2192estimated-label. Source: Figure 8 of (LeCun et al. 1998).\nUsed with kind permission of Yann LeCun. (Compare to Figure 28.4(b) which shows the results of a deep\ngenerative model.)\n16.5.2 Other kinds of neural networks\nOther network topologies are possible besides the ones discussed above. For example, we can\nhave skip arcs that go directly from the input to the output, skipping the hidden layer; we\ncan have sparse connections between the layers; etc. However, the MLP always requires that\nthe weights form a directed acyclic graph. If we allow feedback connections, the model is\nknown as a recurrent neural network ; this de\ufb01nes a nonlinear dynamical system, but does\nnot have a simple probabilistic interpretation. Such RNN models are currently the best approach\nfor language modeling (i.e., performing word prediction in natural language) (Tomas et al. 2011),\nsigni\ufb01cantly outperforming the standard n-gram-based methods discussed in Section 17.2.2.\nIf we allow symmetric connections between the hidden units, the model is known as a Hop-\n\ufb01eld network orassociative memory ; its probabilistic counterpart is known as a Boltzmann\nmachine (see Section 27.7) and can be used for unsupervised learning.\n16.5.3 A brief history of the \ufb01eld\nNeural networks have been the subject of great interest for many decades, due to the desire to\nunderstand the brain, and to build learning machines. It is not possible to review the entire\nhistory here. Instead, we just give a few \u201cedited highlights\u201d.\nThe \ufb01eld is generally viewed as starting with McCulloch and Pitts (McCullich and Pitts 1943),\nwho devised a simple mathematical model of the neuron in 1943, in which they approximated the", "599": "16.5. Feedforwardneuralnetworks(multilayerperceptrons) 569\noutput as a weighted sum of inputs passed through a threshold function, y=I(/summationtext\niwixi>\u03b8),\nfor some threshold \u03b8. This is similar to a sigmoidal activation function. Frank Rosenblatt\ninvented the perceptron learning algorithm in 1957, which is a way to estimate the parameters of\na McCulloch-Pitts neuron (see Section 8.5.4 for details). A very similar model called the adaline\n(for adaptive linear element) was invented in 1960 by Widrow and Hoff.\nIn 1969, Minsky and Papert (Minsky and Papert 1969) published a famous book called \u201cPercep-\ntrons\u201d in which they showed that such linear models, with no hidden layers, were very limitedin their power, since they cannot classify data that is not linearly separable. This considerablyreduced interest in the \ufb01eld.\nIn 1986, Rumelhart, Hinton and Williams (Rumelhart et al. 1986) discovered the backpropa-\ngation algorithm (see Section 16.5.4), which allows one to \ufb01t models with hidden layers. (Thebackpropagation algorithm was originally discovered in (Bryson and Ho 1969), and independentlyin (Werbos 1974); however, it was (Rumelhart et al. 1986) that brought the algorithm to people\u2019sattention.) This spawned a decade of intense interest in these models.\nIn 1987, Sejnowski and Rosenberg (Sejnowski and Rosenberg 1987) created the famous NETtalk\nsystem, that learned a mapping from English words to phonetic symbols which could be fedinto a speech synthesizer. An audio demo of the system as it learns over time can be found athttp://www .cnl.salk. edu/ParallelNetsPronounce/nettalk .mp3. The systems starts by\n\u201cbabbling\u201d and then gradually learns to pronounce English words. NETtalk learned a distributed\nrepresentation (via its hidden layer) of various sounds, and its success spawned a big debate in\npsychology between connectionism, based on neural networks, and computationalism, based\non syntactic rules. This debate lives on to some extent in the machine learning community,wheretherearestillargumentsaboutwhetherlearningisbestperformedusinglow-level, \u201cneural-like\u201d representations, or using more structured models.\nIn 1989, Yann Le Cun and others (LeCun et al. 1989) created the famous LeNet system described\nin Section 16.5.1.\nIn 1992, the support vector machine (see Section 14.5) was invented (Boser et al. 1992). SVMs\nprovide similar prediction accuracy to neural networks while being considerably easier to train(since they use a convex objective function). This spawned a decade of interest in kernel methodsin general.\n7Note, however, that SVMs do not use adaptive basis functions, so they require a fair\namount of human expertise to design the right kernel function.\nIn 2002, Geoff Hinton invented the contrastive divergence training procedure (Hinton 2002),\nwhich provided a way, for the \ufb01rst time, to learn deep networks, by training one layer at a timein an unsupervised fashion (see Section 27.7.2.4 for details). This in turn has spawned renewedinterest in neural networks over the last few years (see Chapter 28).\n16.5.4 The backpropagation algorithm\nUnlike a GLM, the NLL of an MLP is a non-convex function of its parameters. Nevertheless,we can \ufb01nd a locally optimal ML or MAP estimate using standard gradient-based optimizationmethods. Since MLPs have lots of parameters, they are often trained on very large data sets.\n7. It became part of the folklore during the 1990s that to get published in the top machine learning conference known as\nNIPS, which stands for \u201cneural information processing systems\u201d, it was important to ensure your paper did not contain\nthe word \u201cneural network\u201d!", "600": "570 Chapter16. Adaptivebasisfunctionmodels\n\u221210 \u22125 0 5 10\u22121\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.81\n  \ntanh\nsigmoid\nFigure 16.16 Two possible activation functions. tanhmaps Rto[\u22121,+1]and is the preferred nonlin-\nearity for the hidden nodes. sigmmaps Rto[0,1]and is the preferred nonlinearity for binary nodes at\nthe output layer. Figure generated by tanhPlot .\nConsequently it is common to use \ufb01rst-order online methods, such as stochastic gradient descent\n(Section 8.5.2), whereas GLMs are usually \ufb01t with IRLS, which is a second-order offline method.\nWe now discuss how to compute the gradient vector of the NLL by applying the chain rule of\ncalculus. The resulting algorithm is known as backpropagation, for reasons that will become\napparent.\nFor notational simplicity, we shall assume a model with just one hidden layer. It is helpful\nto distinguish the pre- and post-synaptic values of a neuron, that is, before and after we applythe nonlinearity. Let x\nnbe then\u2019th input, an=Vxnbe the pre-synaptic hidden layer, and\nzn=g(an)be the post-synaptic hidden layer, where gis sometransfer function. We typically\nuseg(a) = sigm( a), but we may also use g(a) = tanh( a): see Figure 16.16 for a comparison.\n(When the input to sigmortanhis a vector, we assume it is applied component-wise.)\nWe now convert this hidden layer to the output layer as follows. Let bn=Wznbe the\npre-synaptic output layer, and \u02c6yn=h(bn)be the post-synaptic output layer, where his\nanother nonlinearity, corresponding to the canonical link for the GLM. (We reserve the notationy\nn, without the hat, for the output corresponding to the n\u2019th training case.) For a regression\nmodel, we use h(b)=b; for binary classifcation, we use h(b)=[ s i g m ( b1),...,sigm(bc)];f o r\nmulti-class classi\ufb01cation, we use h(b)=S(b).\nWe can write the overall model as follows:\nxnV\u2192ang\u2192znW\u2192bnh\u2192\u02c6yn (16.65)\nThe parameters of the model are \u03b8=(V,W), the \ufb01rst and second layer weight matrices. Offset\nor bias terms can be accomodated by clamping an element of xnandznto 1.8\n8. In the regression setting, we can easily estimate the variance of the output noise using the empirical variance of the\nresidual errors, \u02c6\u03c32=1\nN||\u02c6y(\u02c6\u03b8)\u2212y||2, after training is complete. There will be one value of \u03c32for each output node,\nif we are performing multi-target regression, as we usually assume.", "601": "16.5. Feedforwardneuralnetworks(multilayerperceptrons) 571\nIn the regression case, with Koutputs, the NLL is given by the squared error:\nJ(\u03b8)=\u2212/summationdisplay\nn/summationdisplay\nk(\u02c6ynk(\u03b8)\u2212ynk)2(16.66)\nIn the classi\ufb01cation case, with Kclasses, the NLL is given by the cross entropy\nJ(\u03b8)=\u2212/summationdisplay\nn/summationdisplay\nkynklog\u02c6ynk(\u03b8) (16.67)\nOur task is to compute \u2207\u03b8J. We will derive this for each nseparately; the overall gradient is\nobtained by summing over n, although often we just use a mini-batch (see Section 8.5.2).\nLet us start by considering the output layer weights. We have\n\u2207wkJn=\u2202Jn\n\u2202bnk\u2207wkbnk=\u2202Jn\n\u2202bnkzn (16.68)\nsincebnk=wT\nkzn. Assuming his the canonical link function for the output GLM, then\nEquation 9.91 tells us that\n\u2202Jn\n\u2202bnk/defines\u03b4w\nnk=(\u02c6ynk\u2212ynk) (16.69)\nwhich is the error signal. So the overall gradient is\n\u2207wkJn=\u03b4w\nnkzn (16.70)\nwhich is the pre-synaptic input to the output layer, namely zn, times the error signal, namely\n\u03b4w\nnk.\nFor the input layer weights, we have\n\u2207vjJn=\u2202Jn\n\u2202anj\u2207vjanj/defines\u03b4v\nnjxn (16.71)\nwhere we exploited the fact that anj=vT\njxn. All that remains is to compute the \ufb01rst level\nerror signal \u03b4v\nnj.W eh a v e\n\u03b4v\nnj=\u2202Jn\n\u2202anj=K/summationdisplay\nk=1\u2202Jn\n\u2202bnk\u2202bnk\n\u2202anj=K/summationdisplay\nk=1\u03b4w\nnk\u2202bnk\n\u2202anj(16.72)\nNow\nbnk=/summationdisplay\njwkjg(anj) (16.73)\nso\n\u2202bnk\n\u2202anj=wkjg/prime(anj) (16.74)\nwhereg/prime(a)=d\ndag(a). For tanh units, g/prime(a)=d\ndatanh(a)=1\u2212tanh2(a)=sech2(a), and\nfor sigmoid units, g/prime(a)=d\nda\u03c3(a)=\u03c3(a)(1\u2212\u03c3(a)). Hence\n\u03b4v\nnj=K/summationdisplay\nk=1\u03b4w\nnkwkjg/prime(anj) (16.75)", "602": "572 Chapter16. Adaptivebasisfunctionmodels\nThus the layer 1 errors can be computed by passing the layer 2 errors back through the Wmatrix;\nhencetheterm\u201cbackpropagation\u201d. Thekeypropertyisthatwecancomputethegradientslocally:\neach node only needs to know about its immediate neighbors. This is supposed to make thealgorithm \u201cneurally plausible\u201d, although this interpretation is somewhat controversial.\nPutting it all together, we can compute all the gradients as follows: we \ufb01rst perform a\nforwards pass to compute a\nn,zn,bnand\u02c6yn. We then compute the error for the output layer,\n\u03b4(2)\nn=\u02c6yn\u2212yn, which we pass backwards through Wusing Equation 16.75 to compute the\nerror for the hidden layer, \u03b4(1)n. We then compute the overall gradient as follows:\n\u2207\u03b8J(\u03b8)=/summationdisplay\nn[\u03b4vnxn,\u03b4wnzn] (16.76)\n16.5.5 Identi\ufb01ability\nIt is easy to see that the parameters of a neural network are not identi\ufb01able. For example, we\ncan change the sign of the weights going into one of the hidden units, so long as we changethe sign of all the weights going out of it; these effects cancel, since tanh is an odd function, sotanh(\u2212a)=\u2212tanh(a). There will be Hsuch sign \ufb02ip symmetries, leading to 2\nHequivalent\nsettings of the parameters. Similarly, we can change the identity of the hidden units withoutaffecting the likelihood. There are H!such permutations. The total number of equivalent\nparameter settings (with the same likelihood) is therefore H!2\nH.\nIn addition, there may be local minima due to the non-convexity of the NLL. This can\nbe a more serious problem, although with enough data, these local optima are often quite\u201cshallow\u201d, and simple stochastic optimization methods can avoid them. In addition, it is commonto perform multiple restarts, and to pick the best solution, or to average over the resultingpredictions. (It does not make sense to average the parameters themselves, since they are notidenti\ufb01able.)\n16.5.6 Regularization\nAs usual, the MLE can over\ufb01t, especially if the number of nodes is large. A simple way to preventthis is called early stopping, which means stopping the training procedure when the error on\nthe validation set \ufb01rst starts to increase. This method works because we usually initialize fromsmall random weights, so the model is initially simple (since the tanhandsigmfunctions are\nnearly linear near the origin). As training progresses, the weights become larger, and the modelbecomes nonlinear. Eventually it will over\ufb01t.\nAnother way to prevent over\ufb01tting, that is more in keeping with the approaches used elsewhere\nin this book, is to impose a prior on the parameters, and then use MAP estimation. It is standardto use aN(0,\u03b1\n\u22121I)prior (equivalent to /lscript2regularization), where \u03b1is the precision (strength)\nof the prior. In the neural networks literature, this is called weight decay, since it encourages\nsmall weights, and hence simpler models. The penalized NLL objective becomes\nJ(\u03b8)=\u2212N/summationdisplay\nn=1logp(yn|xn,\u03b8)+\u03b1\n2[/summationdisplay\nijv2\nij+/summationdisplay\njkw2\njk] (16.77)", "603": "16.5. Feedforwardneuralnetworks(multilayerperceptrons) 573\n(Note that we don\u2019t penalize the bias terms.) The gradient of the modi\ufb01ed objective becomes\n\u2207\u03b8J(\u03b8)=[/summationdisplay\nn\u03b4v\nnxn+\u03b1v,/summationdisplay\nn\u03b4wnzn+\u03b1w] (16.78)\nas in Section 8.3.6.\nIf the regularization is sufficiently strong, it does not matter if we have too many hidden units\n(apart from wasted computation). Hence it is advisable to set Hto be as large as you can afford\n(say 10\u2013100), and then to choose an appropriate regularizer. We can set the \u03b1parameter by\ncross validation or empirical Bayes (see Section 16.5.7.5).\nAs with ridge regression, it is good practice to standardize the inputs to zero mean and unit\nvariance, so that the spherical Gaussian prior makes sense.\n16.5.6.1 Consistent Gaussian priors *\nOne can show (MacKay 1992) that using the same regularization parameter for both the \ufb01rst and\nsecond layer weights results in the lack of a certain desirable invariance property. In particular,suppose we linearly scale and shift the inputs and/or outputs to a neural network regressionmodel. Then we would like the model to learn to predict the same function, by suitably scalingits internal weights and bias terms. However, the amount of scaling needed by the \ufb01rst andsecond layer weights to compensate for a change in the inputs and/or outputs is not the same.Therefore we need to use a different regularization strength for the \ufb01rst and second layer.Fortunately, this is easy to do \u2014 we just use the following prior:\np(\u03b8)=N(W|0,1\n\u03b1wI)N(V|0,1\n\u03b1vI)N(b|0,1\n\u03b1bI)N(c|0,1\n\u03b1cI) (16.79)\nwherebandcare the bias terms.9\nTo get a feeling for the effect of these hyper-parameters, we can sample MLP parameters\nfrom this prior and plot the resulting random functions. Figure 16.17 shows some examples.Decreasing \u03b1\nvallows the \ufb01rst layer weights to get bigger, making the sigmoid-like shape of\nthe functions steeper. Decreasing \u03b1ballows the \ufb01rst layer biases to get bigger, which allows\nthe center of the sigmoid to shift left and right more. Decreasing \u03b1wallows the second layer\nweights to get bigger, making the functions more \u201cwiggly\u201d (greater sensitivity to change in theinput, and hence larger dynamic range). And decreasing \u03b1\ncallows the second layer biases to\nget bigger, allowing the mean level of the function to move up and down more. (In Chapter 15,we will see an easier way to de\ufb01ne priors over functions.)\n16.5.6.2 Weight pruning\nSince there are many weights in a neural network, it is often helpful to encourage sparsity.Various ad-hoc methods for doing this, with names such as \u201coptimal brain damage\u201d, weredevised in the 1990s; see e.g., (Bishop 1995) for details.\n9. Since we are regularizing the output bias terms, it is helpful, in the case of regression, to normalize the target\nresponses in the training set to zero mean, to be consistent with the fact that the prior on the output bias has zero\nmean.", "604": "574 Chapter16. Adaptivebasisfunctionmodels\n\u22121 \u22120.5 0 0.5 1\u221210\u22128\u22126\u22124\u221220246810aw1=0.010, ab1=0.100, aw2=1.000, ab2=1.000\n(a)\u22121 \u22120.5 0 0.5 1\u221210\u22128\u22126\u22124\u221220246810aw1=0.001, ab1=0.100, aw2=1.000, ab2=1.000\n(b)\n\u22121 \u22120.5 0 0.5 1\u221210\u22128\u22126\u22124\u221220246810aw1=0.010, ab1=0.010, aw2=1.000, ab2=1.000\n(c)\u22121 \u22120.5 0 0.5 1\u221210\u22128\u22126\u22124\u221220246810aw1=0.010, ab1=0.100, aw2=0.100, ab2=1.000\n(d)\n\u22121 \u22120.5 0 0.5 1\u221210\u22128\u22126\u22124\u221220246810aw1=0.010, ab1=0.100, aw2=1.000, ab2=0.100\n(e)\nFigure 16.17 The effects of changing the hyper-parameters on an MLP. (a) Default parameter values\n\u03b1v=0.01,\u03b1b=0.1,\u03b1w=1,\u03b1c=1. (b) Decreasing \u03b1vby factor of 10. (c) Decreasing \u03b1bby\nfactor of 10. (d) Decreasing \u03b1wby factor of 10. (e) Decreasing \u03b1cby factor of 10. Figure generated by\nmlpPriorsDemo .", "605": "16.5. Feedforwardneuralnetworks(multilayerperceptrons) 575\nNeural Network\ny\nh44 h43h42 h41h40\nh34 h33h31h30\nh24 h23 h21h20\nh14 h13h22h32\nh12 h11h10\nx1x0\n(a)\u22125 0 5\u22124\u22123\u22122\u2212101234\n  \nData\nDeep Neural Net\n(b)\nFigure 16.18 (a) A deep but sparse neural network. The connections are pruned using /lscript1regularization.\nAt each level, nodes numbered 0 are clamped to 1, so their outgoing weights correspond to the offset/bias\nterms. (b) Predictions made by the model on the training set. Figure generated by sparseNnetDemo ,\nwritten by Mark Schmidt.\nHowever, we can also use the more principled sparsity-promoting techniques we discussed in\nChapter 13. One approach is to use an /lscript1regularizer. See Figure 16.18 for an example. Another\napproach is to use ARD; this is discussed in more detail in Section 16.5.7.5.\n16.5.6.3 Soft weight sharing*\nAnother way to regularize the parameters is to encourage similar weights to share statistical\nstrength. But how do we know which parameters to group together? We can learn this, by usinga mixture model. That is, we model p(\u03b8)as a mixture of (diagonal) Gaussians. Parameters that\nare assigned to the same cluster will share the same mean and variance and thus will havesimilar values (assuming the variance for that cluster is low). This is called soft weight sharing\n(Nowlan and Hinton 1992). In practice, this technique is not widely used. See e.g., (Bishop 2006a,p271) if you want to know the details.\n16.5.6.4 Semi-supervised embedding *\nAn interesting way to regularize \u201cdeep\u201d feedforward neural networks is to encourage the hiddenlayers to assign similar objects to similar representations. This is useful because it is often easyto obtain \u201cside\u201d information consisting of sets of pairs of similar and dissimilar objects. Forexample, in a video classi\ufb01cation task, neighboring frames can be deemed similar, but framesthat are distant in time can be deemed dis-similar (Mobahi et al. 2009). Note that this can bedone without collecting any labels.\nLetS\nij=1if examples iandjare similar, and Sij=0otherwise. Let f(xi)be some\nembedding of item xi, e.g.,f(xi)=z(xi,\u03b8),w h e r ezis the hidden layer of a neural network.\nNow de\ufb01ne a loss function L(f(xi),f(xj),Sij)that depends on the embedding of two objects,", "606": "576 Chapter16. Adaptivebasisfunctionmodels\nand the observed similarity measure. For example, we might want to force similar objects to\nhave similar embeddings, and to force the embeddings of dissimilar objects to be a minimaldistance apart:\nL(f\ni,fj,Sij)=/braceleftbigg\n||fi\u2212fj||2ifSij=1\nmax(0,m \u2212||fi\u2212fj||2)ifSij=0(16.80)\nwheremis some minimal margin. We can now de\ufb01ne an augmented loss function for training\nthe neural network:\n/summationdisplay\ni\u2208LNLL(f(xi),yi)+\u03bb/summationdisplay\ni,j\u2208UL(f(xi),f(xj),Sij) (16.81)\nwhereLis the labeled training set, Uis the unlabeled training set, and \u03bb\u22650is some tradeoff\nparameter. This is called semi-supervised embedding (Weston et al. 2008).\nSuch an objective can be easily optimized by stochastic gradient descent. At each itera-\ntion, pick a random labeled training example, (xn,yn), and take a gradient step to optimize\nNLL(f(xi),yi). Then pick a random pair of similar unlabeled examples xi,xj(these can\nsometimes be generated on the \ufb02y rather than stored in advance), and make a gradient step tooptimize \u03bbL(f(x\ni),f(xj),1), Finally, pick a random unlabeled example xk, which with high\nprobability is dissimilar to xi, and make a gradient step to optimize \u03bbL(f(xi),f(xk),0).\nNote that this technique is effective because it can leverage massive amounts of data. In\na related approach, (Collobert and Weston 2008) trained a neural network to distinguish validEnglish sentences from invalid ones. This was done by taking all 631 million words from EnglishWikipedia (en .wikipedia .org), and then creating windows of length 11 containing neighboring\nwords. This constitutes the positive examples. To create negative examples, the middle word ofeach window was replaced by a random English word (this is likely to be an \u201cinvalid\u201d sentence\u2014 either grammatically and/or semantically \u2014 with high probability). This neural network wasthen trained over the course of 1 week, and its latent representation was then used as the inputto a supervised semantic role labeling task, for which very little labeled training data is available.(See also (Ando and Zhang 2005) for related work.)\n16.5.7 Bayesian inference *\nAlthough MAP estimation is a succesful way to reduce over\ufb01tting, there are still some goodreasons to want to adopt a fully Bayesian approach to \u201c\ufb01tting\u201d neural networks:\n\u2022 Integrating out the parameters instead of optimizing them is a much stronger form of regu-\nlarization than MAP estimation.\n\u2022 We can use Bayesian model selection to determine things like the hyper-parameter settings\nand the number of hidden units. This is likely to be much faster than cross validation,\nespecially if we have many hyper-parameters (e.g., as in ARD).\n\u2022 Modelling uncertainty in the parameters will induce uncertainty in our predictive distribu-\ntions, which is important for certain problems such as active learning and risk-averse decisionmaking.", "607": "16.5. Feedforwardneuralnetworks(multilayerperceptrons) 577\n\u2022 We can use online inference methods, such as the extended Kalman \ufb01lter, to do online\nlearning (Haykin 2001).\nOne can adopt a variety of approximate Bayesian inference techniques in this context. In this\nsection, we discuss the Laplace approximation, \ufb01rst suggested in (MacKay 1992, 1995b). One can\nalso use hybrid Monte Carlo (Neal 1996), or variational Bayes (Hinton and Camp 1993; Barberand Bishop 1998).\n16.5.7.1 Parameter posterior for regression\nWe start by considering regression, following the presentation of (Bishop 2006a, sec 5.7), whichsummarizes the work of (MacKay 1992, 1995b). We will use a prior of the form p(w)=\nN(w|0,(1/\u03b1)I),w h e r ewrepresents all the weights combined. We will denote the precision\nof the noise by \u03b2=1/\u03c3\n2.\nThe posterior can be approximated as follows:\np(w|D,\u03b1,\u03b2)\u221dexp(\u2212E(w)) (16.82)\nE(w)/defines\u03b2ED(w)+\u03b1E W(w) (16.83)\nED(w)/defines1\n2N/summationdisplay\nn=1(yn\u2212f(xn,w))2(16.84)\nEW(w)/defines1\n2wTw (16.85)\nwhereEDis the data error, EWis the prior error, and Eis the overall error (negative log\nprior plus log likelihood). Now let us make a second-order Taylor series approximation of E(w)\naround its minimum (the MAP estimate)\nE(w)\u2248E(wMP)+1\n2(w\u2212wMP)TA(w\u2212wMP) (16.86)\nwhereAis the Hessian of E:\nA=\u2207\u2207E(wMP)=\u03b2H+\u03b1I (16.87)\nwhereH=\u2207\u2207ED(wMP)is the Hessian of the data error. This can be computed exactly\ninO(d2)time, where dis the number of parameters, using a variant of backpropagation (see\n(Bishop 2006a, sec 5.4) for details). Alternatively, if we use a quasi-Newton method to \ufb01ndthe mode, we can use its internally computed (low-rank) approximation to H. (Note that\ndiagonal approximations of Hare usually very inaccurate.) In either case, using this quadratic\napproximation, the posterior becomes Gaussian:\np(w|\u03b1,\u03b2,D)\u2248N(w|w\nMP,A\u22121) (16.88)", "608": "578 Chapter16. Adaptivebasisfunctionmodels\n16.5.7.2 Parameter posterior for classi\ufb01cation\nThe classi\ufb01cation case is the same as the regression case, except \u03b2=1andEDis a cross-\nentropy error of the form\nED(w)/definesN/summationdisplay\nn=1[ynlnf(xn,w)+(1\u2212yn)lnf(xn,w)] (16.89)\n(16.90)\n16.5.7.3 Predictive posterior for regression\nThe posterior predictive density is given by\np(y|x,D,\u03b1,\u03b2)=/integraldisplay\nN(y|f(x,w),1/\u03b2)N(w|wMP,A\u22121)dw (16.91)\nThis is not analytically tractable because of the nonlinearity of f(x,w). Let us therefore\nconstruct a \ufb01rst-order Taylor series approximation around the mode:\nf(x,w)\u2248f(x,wMP)+gT(w\u2212wMP) (16.92)\nwhere\ng=\u2207wf(x,w)|w=wMP (16.93)\nWe now have a linear-Gaussian model with a Gaussian prior on the weights. From Equation 4.126\nwe have\np(y|x,D,\u03b1,\u03b2)\u2248N(y|f(x,wMP),\u03c32(x)) (16.94)\nwhere the predictive variance depends on the input xas follows:\n\u03c32(x)=\u03b2\u22121+gTA\u22121g (16.95)\nThe error bars will be larger in regions of input space where we have little training data. SeeFigure 16.19 for an example.\n16.5.7.4 Predictive posterior for classi\ufb01cation\nIn this section, we discuss how to approximate p(y|x,D)in the case of binary classi\ufb01cation.\nThe situation is similar to the case of logistic regression, discussed in Section 8.4.4, except inaddition the posterior predictive mean is a non-linear function of w. Speci\ufb01cally, we have\n\u03bc=E[y|x,w]=s i g m ( a(x,w)),w h e r e a(x,w)is the pre-synaptic output of the \ufb01nal layer.\nLet us make a linear approximation to this:\na(x,w)\u2248a\nMP(x)+gT(w\u2212wMP) (16.96)\nwhereaMP(x)=a(x,wMP)andg=\u2207xa(x,wMP)can be found by a modi\ufb01ed version of\nbackpropagation. Clearly\np(a|x,D)\u2248N(a(x,wMP),g(x)TA\u22121g(x)) (16.97)", "609": "16.5. Feedforwardneuralnetworks(multilayerperceptrons) 579\n0 0.2 0.4 0.6 0.8 1\u22121.5\u22121\u22120.500.511.5\nInputTarget\n  \ndata\nfunction\nnetwork\nerror bars\n(a)0 0.2 0.4 0.6 0.8 1\u22121.5\u22121\u22120.500.511.5\n  \nData\nFunction\nPrediction\nSamples\n(b)\nFigure 16.19 The posterior predictive density for an MLP with 3 hidden nodes, trained on 16 data points.\nThe dashed green line is the true function. (a) Result of using a Laplace approximation, after performing\nempirical Bayes to optimize the hyperparameters. The solid red line is the posterior mean prediction,and the dotted blue lines are 1 standard deviation above and below the mean. Figure generated bymlpRegEvidenceDemo . (b) Result of using hybrid Monte Carlo, using the same trained hyperparameters\nas in (a). The solid red line is the posterior mean prediction, and the dotted blue lines are samples fromthe posterior predictive. Figure generated by mlpRegHmcDemo , written by Ian Nabney.\nHence the posterior predictive for the output is\np(y=1|x,D)=/integraldisplay\nsigm(a)p(a|x,D)da\u2248sigm(\u03ba(\u03c32\na)bTwMP) (16.98)\nwhere\u03bais de\ufb01ned by Equation 8.70, which we repeat here for convenience:\n\u03ba(\u03c32)/defines(1+\u03c0\u03c32/8)\u22121\n2 (16.99)\nOf course, a simpler (and potentially more accurate) alternative to this is to draw a few samples\nfrom the Gaussian posterior and to approximate the posterior predictive using Monte Carlo.\nIn either case, the effect of taking uncertainty of the parameters into account, as in Sec-\ntion 8.4.4, is to \u201cmoderate\u201d the con\ufb01dence of the output; the decision boundary itself is unaf-\nfected, however.\n16.5.7.5 ARD for neural networks\nOnce we have made the Laplace approximation to the posterior, we can optimize the marginallikelihood wrt the hyper-parameters \u03b1using the same \ufb01xed-point equations as in Section 13.7.4.2.\nTypically we use one hyper-parameter for the weight vector leaving each node, to achieve aneffect similar to group lasso (Section 13.5.1). That is, the prior has the form\np(\u03b8)=\nD/productdisplay\ni=1N(v:,i|0,1\n\u03b1v,iI)H/productdisplay\nj=1N(w:,j|0,1\n\u03b1w,jI) (16.100)\nIf we \ufb01nd \u03b1v,i=\u221e, then input feature iis irrelevant, and its weight vector v:,iis pruned out.\nSimilarly, if we \ufb01nd \u03b1w,j=\u221e, then hidden feature jis irrelevant. This is known as automatic", "610": "580 Chapter16. Adaptivebasisfunctionmodels\nrelevancy determination or ARD, which was discussed in detail in Section 13.7. Applying this to\nneural networks gives us an efficient means of variable selection in non-linear models.\nThe software package NETLABcontains a simple example of ARD applied to a neural network,\ncalled demard. This demo creates some data according to a nonlinear regression function\nf(x1,x2,x3)=s i n ( 2 \u03c0x1)+/epsilon1,w h e r ex2is a noisy copy of x1. We see that x2andx3are\nirrelevant for predicting the target. However, x2is correlated with x1, which is relevant. Using\nARD, the \ufb01nal hyper-parameters are as follows:\n\u03b1=[ 0.2,21.4,249001.8] (16.101)\nThis clearly indicates that feature 3 is irrelevant, feature 2 is only weakly relevant, and feature 1is very relevant.\n16.6 Ensemble learning\nEnsemble learning refers to learning a weighted combination of base models of the form\nf(y|x,\u03c0)=/summationdisplay\nm\u2208Mwmfm(y|x) (16.102)\nwhere the wmare tunable parameters. Ensemble learning is sometimes called a committee\nmethod, since each base model fmgets a weighted \u201cvote\u201d.\nClearly ensemble learning is closely related to learning adaptive-basis function models. In\nfact, one can argue that a neural net is an ensemble method, where fmrepresents the m\u2019th\nhidden unit, and wmare the output layer weights. Also, we can think of boosting as kind of\nensemble learning, where the weights on the base models are determined sequentially. Belowwe describe some other forms of ensemble learning.\n16.6.1 Stacking\nAn obvious way to estimate the weights in Equation 16.102 is to use\n\u02c6w=a r g m i n\nwN/summationdisplay\ni=1L(yi,M/summationdisplay\nm=1wmfm(x)) (16.103)\nHowever, this will result in over\ufb01tting, with wmbeing large for the most complex model. A\nsimple solution to this is to use cross-validation. In particular, we can use the LOOCV estimate\n\u02c6w=a r g m i n\nwN/summationdisplay\ni=1L(yi,M/summationdisplay\nm=1wm\u02c6f\u2212i\nm(x)) (16.104)\nwhere\u02c6f\u2212i\nm(x)is the predictor obtained by training on data excluding (xi,yi). This is known\nasstacking, which stands for \u201cstacked generalization\u201d (Wolpert 1992). This technique is more\nrobust to the case where the \u201ctrue\u201d model is not in the model class than standard BMA (Clarke2003). This approach was used by the Net\ufb02ix team known as \u201cThe Ensemble\u201d, which tied thesubmission of the winning team (BellKor\u2019s Pragmatic Chaos) in terms of accuracy (Sill et al.2009). Stacking has also been used for problems such as image segmentation and labeling.", "611": "16.6. Ensemblelearning 581\nClassC1C2C3C4C5C6\u00b7\u00b7\u00b7C15\n0 110000 \u00b7\u00b7\u00b71\n1 001111 \u00b7\u00b7\u00b70\n...\n90 1 1 1 0 0 \u00b7\u00b7\u00b70\nTable 16.2 Part of a 15-bit error-correcting output code for a 10-class problem. Each row de\ufb01nes a\ntwo-class problem. Based on Table 16.1 of (Hastie et al. 2009).\n16.6.2 Error-correcting output codes\nAn interesting form of ensemble learning is known as error-correcting output codes orECOC\n(Dietterich and Bakiri 1995), which can be used in the context of multi-class classi\ufb01cation. The\nidea is that we are trying to decode a symbol (namely the class label) which has Cpossible\nstates. We could use a bit vector of length B=\u2308log2C\u2309to encode the class label, and train\nBseparate binary classi\ufb01ers to predict each bit. However, by using more bits, and by designing\nthe codewords to have maximal Hamming distance from each other, we get a method that ismore resistant to individual bit-\ufb02ipping errors (misclassi\ufb01cation). For example, in Table 16.2, weuseB=1 5bits to encode a C=1 0class problem. The minimum Hamming distance between\nany pair of rows is 7. The decoding rule is\n\u02c6c(x)=m i n\ncB/summationdisplay\nb=1|Ccb\u2212\u02c6pb(x)| (16.105)\nwhereCcbis theb\u2019th bit of the codeword for class c. (James and Hastie 1998) showed that a\nrandom code worked just as well as the optimal code: both methods work by averaging theresults of multiple classi\ufb01ers, thereby reducing variance.\n16.6.3 Ensemble learning is not equivalent to Bayes model averaging\nIn Section 5.3, we discussed Bayesian model selection. An alternative to picking the best model,and then using this to make predictions, is to make a weighted average of the predictions madeby each model, i.e., we compute\np(y|x,D)=/summationdisplay\nm\u2208Mp(y|x,m,D)p(m|D) (16.106)\nThis is called Bayes model averaging (BMA), and can sometimes give better performance than\nusing any single model (Hoeting et al. 1999). Of course, averaging over all models is typicallycomputationally infeasible (analytical integration is obviously not possible in a discrete space,although one can sometimes use dynamic programming to perform the computation exactly,e.g., (Meila and Jaakkola 2006)). A simple approximation is to sample a few models from theposterior. An even simpler approximation (and the one most widely used in practice) is to justuse the MAP model.\nIt is important to note that BMA is not equivalent to ensemble learning (Minka 2000c). This\nlatter technique corresponds to enlarging the model space, by de\ufb01ning a single new model", "612": "582 Chapter16. Adaptivebasisfunctionmodels\nmodel 1st2nd3rd4th5th6th7th8th9th10th\nbst-dt 0.580 0.228 0.160 0.023 0.009 0.000 0.000 0.000 0.000 0.000\nrf 0.390 0.525 0.084 0.001 0.000 0.000 0.000 0.000 0.000 0.000\nbag-dt 0.030 0.232 0.571 0.150 0.017 0.000 0.000 0.000 0.000 0.000\nsvm 0.000 0.008 0.148 0.574 0.240 0.029 0.001 0.000 0.000 0.000\nann 0.000 0.007 0.035 0.230 0.606 0.122 0.000 0.000 0.000 0.000\nknn 0.000 0.000 0.000 0.009 0.114 0.592 0.245 0.038 0.002 0.000\nbst-stmp 0.000 0.000 0.002 0.013 0.014 0.257 0.710 0.004 0.000 0.000\ndt 0.000 0.000 0.000 0.000 0.000 0.000 0.004 0.616 0.291 0.089\nlogreg 0.000 0.000 0.000 0.000 0.000 0.000 0.040 0.312 0.423 0.225\nnb 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.030 0.284 0.686\nTable 16.3 Fraction of time each method achieved a speci\ufb01ed rank, when sorting by mean performance\nacross 11 datasets and 8 metrics. Based on Table 4 of (Caruana and Niculescu-Mizil 2006). Used with kind\npermission of Alexandru Niculescu-Mizil.\nwhich is a convex combination of base models, as follows:\np(y|x,\u03c0)=/summationdisplay\nm\u2208M\u03c0mp(y|x,m) (16.107)\nIn principle, we can now perform Bayesian inference to compute p(\u03c0|D); we then make pre-\ndictions using p(y|x,D)=/integraltext\np(y|x,\u03c0)p(\u03c0|D)d\u03c0. However, it is much more common to use\npoint estimation methods for \u03c0,a sw es a wa b o v e .\n16.7 Experimental comparison\nWe have described many different methods for classi\ufb01cation and regression. Which one should\nyou use? That depends on which inductive bias you think is most appropriate for your domain.Usually this is hard to assess, so it is common to just try several different methods, andsee how they perform empirically. Below we summarize two such comparisons that werecarefully conducted (although the data sets that were used are relatively small). See the websitemlcomp.orgfor a distributed way to perform large scale comparisons of this kind. Of course,\nwe must always remember the no free lunch theorem (Section 1.4.9), which tells us that there isno universally best learning method.\n16.7.1 Low-dimensional features\nIn 2006, Rich Caruana and Alex Niculescu-Mizil (Caruana and Niculescu-Mizil 2006) conducteda very extensive experimental comparison of 10 different binary classi\ufb01cation methods, on 11different data sets. The 11 data sets all had 5000 training cases, and had test sets containing\u223c10,000examples on average. The number of features ranged from 9 to 200, so this is much\nlower dimensional than the NIPS 2003 feature selection challenge. 5-fold cross validation wasused to assess average test error. (This is separate from any internal CV a method may need touse for model selection.)", "613": "16.7. Experimentalcomparison 583\nThe methods they compared are as follows (listed in roughly decreasing order of performance,\nas assessed by Table 16.3):\n\u2022 BST-DT: boosted decision trees\n\u2022 RF: random forest\u2022 BAG-DT: bagged decision trees\u2022 SVM: support vector machine\u2022 ANN: arti\ufb01cial neural network\u2022 KNN: K-nearest neighbors\u2022 BST-STMP: boosted stumps\u2022 DT: decision tree\u2022 LOGREG: logistic regression\u2022 NB: naive Bayes\nThey used 8 different performance measures, which can be divided into three groups. Thresh-\nold metrics just require a point estimate as output. These include accuracy, F-score (Sec-\ntion 5.7.2.3), etc. Ordering/ ranking metrics measure how well positive cases are ordered beforethe negative cases. These include area under the ROC curve (Section 5.7.2.1), average precision,and the precision/recall break even point. Finally, the probability metrics included cross-entropy(log-loss) and squared error, (y\u2212\u02c6p)\n2. Methods such as SVMs that do not produce calibrated\nprobabilities were post-processed using Platt\u2019s logistic regression trick (Section 14.5.2.3), or usingisotonic regression. Performance measures were standardized to a 0:1 scale so they could becompared.\nObviously the results vary by dataset and by metric. Therefore just averaging the performance\ndoes not necessarily give reliable conclusions. However, one can perform a bootstrap analysis,which shows how robust the conclusions are to such changes. The results are shown inTable 16.3. We see that most of the time, boosted decision trees are the best method, followedby random forests, bagged decision trees, SVMs and neural networks. However, the followingmethods all did relatively poorly: KNN, stumps, single decision trees, logistic regression andnaive Bayes.\nThese results are generally consistent with conventional wisdom of practioners in the \ufb01eld.\nOf course, the conclusions may change if there the features are high dimensional and/ or thereare lots of irrelevant features (as in Section 16.7.2), or if there is lots of noise, etc.\n16.7.2 High-dimensional features\nIn 2003, the NIPS conference ran a competition where the goal was to solve binary classi\ufb01cationproblems with large numbers of (mostly irrelevant) features, given small training sets. (Thiswas called a \u201cfeature selection\u201d challenge, but performance was measured in terms of predictiveaccuracy, not in terms of the ability to select features.) The \ufb01ve datasets that were used aresummarized in Table 16.4. The term proberefers to arti\ufb01cal variables that were added to the\nproblem to make it harder. These have no predictive power, but are correlated with the originalfeatures.\nResults of the competition are discussed in (Guyon et al. 2006). The overall winner was an\napproach based on Bayesian neural networks (Neal and Zhang 2006). In a follow-up study", "614": "584 Chapter16. Adaptivebasisfunctionmodels\nDataset Domain Type D %probes NtrainNvalNtest\nAracene Mass spectrometry Dense 10,000 30 100 100 700\nDexter Text classi\ufb01cation Sparse 20,000 50 300 300 2000Dorothea Drug discovery Sparse 100,000 50 800 350 800Gisette Digit recognition Dense 5000 30 6000 1000 6500Madelon Arti\ufb01cial Dense 500 96 2000 600 1800\nTable 16.4 Summary of the data used in the NIPS 2003 \u201cfeature selection\u201d challenge. For the Dorothea\ndatasets, the features are binary. For the others, the features are real-valued.\nScreened features ARD\nMethod Avg rank Avg time Avg rank Avg time\nHMC MLP 1.5 384 (138) 1.6 600 (186)\nBoosted MLP 3.8 9.4 (8.6) 2.2 35.6 (33.5)\nBagged MLP 3.6 3.5 (1.1) 4.0 6.4 (4.4)\nBoosted trees 3.4 3.03 (2.5) 4.0 34.1 (32.4)\nRandom forests 2.7 1.9 (1.7) 3.2 11.2 (9.3)\nTable 16.5 Performance of different methods on the NIPS 2003 \u201cfeature selection\u201d challenge. (HMC\nstands for hybrid Monte Carlo; see Section 24.5.4.) We report the average rank (lower is better) across the\n5 datasets. We also report the average training time in minutes (standard error in brackets). The MCMCand bagged MLPs use two hidden layers of 20 and 8 units. The boosted MLPs use one hidden layer with 2or 4 hidden units. The boosted trees used depths between 2 and 9, and shrinkage between 0.001 and 0.1.Each tree was trained on 80% of the data chosen at random at each step (so-called stochastic gradient\nboosting). From Table 11.3 of (Hastie et al. 2009).\n(Johnson 2009), Bayesian neural nets (MLPs with 2 hidden layers) were compared to several other\nmethods based on bagging and boosting. Note that all of these methods are quite similar: ineach case, the prediction has the form\n\u02c6f(x\n\u2217)=M/summationdisplay\nm=1wmE[y|x\u2217,\u03b8m] (16.108)\nThe Bayesian MLP was \ufb01t by MCMC (hybrid Monte Carlo), so we set wm=1/Mand set\u03b8m\nto a draw from the posterior. In bagging, we set wm=1/Mand\u03b8mis estimated by \ufb01tting\nthe model to a bootstrap sample from the data. In boosting, we set wm=1and the\u03b8mare\nestimated sequentially.\nTo improve computational and statistical performance, some feature selection was performed.\nTwo methods were considered: simple uni-variate screening using T-tests, and a method basedon MLP+ARD. Results of this follow-up study are shown in Table 16.5. We see that Bayesian MLPsare again the winner. In second place are either random forests or boosted MLPs, dependingon the preprocessing. However, it is not clear how statistically signi\ufb01cant these differences are,since the test sets are relatively small.\nIn terms of training time, we see that MCMC is much slower than the other methods. It would\nbe interesting to see how well deterministic Bayesian inference (e.g., Laplace approximation)would perform. (Obviously it will be much faster, but the question is: how much would one lose", "615": "16.8. Interpretingblack-boxmodels 585\n0.2 0.4 0.6 0.88 1 01 21 41 61 82 0\nx1partial dependence\n0.2 0.4 0.6 0.88 1 01 21 41 61 82 0\nx20.2 0.4 0.6 0.88 1 01 21 41 61 82 0\nx30.2 0.4 0.6 0.8 1.08 1 01 21 41 61 82 0\nx40.2 0.4 0.6 0.88 1 01 21 41 61 82 0\nx5\n0.2 0.4 0.6 0.88 1 01 21 41 61 82 0\nx6partial dependence\n0.2 0.4 0.6 0.8 1.08 1 01 21 41 61 82 0\nx70.2 0.4 0.6 0.88 1 01 21 41 61 82 0\nx80.2 0.4 0.6 0.88 1 01 21 41 61 82 0\nx90.2 0.4 0.6 0.88 1 01 21 41 61 82 0\nx10\nFigure 16.20 Partial dependence plots for the 10 predictors in Friedman\u2019s synthetic 5-dimensional re-\ngression problem. Source: Figure 4 of (Chipman et al. 2010) . Used with kind permission of Hugh\nChipman.\nin statistical performance?)\n16.8 Interpreting black-box models\nLinear models are popular in part because they are easy to interpet. However, they often are\npoor predictors, which makes them a poor proxy for \u201cnature\u2019s mechanism\u201d. Thus any conclusionsabout the importance of particular variables should only be based on models that have goodpredictive accuracy (Breiman 2001b). (Interestingly, many standard statistical tests of \u201cgoodnessof \ufb01t\u201d do not test the predictive accuracy of a model.)\nIn this chapter, we studied black-box models, which do have good predictive accuracy.\nUnfortunately, they are hard to interpret directly. Fortunately, there are various heuristics we canuse to \u201cprobe\u201d such models, in order to assess which input variables are the most important.\nAs a simple example, consider the following non-linear function, \ufb01rst proposed (Friedman\n1991) to illustrate the power of MARS:\nf(x)=1 0s i n ( \u03c0x\n1x2)+20(x3\u22120.5)2+10x4+5x5+/epsilon1 (16.109)\nwhere/epsilon1\u223cN(0,1). We see that the output is a complex function of the inputs. By augmenting\nthexvector with additional irrelevant random variables, all drawn uniform on [0,1], we can\ncreate a challenging feature selection problem. In the experiments below, we add 5 extra dummyvariables.", "616": "586 Chapter16. Adaptivebasisfunctionmodels\n1111\n1\n1 1 1 1 1\n2468 1 00.00 0.05 0.10 0.15 0.20 0.25usage2\n222\n2\n2 2 2 2 23333\n3\n3 33 3 34\n444\n4\n4444 45555\n5\n5 5 555\nFigure 16.21 Average usage of each variable in a BART model \ufb01t to data where only the \ufb01rst 5 features are\nrelevant. The different coloured lines correspond to different numbers of trees in the ensemble. Source:\nFigure 5 of (Chipman et al. 2010) . Used with kind permission of Hugh Chipman.\nOne useful way to measure the effect of a set sof variables on the output is to compute a\npartial dependence plot (Friedman 2001). This is a plot of f(xs)vsxs,w h e r ef(xs)is de\ufb01ned\nas the response to xswith the other predictors averaged out:\nf(xs)=1\nNN/summationdisplay\ni=1f(xs,xi,\u2212s) (16.110)\nFigure 16.20 shows an example where we use sets corresponding to each single variable. The data\nwas generated from Equation 16.109, with 5 irrelevant variables added. We then \ufb01t a BART model(Section 16.2.5) and computed the partial dependence plots. We see that the predicted responseis invariant for s\u2208{6,...,10}, indicating that these variables are (marginally) irrelevant. The\nresponse is roughly linear in x\n4andx5, and roughly quadratic in x3. (The error bars are obtained\nby computing empirical quantiles of f(x,\u03b8)based on posterior samples of \u03b8; alternatively, we\ncan use bootstrap.)\nAnother very useful summary computes the relative importance of predictor variables.\nThis can be thought of as a nonlinear, or even \u201cmodel free\u201d, way of performing variable selection,although the technique is restricted to ensembles of trees. The basic idea, originally proposedin (Breiman et al. 1984), is to count how often variable jis used as a node in any of the trees.\nIn particular, let v\nj=1\nM/summationtextM\nm=1I(j\u2208Tm)be the proportion of all splitting rules that use xj,\nwhereTmis them\u2019th tree. If we can sample the posterior of trees, p(T1:M|D), we can easily\ncompute the posterior for vj. Alternatively, we can use bootstrap.\nFigure 16.21 gives an example, using BART. We see that the \ufb01ve relevant variables are chosen\nmuch more than the \ufb01ve irrelevant variables. As we increase the number Mof trees, all the\nvariables are more likely to be chosen, reducing the sensitivity of this method, but for small M,\nthe method is farily diagnostic.", "617": "16.8. Interpretingblack-boxmodels 587\nExercises\nExercise 16.1 Nonlinear regression for inverse dynamics\nIn this question, we \ufb01t a model which can predict what torques a robot needs to apply in order to make\nits arm reach a desired point in space. The data was collected from a SARCOS robot arm with 7 degrees of\nfreedom. The input vector x\u2208R21encodes the desired position, velocity and accelaration of the 7 joints.\nThe output vector y\u2208R7encodes the torques that should be applied to the joints to reach that point.\nThe mapping from xtoyis highly nonlinear.\nWe have N=4 8,933training points and Ntest=4,449testing points. For simplicity, we following\nstandard practice and focus on just predicting a scalar output, namely the torque for the \ufb01rst joint.\nDownload the data from http://www .gaussianprocess .org/gpml . Standardize the inputs so they\nhave zero mean and unit variance on the training set, and center the outputs so they have zero mean\non the training set. Apply the corresponding transformations to the test data. Below we will describevarious models which you should \ufb01t to this transformed data. Then make predictions and compute thestandardized mean squared error on the test set as follows:\nSMSE=\n1\nNtest/summationtextNtest\ni=1(yi\u2212\u02c6yi)2\n\u03c32(16.111)\nwhere\u03c32=1\nNtrain/summationtextNtrain\ni=1(yi\u2212y)2is the variance of the output computed on the training set.\na. The \ufb01rst method you should try is standard linear regression. Turn in your numbers and code.\n(According to (Rasmussen and Williams 2006, p24), you should be able to achieve a SMSE of 0.075\nusing this method.)\nb. Now try running K-means clustering (using cross validation to pick K). Then \ufb01t an RBF network to\nthe data, using the \u03bckestimated by K-means. Use CV to estimate the RBF bandwidth. What SMSE do\nyou get? Turn in your numbers and code. (According to (Rasmussen and Williams 2006, p24), Gaussianprocess regression can get an SMSE of 0.011, so the goal is to get close to that.)\nc. Now try \ufb01tting a feedforward neural network. Use CV to pick the number of hidden units and the\nstrength of the /lscript\n2regularizer. What SMSE do you get? Turn in your numbers and code.", "618": "", "619": "17 Markov and hidden Markov models\n17.1 Introduction\nIn this chapter, we discuss probabilistic models for sequences of observations, X1,...,X T,o f\narbitrary length T. Such models have applications in computational biology, natural language\nprocessing, time series forecasting, etc. We focus on the case where we the observations occur\nat discrete \u201ctime steps\u201d, although \u201ctime\u201d may also refer to locations within a sequence.\n17.2 Markov models\nRecall from Section 10.2.2 that the basic idea behind a Markov chain is to assume that Xt\ncaptures all the relevant information for predicting the future (i.e., we assume it is a sufficientstatistic). If we assume discrete time steps, we can write the joint distribution as follows:\np(X\n1:T)=p(X1)p(X2|X1)p(X3|X2)...=p(X1)T/productdisplay\nt=2p(Xt|Xt\u22121) (17.1)\nThis is called a Markov chain orMarkov model.\nIf we assume the transition function p(Xt|Xt\u22121)is independent of time, then the chain is\ncalledhomogeneous, stationary,o r time-invariant. This is an example of parameter tying,\nsince the same parameter is shared by multiple variables. This assumption allows us to modelan arbitrary number of variables using a \ufb01xed number of parameters; such models are calledstochastic processes.\nIf we assume that the observed variables are discrete, so X\nt\u2208{1,...,K}, this is called a\ndiscrete-state or \ufb01nite-state Markov chain. We will make this assumption throughout the rest ofthis section.\n17.2.1 Transition matrix\nWhenXtis discrete, so Xt\u2208{1,...,K}, the conditional distribution p(Xt|Xt\u22121)can be\nwritten as a K\u00d7Kmatrix, known as the transition matrix A,w h e r e Aij=p(Xt=\nj|Xt\u22121=i)is the probability of going from state ito statej. Each row of the matrix sums to\none,/summationtext\njAij=1, so this is called a stochastic matrix.", "620": "590 Chapter17. MarkovandhiddenMarkovmodels\n1 2\u03b1\n\u03b21\u2212\u03b11\u2212\u03b2\n(a)1 2 3A12 A23A11 A22 A33\n(b)\nFigure 17.1 State transition diagrams for some simple Markov chains. Left: a 2-state chain. Right: a\n3-state left-to-right chain.\nA stationary, \ufb01nite-state Markov chain is equivalent to a stochastic automaton. It is common\nto visualize such automata by drawing a directed graph, where nodes represent states and arrows\nrepresent legal transitions, i.e., non-zero elements of A. This is known as a state transition\ndiagram. The weights associated with the arcs are the probabilities. For example, the following2-state chain\nA=/parenleftbigg1\u2212\u03b1\u03b1\n\u03b21\u2212\u03b2/parenrightbigg\n(17.2)\nis illustrated in Figure 17.1(left). The following 3-state chain\nA=\u239b\n\u239dA\n11A120\n0A22A23\n001\u239e\u23a0 (17.3)\nis illustrated in Figure 17.1(right). This is called a left-to-right transition matrix, and is com-\nmonly used in speech recognition (Section 17.6.2).\nTheA\nijelement of the transition matrix speci\ufb01es the probability of getting from itojin\none step. The n-step transition matrix A(n)is de\ufb01ned as\nAij(n)/definesp(Xt+n=j|Xt=i) (17.4)\nwhich is the probability of getting from itojin exactly nsteps. Obviously A(1) =A. The\nChapman-Kolmogorov equations state that\nAij(m+n)=K/summationdisplay\nk=1Aik(m)Akj(n) (17.5)\nIn words, the probability of getting from itojinm+nsteps is just the probability of getting\nfromitokinmsteps, and then from ktojinnsteps, summed up over all k. We can write\nthe above as a matrix multiplication\nA(m+n)=A(m)A(n) (17.6)\nHence\nA(n)=AA(n\u22121) =AAA(n\u22122) =\u00b7\u00b7\u00b7=An(17.7)\nThus we can simulate multiple steps of a Markov chain by \u201cpowering up\u201d the transition matrix.", "621": "17.2. Markovmodels 591\nSAYS IT\u2019S NOT IN THE CARDS LEGENDARY RECONNAISSANCE BY ROLLIE\nDEMOCRACIES UNSUSTAINABLE COULD STRIKE REDLINING VISITS TO PROFIT\nBOOKING WAIT HERE AT MADISON SQUARE GARDEN COUNTY COURTHOUSE WHERE HE\nHAD BEEN DONE IN THREE ALREADY IN ANY WAY IN WHICH A TEACHER\nTable 17.1 Example output from an 4-gram word model, trained using backoff smoothing on the Broadcast\nNews corpus. The \ufb01rst 4 words are speci\ufb01ed by hand, the model generates the 5th word, and then the\nresults are fed back into the model. Source: http://www .fit.vutbr .cz/~imikolov/rnnlm/gen-4gra\nm.txt.\n17.2.2 Application: Language modeling\nOne important application of Markov models is to make statistical language models, which are\nprobability distributions over sequences of words. We de\ufb01ne the state space to be all the words\nin English (or some other language). The marginal probabilities p(Xt=k)are called unigram\nstatistics. If we use a \ufb01rst-order Markov model, then p(Xt=k|Xt\u22121=j)is called a bigram\nmodel. If we use a second-order Markov model, then p(Xt=k|Xt\u22121=j,Xt\u22122=i)is\ncalled atrigram model. And so on. In general these are called n-gram models. For example,\nFigure 17.2 shows 1-gram and 2-grams counts for the letters{a,...,z,\u2212} (where - represents\nspace) estimated from Darwin\u2019s OnTheOriginOfSpecies.\nLanguage models can be used for several things, such as the following:\n\u2022Sentence completion A language model can predict the next word given the previous\nwords in a sentence. This can be used to reduce the amount of typing required, which isparticularly important for disabled users (see e.g., David Mackay\u2019s Dasher system\n1), or uses of\nmobile devices.\n\u2022Data compression Any density model can be used to de\ufb01ne an encoding scheme, by\nassigning short codewords to more probable strings. The more accurate the predictive model,the fewer the number of bits it requires to store the data.\n\u2022Text classi\ufb01cation Any density model can be used as a class-conditional density and hence\nturned into a (generative) classi\ufb01er. Note that using a 0-gram class-conditional density (i.e.,only unigram statistics) would be equivalent to a naive Bayes classi\ufb01er (see Section 3.5).\n\u2022Automatic essay writing One can sample from p(x\n1:t)to generate arti\ufb01cial text. This is\none way of assessing the quality of the model. In Table 17.1, we give an example of textgenerated from a 4-gram model, trained on a corpus with 400 million words. ((Tomas et al.2011) describes a much better language model, based on recurrent neural networks, whichgenerates much more semantically plausible text.)\n1.http://www .inference .phy.cam.ac.uk/dasher/", "622": "592 Chapter17. MarkovandhiddenMarkovmodels\n 1   0.16098   _\n 2   0.06687   a\n 3   0.01414   b\n 4   0.02938   c\n 5   0.03107   d\n 6   0.11055   e\n 7   0.02325   f\n 8   0.01530   g\n 9   0.04174   h\n10   0.06233   i\n11   0.00060   j\n12   0.00309   k\n13   0.03515   l\n14   0.02107   m\n15   0.06007   n\n16   0.06066   o\n17   0.01594   p\n18   0.00077   q\n19   0.05265   r\n20   0.05761   s\n21   0.07566   t\n22   0.02149   u\n23   0.00993   v\n24   0.01341   w\n25   0.00208   x\n26   0.01381   y\n27   0.00039   zUnigrams_abcdefghijklmnopqrstuvwxyz\n_\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\no\np\nq\nr\ns\nt\nu\nv\nw\nx\ny\nzBigrams\nFigure 17.2 Unigram and bigram counts from Darwin\u2019s OnTheOriginOfSpecies. The 2D picture on the\nright is a Hinton diagram of the joint distribution. The size of the white squares is proportional to the\nvalue of the entry in the corresponding vector/ matrix. Based on (MacKay 2003, p22). Figure generated byngramPlot .\n17.2.2.1 MLE for Markov language models\nWe now discuss a simple way to estimate the transition matrix from training data. The proba-\nbility of any particular sequence of length Tis given by\np(x1:T|\u03b8)=\u03c0 (x1)A(x1,x2)...A(xT\u22121,xT) (17.8)\n=K/productdisplay\nj=1(\u03c0j)I(x1=j)T/productdisplay\nt=2K/productdisplay\nj=1K/productdisplay\nk=1(Ajk)I(xt=k,xt\u22121=j)(17.9)\nHence the log-likelihood of a set of sequences D=(x1,...,xN),w h e r exi=(xi1,...,x i,Ti)\nis a sequence of length Ti, is given by\nlogp(D|\u03b8)=N/summationdisplay\ni=1logp(xi|\u03b8)=/summationdisplay\njN1\njlog\u03c0j+/summationdisplay\nj/summationdisplay\nkNjklogAjk (17.10)\nwhere we de\ufb01ne the following counts:\nN1\nj/definesN/summationdisplay\ni=1I(xi1=j),Njk/definesN/summationdisplay\ni=1Ti\u22121/summationdisplay\nt=1I(xi,t=j,xi,t+1=k) (17.11)", "623": "17.2. Markovmodels 593\nHence we can write the MLE as the normalized counts:\n\u02c6\u03c0j=N1\nj/summationtext\njN1\nj,\u02c6Ajk=Njk/summationtext\nkNjk(17.12)\nThese results can be extended in a straightforward way to higher order Markov models.\nHowever, the problem of zero-counts becomes very acute whenever the number of states K,\nand/or the order of the chain, n, is large. An n-gram models has O(Kn)parameters. If we have\nK\u223c50,000words in our vocabulary, then a bi-gram model will have about 2.5 billion free\nparameters, corresponding to all possible word pairs. It is very unlikely we will see all of these\nin our training data. However, we do not want to predict that a particular word string is totallyimpossible just because we happen not to have seen it in our training text \u2014 that would be asevere form of over\ufb01tting.\n2\nA simple solution to this is to use add-one smoothing, where we simply add one to all the\nempirical counts before normalizing. The Bayesian justi\ufb01cation for this is given in Section 3.3.4.1.However add-one smoothing assumes all n-grams are equally likely, which is not very realistic.A more sophisticated Bayesian approach is discussed in Section 17.2.2.2.\nAn alternative to using smart priors is to gather lots and lots of data. For example, Google\nhas \ufb01t n-gram models (for n=1:5) based on one trillion words extracted from the web. Their\ndata, which is over 100GB when uncompressed, is publically available.\n3An example of their\ndata, for a set of 4-grams, is shown below.\nserve as the incoming 92\nserve as the incubator 99\nserve as the independent 794\nserve as the index 223\nserve as the indication 72\nserve as the indicator 120\nserve as the indicators 45\nserve as the indispensable 111\nserve as the indispensible 40\nserve as the individual 234\n...\nAlthough such an approach, based on \u201cbrute force and ignorance\u201d, can be successful, it is\nrather unsatisfying, since it is clear that this is not how humans learn (see e.g., (Tenenbaum\nand Xu 2000)). A more re\ufb01ned Bayesian approach, that needs much less data, is described inSection 17.2.2.2.\n17.2.2.2 Empirical Bayes version of deleted interpolation\nA common heuristic used to \ufb01x the sparse data problem is called deleted interpolation (Chen\nand Goodman 1996). This de\ufb01nes the transition matrix as a convex combination of the bigram\n2. A famous example of an improbable, but syntactically valid, English word string, due to Noam Chomsky, is \u201ccolourless\ngreen ideas sleep furiously\u201d. We would not want our model to predict that this string is impossible. Even ungrammatical\nconstructs should be allowed by our model with a certain probability, since people frequently violate grammatical rules,\nespecially in spoken language.\n3. See http://googleresearch .blogspot .com/2006/08/all-our-n-gram-are-belong-to-you .htmlfor de-\ntails.", "624": "594 Chapter17. MarkovandhiddenMarkovmodels\nfrequencies fjk=Njk/Njand the unigram frequencies fk=Nk/N:\nAjk=( 1\u2212\u03bb)fjk+\u03bbfk (17.13)\nThe term \u03bbis usually set by cross validation. There is also a closely related technique called\nbackoff smoothing; the idea is that if fjkis too small, we \u201cback off\u201d to a more reliable estimate,\nnamelyfk.\nWe will now show that the deleted interpolation heuristic is an approximation to the predic-\ntions made by a simple hierarchical Bayesian model. Our presentation follows (McKay and Peto\n1995). First, let us use an independent Dirichlet prior on each row of the transition matrix:\nAj\u223cDir(\u03b10m1,...,\u03b1 0mK)=D i r (\u03b10m)=D i r ( \u03b1) (17.14)\nwhereAjis rowjof the transition matrix, mis the prior mean (satisfying/summationtext\nkmk=1) and\n\u03b10is the prior strength. We will use the same prior for each row: see Figure 17.3.\nThe posterior is given by Aj\u223cDir(\u03b1+Nj),w h e r eNj=(Nj1,...,N jK)is the vector\nthat records the number of times we have transitioned out of state jto each of the other states.\nFrom Equation 3.51, the posterior predictive density is\np(Xt+1=k|Xt=j,D)=Ajk=Njk+\u03b1mk\nNj+\u03b10=fjkNj+\u03b1mk\nNj+\u03b10=( 1\u2212\u03bbj)fjk+\u03bbjmk(17.15)\nwhereAjk=E[Ajk|D,\u03b1]and\n\u03bbj=\u03b1\nNj+\u03b10(17.16)\nThis is very similar to Equation 17.13 but not identical. The main difference is that the Bayesian\nmodel uses a context-dependent weight \u03bbjto combine mkwith the empirical frequency fjk,\nrather than a \ufb01xed weight \u03bb. This is like adaptive deleted interpolation. Furthermore, rather\nthan backing off to the empirical marginal frequencies fk, we back off to the model parameter\nmk.\nThe only remaining question is: what values should we use for \u03b1andm? Let\u2019s use empirical\nBayes. Since we assume each row of the transition matrix is a priori independent given \u03b1, the\nmarginal likelihood for our Markov model is found by applying Equation 5.24 to each row:\np(D|\u03b1)=/productdisplay\njB(Nj+\u03b1)\nB(\u03b1)(17.17)\nwhereNj=(Nj1,...,N jK)are the counts for leaving state jandB(\u03b1)is the generalized\nbeta function.\nWe can \ufb01t this using the methods discussed in (Minka 2000e). However, we can also use the\nfollowing approximation (McKay and Peto 1995, p12):\nmk\u221d| {j:Njk>0}| (17.18)\nThis says that the prior probability of word kis given by the number of different contexts in\nwhich it occurs, rather than the number of times it occurs. To justify the reasonableness of thisresult, Mackay and Peto (McKay and Peto 1995) give the following example.", "625": "17.2. Markovmodels 595\nFigure 17.3 A Markov chain in which we put a different Dirichlet prior on every row of the transition\nmatrixA, but the hyperparameters of the Dirichlet are shared.\nImagine, you see, that the language, you see, has, you see, a\nfrequently occuring couplet \u2019you see\u2019, you see, in which the second\nword of the couplet, see, follows the first word, you, with very high\nprobability, you see. Then the marginal statistics, you see, are going\nto become hugely dominated, you see, by the words you and see, with\nequal frequency, you see.\nIf we use the standard smoothing formula, Equation 17.13, then P(you |novel) and P(see |novel),\nfor some novel context word not seen before, would turn out to be the same, since the marginal\nfrequencies of \u2019you\u2019 and \u2019see\u2019 are the same (11 times each). However, this seems unreasonable.\n\u2019You\u2019 appears in many contexts, so P(you |novel) should be high, but \u2019see\u2019 only follows \u2019you\u2019, so\nP(see|novel) should be low. If we use the Bayesian formula Equation 17.15, we will get this effect\nfor free, since we back off to mknotfk, andmkwill be large for \u2019you\u2019 and small for \u2019see\u2019 by\nEquation 17.18.\nUnfortunately, although elegant, this Bayesian model does not beat the state-of-the-art lan-\nguage model, known as interpolated Kneser-Ney (Kneser and Ney 1995; Chen and Goodman\n1998). However, in (Teh 2006), it was shown how one can build a non-parametric Bayesian\nmodel which outperforms interpolated Kneser-Ney, by using variable-length contexts. In (Wood\net al. 2009), this method was extended to create the \u201csequence memoizer\u201d, which is currently\n(2010) the best-performing language model.4\n17.2.2.3 Handling out-of-vocabulary words\nWhile the above smoothing methods handle the case where the counts are small or even zero,\nnone of them deal with the case where the test set may contain a completely novel word. In\nparticular, they all assume that the words in the vocabulary (i.e., the state space of Xt)i s\ufb01 x e d\nand known (typically it is the set of unique words in the training data, or in some dictionary).\n4. Interestingly, these non-parametric methods are based on posterior inference using MCMC (Section 24.1) and/or\nparticle \ufb01ltering (Section 23.5), rather than optimization methods such as EB. Despite this, they are quite efficient.", "626": "596 Chapter17. MarkovandhiddenMarkovmodels\n1 2 31.0 0.5\n0.51.0\n(a)12340.9\n0.90.5 0.51.0 0.1 0.1\n(b)\nFigure 17.4 Some Markov chains. (a) A 3-state aperiodic chain. (b) A reducible 4-state chain.\nEven if all Ajk\u2019s are non-zero, none of these models will predict a novel word outside of this set,\nand hence will assign zero probability to a test sentence with an unfamiliar word. (Unfamiliar\nwords are bound to occur, because the set of words is an open class. For example, the set ofproper nouns (names of people and places) is unbounded.)\nA standard heuristic to solve this problem is to replace all novel words with the special symbol\nunk, which stands for \u201cunknown\u201d. A certain amount of probability mass is held aside for thisevent.\nAmoreprincipledsolutionwouldbetouseaDirichletprocess, whichcangenerateacountably\nin\ufb01nite state space, as the amount of data increases (see Section 25.2.2). If all novel words are\u201caccepted\u201d as genuine words, then the system has no predictive power, since any misspellingwill be considered a new word. So the novel word has to be seen frequently enough to warrantbeing added to the vocabulary. See e.g., (Friedman and Singer 1999; Griffiths and Tenenbaum2001) for details.\n17.2.3 Stationary distribution of a Markov chain *\nWe have been focussing on Markov models as a way of de\ufb01ning joint probability distributionsover sequences. However, we can also interpret them as stochastic dynamical systems, wherewe \u201chop\u201d from one state to another at each time step. In this case, we are often interested in thelong term distribution over states, which is known as the stationary distribution of the chain.\nIn this section, we discuss some of the relevant theory. Later we will consider two importantapplications: Google\u2019s PageRank algorithm for ranking web pages (Section 17.2.4), and the MCMCalgorithm for generating samples from hard-to-normalize probability distributions (Chapter 24).\n17.2.3.1 What is a stationary distribution?\nLetA\nij=p(Xt=j|Xt\u22121=i)be the one-step transition matrix, and let \u03c0t(j)=p(Xt=j)\nbe the probability of being in state jat timet. It is conventional in this context to assume that\n\u03c0is arowvector. If we have an initial distribution over states of \u03c00, then at time 1 we have\n\u03c01(j)=/summationdisplay\ni\u03c00(i)Aij (17.19)\nor, in matrix notation,\n\u03c01=\u03c00A (17.20)", "627": "17.2. Markovmodels 597\nWe can imagine iterating these equations. If we ever reach a stage where\n\u03c0=\u03c0A (17.21)\nthen we say we have reached the stationary distribution (also called the invariant distribution\norequilibrium distribution). Once we enter the stationary distribution, we will never leave.\nFor example, consider the chain in Figure 17.4(a). To \ufb01nd its stationary distribution, we write\n/parenleftbig\u03c01\u03c02\u03c03/parenrightbig\n=/parenleftbig\u03c01\u03c02\u03c03/parenrightbig\u239b\n\u239d1\u2212A12\u2212A13 A12 A13\nA21 1\u2212A21\u2212A23 A23\nA31 A32 1\u2212A31\u2212A32\u239e\u23a0(17.22)\nso\n\u03c0\n1=\u03c01(1\u2212A12\u2212A12)+\u03c02A21+\u03c03A31 (17.23)\nor\n\u03c01(A12+A13)=\u03c02A21+\u03c03A31 (17.24)\nIn general, we have\n\u03c0i/summationdisplay\nj/negationslash=iAij=/summationdisplay\nj/negationslash=i\u03c0jAji (17.25)\nIn other words, the probability of being in state itimes the net \ufb02ow out of state imust equal\nthe probability of being in each other state jtimes the net \ufb02ow from that state into i. These\nare called the global balance equations. We can then solve these equations, subject to the\nconstraint that/summationtext\nj\u03c0j=1.\n17.2.3.2 Computing the stationary distribution\nTo \ufb01nd the stationary distribution, we can just solve the eigenvector equation ATv=v, and\nthen to set \u03c0=vT,w h e r evis an eigenvector with eigenvalue 1. (We can be sure such an\neigenvector exists, since Ais a row-stochastic matrix, so A1=1; also recall that the eigenvalues\nofAandATare the same.) Of course, since eigenvectors are unique only up to constants of\nproportionality, we must normalize vat the end to ensure it sums to one.\nNote, however, that the eigenvectors are only guaranteed to be real-valued if the matrix is\npositive,Aij>0(and hence Aij<1, due to the sum-to-one constraint). A more general\napproach, which can handle chains where some transition probabilities are 0 or 1 (such as\nFigure 17.4(a)), is as follows (Resnick 1992, p138). We have Kconstraints from \u03c0(I\u2212A)=0K\u00d71\nand 1 constraint from \u03c01K\u00d71=0. Since we only have Kunknowns, this is overconstrained.\nSo let us replace any column (e.g., the last) of I\u2212Awith1, to get a new matrix, call it M.\nNext we de\ufb01ne r=[ 0,0,...,1], where the 1 in the last position corresponds to the column of\nall 1s inM. We then solve \u03c0M=r. For example, for a 3 state chain we have to solve this\nlinear system:\n/parenleftbig\n\u03c01\u03c02\u03c03/parenrightbig\u239b\n\u239d1\u2212A11\u2212A121\n\u2212A211\u2212A221\n\u2212A31\u2212A321\u239e\u23a0=/parenleftbig001/parenrightbig\n(17.26)", "628": "598 Chapter17. MarkovandhiddenMarkovmodels\nFor the chain in Figure 17.4(a) we \ufb01nd \u03c0=[ 0.4,0.4,0.2]. We can easily verify this is correct,\nsince\u03c0=\u03c0A. See mcStatDist for some Matlab code.\nUnfortunately, not all chains have a stationary distribution. as we explain below.\n17.2.3.3 When does a stationary distribution exist? *\nConsider the 4-state chain in Figure 17.4(b). If we start in state 4, we will stay there forever, since\n4i sa nabsorbing state. Thus \u03c0=( 0,0,0,1)is one possible stationary distribution. However,\nif we start in 1 or 2, we will oscillate between those two states for ever. So \u03c0=( 0.5,0.5,0,0)\nis another possible stationary distribution. If we start in state 3, we could end up in either ofthe above stationary distributions.\nWe see from this example that a necessary condition to have a unique stationary distribution\nis that the state transition diagram be a singly connected component, i.e., we can get from anystate to any other state. Such chains are called irreducible.\nNow consider the 2-state chain in Figure 17.1(a). This is irreducible provided \u03b1,\u03b2 >0.\nSuppose\u03b1=\u03b2=0.9. It is clear by symmetry that this chain will spend 50% of its time in\neach state. Thus \u03c0=( 0.5,0.5). But now suppose \u03b1=\u03b2=1. In this case, the chain will\noscillate between the two states, but the long-term distribution on states depends on where youstart from. If we start in state 1, then on every odd time step (1,3,5,...) we will be in state 1; butif we start in state 2, then on every odd time step we will be in state 2.\nThis example motivates the following de\ufb01nition. Let us say that a chain has a limiting\ndistribution if\u03c0\nj= limn\u2192\u221eAn\nijexists and is independent of i, for allj. If this holds, then\nthe long-run distribution over states will be independent of the starting state:\nP(Xt=j)=/summationdisplay\niP(X0=i)Aij(t)\u2192\u03c0jast\u2192\u221e (17.27)\nLet us now characterize when a limiting distribution exists. De\ufb01ne the periodof stateito be\nd(i)=gcd{t :Aii(t)>0} (17.28)\nwhere gcd stands for greatest common divisor, i.e., the largest integer that divides all the\nmembers of the set. For example, in Figure 17.4(a), we have d(1) =d(2) =gcd(2,3,4,6,...)=1\nandd(3) =gcd(3,5,6,...)=1 .W es a yas t a t e iisaperiodic ifd(i)=1. (A sufficient condition\nto ensure this is if state ihas a self-loop, but this is not a necessary condition.) We say a chain\nis aperiodic if all its states are aperiodic. One can show the following important result:\nTheorem 17.2.1. Every irreducible (singly connected), aperiodic \ufb01nite state Markov chain has a\nlimitingdistribution,whichisequalto \u03c0,itsuniquestationarydistribution.\nA special case of this result says that every regular \ufb01nite state chain has a unique stationary\ndistribution, where a regularchain is one whose transition matrix satis\ufb01es An\nij>0for some\nintegernand alli,j, i.e., it is possible to get from any state to any other state in nsteps.\nConsequently, after nsteps, the chain could be in any state, no matter where it started. One\ncan show that sufficient conditions to ensure regularity are that the chain be irreducible (singly\nconnected) and that every state have a self-transition.\nTo handle the case of Markov chains whose state-space is not \ufb01nite (e.g, the countable set of\nall integers, or all the uncountable set of all reals), we need to generalize some of the earlier", "629": "17.2. Markovmodels 599\nde\ufb01nitions. Since the details are rather technical, we just brie\ufb02y state the main results without\nproof. See e.g., (Grimmett and Stirzaker 1992) for details.\nFor a stationary distribution to exist, we require irreducibility (singly connected) and aperiod-\nicity, as before. But we also require that each state is recurrent. (A chain in which all states\nare recurrent is called a recurrent chain.) Recurrent means that you will return to that statewith probability 1. As a simple example of a non-recurrent state (i.e., a transient state), consider\nFigure 17.4(b): states 3 is transient because one immediately leaves it and either spins aroundstate 4 forever, or oscillates between states 1 and 2 forever. There is no way to return to state 3.\nIt is clear that any \ufb01nite-state irreducible chain is recurrent, since you can always get back to\nwhere you started from. But now consider an example with an in\ufb01nite state space. Suppose weperform a random walk on the integers, X={...,\u22122,\u22121,0,1,2,...}.L e tA\ni,i+1=pbe the\nprobability of moving right, and Ai,i\u22121=1\u2212pbe the probability of moving left. Suppose we\nstart atX1=0.I fp>0.5, we will shoot off to +\u221e; we are not guaranteed to return. Similarly,\nifp<0.5, we will shoot off to \u2212\u221e. So in both cases, the chain is not recurrent, even though\nit is irreducible.\nIt should be intuitively obvious that we require all states to be recurrent for a stationary\ndistribution to exist. However, this is not sufficient. To see this, consider the random walkon the integers again, and suppose p=0.5. In this case, we can return to the origin an\nin\ufb01nite number of times, so the chain is recurrent. However, it takes in\ufb01nitely long to doso. This prohibits it from having a stationary distribution. The intuitive reason is that thedistribution keeps spreading out over a larger and larger set of the integers, and never convergesto a stationary distribution. More formally, we de\ufb01ne a state to be non-null recurrent if the\nexpected time to return to this state is \ufb01nite. A chain in which all states are non-null is called anon-null chain.\nFor brevity, we we say that a state is ergodicif it is aperiodic, recurrent and non-null, and\nwe say a chain is ergodic if all its states are ergodic.\nWe can now state our main theorem:\nTheorem 17.2.2. Every irreducible (singly connected), ergodic Markov chain has a limiting distri-\nbution,whichisequalto \u03c0,itsuniquestationarydistribution.\nThis generalizes Theorem 17.2.1, since for irreducible \ufb01nite-state chains, all states are recurrent\nand non-null.\n17.2.3.4 Detailed balance\nEstablishing ergodicity can be difficult. We now give an alternative condition that is easier toverify.\nWe say that a Markov chain Aistime reversible if there exists a distribution \u03c0such that\n\u03c0\niAij=\u03c0jAji (17.29)\nThese are called the detailed balance equations. This says that the \ufb02ow from itojmust\nequal the \ufb02ow from jtoi, weighted by the appropriate source probabilities.\nWe have the following important result.\nTheorem 17.2.3. If a Markov chain with transition matrix Ais regular and satis\ufb01es detailed\nbalancewrtdistribution \u03c0,then\u03c0isastationarydistributionofthechain.", "630": "600 Chapter17. MarkovandhiddenMarkovmodels\nX1\nX2\nX3X4\nX5 X6\nFigure 17.5 A very small world wide web. Figure generated by pagerankDemo , written by Tim Davis.\nProof.To see this, note that\n/summationdisplay\ni\u03c0iAij=/summationdisplay\ni\u03c0jAji=\u03c0j/summationdisplay\niAji=\u03c0j (17.30)\nand hence \u03c0=A\u03c0.\nNote that this condition is sufficient but not necessary (see Figure 17.4(a) for an example of a\nchain with a stationary distribution which does not satisfy detailed balance).\nIn Section 24.1, we will discuss Markov chain Monte Carlo or MCMC methods. These take\nas input a desired distribution \u03c0and construct a transition matrix (or in general, a transition\nkernel)Awhich satis\ufb01es detailed balance wrt \u03c0. Thus by sampling states from such a chain,\nwe will eventually enter the stationary distribution, and will visit states with probabilities given\nby\u03c0.\n17.2.4 Application: Google\u2019s PageRank algorithm for web page ranking *\nThe results in Section 17.2.3 form the theoretical underpinnings to Google\u2019s PageRank algorithm,\nwhich is used for information retrieval on the world-wide web. We sketch the basic idea below;see (Byran and Leise 2006) for a more detailed explanation.\nWe will treat the web as a giant directed graph, where nodes represent web pages (documents)\nand edges represent hyper-links.\n5We then perform a process called web crawling. We start at\na few designated root nodes, such as dmoz. org, the home of the Open Directory Project, and\nthen follows the links, storing all the pages that we encounter, until we run out of time.\nNext, all of the words in each web page are entered into a data structure called an inverted\nindex. That is, for each word, we store a list of the documents where this word occurs. (Inpractice, we store a list of hash codes representing the URLs.) At test time, when a user enters\n5. In 2008, Google said it had indexed 1 trillion (1012)unique URLs. If we assume there are about 10 URLs per page\n(on average), this means there were about 100 billion unique web pages. Estimates for 2010 are about 121 billion unique\nweb pages. Source: thenextweb .com/shareables/2011/01/11/infographic-how-big-is-the-internet .", "631": "17.2. Markovmodels 601\na query, we can just look up all the documents containing each word, and intersect these\nlists (since queries are de\ufb01ned by a conjunction of search terms). We can get a re\ufb01ned searchby storing the location of each word in each document. We can then test if the words in adocument occur in the same order as in the query.\nLet us give an example, from http://en .wikipedia .org/wiki/Inverted_index .W e\nhave 3 documents, T\n0= \u201cit is what it is\u201d, T1= \u201cwhat is it\u201d and T2= \u201cit is a banana\u201d. Then\nwe can create the following inverted index, where each pair represents a document and wordlocation:\n\"a\": {(2, 2)}\n\"banana\": {(2, 3)}\"is\": {(0, 1), (0, 4), (1, 1), (2, 1)}\"it\": {(0, 0), (0, 3), (1, 2), (2, 0)}\"what\": {(0, 2), (1, 0)}\nFor example, we see that the word \u201cwhat\u201d occurs at location 2 (counting from 0) in document\n0, and location 0 in document 1. Suppose we search for \u201cwhat is it\u201d. If we ignore word order,we retrieve the following documents:\n{T\n0,T1}\u2229{T0,T1,T2}\u2229{T0,T1,T2}={T0,T1} (17.31)\nIf we require that the word order matches, only document T1would be returned. More generally,\nwe can allow out-of-order matches, but can give \u201cbonus points\u201d to documents whose word ordermatches the query\u2019s word order, or to other features, such as if the words occur in the title ofa document. We can then return the matching documents in decreasing order of their score/relevance. This is called document ranking.\nSo far, we have described the standard process of information retrieval. But the link structure\nof the web provides an additional source of information. The basic idea is that some web pagesare more authoritative than others, so these should be ranked higher (assuming they matchthe query). A web page is an authority if it is linked to by many other pages. But to protectagainst the effect of so-called link farms, which are dummy pages which just link to a given\nsite to boost its apparent relevance, we will weight each incoming link by the source\u2019s authority.Thus we get the following recursive de\ufb01nition for the authoritativeness of page j, also called its\nPageRank:\n\u03c0\nj=/summationdisplay\niAij\u03c0i (17.32)\nwhereAijis the probability of following a link from itoj. We recognize Equation 17.32 as the\nstationary distribution of a Markov chain.\nIn the simplest setting, we de\ufb01ne Ai.as a uniform distribution over all states that iis\nconnected to. However, to ensure the distribution is unique, we need to make the chain into aregular chain. This can be done by allowing each state ito jump to any other state (including\nitself) with some small probability. This effectively makes the transition matrix aperiodic andfully connected (although the adjacency matrix G\nijof the web itself is highly sparse).\nWe discuss efficient methods for computing the leading eigenvector of this giant matrix below.\nBut \ufb01rst, let us give an example of the PageRank algorithm. Consider the small web in Figure 17.5.", "632": "602 Chapter17. MarkovandhiddenMarkovmodels\n0 100 200 300 400 5000\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\nnz = 2636\n(a)0 100 200 300 400 50000.0020.0040.0060.0080.010.0120.0140.0160.0180.02\n(b)\nFigure 17.6 (a) Web graph of 500 sites rooted at www.harvard .edu. (b) Corresponding page rank vector.\nFigure generated by pagerankDemoPmtk , Based on code by Cleve Moler (Moler 2004).\nWe \ufb01nd that the stationary distribution is\n\u03c0=( 0.3209,0.1706,0.1065,0.1368,0.0643,0.2008) (17.33)\nSo a random surfer will visit site 1 about 32% of the time. We see that node 1 has a higher\nPageRank than nodes 4 or 6, even though they all have the same number of in-links. This isbecause being linked to from an in\ufb02uential nodehelps increase your PageRank score more thanbeing linked to by a less in\ufb02uential node.\nAs a slightly larger example, Figure 17.6(a) shows a web graph, derived from the root of\nharvard.edu. Figure 17.6(b) shows the corresponding PageRank vector.\n17.2.4.1 Efficiently computing the PageRank vector\nLetG\nij=1iff there is a link from jtoi. Now imagine performing a random walk on\nthis graph, where at every time step, with probability p=0.85you follow one of the outlinks\nuniformly at random, and with probability 1\u2212pyou jump to a random node, again chosen\nuniformly at random. If there are no outlinks, you just jump to a random page. (These randomjumps, including self-transitions, ensure the chain is irreducible (singly connected) and regular.Hence we can solve for its unique stationary distribution using eigenvector methods.) Thisde\ufb01nes the following transition matrix:\nM\nij=/braceleftbiggpGij/cj+\u03b4ifcj/negationslash=0\n1/n ifcj=0(17.34)\nwherenis the number of nodes, \u03b4=( 1\u2212p)/nis the probability of jumping from one page\nto another without following a link and cj=/summationtext\niGijrepresents the out-degree of page j. (If\nn=4\u00b7109andp=0.85, then \u03b4=3.75\u00b710\u221211.) HereMis a stochastic matrix in which\ncolumnssum to one. Note that M=ATin our earlier notation.\nWe can represent the transition matrix compactly as follows. De\ufb01ne the diagonal matrix D\nwith entries\ndjj=/braceleftbigg\n1/cjifcj/negationslash=0\n0ifcj=0(17.35)", "633": "17.3. HiddenMarkovmodels 603\nDe\ufb01ne the vector zwith components\nzj=/braceleftbigg\n\u03b4ifcj/negationslash=0\n1/nifcj=0(17.36)\nThen we can rewrite Equation 17.34 as follows:\nM=pGD+1zT(17.37)\nThe matrix Mis not sparse, but it is a rank one modi\ufb01cation of a sparse matrix. Most of the\nelements of Mare equal to the small constant \u03b4. Obviously these do not need to be stored\nexplicitly.\nOur goal is to solve v=Mv,w h e r ev=\u03c0T. One efficient method to \ufb01nd the leading\neigenvector of a large matrix is known as the power method. This simply consists of repeated\nmatrix-vector multiplication, followed by normalization:\nv\u221dMv=pGDv+1zTv (17.38)\nIt is possible to implement the power method without using any matrix multiplications, by\nsimply sampling from the transition matrix and counting how often you visit each state. This is\nessentially a Monte Carlo approximation to the sum implied by v=Mv. Applying this to the\ndata in Figure 17.6(a) yields the stationary distribution in Figure 17.6(b). This took 13 iterations toconverge, starting from a uniform distribution. (See also the function pagerankDemo ,b yT i m\nDavis, for an animation of the algorithm in action, applied to the small web example.) To handlechanging web structure, we can re-run this algorithm every day or every week, starting voff at\nthe old distribution (Langville and Meyer 2006).\nFor details on how to perform this Monte Carlo power method in a parallel distributed\ncomputing environment, see e.g., (Rajaraman and Ullman 2010).\n17.2.4.2 Web spam\nPageRank is not foolproof. For example, consider the strategy adopted by JC Penney, a depart-ment store in the USA. During the Christmas season of 2010, it planted many links to its homepage on 1000s of irrelevant web pages, thus increasing its ranking on Google\u2019s search engine(Segal 2011). Even though each of these source pages has low PageRank, there were so manyof them that their effect added up. Businesses call this search engine optimization; Google\ncalls itweb spam . When Google was noti\ufb01ed of this scam (by the NewYorkTimes ), it manually\ndownweighted JC Penney, since such behavior violates Google\u2019s code of conduct. The resultwas that JC Penney dropped from rank 1 to rank 65, essentially making it disappear from view.Automatically detecting such scams relies on various techniques which are beyond the scope ofthis chapter.\n17.3 Hidden Markov models\nAs we mentioned in Section 10.2.2, a hidden Markov model orHMMconsists of a discrete-time,\ndiscrete-state Markov chain, with hidden states zt\u2208{1,...,K}, plus an observation model", "634": "604 Chapter17. MarkovandhiddenMarkovmodels\n\u221220 \u221215 \u221210 \u22125 0 5 10 15 20\u221210\u2212505101520\n12 3 4567\n89\n101112\n13\n14\n151617\n18\n1920\n(a)2 4 6 8 10 12 14 16 18 2011.21.41.61.822.22.42.62.83\n(b)\nFigure 17.7 (a) Some 2d data sampled from a 3 state HMM. Each state emits from a 2d Gaussian. (b) The\nhidden state sequence. Based on Figure 13.8 of (Bishop 2006b). Figure generated by hmmLillypadDemo .\np(xt|zt). The corresponding joint distribution has the form\np(z1:T,x1:T)=p( z1:T)p(x1:T|z1:T)=/bracketleftBigg\np(z1)T/productdisplay\nt=2p(zt|zt\u22121)/bracketrightBigg/bracketleftBiggT/productdisplay\nt=1p(xt|zt)/bracketrightBigg\n(17.39)\nThe observations in an HMM can be discrete or continuous. If they are discrete, it is common\nfor the observation model to be an observation matrix:\np(xt=l|zt=k,\u03b8)=B(k,l) (17.40)\nIf the observations are continuous, it is common for the observation model to be a conditionalGaussian:\np(x\nt|zt=k,\u03b8)=N(xt|\u03bck,\u03a3k) (17.41)\nFigure 17.7 shows an example where we have 3 states, each of which emits a different Gaussian.The resulting model is similar to a Gaussian mixture model, except the cluster membershiphas Markovian dynamics. (Indeed, HMMs are sometimes called Markov switching models\n(Fruhwirth-Schnatter 2007).) We see that we tend to get multiple observations in the samelocation, and then a sudden jump to a new cluster.\n17.3.1 Applications of HMMs\nHMMs can be used as black-box density models on sequences. They have the advantageover Markov models in that they can represent long-range dependencies between observations,mediated via the latent variables. In particular, note that they do not assume the Markovproperty holds for the observations themselves. Such black-box models are useful for time-series prediction (Fraser 2008). They can also be used to de\ufb01ne class-conditional densitiesinside a generative classi\ufb01er.\nHowever, it is more common to imbue the hidden states with some desired meaning, and to\nthen try to estimate the hidden states from the observations, i.e., to compute p(z\nt|x1:t)if we are", "635": "17.3. HiddenMarkovmodels 605\nx x ...x\nA G ---C\nA- AG- C\nAG- AA-\n- - AAAC\nA G ---Cbat\nrat\ncat\ngnat\ngoat\n2. . . 13\n(a)\nM\n0M End Begin MMMIIIIDDD\n1 02 3 4\n(b)\nFigure 17.8 (a) Some DNA sequences. (b) State transition diagram for a pro\ufb01le HMM. Source: Figure 5.7\nof (Durbin et al. 1998). Used with kind permission of Richard Durbin.\nin an online scenario, or p(zt|x1:T)if we are in an offline scenario (see Section 17.4.1 for further\ndiscussion of the differences between these two approaches). Below we give some examples of\napplications which use HMMs in this way:\n\u2022Automatic speech recognition.H e r e xtrepresents features extracted from the speech\nsignal, and ztrepresents the word that is being spoken. The transition model p(zt|zt\u22121)\nrepresents the language model, and the observation model p(xt|zt)represents the acoustic\nmodel. See e.g., (Jelinek 1997; Jurafsky and Martin 2008) for details.\n\u2022Activity recognition.H e r e xtrepresents features extracted from a video frame, and ztis\nthe class of activity the person is engaged in (e.g., running, walking, sitting, etc.) See e.g.,\n(Szeliski 2010) for details.\n\u2022Part of speech tagging.H e r e xtrepresents a word, and ztrepresents its part of speech\n(noun, verb, adjective, etc.) See Section 19.6.2.1 for more information on POS tagging and", "636": "606 Chapter17. MarkovandhiddenMarkovmodels\nrelated tasks.\n\u2022Gene \ufb01nding.H e r e xtrepresents the DNA nucleotides (A,C,G,T), and ztrepresents whether\nwe are inside a gene-coding region or not. See e.g., (Schweikerta et al. 2009) for details.\n\u2022Protein sequence alignment.H e r e xtrepresents an amino acid, and ztrepresents whether\nthis matches the latent consensus sequence at this location. This model is called a pro\ufb01le\nHMMand is illustrated in Figure 17.8. The HMM has 3 states, called match, insert and delete.\nIfztis a match state, then xtis equal to the t\u2019th value of the consensus. If ztis an insert\nstate, then xtis generated from a uniform distribution that is unrelated to the consensus\nsequence. If ztis a delete state, then xt=\u2212. In this way, we can generate noisy copies of\nthe consensus sequence of different lengths. In Figure 17.8(a), the consensus is \u201cAGC\u201d, and\nwe see various versions of this below. A path through the state transition diagram, shownin Figure 17.8(b), speci\ufb01es how to align a sequence to the consensus, e.g., for the gnat, themost probable path is D,D,I,I,I,M . This means we delete the A and G parts of the\nconsensus sequence, we insert 3 A\u2019s, and then we match the \ufb01nal C. We can estimate themodel parameters by counting the number of such transitions, and the number of emissionsfrom each kind of state, as shown in Figure 17.8(c). See Section 17.5 for more information ontraining an HMM, and (Durbin et al. 1998) for details on pro\ufb01le HMMs.\nNote that for some of these tasks, conditional random \ufb01elds, which are essentially discrimi-\nnative versions of HMMs, may be more suitable; see Chapter 19 for details.\n17.4 Inference in HMMs\nWe now discuss how to infer the hidden state sequence of an HMM, assuming the parametersare known. Exactly the same algorithms apply to other chain-structured graphical models, suchas chain CRFs (see Section 19.6.1). In Chapter 20, we generalize these methods to arbitrarygraphs. And in Section 17.5.2, we show how we can use the output of inference in the contextof parameter estimation.\n17.4.1 Types of inference problems for temporal models\nThere are several different kinds of inferential tasks for an HMM (and SSM in general). Toillustrate the differences, we will consider an example called the occasionally dishonest casino,\nfrom (Durbin et al. 1998). In this model, x\nt\u2208{1,2,...,6}represents which dice face shows\nup, andztrepresents the identity of the dice that is being used. Most of the time the casino\nuses a fair dice, z=1, but occasionally it switches to a loaded dice, z=2, for a short period.\nIfz=1the observation distribution is a uniform multinoulli over the symbols {1,...,6}.I f\nz=2, the observation distribution is skewed towards face 6 (see Figure 17.9). If we sample from\nthis model, we may observe data such as the following:\nListing 17.1 Example output of casinoDemo\nRolls: 664153216162115234653214356634261655234232315142464156663246\nDie: LLLLLLLLLLLLLLFFFFFFLLLLLLLLLLLLLLFFFFFFFFFFFFFFFFFFLLLLLLLL\nHere \u201crolls\u201d refers to the observed symbol and \u201cdie\u201d refers to the hidden state (L is loaded and\nF is fair). Thus we see that the model generates a sequence of symbols, but the statistics of the", "637": "17.4. InferenceinHMMs 607\n\u0014\u001d\u0003\u0003\u0014\u0012\u0019\n\u0015\u001d\u0003\u0003\u0014\u0012\u0019\u0016\u001d\u0003\u0003\u0014\u0012\u0019\u0017\u001d\u0003\u0003\u0014\u0012\u0019\u0018\u001d\u0003\u0003\u0014\u0012\u0019\u0019\u001d\u0003\u0003\u0014\u0012\u0019\u0014\u001d\u0003\u0003\u0014\u0012\u0014\u0013\u0015\u001d\u0003\u0003\u0014\u0012\u0014\u0013\u0016\u001d\u0003\u0003\u0014\u0012\u0014\u0013\u0017\u001d\u0003\u0003\u0014\u0012\u0014\u0013\u0018\u001d\u0003\u0003\u0014\u0012\u0014\u0013\u0019\u001d\u0003\u0003\u0018\u0012\u0014\u0013\u0013\u0011\u001c\u0018 \u0013\u0011\u001c\u0013\n\u0013\u0011\u0014\n\u0013\u0011\u0013\u0018\nFigure 17.9 An HMM for the occasionally dishonest casino. The blue arrows visualize the state transition\ndiagramA. Based on (Durbin et al. 1998, p54).\n0 50 100 150 200 250 30000.51\nroll numberp(loaded)filtered\n(a)0 50 100 150 200 250 30000.51\nroll numberp(loaded)smoothed\n(b)0 50 100 150 200 250 30000.51\nroll numberMAP state (0=fair,1=loaded)Viterbi\n(c)\nFigure 17.10 Inference in the dishonest casino. Vertical gray bars denote the samples that we generated\nusing a loaded die. (a) Filtered estimate of probability of using a loaded dice. (b) Smoothed estimates. (c)\nMAP trajectory. Figure generated by casinoDemo .\ndistribution changes abruptly every now and then. In a typical application, we just see the rolls\nand want to infer which dice is being used. But there are different kinds of inference, which wesummarize below.\n\u2022Filtering means to compute the belief state p(z\nt|x1:t)online, or recursively, as the data\nstreams in. This is called \u201c\ufb01ltering\u201d because it reduces the noise more than simply estimating\nthe hidden state using just the current estimate, p(zt|xt). We will see below that we can\nperform \ufb01ltering by simply applying Bayes rule in a sequential fashion. See Figure 17.10(a) foran example.\n\u2022Smoothing means to compute p(z\nt|x1:T)offline, given all the evidence. See Figure 17.10(b)\nfor an example. By conditioning on past and future data, our uncertainty will be signi\ufb01cantlyreduced. To understand this intuitively, consider a detective trying to \ufb01gure out who com-mitted a crime. As he moves through the crime scene, his uncertainty is high until he \ufb01ndsthe key clue; then he has an \u201caha\u201d moment, his uncertainty is reduced, and all the previouslyconfusing observations are, in hindsight, easy to explain.", "638": "608 Chapter17. MarkovandhiddenMarkovmodels\nILOWHULQJ\nSUHGLFWLRQ\nIL[HG\u0010ODJ\u0003\nVPRRWKLQJ\nIL[HG\u0010ODJ\u0003\nVPRRWKLQJ\u0003\n\u000bRIIOLQH\fW\nW\nW\nWK\n7O\nFigure 17.11 The main kinds of inference for state-space models. The shaded region is the interval for\nwhich we have data. The arrow represents the time step at which we want to perform inference. tis the\ncurrent time, Tis the sequence length, /lscriptis the lag and his the prediction horizon. See text for details.\n\u2022Fixed lag smoothing is an interesting compromise between online and offline estimation; it\ninvolves computing p(zt\u2212/lscript|x1:t),w h e r e/lscript>0is called the lag. This gives better performance\nthan \ufb01ltering, but incurs a slight delay. By changing the size of the lag, one can trade off\naccuracy vs delay.\n\u2022Prediction Instead of predicting the past given the future, as in \ufb01xed lag smoothing, we\nmight want to predict the future given the past, i.e., to compute p(zt+h|x1:t),w h e r eh>0\nis called the prediction horizon. For example, suppose h=2; then we have\np(zt+2|x1:t)=/summationdisplay\nzt+1/summationdisplay\nztp(zt+2|zt+1)p(zt+1|zt)p(zt|x1:t) (17.42)\nIt is straightforward to perform this computation: we just power up the transition matrix andapply it to the current belief state. The quantity p(z\nt+h|x1:t)is a prediction about future\nhidden states; it can be converted into a prediction about future observations using\np(xt+h|x1:t)=/summationdisplay\nzt+hp(xt+h|zt+h)p(zt+h|x1:t) (17.43)\nThis is the posterior predictive density, and can be used for time-series forecasting (see(Fraser 2008) for details). See Figure 17.11 for a sketch of the relationship between \ufb01ltering,smoothing, and prediction.\n\u2022MAP estimation This means computing argmax\nz1:Tp(z1:T|x1:T), which is a most prob-\nable state sequence. In the context of HMMs, this is known as Viterbi decoding (see", "639": "17.4. InferenceinHMMs 609\nSection 17.4.4). Figure 17.10 illustrates the difference between \ufb01ltering, smoothing and MAP\ndecoding for the occasionally dishonest casino HMM. We see that the smoothed (offline)estimate is indeed smoother than the \ufb01ltered (online) estimate. If we threshold the estimatesat 0.5 and compare to the true sequence, we \ufb01nd that the \ufb01ltered method makes 71 errorsout of 300, and the smoothed method makes 49/300; the MAP path makes 60/300 errors. It isnot surprising that smoothing makes fewer errors than Viterbi, since the optimal way to min-imize bit-error rate is to threshold the posterior marginals (see Section 5.7.1.1). Nevertheless,for some applications, we may prefer the Viterbi decoding, as we discuss in Section 17.4.4.\n\u2022Posterior samples If there is more than one plausible interpretation of the data, it can be\nuseful to sample from the posterior, z\n1:T\u223cp(z1:T|x1:T). These sample paths contain much\nmore information than the sequence of marginals computed by smoothing.\n\u2022Probability of the evidence We can compute the probability of the evidence, p(x1:T),\nby summing up over all hidden paths, p(x1:T)=/summationtext\nz1:Tp(z1:T,x1:T). This can be used to\nclassify sequences (e.g., if the HMM is used as a class conditional density), for model-basedclustering, for anomaly detection, etc.\n17.4.2 The forwards algorithm\nWe now describe how to recursively compute the \ufb01ltered marginals, p(zt|x1:t)in an HMM.\nThe algorithm has two steps. First comes the prediction step, in which we compute the\none-step-ahead predictive density; this acts as the new prior for time t:\np(zt=j|x1:t\u22121)=/summationdisplay\nip(zt=j|zt\u22121=i)p(zt\u22121=i|x1:t\u22121) (17.44)\nNext comes the update step, in which we absorb the observed data from time tusing Bayes\nrule:\n\u03b1t(j)/definesp(zt=j|x1:t)=p(zt=j|xt,x1:t\u22121) (17.45)\n=1\nZtp(xt|zt=j,\u0018\u0018\u0018x1:t\u22121)p(zt=j|x1:t\u22121) (17.46)\nwhere the normalization constant is given by\nZt/definesp(xt|x1:t\u22121)=/summationdisplay\njp(zt=j|x1:t\u22121)p(xt|zt=j) (17.47)\nThis process is known as the predict-update cycle. The distribution p(zt|x1:t)is called the\n(\ufb01ltered)belief state at timet, and is a vector of Knumbers, often denoted by \u03b1t. In matrix-\nvector notation, we can write the update in the following simple form:\n\u03b1t\u221d\u03c8t\u2299(\u03a8T\u03b1t\u22121) (17.48)\nwhere\u03c8t(j)=p(xt|zt=j)is the local evidence at time t,\u03a8(i,j)=p(zt=j|zt\u22121=i)is\nthe transition matrix, and u\u2299vis theHadamard product, representing elementwise vector\nmultiplication. See Algorithm 6 for the pseudo-code, and hmmFilter for some Matlab code.", "640": "610 Chapter17. MarkovandhiddenMarkovmodels\nIn addition to computing the hidden states, we can use this algorithm to compute the log\nprobability of the evidence:\nlogp(x1:T|\u03b8)=T/summationdisplay\nt=1logp(xt|x1:t\u22121)=T/summationdisplay\nt=1logZt (17.49)\n(We need to work in the log domain to avoid numerical under\ufb02ow.)\nAlgorithm 17.1: Forwards algorithm\n1Input: Transition matrices \u03c8(i,j)=p(zt=j|zt\u22121=i), local evidence vectors\n\u03c8t(j)=p(xt|zt=j), initial state distribution \u03c0(j)=p(z1=j);\n2[\u03b11,Z1]=normalize( \u03c81\u2299\u03c0);\n3fort=2:Tdo\n4[\u03b1t,Zt]=normalize( \u03c8t\u2299(\u03a8T\u03b1t\u22121));\n5Return\u03b11:Tandlogp(y1:T)=/summationtext\ntlogZt;\n6Subroutine: [v,Z]=normalize( u):Z=/summationtext\njuj;vj=uj/Z;\n17.4.3 The forwards-backwards algorithm\nIn Section 17.4.2, we explained how to compute the \ufb01ltered marginals p(zt=j|x1:t)using\nonline inference. We now discuss how to compute the smoothed marginals, p(zt=j|x1:T),\nusing offline inference.\n17.4.3.1 Basic idea\nThe key decomposition relies on the fact that we can break the chain into two parts, the past\nand the future, by conditioning on zt:\np(zt=j|x1:T)\u221dp(zt=j,xt+1:T|x1:t)\u221dp(zt=j|x1:t)p(xt+1:T|zt=j,\u0018\u0018x1:t)(17.50)\nLet\u03b1t(j)/definesp(zt=j|x1:t)be the \ufb01ltered belief state as before. Also, de\ufb01ne\n\u03b2t(j)/definesp(xt+1:T|zt=j) (17.51)\nas the conditional likelihood of future evidence given that the hidden state at time tisj.\n(Note that this is not a probability distribution over states, since it does not need to satisfy/summationtext\nj\u03b2t(j)=1.) Finally, de\ufb01ne\n\u03b3t(j)/definesp(zt=j|x1:T) (17.52)\nas the desired smoothed posterior marginal. From Equation 17.50, we have\n\u03b3t(j)\u221d\u03b1t(j)\u03b2t(j) (17.53)", "641": "17.4. InferenceinHMMs 611\nWe have already described how to recursively compute the \u03b1\u2019s in a left-to-right fashion in\nSection 17.4.2. We now describe how to recursively compute the \u03b2\u2019s in a right-to-left fashion. If\nwe have already computed \u03b2t, we can compute \u03b2t\u22121as follows:\n\u03b2t\u22121(i)=p( xt:T|zt\u22121=i) (17.54)\n=/summationdisplay\njp(zt=j,xt,xt+1:T|zt\u22121=i) (17.55)\n=/summationdisplay\njp(xt+1:T|zt=j,\u0018\u0018\u0018\u0018zt\u22121=i,\u001a\u001axt)p(zt=j,xt|zt\u22121=i) (17.56)\n=/summationdisplay\njp(xt+1:T|zt=j)p(xt|zt=j,\u0018\u0018\u0018\u0018zt\u22121=i)p(zt=j|zt\u22121=i) (17.57)\n=/summationdisplay\nj\u03b2t(j)\u03c8t(j)\u03c8(i,j) (17.58)\nWe can write the resulting equation in matrix-vector form as\n\u03b2t\u22121=\u03a8(\u03c8t\u2299\u03b2t) (17.59)\nThe base case is\n\u03b2T(i)=p(xT+1:T|zT=i)=p(\u2205|zT=i)=1 (17.60)\nwhich is the probability of a non-event.\nHaving computed the forwards and backwards messages, we can combine them to compute\n\u03b3t(j)\u221d\u03b1t(j)\u03b2t(j). The overall algorithm is known as the forwards-backwards algorithm.\nThe pseudo code is very similar to the forwards case; see hmmFwdBack for an implementation.\nWe can think of this algorithm as passing \u201cmessages\u201d from left to right, and then from right\nto left, and then combining them at each node. We will generalize this intuition in Section 20.2,\nwhen we discuss belief propagation.\n17.4.3.2 Two-slice smoothed marginals\nWhen we estimate the parameters of the transition matrix using EM (see Section 17.5), we willneed to compute the expected number of transitions from state ito statej:\nN\nij=T\u22121/summationdisplay\nt=1E[I(zt=i,zt+1=j)|x1:T]=T\u22121/summationdisplay\nt=1p(zt=i,zt+1=j|x1:T) (17.61)\nThe term p(zt=i,zt+1=j|x1:T)is called a (smoothed) two-slice marginal, and can be\ncomputed as follows\n\u03bet,t+1(i,j)/definesp(zt=i,zt+1=j|x1:T) (17.62)\n\u221dp(zt|x1:t)p(zt+1|zt,xt+1:T) (17.63)\n\u221dp(zt|x1:t)p(xt+1:T|zt,zt+1)p(zt+1|zt) (17.64)\n\u221dp(zt|x1:t)p(xt+1|zt+1)p(xt+2:T|zt+1)p(zt+1|zt) (17.65)\n=\u03b1t(i)\u03c6t+1(j)\u03b2t+1(j)\u03c8(i,j) (17.66)", "642": "612 Chapter17. MarkovandhiddenMarkovmodels\nIn matrix-vector form, we have\n\u03bet,t+1\u221d\u03a8\u2299(\u03b1t(\u03c6t+1\u2299\u03b2t+1)T) (17.67)\nFor another interpretation of these equations, see Section 20.2.4.3.\n17.4.3.3 Time and space complexity\nIt is clear that a straightforward implementation of FB takes O(K2T)time, since we must\nperform a K\u00d7Kmatrix multiplication at each step. For some applications, such as speech\nrecognition, Kis very large, so the O(K2)term becomes prohibitive. Fortunately, if the\ntransition matrix is sparse, we can reduce this substantially. For example, in a left-to-right\ntransition matrix, the algorithm takes O(TK)time.\nIn some cases, we can exploit special properties of the state space, even if the transition\nmatrix is not sparse. In particular, suppose the states represent a discretization of an underlyingcontinuous state-space, and the transition matrix has the form \u03c8(i,j)\u221dexp(\u2212\u03c3\n2|zi\u2212zj|),\nwhereziis the continuous vector represented by state i. Then one can implement the forwards-\nbackwards algorithm in O(TKlogK)time. This is very useful for models with large state\nspaces. See Section 22.2.6.1 for details.\nIn some cases, the bottleneck is memory, not time. The expected sufficient statistics needed\nby EM are/summationtext\nt\u03bet\u22121,t(i,j); this takes constant space (independent of T); however, to compute\nthem, we need O(KT)working space, since we must store \u03b1tfort=1,...,Tuntil we do the\nbackwards pass. It is possible to devise a simple divide-and-conquer algorithm that reduces thespace complexity from O(KT)toO(KlogT)at the cost of increasing the running time from\nO(K\n2T)toO(K2TlogT): see (Binder et al. 1997; Zweig and Padmanabhan 2000) for details.\n17.4.4 The Viterbi algorithm\nTheViterbialgorithm (Viterbi 1967) can be used to compute the most probable sequence of\nstates in a chain-structured graphical model, i.e., it can compute\nz\u2217=a r gm a x\nz1:Tp(z1:T|x1:T) (17.68)\nThis is equivalent to computing a shortest path through the trellis diagram in Figure 17.12,\nwhere the nodes are possible states at each time step, and the node and edge weights are logprobabilities. That is, the weight of a path z\n1,z2,...,z Tis given by\nlog\u03c01(z1)+log\u03c61(z1)+T/summationdisplay\nt=2[log\u03c8(zt\u22121,zt)+log\u03c6t(zt)] (17.69)\n17.4.4.1 MAP vs MPE\nBefore discussing how the algorithm works, let us make one important remark: the (jointly)most\nprobablesequenceofstatesisnotnecessarilythesameasthesequenceof(marginally)mostprobablestates. The former is given by Equation 17.68, and is what Viterbi computes, whereas the latter isgiven by the maximizer of the posterior marginals or MPM:\n\u02c6z= (argmax\nz1p(z1|x1:T),...,argmax\nzTp(zT|x1:T)) (17.70)", "643": "17.4. InferenceinHMMs 613\n\u0011\n\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\n1\n\u0016\n\u0015\n\u0014\n\u0014 \u0015 \u0016 7\n67$7(\n2%6(59$7,21\u0011\n\u0011\u0011\n\u0011\u0003\u0011\u0003\u0011\u0011\u0003\u0011\u0003\u0011\u0011\u0003\u0011\u0003\u0011\u0011\u0003\u0011\u0003\u0011\u0011\u0003\u0011\u0003\u0011\nFigure 17.12 The trellis of states vs time for a Markov chain. Based on (Rabiner 1989).\nAs a simple example of the difference, consider a chain with two time steps, de\ufb01ning the\nfollowing joint:\nX1=0X1=1\nX2=00.04 0.3 0.34\nX2=10.36 0.3 0.66\n0.4 0.6\nThe joint MAP estimate is (0,1), whereas the sequence of marginal MPMs is (1,1).\nThe advantage of the joint MAP estimate is that is is always globally consistent. For example,\nsuppose we are performing speech recognition and someones says \u201crecognize speech\u201d. This\ncould be mis-heard as \u201cwreck a nice beach\u201d. Locally it may appear that \u201cbeach\u201d is the mostprobable interpretation of that particular window of sound, but when we add the requirementthat the data be explained by a single linguistically plausible path, this interpretation becomesless likely.\nOn the other hand, the MPM estimates can be more robust (Marroquin et al. 1987). To see\nwhy, note that in Viterbi, when we estimate z\nt, we \u201cmax out\u201d the other variables:\nz\u2217\nt=a r gm a x\nztmax\nz1:t\u22121,zt+1:Tp(z1:t\u22121,zt,zt+1:T|x1:T) (17.71)\nwhereas we when we use forwards-backwards, we sum out the other variables:\np(zt|x1:T)=/summationdisplay\nz1:t\u22121,zt+1:Tp(z1:t\u22121,zt,zt+1:T|x1:T) (17.72)\nThis makes the MPM in Equation 17.70 more robust, since we estimate each node averaging overits neighbors, rather than conditioning on a speci\ufb01c value of its neighbors.\n6\n6. In general, we may want to mix max and sum. For example, consider a joint distribution where we observe", "644": "614 Chapter17. MarkovandhiddenMarkovmodels\n17.4.4.2 Details of the algorithm\nIt is tempting to think that we can implement Viterbi by just replacing the sum-operator in\nforwards-backwards with a max-operator. The former is called the sum-product, and the\nlatter the max-product algorithm. If there is a unique mode, running max-product and then\ncomputing using Equation 17.70 will give the same result as using Equation 17.68 (Weiss andFreeman 2001b), but in general, it can lead to incorrect results if there are multiple equallyprobably joint assignments. The reasons is that each node breaks ties independently and hencemay do so in a manner that is inconsistent with its neighbors. The Viterbi algorithm is thereforenot quite as simple as replacing sum with max. In particular, the forwards pass does use max-product, but the backwards pass uses a traceback procedure to recover the most probable path\nthrough the trellis of states. Essentially, once z\ntpicks its most probable state, the previous nodes\ncondition on this event, and therefore they will break ties consistently.\nIn more detail, de\ufb01ne\n\u03b4t(j)/definesmax\nz1,...,zt\u22121p(z1:t\u22121,zt=j|x1:t) (17.73)\nThis is the probability of ending up in state jat timet, given that we take the most probable\npath. The key insight is that the most probable path to state jat timetmust consist of the\nmost probable path to some other state iat timet\u22121, followed by a transition from itoj.\nHence\n\u03b4t(j)=m a x\ni\u03b4t\u22121(i)\u03c8(i,j)\u03c6t(j) (17.74)\nWe also keep track of the most likely previous state, for each possible state that we end up in:\nat(j) = argmax\ni\u03b4t\u22121(i)\u03c8(i,j)\u03c6t(j) (17.75)\nThat is,at(j)tells us the most likely previous state on the most probable path to zt=j.W e\ninitialize by setting\n\u03b41(j)=\u03c0j\u03c61(j) (17.76)\nand we terminate by computing the most probable \ufb01nal state z\u2217\nT:\nz\u2217\nT=a r gm a x\ni\u03b4T(i) (17.77)\nWe can then compute the most probable sequence of states using traceback:\nz\u2217\nt=at+1(z\u2217\nt+1) (17.78)\nAs usual, we have to worry about numerical under\ufb02ow. We are free to normalize the \u03b4tterms\nat each step; this will not affect the maximum. However, unlike the forwards-backwards case,\nvand we want to query q;l e tnbe the remaining nuisance variables. We de\ufb01ne the MAP estimate as x\u2217\nq=\nargmax xq/summationtext\nxnp(xq,xn|xv), where we max over xqand sum over xn. By contrast, we de\ufb01ne the MPEor\nmost probable explanation as (x\u2217q,x\u2217n) = argmax xq,xnp(xq,xn|xv), where we max over both xqandxn. This\nterminology is due to (Pearl 1988), although it is not widely used outside the Bayes net literatire. Obviously MAP=MPE if\nn=\u2205. However, if n/negationslash=\u2205, then summing out the nuisance variables can give different results than maxing them out.\nSumming out nuisance variables is more sensible, but computationally harder, because of the need to combine max and\nsum operations (Lerner and Parr 2001).", "645": "17.4. InferenceinHMMs 615\n&\u0014 \u0013\u0011\u0018 \u0013 \u0013\n&\u0015 \u0013\u0011\u0016 \u0013 \u0013\n&\u0016 \u0013\u0011\u0015 \u0013\u0011\u0015 \u0013\n&\u0017 \u0013 \u0013\u0011\u001a \u0013\u0011\u0014\n&\u0018 \u0013 \u0013\u0011\u0014 \u0013\n&\u0019 \u0013 \u0013 \u0013\u0011\u0018\n&\u001a \u0013 \u0013 \u0013\u0011\u0017\n(a)6\u0014\n\u0013\u0011\u00186\u0014\n\u0013\u0011\u0013\u0017\u00186\u0014\n\u0013\u0011\u00136\u0014\n\u0013\u0011\u0013\n6\u0015\n\u0013\u0011\u00136\u0015\n\u0013\u0011\u0013\u001a6\u0015\n\u0013\u0011\u0013\u0017\u0017\u00146\u0015\n\u0013\u0011\u0013\n6\u0016\n\u0013\u0011\u00136\u0016\n\u0013\u0011\u00136\u0016\n\u0013\u0011\u0013\u0013\u0013\u001a6\u0016\n\u0013\u0011\u0013\u0013\u0015\u0015\u0013\u0011\u0016\u000f\u0003\u0013\u0011\u0016\u0013 \u0011\u0016\u000f\u0003\u0013\u0011\u0013\n\u0013\u0011\u001a\u000f\u0003\u0013\u0011\u0015\u0013 \u0011\u001a\u000f\u0003\u0013\u0011\u001a\n\u0013\u0011\u001c\u000f\u0003\u0013\u0011\u001a\u0013 \u0011\u001c\u000f\u0003\u0013\u0011\u0013\n\u0013\u0011\u0014\u000f\u0003\u0013\u0011\u0014\u0013 \u0011\u0014\u000f\u0003\u0013\u0011\u0018\n\u0013\u0011\u0017\u000f\u0003\u0013\u0011\u0018\n(b)\nFigure 17.13 Illustration of Viterbi decoding in a simple HMM for speech recognition. (a) A 3-state HMM\nfor a single phone. We are visualizing the state transition diagram. We assume the observations have been\nvector quantized into 7 possible symbols, C1,...,C 7. Each state z1,z2,z3has a different distribution over\nthese symbols. Based on Figure 15.20 of (Russell and Norvig 2002). (b) Illustration of the Viterbi algorithm\napplied to this model, with data sequence C1,C3,C4,C6. The columns represent time, and the rows\nrepresent states. An arrow from state iatt\u22121to statejattis annotated with two numbers: the \ufb01rst\nis the probability of the i\u2192jtransition, and the second is the probability of generating observation xt\nfrom state j. The bold lines/ circles represent the most probable sequence of states. Based on Figure 24.27\nof (Russell and Norvig 1995).\nwe can also easily work in the log domain. The key difference is that logmax = maxlog ,\nwhereaslog/summationtext/negationslash=/summationtextlog. Hence we can use\nlog\u03b4t(j)/definesmax\nz1:t\u22121logp(z1:t\u22121,zt=j|x1:t) (17.79)\n=m a x\nilog\u03b4t\u22121(i)+log\u03c8(i,j)+log\u03c6t(j) (17.80)\nIn the case of Gaussian observation models, this can result in a signi\ufb01cant (constant factor)\nspeedup, since computing logp(xt|zt)can be much faster than computing p(xt|zt)for a high-\ndimensional Gaussian. This is one reason why the Viterbi algorithm is widely used in the E step\nof EM (Section 17.5.2) when training large speech recognition systems based on HMMs.\n17.4.4.3 Example\nFigure 17.13 gives a worked example of the Viterbi algorithm, based on (Russell et al. 1995).\nSuppose we observe the discrete sequence of observations x1:4=(C1,C3,C4,C6), representing\ncodebook entries in a vector-quantized version of a speech signal. The model starts in state\nz1. The probability of generating C1inz1is 0.5, so we have \u03b41(1) = 0.5, and\u03b41(i)=0for\nall other states. Next we can self-transition to z1with probability 0.3, or transition to z2with\nproabability 0.7. If we end up in z1, the probability of generating C3is 0.3; if we end up in z2,", "646": "616 Chapter17. MarkovandhiddenMarkovmodels\nthe probability of generating C3is 0.2. Hence we have\n\u03b42(1) = \u03b41(1)\u03c8(1,1)\u03c62(1) = 0.5\u00b70.3\u00b70.3=0.045 (17.81)\n\u03b42(2) = \u03b41(1)\u03c8(1,2)\u03c62(2) = 0.5\u00b70.7\u00b70.2=0.07 (17.82)\nThus state 2 is more probable at t=2; see the second column of Figure 17.13(b). In time step\n3, we see that there are two paths into z2,f r o mz1and from z2. The bold arrow indicates that\nthe latter is more probable. Hence this is the only one we have to remember. The algorithm\ncontinues in this way until we have reached the end of the sequence. One we have reached theend, we can follow the black arrows back to recover the MAP path (which is 1,2,2,3).\n17.4.4.4 Time and space complexity\nThe time complexity of Viterbi is clearly O(K\n2T)in general, and the space complexity\nisO(KT), both the same as forwards-backwards. If the transition matrix has the form\n\u03c8(i,j)\u221dexp(\u2212\u03c32||zi\u2212zj||2),w h e r eziis the continuous vector represented by state i,w e\ncan implement Viterbi in O(TK)time, instead of O(TKlogK)needed by forwards-backwards.\nSee Section 22.2.6.1 for details.\n17.4.4.5 N-best list\nThe Viterbi algorithm returns one of the most probable paths. It can be extended to return thetopNpaths (Schwarz and Chow 1990; Nilsson and Goldberger 2001). This is called the N-best\nlist. Once can then use a discriminative method to rerank the paths based on global featuresderived from the fully observed state sequence (as well as the visible features). This techniqueis widely used in speech recognition. For example, consider the sentence \u201crecognize speech\u201d. Itis possible that the most probable interpretation by the system of this acoustic signal is \u201cwrecka nice speech\u201d, or maybe \u201cwreck a nice beach\u201d. Maybe the correct interpretation is much lowerdown on the list. However, by using a re-ranking system, we may be able to improve the scoreof the correct interpretation based on a more global context.\nOne problem with the N-best list is that often the top Npaths are very similar to each other,\nrather than representing qualitatively different interpretations of the data. Instead we might wantto generate a more diverse set of paths to more accurately represent posterior uncertainty. Oneway to do this is to sample paths from the posterior, as we discuss below. For some other waysto generate diverse MAP estimates, see e.g., (Yadollahpour et al. 2011; Kulesza and Taskar 2011).\n17.4.5 Forwards \ufb01ltering, backwards sampling\nIt is often useful to sample paths from the posterior:\nzs\n1:T\u223cp(z1:T|x1:T) (17.83)\nWe can do this is as follow: run forwards backwards, to compute the two-slice smoothed posteri-\nors,p(zt\u22121,t|x1:T); next compute the conditionals p(zt|zt\u22121,x1:T)by normalizing; sample from\nthe initial pair of states, z\u2217\n1,2\u223cp(z1,2|x1:T); \ufb01nally, recursively sample z\u2217\nt\u223cp(zt|z\u2217\nt\u22121,x1:T).\nNote that the above solution requires a forwards-backwards pass, and then an additional\nforwards sampling pass. An alternative is to do the forwards pass, and then perform sampling", "647": "17.5. LearningforHMMs 617\nin the backwards pass. The key insight into how to do this is that we can write the joint from\nright to left using\np(z1:T|x1:T)=p(zT|x1:T)1/productdisplay\nt=T\u22121p(zt|zt+1,x1:T) (17.84)\nWe can then sample ztgiven future sampled states using\nzs\nt\u223cp(zt|zt+1:T,x1:T)=p(zt|zt+1,\u0018\u0018\u0018zt+2:T,x1:t,\u0018\u0018\u0018xt+1:T)=p(zt|zs\nt+1,x1:t)(17.85)\nThe sampling distribution is given by\np(zt=i|zt+1=j,x1:t)=p( zt|zt+1,x1:t,\u0018\u0018\u0018xt+1) (17.86)\n=p(zt+1,zt|x1:t+1)\np(zt+1|x1:t+1)(17.87)\n\u221dp(xt+1|zt+1,\u001a\u001azt,\u0018\u0018x1:t)p(zt+1,zt|x1:t)\np(zt+1|x1:t+1)(17.88)\n=p(xt+1|zt+1)p(zt+1|zt,\u0018\u0018x1:t)p(zt|x1:t)\np(zt+1|x1:t+1)(17.89)\n=\u03c6t+1(j)\u03c8(i,j)\u03b1t(i)\n\u03b1t+1(j)(17.90)\nThe base case is\nzs\nT\u223cp(zT=i|x1:T)=\u03b1T(i) (17.91)\nThis algorithm forms the basis of blocked-Gibbs sampling methods for parameter inference,\nas we will see below.\n17.5 Learning for HMMs\nWe now discuss how to estimate the parameters \u03b8=(\u03c0,A,B),w h e r e\u03c0(i)=p(z1=i)is\nthe initial state distribution, A(i,j)=p(zt=j|zt\u22121=i)is the transition matrix, and Bare\nthe parameters of the class-conditional densities p(xt|zt=j). We \ufb01rst consider the case where\nz1:Tis observed in the training set, and then the harder case where z1:Tis hidden.\n17.5.1 Training with fully observed data\nIf we observe the hidden state sequences, we can compute the MLEs for Aand\u03c0exactly as in\nSection 17.2.2.1. If we use a conjugate prior, we can also easily compute the posterior.\nThe details on how to estimate Bdepend on the form of the observation model. The\nsituation is identical to \ufb01tting a generative classi\ufb01er. For example, if each state has a multinoullidistribution associated with it, with parameters B\njl=p(Xt=l|zt=j),w h e r el\u2208{1,...,L}\nrepresents the observed symbol, the MLE is given by\n\u02c6Bjl=NX\njl\nNj,NX\njl/definesN/summationdisplay\ni=1Ti/summationdisplay\nt=1I(zi,t=j,xi,t=l) (17.92)", "648": "618 Chapter17. MarkovandhiddenMarkovmodels\nThis result is quite intuitive: we simply add up the number of times we are in state jand we\nsee a symbol l, and divide by the number of times we are in state j.\nSimilarly, if each state has a Gaussian distribution associated with it, we have (from Sec-\ntion 4.2.4) the following MLEs:\n\u02c6\u03bck=xk\nNk,\u02c6\u03a3k=(xx)T\nk\u2212Nk\u02c6\u03bck\u02c6\u03bcT\nk\nNk(17.93)\nwhere the sufficient statistics are given by\nxk/definesN/summationdisplay\ni=1Ti/summationdisplay\nt=1I(zi,t=k)xi,t (17.94)\n(xx)T\nk/definesN/summationdisplay\ni=1Ti/summationdisplay\nt=1I(zi,t=k)xi,txTi,t(17.95)\nAnalogous results can be derived for other kinds of distributions. One can also easily extend all\nof these results to compute MAP estimates, or even full posteriors over the parameters.\n17.5.2 EM for HMMs (the Baum-Welch algorithm)\nIf theztvariables are not observed, we are in a situation analogous to \ufb01tting a mixture model.\nThe most common approach is to use the EM algorithm to \ufb01nd the MLE or MAP parameters,although of course one could use other gradient-based methods (see e.g., (Baldi and Chauvin1994)). In this Section, we derive the EM algorithm. When applied to HMMs, this is also knownas theBaum-Welch algorithm (Baum et al. 1970).\n17.5.2.1 E step\nIt is straightforward to show that the expected complete data log likelihood is given by\nQ(\u03b8,\u03b8\nold)=K/summationdisplay\nk=1E/bracketleftbig\nN1\nk/bracketrightbig\nlog\u03c0k+K/summationdisplay\nj=1K/summationdisplay\nk=1E[Njk]logAjk (17.96)\n+N/summationdisplay\ni=1Ti/summationdisplay\nt=1K/summationdisplay\nk=1p(zt=k|xi,\u03b8old)logp(xi,t|\u03c6k) (17.97)\nwhere the expected counts are given by\nE/bracketleftbig\nN1\nk/bracketrightbig\n=N/summationdisplay\ni=1p(zi1=k|xi,\u03b8old) (17.98)\nE[Njk]=N/summationdisplay\ni=1Ti/summationdisplay\nt=2p(zi,t\u22121=j,zi,t=k|xi,\u03b8old) (17.99)\nE[Nj]=N/summationdisplay\ni=1Ti/summationdisplay\nt=1p(zi,t=j|xi,\u03b8old) (17.100)", "649": "17.5. LearningforHMMs 619\nThese expected sufficient statistics can be computed by running the forwards-backwards algo-\nrithm on each sequence. In particular, this algorithm computes the following smoothed nodeand edge marginals:\n\u03b3\ni,t(j)/definesp(zt=j|xi,1:Ti,\u03b8) (17.101)\n\u03bei,t(j,k)/definesp(zt\u22121=j,zt=k|xi,1:Ti,\u03b8) (17.102)\n17.5.2.2 M step\nBased on Section 11.3, we have that the M step for Aand\u03c0is to just normalize the expected\ncounts:\n\u02c6Ajk=E[Njk]/summationtext\nk/primeE[Njk/prime],\u02c6\u03c0k=E/bracketleftbig\nN1\nk/bracketrightbig\nN(17.103)\nThis result is quite intuitive: we simply add up the expected number of transitions from jtok,\nand divide by the expected number of times we transition from jto anything else.\nFor a multinoulli observation model, the expected sufficient statistics are\nE[Mjl]=N/summationdisplay\ni=1Ti/summationdisplay\nt=1\u03b3i,t(j)I(xi,t=l)=N/summationdisplay\ni=1/summationdisplay\nt:xi,t=l\u03b3i,t(j) (17.104)\nThe M step has the form\n\u02c6Bjl=E[Mjl]\nE[Nj](17.105)\nThis result is quite intuitive: we simply add up the expected number of times we are in state j\nand we see a symbol l, and divide by the expected number of times we are in state j.\nFor a Gaussian observation model, the expected sufficient statistics are given by\nE[xk]=N/summationdisplay\ni=1Ti/summationdisplay\nt=1\u03b3i,t(k)xi,t (17.106)\nE/bracketleftbig\n(xx)T\nk/bracketrightbig\n=N/summationdisplay\ni=1Ti/summationdisplay\nt=1\u03b3i,t(k)xi,txTi,t(17.107)\nThe M step becomes\n\u02c6\u03bck=E[xk]\nE[Nk],\u02c6\u03a3k=E/bracketleftbig\n(xx)T\nk/bracketrightbig\n\u2212E[Nk]\u02c6\u03bck\u02c6\u03bcT\nk\nE[Nk](17.108)\nThis can (and should) be regularized in the same way we regularize GMMs.\n17.5.2.3 Initialization\nAs usual with EM, we must take care to ensure that we initialize the parameters carefully, to\nminimize the chance of getting stuck in poor local optima. There are several ways to do this,such as", "650": "620 Chapter17. MarkovandhiddenMarkovmodels\n\u2022 Use some fully labeled data to initialize the parameters.\n\u2022 Initially ignore the Markov dependencies, and estimate the observation parameters using the\nstandard mixture model estimation methods, such as K-means or EM.\n\u2022 Randomly initialize the parameters, use multiple restarts, and pick the best solution.\nTechniques such as deterministic annealing (Ueda and Nakano 1998; Rao and Rose 2001)\ncan help mitigate the effect of local minima. Also, just as K-means is often used to initialize\nEM for GMMs, so it is common to initialize EM for HMMs using Viterbi training, which\nmeans approximating the posterior over paths with the single most probable path. (This is notnecessarily a good idea, since initially the parameters are often poorly estimated, so the Viterbipath will be fairly arbitrary. A safer option is to start training using forwards-backwards, and toswitch to Viterbi near convergence.)\n17.5.3 Bayesian methods for \u201c\ufb01tting\u201d HMMs *\nEM returns a MAP estimate of the parameters. In this section, we brie\ufb02y discuss some methodsfor Bayesian parameter estimation in HMMs. (These methods rely on material that we will coverlater in the book.)\nOne approach is to use variational Bayes EM (VBEM), which we discuss in general terms in\nSection 21.6. The details for the HMM case can be found in (MacKay 1997; Beal 2003), butthe basic idea is this: The E step uses forwards-backwards, but where (roughly speaking) weplug in the posterior mean parameters instead of the MAP estimates. The M step updates theparameters of the conjugate posteriors, instead of updating the parameters themselves.\nAn alternative to VBEM is to use MCMC. A particularly appealing algorithm is block Gibbs\nsampling, which we discuss in general terms in Section 24.2.8. The details for the HMM casecan be found in (Fruhwirth-Schnatter 2007), but the basic idea is this: we sample z\n1:Tgiven\nthe data and parameters using forwards-\ufb01ltering, backwards-sampling, and we then sample theparameters from their posteriors, conditional on the sampled latent paths. This is simple toimplement, but one does need to take care of unidenti\ufb01ability (label switching), just as withmixture models (see Section 11.3.1).\n17.5.4 Discriminative training\nSometimes HMMs are used as the class conditional density inside a generative classi\ufb01er. In thiscase,p(x|y=c,\u03b8)can be computed using the forwards algorithm. We can easily maximize the\njoint likelihood/producttext\nN\ni=1p(xi,yi|\u03b8)by using EM (or some other method) to \ufb01t the HMM for each\nclass-conditional density separately.\nHowever, we might like to \ufb01nd the parameters that maximize the conditional likelihood\nN/productdisplay\ni=1p(yi|xi,\u03b8)=/productdisplay\nip(yi|\u03b8)p(xi|yi,\u03b8)/summationtext\ncp(yi=c|\u03b8)p(xi|c,\u03b8)(17.109)\nThis is more expensive than maximizing the joint likelihood, since the denominator couples all C\nclass-conditional HMMs together. Furthermore, EM can no longer be used, and one must resort", "651": "17.6. GeneralizationsofHMMs 621\nto generic gradient based methods. Nevertheless, discriminative training can result in improved\naccuracies. The standard practice in speech recognition is to initially train the generative modelsseparately using EM, and then to \ufb01ne tune them discriminatively (Jelinek 1997).\n17.5.5 Model selection\nIn HMMs, the two main model selection issues are: how many states, and what topology to usefor the state transition diagram. We discuss both of these issues below.\n17.5.5.1 Choosing the number of hidden states\nChoosing the number of hidden states Kin an HMM is analogous to the problem of choosing\nthe number of mixture components. Here are some possible solutions:\n\u2022 Use grid-search over a range of K\u2019s, using as an objective function cross-validated likelihood,\nthe BIC score, or a variational lower bound to the log-marginal likelihood.\n\u2022 Use reversible jump MCMC. See (Fruhwirth-Schnatter 2007) for details. Note that this is very\nslow and is not widely used.\n\u2022 Use variational Bayes to \u201cextinguish\u201d unwanted components, by analogy to the GMM case\ndiscussed in Section 21.6.1.6. See (MacKay 1997; Beal 2003) for details.\n\u2022 Use an \u201cin\ufb01nite HMM\u201d, which is based on the hierarchical Dirichlet process. See e.g., (Beal\net al. 2002; Teh et al. 2006) for details.\n17.5.5.2 Structure learning\nThe term structure learning in the context of HMMs refers to learning a sparse transition\nmatrix. That is, we want to learn the structure of the state transition diagram, not the structure\nof the graphical model (which is \ufb01xed). A large number of heuristic methods have been proposed.Most alternate between parameter estimation and some kind of heuristic split merge method\n(see e.g., (Stolcke and Omohundro 1992)).\nAlternatively, one can pose the problem as MAP estimation using a minimum entropy prior,\nof the form\np(A\ni,:)\u221dexp(\u2212 H(Ai,:)) (17.110)\nThis prior prefers states whose outgoing distribution is nearly deterministic, and hence has lowentropy (Brand 1999). The corresponding M step cannot be solved in closed form, but numericalmethods can be used. The trouble with this is that we might prune out all incoming transitionsto a state, creating isolated \u201cislands\u201d in state-space. The in\ufb01nite HMM presents an interestingalternative to these methods. See e.g., (Beal et al. 2002; Teh et al. 2006) for details.\n17.6 Generalizations of HMMs\nMany variants of the basic HMM model have been proposed. We brie\ufb02y discuss some of thembelow.", "652": "622 Chapter17. MarkovandhiddenMarkovmodels\nDt\u22121 DtDt+1\nQt\u22121 QtQt\u22121\nXt\u22121 XtXt+1\nFigure 17.14 Encoding a hidden semi-Markov model as a DGM. Dtare deterministic duration counters.\n17.6.1 Variable duration (semi-Markov) HMMs\nIn a standard HMM, the probability we remain in state ifor exactly dsteps is\np(ti=d)=( 1\u2212Aii)Ad\nii\u221dexp(dlogAii) (17.111)\nwhereAiiis the self-loop probability. This is called the geometric distribution. However, this\nkind of exponentially decaying function of dis sometimes unrealistic.\nTo allow for more general durations, one can use a semi-Markov model. It is called semi-\nMarkov because to predict the next state, it is not sufficient to condition on the past state: we\nalso need to know how long we\u2019ve been in that state. When the state space is not observeddirectly, the result is called a hidden semi-Markov model (HSMM), a variable duration HMM,\nor anexplicit duration HMM.\nHSMMs are widely used in many gene \ufb01nding programs, since the length distribution of\nexons and introns is not geometric (see e.g., (Schweikerta et al. 2009)), and in some chip-Seqdata analysis programs (see e.g., (Kuan et al. 2009)).\nHSMMs are useful not only because they can model the waiting time of each state more\naccurately, but also because they can model the distribution of a whole batch of observations atonce, instead of assuming all observations are conditionally iid. That is, they can use likelihoodmodels of the form p(x\nt:t+l|zt=k,dt=l), which generate lcorrelated observations if the\nduration in state kis forltime steps. This is useful for modeling data that is piecewise linear,\nor shows other local trends (Ostendorf et al. 1996).\n17.6.1.1 HSMM as augmented HMMs\nOne way to represent a HSMM is to use the graphical model shown in Figure 17.14. (In this\ufb01gure, we have assumed the observations are iid within each state, but this is not required,as mentioned above.) The D\nt\u2208{0,1,...,D}node is a state duration counter, where Dis\nthe maximum duration of any state. When we \ufb01rst enter state j, we sample Dtfrom the\nduration distribution for that state, Dt\u223cpj(\u00b7). Thereafer, Dtdeterministically counts down", "653": "17.6. GeneralizationsofHMMs 623\n1 2 3 41\u2212p 1\u2212p 1\u2212pp p p p\n(a)0 100 200 300 400 500 60000.0020.0040.0060.0080.010.012\n  \nn=1\nn=2\nn=5\n(b)\nFigure 17.15 (a) A Markov chain with n=4repeated states and self loops. (b) The resulting distribution\nover sequence lengths, for p=0.99and various n. Figure generated by hmmSelfLoopDist .\nuntilDt=0. WhileDt>0, the state ztis not allowed to change. When Dt=0, we make a\nstochastic transition to a new state.\nMore precisely, we de\ufb01ne the CPDs as follows:\np(Dt=d/prime|Dt\u22121=d,zt=j)=\u23a7\n\u23a8\n\u23a9pj(d/prime)ifd=0\n1ifd/prime=d\u22121andd\u22651\n0otherwise(17.112)\np(zt=k|zt\u22121=j,Dt\u22121=d)=\u23a7\n\u23a8\n\u23a91ifd>0andj=k\nAjkifd=0\n0otherwise(17.113)\nNote that pj(d)could be represented as a table (a non-parametric approach) or as some kind\nof parametric distribution, such as a Gamma distribution. If pj(d)is a geometric distribution,\nthis emulates a standard HMM.\nOne can perform inference in this model by de\ufb01ning a mega-variable Yt=(Dt,zt). However,\nthis is rather inefficient, since Dtis deterministic. It is possible to marginalize Dtout, and derive\nspecial purpose inference procedures. See (Guedon 2003; Yu and Kobayashi 2006) for details.\nUnfortunately, all these methods take O(TK2D)time, where Tis the sequence length, Kis\nthe number of states, and Dis the maximum duration of any state.\n17.6.1.2 Approximations to semi-Markov models\nA more efficient, but less \ufb02exible, way to model non-geometric waiting times is to replace eachstate with nnew states, each with the same emission probabilities as the original state. For\nexample, consider the model in Figure 17.15(a). Obviously the smallest sequence this can generateis of length n=4. Any path of length dthrough the model has probability p\nd\u2212n(1\u2212p)n;\nmultiplying by the number of possible paths we \ufb01nd that the total probability of a path of lengthdis\np(d)=/parenleftbiggd\u22121\nn\u22121/parenrightbigg\np\nd\u2212n(1\u2212p)n(17.114)", "654": "624 Chapter17. MarkovandhiddenMarkovmodels\nneed on the words\nphones\nsub-\nphonesaa n end n iy d dh\nnax\niyend\nend\nend end\nFigure 17.16 An example of an HHMM for an ASR system which can recognize 3 words. The top level\nrepresents bigram word probabilities. The middle level represents the phonetic spelling of each word. The\nbottom level represents the subphones of each phone. (It is traditional to represent a phone as a 3 stateHMM, representing the beginning, middle and end.) Based on Figure 7.5 of (Jurafsky and Martin 2000).\nThis is equivalent to the negative binomial distribution. By adjusting nand the self-loop\nprobabilities pof each state, we can model a wide range of waiting times: see Figure 17.15(b).\nLetEbe the number of expansions of each state needed to approximate pj(d). Forwards-\nbackwards on this model takes O(T(KE)Fin)time, where Finis the average number of\npredecessor states, compared to O(TK(Fin+D))for the HSMM. For typical speech recognition\napplications, Fin\u223c3,D\u223c50,K\u223c106,T\u223c105. (Similar \ufb01gures apply to problems such\nas gene \ufb01nding, which also often uses HSMMs.) Since Fin+D/greatermuchEFin, the expanded state\nmethod is much faster than an HSMM. See (Johnson 2005) for details.\n17.6.2 Hierarchical HMMs\nAhierarchical HMM (HHMM) (Fine et al. 1998) is an extension of the HMM that is designed to\nmodel domains with hierarchical structure. Figure 17.16 gives an example of an HHMM used in\nautomatic speech recognition. The phone and subphone models can be \u201ccalled\u201d from differenthigher level contexts. We can always \u201c\ufb02atten\u201d an HHMM to a regular HMM, but a factoredrepresentation is often easier to interpret, and allows for more efficient inference and model\ufb01tting.\nHHMMs have been used in many application domains, e.g., speech recognition (Bilmes 2001),\ngene \ufb01nding (Hu et al. 2000), plan recognition (Bui et al. 2002), monitoring transportationpatterns (Liao et al. 2007), indoor robot localization (Theocharous et al. 2004), etc. HHMMs areless expressive than stochastic context free grammars (SCFGs), since they only allow hierarchiesof bounded depth, but they support more efficient inference. In particular, inference in SCFGs(using the inside outside algorithm, (Jurafsky and Martin 2008)) takes O(T\n3)whereas inference\nin an HHMM takes O(T)time (Murphy and Paskin 2001).\nWe can represent an HHMM as a directed graphical model as shown in Figure 17.17. Q/lscript\nt\nrepresents the state at time tand level /lscript. A state transition at level /lscriptis only \u201callowed\u201d if the", "655": "17.6. GeneralizationsofHMMs 625\nF1\n1 F1\n2 F1\n3\nQ1\n1 Q12 Q13\nF2\n1 F2\n2 F2\n3\nQ2\n1 Q22 Q23\nF3\n1 F3\n2 F3\n3\nQ3\n1 Q32 Q33\nY1 Y2 Y3\nFigure 17.17 An HHMM represented as a DGM. Q/lscript\ntis the state at time t,l e v e l/lscript;F/lscript\nt=1if the HMM at\nlevel/lscripthas \ufb01nished (entered its exit state), otherwise F/lscript\nt=0. Shaded nodes are observed; the remaining\nnodes are hidden. We may optionally clamp F/lscript\nT=1,w h e r eTis the length of the observation sequence,\nto ensure all models have \ufb01nished by the end of the sequence. Source: Figure 2 of (Murphy and Paskin\n2001).\nchain at the level below has \u201c\ufb01nished\u201d, as determined by the F/lscript\u22121\ntnode. (The chain below\n\ufb01nishes when it chooses to enter its end state.) This mechanism ensures that higher level chains\nevolve more slowly than lower level chains, i.e., lower levels are nested within higher levels.\nA variable duration HMM can be thought of as a special case of an HHMM, where the top\nlevel is a deterministic counter, and the bottom level is a regular HMM, which can only changestates once the counter has \u201ctimed out\u201d. See (Murphy and Paskin 2001) for further details.\n17.6.3 Input-output HMMs\nIt is straightforward to extend an HMM to handle inputs, as shown in Figure 17.18(a). This de\ufb01nesa conditional density model for sequences of the form\np(y\n1:T,z1:T|u1:T,\u03b8) (17.115)\nwhereutis the input at time t; this is sometimes called a control signal. If the inputs and\noutputs are continuous, a typical parameterization would be\np(zt|xt,zt\u22121=i,\u03b8)=C a t ( zt|S(Wiut)) (17.116)\np(yt|xt,zt=j,\u03b8)=N (yt|Vjut,\u03a3j) (17.117)\nThus the transition matrix is a logistic regression model whose parameters depend on theprevious state. The observation model is a Gaussian whose parameters depend on the current", "656": "626 Chapter17. MarkovandhiddenMarkovmodels\nyt\u22121 ytzt\u22121 ztut\u22121 ut\n(a)x1x2 xTz1z2 zT\n(b) (c)\nFigure 17.18 (a) Input-output HMM. (b) First-order auto-regressive HMM. (c) A second-order buried Markov\nmodel. Depending on the value of the hidden variables, the effective graph structure between the com-\nponents of the observed variables (i.e., the non-zero elements of the regression matrix and the precisionmatrix) can change, although this is not shown.\nstate. The whole model can be thought of as a hidden version of a maximum entropy Markov\nmodel (Section 19.6.1).\nConditional on the inputs u1:Tand the parameters \u03b8, one can apply the standard forwards-\nbackwards algorithm to estimate the hidden states. It is also straightforward to derive an EMalgorithm to estimate the parameters (see (Bengio and Frasconi 1996) for details).\n17.6.4 Auto-regressive and buried HMMs\nThe standard HMM assumes the observations are conditionally independent given the hiddenstate. In practice this is often not the case. However, it is straightforward to have direct arcs fromx\nt\u22121toxtas well as from zttoxt, as in Figure 17.18(b). This is known as an auto-regressive\nHMM,o raregime switching Markov model. For continuous data, the observation modelbecomes\np(x\nt|xt\u22121,zt=j,\u03b8)=N(xt|Wjxt\u22121+\u03bcj,\u03a3j) (17.118)\nThis is a linear regression model, where the parameters are chosen according to the currenthidden state. We can also consider higher-order extensions, where we condition on the last L\nobservations:\np(x\nt|xt\u2212L:t\u22121,zt=j,\u03b8)=N(xt|L/summationdisplay\n/lscript=1Wj,/lscriptxt\u2212/lscript+\u03bcj,\u03a3j) (17.119)\nSuch models are widely used in econometrics (Hamilton 1990). Similar models can be de\ufb01nedfor discrete observations.\nThe AR-HMM essentially combines two Markov chains, one on the hidden variables, to capture\nlong range dependencies, and one on the observed variables, to capture short range dependen-cies (Berchtold 1999). Since the Xnodes are observed, the connections between them only", "657": "17.6. GeneralizationsofHMMs 627\nx1 x2 x3z3,1 z3,2 z3,3z2,1 z2,2 z2,3z1,1 z1,2 z1,3\n(a)x31 x32 x33z31 z32 z33x21 x22 x23z21 z22 z23x11 x12 x13z11 z12 z13\n(b)\nFigure 17.19 (a) A factorial HMM with 3 chains. (b) A coupled HMM with 3 chains.\nchange the computation of the local evidence; inference can still be performed using the stan-\ndard forwards-backwards algorithm. Parameter estimation using EM is also straightforward: theE step is unchanged, as is the M step for the transition matrix. If we assume scalar observationsfor notational simplicty, the M step involves minimizing\n/summationdisplay\ntE/bracketleftbigg1\n\u03c32(st)(yt\u2212yT\nt\u2212L:t\u22121w(st))2+log\u03c32(st)/bracketrightbigg\n(17.120)\nFocussing on the wterms, we see that this requires solving Kweighted least squares problems:\nJ(w1:K)=/summationdisplay\nj/summationdisplay\nt\u03b3t(j)\n\u03c32(j)(yt\u2212yT\nt\u2212L:t\u22121wj)2(17.121)\nwhere\u03b3t(j)=p(zt=k|x1:T)is the smoothed posterior marginal. This is a weighted linear\nregression problem, where the design matrix has a Toeplitz form. This subproblem can be solvedefficiently using the Levinson-Durbin method (Durbin and Koopman 2001).\nBuried Markov models generalize AR-HMMs by allowing the dependency structure between\nthe observable nodes to change based on the hidden state, as in Figure 17.18(c). Such a modelis called a dynamic Bayesian multi net, since it is a mixture of different networks. In the\nlinear-Gaussian setting, we can change the structure of the of x\nt\u22121\u2192xtarcs by using sparse\nregression matrices, Wj, and we can change the structure of the connections within the\ncomponents of xtby using sparse Gaussian graphical models, either directed or undirected. See\n(Bilmes 2000) for details.\n17.6.5 Factorial HMM\nAn HMM represents the hidden state using a single discrete random variable zt\u2208{1,...,K}.\nTo represent 10 bits of information would require K=210= 1024states. By contrast, consider\nadistributed representation of the hidden state, where each zc,t\u2208{0,1}represents the c\u2019th", "658": "628 Chapter17. MarkovandhiddenMarkovmodels\nbit of the t\u2019th hidden state. Now we can represent 10 bits using just 10 binary variables, as\nillustrated in Figure 17.19(a). This model is called a factorial HMM (Ghahramani and Jordan\n1997). The hope is that this kind of model could capture different aspects of a signal, e.g., one\nchain would represent speaking style, another the words that are being spoken.\nUnfortunately, conditioned on xt, all the hidden variables are correlated (due to explaining\naway the common observed child xt). This make exact state estimation intractable. However,\nwe can derive efficient approximate inference algorithms, as we discuss in Section 21.4.1.\n17.6.6 Coupled HMM and the in\ufb02uence model\nIf we have multiple related data streams, we can use a coupled HMM (Brand 1996), as illustrated\nin Figure 17.19(b). This is a series of HMMs where the state transitions depend on the states ofneighboring chains. That is, we represent the joint conditional distribution as\np(z\nt|zt\u22121)=/productdisplay\ncp(zct|zt\u22121) (17.122)\np(zct|zt\u22121)=p( zct|zc,t\u22121,zc\u22121,t\u22121,zc+1,t\u22121) (17.123)\nThis has been used for various tasks, such as audio-visual speech recognition (Ne\ufb01an et al.\n2002) and modeling freeway traffic \ufb02ows (Kwon and Murphy 2000).\nThe trouble with the above model is that it requires O(CK4)parameters to specify, if there\nareCchains with Kstates per chain, because each state depends on its own past plus the\npast of its two neighbors. There is a closely related model, known as the in\ufb02uence model\n(Asavathiratham 2000), which uses fewer parameters. It models the joint conditional distributionas\np(z\nct|zt\u22121)=C/summationdisplay\nc/prime=1\u03b1c,c/primep(zct|zc/prime,t\u22121) (17.124)\nwhere/summationtext\nc/prime\u03b1c,c/prime=1for eachc. That is, we use a convex combination of pairwise transition\nmatrices. The \u03b1c,c/primeparameter speci\ufb01es how much in\ufb02uence chain chas on chain c/prime. This\nmodel only takes O(C2+CK2)parameters to specify. Furthermore, it allows each chain to\nbe in\ufb02uenced by all the other chains, not just its nearest neighbors. (Hence the correspondinggraphical model is similar to Figure 17.19(b), except that each node has incoming edges fromall the previous nodes.) This has been used for various tasks, such as modeling conversationalinteractions between people (Basu et al. 2001).\nUnfortunately, inference in both of these models takes O(T(K\nC)2)time, since all the chains\nbecome fully correlated even if the interaction graph is sparse. Various approximate inferencemethods can be applied, as we discuss later.\n17.6.7 Dynamic Bayesian networks (DBNs)\nAdynamic Bayesian network is just a way to represent a stochastic process using a directed\ngraphical model.7Note that the network is not dynamic (the structure and parameters are \ufb01xed),\n7. The acronym DBNcan stand for either \u201cdynamic Bayesian network\u201d or \u201cdeep belief network\u201d (Section 28.1) depending\non the context. Geoff Hinton (who invented the term \u201cdeep belief network\u201d) has suggested the acronyms DyBNand\nDeeBNto avoid this ambiguity.", "659": "17.6. GeneralizationsofHMMs 629\n1471917162023212527\n6\n12\n13\n10\n8\n4\n132826222418\n15\n119\n5\n2SensorValid1\nFYdotDiff1\nFcloseSlow1Xdot0 Xdot1\nInLane0 InLane1LeftClr0 LeftClr1\nRightClr0 RightClr1\nLatAction0 LatAction1\nFwdAction0 FwdAction1\nYdot0 Ydot1\nStopped0 Stopped1BXdot1\nEngStatus0 EngStatus1 BcloseFast1\nFrontBackStatus0 FrontBackStatus1 BYdotDiff1Fclr1\nBclr1XdotSens1\nYdotSens1LeftClrSens1\nRightClrSens1\nTurnSignal1\nFYdotDiffSens1\nFclrSens1\nBXdotSens1\nBclrSens1\nBYdotDiffSens1\nslice ts lice t+1 evidence\nFigure 17.20 The BATnet DBN. The transient nodes are only shown for the second slice, to minimize\nclutter. The dotted lines can be ignored. Used with kind permission of Daphne Koller.\nrather it is a network representation of a dynamical system. All of the HMM variants we have\nseen above could be considered to be DBNs. However, we prefer to reserve the term \u201cDBN\u201dfor graph structures that are more \u201cirregular\u201d and problem-speci\ufb01c. An example is shown inFigure 17.20, which is a DBN designed to monitor the state of a simulated autonomous carknown as the \u201cBayesian Automated Taxi\u201d, or \u201cBATmobile\u201d (Forbes et al. 1995).\nDe\ufb01ning DBNs is straightforward: you just need to specify the structure of the \ufb01rst time-slice,\nthe structure between two time-slices, and the form of the CPDs. Learning is also easy. Themain problem is that exact inference can be computationally expensive, because all the hiddenvariables become correlated over time (this is known as entanglement \u2014 see e.g., (Koller and\nFriedman 2009, Sec. 15.2.4) for details). Thus a sparse graph does not necessarily result intractable exact inference. However, later we will see algorithms that can exploit the graphstructure for efficient approximate inference.\nExercises\nExercise 17.1 Derivation of Qfunction for HMM\nDerive Equation 17.97.\nExercise 17.2 Two \ufb01lter approach to smoothing in HMMs\nAssuming that \u03a0t(i)=p(St=i)>0for alliandt, derive a recursive algorithm for updating rt(i)=\np(St=i|xt+1:T). Hint: it should be very similar to the standard forwards algorithm, but using a time-\nreversed transition matrix. Then show how to compute the posterior marginals \u03b3t(i)=p(St=i|x1:T)", "660": "630 Chapter17. MarkovandhiddenMarkovmodels\nfrom the backwards \ufb01ltered messages rt(i), the forwards \ufb01ltered messages \u03b1t(i), and the stationary\ndistribution \u03a0t(i).\nExercise 17.3 EM for for HMMs with mixture of Gaussian observations\nConsider an HMM where the observation model has the form\np(xt|zt=j,\u03b8)=/summationdisplay\nkwjkN(xt|\u03bcjk,\u03a3jk) (17.125)\n\u2022 Draw the DGM.\n\u2022 Derive the E step.\u2022 Derive the M step.\nExercise 17.4 EM for for HMMs with tied mixtures\nIn many applications, it is common that the observations are high-dimensional vectors (e.g., in speech\nrecognition, x\ntis often a vector of cepstral coefficients and their derivatives, so xt\u2208R39), so estimating a\nfull covariance matrix for KMvalues (where Mis the number of mixture components per hidden state),\nas in Exercise 17.3, requires a lot of data. An alternative is to use just MGaussians, rather than MK\nGaussians, and to let the state in\ufb02uence the mixing weights but not the means and covariances. This is\ncalled asemi-continuous HMM ortied-mixture HMM.\n\u2022 Draw the corresponding graphical model.\n\u2022 Derive the E step.\u2022 Derive the M step.", "661": "18 State space models\n18.1 Introduction\nAstate space model orSSMis just like an HMM, except the hidden states are continuous. The\nmodel can be written in the following generic form:\nzt=g(ut,zt\u22121,/epsilon1t) (18.1)\nyt=h(zt,ut,\u03b4t) (18.2)\nwhereztis the hidden state, utis an optional input or control signal, ytis the observation, g\nis thetransition model, his theobservation model, /epsilon1tis the system noise at time t, and\u03b4t\nis the observation noise at time t. We assume that all parameters of the model, \u03b8, are known;\nif not, they can be included into the hidden state, as we discuss below.\nOneoftheprimarygoalsinusingSSMsistorecursivelyestimatethebeliefstate, p(zt|y1:t,u1:t,\u03b8).\n(Note: we will often drop the conditioning on uand\u03b8for brevity.) We will discuss algorithms for\nthis later in this chapter. We will also discuss how to convert our beliefs about the hidden state\ninto predictions about future observables by computing the posterior predictive p(yt+1|y1:t).\nAn important special case of an SSM is where all the CPDs are linear-Gaussian. In other\nwords, we assume\n\u2022 The transition model is a linear function\nzt=Atzt\u22121+Btut+/epsilon1t (18.3)\n\u2022 The observation model is a linear function\nyt=Ctzt+Dtut+\u03b4t (18.4)\n\u2022 The system noise is Gaussian\n/epsilon1t\u223cN(0,Qt) (18.5)\n\u2022 The observation noise is Gaussian\n\u03b4t\u223cN(0,Rt) (18.6)\nThis model is called a linear-Gaussian SSM (LG-SSM)o ralinear dynamical system (LDS).\nIf the parameters \u03b8t=(At,Bt,Ct,Dt,Qt,Rt)are independent of time, the model is called\nstationary.", "662": "632 Chapter18. Statespacemodels\n10 12 14 16 18 20 22468101214\n  \nobserved\ntruth\n(a)8 10 12 14 16 18 20 22 2446810121416\n  \nobserved\nfiltered\n(b)10 15 20 25468101214\n  \nobserved\nsmoothed\n(c)\nFigure 18.1 Illustration of Kalman \ufb01ltering and smoothing. (a) Observations (green cirles) are generated\nby an object moving to the right (true location denoted by black squares). (b) Filtered estimated is shown\nby dotted red line. Red cross is the posterior mean, blue circles are 95% con\ufb01dence ellipses derived fromthe posterior covariance. For clarity, we only plot the ellipses every other time step. (c) Same as (b), butusing offline Kalman smoothing. Figure generated by kalmanTrackingDemo .\nThe LG-SSM is important because it supports exact inference, as we will see. In particular,\nif the initial belief state is Gaussian, p(z1)=N(\u03bc1|0,\u03a31|0), then all subsequent belief states\nwill also be Gaussian; we will denote them by p(zt|y1:t)=N(\u03bct|t,\u03a3t|t). (The notation \u03bct|\u03c4\ndenotes E[zt|y1:\u03c4], and similarly for \u03a3t|t; thus\u03bct|0denotes the prior for z1before we have\nseen any data. For brevity we will denote the posterior belief states using \u03bct|t=\u03bctand\n\u03a3t|t=\u03a3t.) We can compute these quantities efficiently using the celebrated Kalman \ufb01lter,\nas we show in Section 18.3.1. But before discussing algorithms, we discuss some important\napplications.\n18.2 Applications of SSMs\nSSMs have many applications, some of which we discuss in the sections below. We mostlyfocus on LG-SSMs, for simplicity, although non-linear and/or non-Gaussian SSMs are even morewidely used.\n18.2.1 SSMs for object tracking\nOne of the earliest applications of Kalman \ufb01ltering was for tracking objects, such as airplanesand missiles, from noisy measurements, such as radar. Here we give a simpli\ufb01ed example toillustrate the key ideas. Consider an object moving in a 2D plane. Let z\n1tandz2tbe the\nhorizontal and vertical locations of the object, and \u02d9z1tand\u02d9z2tbe the corresponding velocity.\nWe can represent this as a state vector zt\u2208R4as follows:\nzT\nt=/parenleftbigz1tz2t\u02d9z1t\u02d9z2t/parenrightbig\n. (18.7)", "663": "18.2. ApplicationsofSSMs 633\nLet us assume that the object is moving at constant velocity, but is \u201cperturbed\u201d by random\nGaussian noise (e.g., due to the wind). Thus we can model the system dynamics as follows:\nzt=Atzt\u22121+/epsilon1t (18.8)\u239b\n\u239c\u239c\u239dz1t\nz2t\n\u02d9z1t\n\u02d9z2t\u239e\n\u239f\u239f\u23a0=\u239b\n\u239c\u239c\u239d10\u03940\n010\u0394001 0000 1\u239e\n\u239f\u239f\u23a0\u239b\n\u239c\u239c\u239dz\n1,t\u22121\nz2,t\u22121\n\u02d9z1,t\u22121\n\u02d9z2,t\u22121\u239e\n\u239f\u239f\u23a0+\u239b\n\u239c\u239c\u239d/epsilon11t\n/epsilon12t\n/epsilon13t\n/epsilon14t\u239e\n\u239f\u239f\u23a0(18.9)\nwhere/epsilon1t\u223cN(0,Q)is the system noise, and \u0394is thesampling period. This says that the\nnew location zj,tis the old location zj,t\u22121plus\u0394times the old velocity \u02d9zj,t\u22121, plus random\nnoise,/epsilon1jt,f o rj=1:2. Also, the new velocity \u02d9zj,tis the old velocity \u02d9zj,t\u22121plus random\nnoise,/epsilon1jt,f o rj=3:4. This is called a random accelerations model, since the object moves\naccording to Newton\u2019s laws, but is subject to random changes in velocity.\nNow suppose that we can observe the location of the object but not its velocity. Let yt\u2208R2\nrepresent our observation, which we assume is subject to Gaussian noise. We can model this as\nfollows:\nyt=Ctzt+\u03b4t (18.10)\n/parenleftbiggy1t\ny2t/parenrightbigg\n=/parenleftbigg10000100/parenrightbigg\u239b\n\u239c\u239c\u239dz\n1t\nz2t\n\u02d9z1t\n\u02d9z2t\u239e\n\u239f\u239f\u23a0+\u239b\n\u239c\u239c\u239d\u03b41t\n\u03b42t\n\u03b43t\n\u03b44t\u239e\n\u239f\u239f\u23a0(18.11)\nwhere\u03b4t\u223cN(0,R)is the measurement noise.\nFinally, we need to specify our initial (prior) beliefs about the state of the object, p(z1).W e\nwill assume this is a Gaussian, p(z1)=N(z1|\u03bc1|0,\u03a31|0). We can represent prior ignorance by\nmaking\u03a31|0suitably \u201cbroad\u201d, e.g., \u03a31|0=\u221eI. We have now fully speci\ufb01ed the model and can\nperform sequential Bayesian updating to compute p(zt|y1:t)using an algorithm known as the\nKalman \ufb01lter, to be described in Section 18.3.1.\nFigure 18.1(a) gives an example. The object moves to the right and generates an observation\nat each time step (think of \u201cblips\u201d on a radar screen). We observe these blips and \ufb01lter out\nthe noise by using the Kalman \ufb01lter. At every step, we have p(zt|y1:t), from which we can\ncompute p(z1t,z2t|y1:t)by marginalizing out the dimensions corresponding to the velocities.\n(This is easy to do since the posterior is Gaussian.) Our \u201cbest guess\u201d about the location of theobject is the posterior mean, E[z\nt|y1:t], denoted as a red cross in Figure 18.1(b). Our uncertainty\nassociated with this is represented as an ellipse, which contains 95% of the probability mass. Wesee that our uncertainty goes down over time, as the effects of the initial uncertainty get \u201cwashedout\u201d. We also see that the estimated trajectory has \u201c\ufb01ltered out\u201d some of the noise. To obtainthe much smoother plot in Figure 18.1(c), we need to use the Kalman smoother, which computesp(z\nt|y1:T); this depends on \u201cfuture\u201d as well as \u201cpast\u201d data, as discussed in Section 18.3.2.\n18.2.2 Robotic SLAM\nConsider a robot moving around an unknown 2d world. It needs to learn a map and keeptrack of its location within that map. This problem is known as simultaneous localization and", "664": "634 Chapter18. Statespacemodels\nL2Y1 Y2 YTY1 Y3\nX1 X2 X3\n...XTL1\nFigure 18.2 Illustration of graphical model underlying SLAM. Liis the \ufb01xed location of landmark i,xt\nis the location of the robot, and ytis the observation. In this trace, the robot sees landmarks 1 and 2 at\ntime step 1, then just landmark 2, then just landmark 1, etc. Based on Figure 15.A.3 of (Koller and Friedman\n2009).\nRobot pose\n(a) (b)\nFigure 18.3 Illustration of the SLAM problem. (a) A robot starts at the top left and moves clockwise in a\ncircle back to where it started. We see how the posterior uncertainty about the robot\u2019s location increases\nand then decreases as it returns to a familar location, closing the loop. If we performed smoothing, this\nnew information would propagate backwards in time to disambiguate the entire trajectory. (b) We show the\nprecision matrix, representing sparse correlations between the landmarks, and between the landmarks and\nthe robot\u2019s position (pose). This sparse precision matrix can be visualized as a Gaussian graphical model,\nas shown. Source: Figure 15.A.3 of (Koller and Friedman 2009) . Used with kind permission of Daphne\nKoller.", "665": "18.2. ApplicationsofSSMs 635\nmapping,o r SLAMfor short, and is widely used in mobile robotics, as well as other applications\nsuch as indoor navigation using cellphones (since GPS does not work inside buildings).\nLet us assume we can represent the map as the 2d locations of a \ufb01xed set of Klandmarks,\ndenote them by L1,...,LK(each is a vector in R2). For simplicity, we will assume these are\nuniquely identi\ufb01able. Let xtrepresent the unknown location of the robot at time t. We de\ufb01ne\nthe state space to be zt=(xt,L1:K); we assume the landmarks are static, so their motion\nmodel is a constant, and they have no system noise. If ytmeasures the distance from xtto\nthe set of closest landmarks, then the robot can update its estimate of the landmark locations\nbased on what it sees. Figure 18.2 shows the corresponding graphical model for the case whereK=2, and where on the \ufb01rst step it sees landmarks 1 and 2, then just landmark 2, then just\nlandmark 1, etc.\nIf we assume the observation model p(y\nt|zt,L)is linear-Gaussian, and we use a Gaussian\nmotion model for p(xt|xt\u22121,ut), we can use a Kalman \ufb01lter to maintain our belief state about\nthe location of the robot and the location of the landmarks (Smith and Cheeseman 1986; Chosetand Nagatani 2001).\nOver time, the uncertainty in the robot\u2019s location will increase, due to wheel slippage etc.,\nbut when the robot returns to a familiar location, its uncertainty will decrease again. This iscalledclosing the loop, and is illustrated in Figure 18.3(a), where we see the uncertainty ellipses,\nrepresenting cov[x\nt|y1:t,u1:t], grow and then shrink. (Note that in this section, we assume that\na human is joysticking the robot through the environment, so u1:tis given as input, i.e., we do\nnot address the decision-theoretic issue of choosing where to explore.)\nSince the belief state is Gaussian, we can visualize the posterior covariance matrix \u03a3t. Ac-\ntually, it is more interesting to visualize the posterior precision matrix, \u039bt=\u03a3\u22121\nt, since that\nis fairly sparse, as shown in Figure 18.3(b). The reason for this is that zeros in the precisionmatrix correspond to absent edges in the corresponding undirected Gaussian graphical model(see Section 19.4.4). Initially all the landmarks are uncorrelated (assuming we have a diagonalprior onL), so the GGM is a disconnected graph, and \u039b\ntis diagonal. However, as the robot\nmoves about, it will induce correlation between nearby landmarks. Intuitively this is because therobot is estimating its position based on distance to the landmarks, but the landmarks\u2019 locationsare being estimated based on the robot\u2019s position, so they all become inter-dependent. This canbe seen more clearly from the graphical model in Figure 18.2: it is clear that L\n1andL2are not\nd-separated by y1:t, because there is a path between them via the unknown sequence of x1:t\nnodes. As a consequence of the precision matrix becoming denser, exact inference takes O(K3)\ntime. (This is an example of the entanglement problem for inference in DBNs.) This preventsthe method from being applied to large maps.\nThere are two main solutions to this problem. The \ufb01rst is to notice that the correlation pattern\nmoves along with the location of the robot (see Figure 18.3(b)). The remaining correlationsbecome weaker over time. Consequently we can dynamically \u201cprune out\u201d weak edges fromthe GGM using a technique called the thin junction tree \ufb01lter (Paskin 2003) (junction trees areexplained in Section 20.4).\nA second approach is to notice that, conditional on knowing the robot\u2019s path, x\n1:t, the\nlandmark locations are independent. That is, p(L|x1:t,y1:t)=/producttextK\nk=1p(Lk|x1:t,y1:t). This\nforms the basis of a method known as FastSLAM, which combines Kalman \ufb01ltering and particle\n\ufb01ltering, as discussed in Section 23.6.3.\n(Thrun et al. 2006) provides a more detailed account of SLAM and mobile robotics.", "666": "636 Chapter18. Statespacemodels\nxt\u22121 xtyt\u22121 yt\u03b8t\u22121 \u03b8t\n(a)0 5 10 15 20 25\u22128\u22126\u22124\u22122024\ntimeweightsonline linear regression\n  \nw0\nw1\nw0 batch\nw1 batch\n(b)\nFigure 18.4 (a) A dynamic generalization of linear regression. (b) Illustration of the recursive least squares\nalgorithm applied to the model p(y|x,\u03b8)=N(y|w0+w1x,\u03c32). We plot the marginal posterior of w0\nandw1vs number of data points. (Error bars represent E[wj|y1:t]\u00b1/radicalbig\nvar[wj|y1:t].) After seeing all\nthe data, we converge to the offline ML (least squares) solution, represented by the horizontal lines. Figure\ngenerated by linregOnlineDemoKalman .\n18.2.3 Online parameter learning using recursive least squares\nWe can perform online Bayesian inference for the parameters of various statistical models using\nSSMs. In this section, we focus on linear regression; in Section 18.5.3.2, we discuss logisticregression.\nThe basic idea is to let the hidden state represent the regression parameters, and to let the\n(time-varying) observation model represent the current data vector. In more detail, de\ufb01ne theprior to be p(\u03b8)=N(\u03b8|\u03b8\n0,\u03a30). (If we want to do online ML estimation, we can just set\n\u03a30=\u221eI.) Let the hidden state be zt=\u03b8; if we assume the regression parameters do not\nchange, we can set At=IandQt=0I,s o\np(\u03b8t|\u03b8t\u22121)=N(\u03b8t|\u03b8t\u22121,0I)=\u03b4\u03b8t\u22121(\u03b8t) (18.12)\n(If we do let the parameters change over time, we get a so-called dynamic linear model\n(Harvey 1990; West and Harrison 1997; Petris et al. 2009).) Let Ct=xT\nt, andRt=\u03c32, so the\n(non-stationary) observation model has the form\nN(yt|Ctzt,Rt)=N(yt|xT\nt\u03b8t,\u03c32) (18.13)\nApplying the Kalman \ufb01lter to this model provides a way to update our posterior beliefs about\nthe parameters as the data streams in. This is known as the recursive least squares orRLS\nalgorithm.\nWe can derive an explicit form for the updates as follows. In Section 18.3.1, we show that the\nKalman update for the posterior mean has the form\n\u03bct=At\u03bct\u22121+Kt(yt\u2212CtAt\u03bct\u22121) (18.14)", "667": "18.2. ApplicationsofSSMs 637\nwhereKtis known as the Kalman gain matrix. Based on Equation 18.39, one can show that\nKt=\u03a3tCT\ntR\u22121\nt. In this context, we have Kt=\u03a3txt/\u03c32. Hence the update for the\nparameters becomes\n\u02c6\u03b8t=\u02c6\u03b8t\u22121+1\n\u03c32\u03a3t|t(yt\u2212xT\nt\u02c6\u03b8t\u22121)xt (18.15)\nIf we approximate1\n\u03c32\u03a3t|t\u22121with\u03b7tI, we recover the least mean squares orLMSalgorithm,\ndiscussed in Section 8.5.3. In LMS, we need to specify how to adapt the update parameter\n\u03b7tto ensure convergence to the MLE. Furthermore, the algorithm may take multiple passes\nthrough the data. By contrast, the RLS algorithm automatically performs step-size adaptation,and converges to the optimal posterior in one pass over the data. See Figure 18.4 for an example.\n18.2.4 SSM for time series forecasting *\nSSMs are very well suited for time-series forecasting, as we explain below. We focus on the caseof scalar (one dimensional) time series, for simplicity. Our presentation is based on (Varian 2011).See also (Aoki 1987; Harvey 1990; West and Harrison 1997; Durbin and Koopman 2001; Petriset al. 2009; Prado and West 2010) for good books on this topic.\nAt \ufb01rst sight, it might not be apparent why SSMs are useful, since the goal in forecasting is\nto predict future visible variables, not to estimate hidden states of some system. Indeed, mostclassical methods for time series forecasting are just functions of the form \u02c6y\nt+1=f(y1:t,\u03b8),\nwhere hidden variables play no role (see Section 18.2.4.4). The idea in the state-space approach totime series is to create a generative model of the data in terms of latent processes, which capturedifferent aspects of the signal. We can then integrate out the hidden variables to compute theposterior predictive of the visibles.\nSince the model is linear-Gaussian, we can just add these processes together to explain the\nobserved data. This is called a structural time series model. Below we explain some of the\nbasic building blocks.\n18.2.4.1 Local level model\nThe simplest latent process is known as the local level model, which has the form\ny\nt=at+/epsilon1y\nt,/epsilon1yt\u223cN(0,R) (18.16)\nat=at\u22121+/epsilon1a\nt,/epsilon1at\u223cN(0,Q) (18.17)\nwhere the hidden state is just zt=at. This model asserts that the observed data yt\u2208Ris\nequal to some unknown level term at\u2208R, plus observation noise with variance R. In addition,\nthe levelatevolves over time subject to system noise with variance Q. See Figure 18.5 for some\nexamples.", "668": "638 Chapter18. Statespacemodels\nat\u22121 at\nyt\n(a)0 20 40 60 80 100 120 140 160 180 200\u22128\u22126\u22124\u221220246local level, a=1.000\n  \nQ=0.0, R=0.1\nQ=0.1, R=0.0\nQ=0.1, R=0.1\n(b)\nFigure 18.5 (a) Local level model. (b) Sample output, for a0=1 0. Black solid line: Q=0,R=1\n(deterministic system, noisy observations). Red dotted line: Q=0.1,R=0(noisy system, deterministic\nobservation). Blue dot-dash line: Q=0.1,R=1(noisy system and observations). Figure generated by\nssmTimeSeriesSimple .\nat\u22121 at\nbt\u22121 bt\nyt\n(a)0 10 20 30 40 50 60 70 80 90 100\u22121000\u2212800\u2212600\u2212400\u22122000200local trend, a=10.000, b=1.000\n  \nQ=0.0, R=100.0\nQ=1.0, R=0.0\nQ=1.0, R=100.0\n(b)\nFigure 18.6 (a) Local Trend. (b) Sample output, for a0=1 0,b0=1. Color code as in Figure 18.5. Figure\ngenerated by ssmTimeSeriesSimple .\n18.2.4.2 Local linear trend\nMany time series exhibit linear trends upwards or downwards, at least locally. We can model\nthis by letting the level atchange by an amount btat each step as follows:\nyt=at+/epsilon1y\nt,/epsilon1yt\u223cN(0,R) (18.18)\nat=at\u22121+bt\u22121+/epsilon1a\nt,/epsilon1at\u223cN(0,Qa) (18.19)\nbt=bt\u22121+/epsilon1bt,/epsilon1bt\u223cN(0,Qb) (18.20)\nSee Figure 18.6(a). We can write this in standard form by de\ufb01ning zt=(at,bt)and\nA=/parenleftbigg11\n01/parenrightbigg\n,C=/parenleftbig10/parenrightbig\n,Q=/parenleftbiggQa0\n0Qb/parenrightbigg\n(18.21)\nWhenQb=0,w eh a v e bt=b0, which is some constant de\ufb01ning the slope of the line. If in\naddition we have Qa=0,w eh a v ea t=at\u22121+b0t. Unrolling this, we have at=a0+b0t, and", "669": "18.2. ApplicationsofSSMs 639\nat\u22121 at\nbt\u22121 bt\nytc1\nt\u22121 c1\ntc2\nt\u22121 c2\ntc3\nt\u22121 c3\nt-1\n-1\n-1\n0 2 4 6 8 10 12 14 16 18 20\u221230\u221220\u221210010203040seasonal model, s=4, a=0.000, b=0.000\n  \nQ=0.0, R=1.0\nQ=1.0, R=0.0\nQ=1.0, R=1.0\nFigure 18.7 (a) Seasonal model. (b) Sample output, for a0=b0=0,c0=( 1,1,1), with a period of 4.\nColor code as in Figure 18.5. Figure generated by ssmTimeSeriesSimple .\nhence E[yt|y1:t\u22121]=a0+tb0. This is thus a generalization of the classic constant linear trend\nmodel, an example of which is shown in the black line of Figure 18.6(b).\n18.2.4.3 Seasonality\nMany time series \ufb02uctuate periodically, as illustrated in Figure 18.7(b). This can be modeled by\nadding a latent process consisting of a series offset terms, ct, which sum to zero (on average)\nover a complete cycle of Ssteps:\nct=\u2212S\u22121/summationdisplay\ns=1ct\u2212s+/epsilon1c\nt,/epsilon1ct\u223cN(0,Qc) (18.22)\nSee Figure 18.7(a) for the graphical model for the case S=4(we only need 3 seasonal vari-\nable because of the sum-to-zero constraint). Writing this in standard LG-SSM form is left to\nExercise 18.2.\n18.2.4.4 ARMA models *\nThe classical approach to time-series forecasting is based on ARMAmodels. \u201cARMA\u201d stands for\nauto-regressive moving-average, and refers to a model of the form\nxt=p/summationdisplay\ni=1\u03b1ixt\u2212i+q/summationdisplay\nj=1\u03b2jwt\u2212j+vt (18.23)\nwherevt,wt\u223cN(0,1)are independent Gaussian noise terms. If q=0,w eh a v eap u r eA R\nmodel, where xt\u22a5xi|xt\u22121:t\u2212p,f o ri<t\u2212p. For example, if p=1, we have the AR(1) model", "670": "640 Chapter18. Statespacemodels\nx1 x2 x3 x4\n(a)w1 w3 w2\nx1 x2 x3 x4\n(b)\nw1 w3 w2\nx1 x2 x3 x4\n(c)\nFigure 18.8 (a) An AR(1) model. (b) An MA(1) model represented as a bi-directed graph. (c) An ARMA(1,1)\nmodel. Source: Figure 5.14 of (Choi 2011). Used with kind permission of Myung Choi.\nshown in Figure 18.8(a). (The vtnodes are implicit in the Gaussian CPD for xt.) This is just a\n\ufb01rst-order Markov chain. If p=0, we have a pure MA model, where xt\u22a5xi,f o ri<t\u2212q.\nFor example, if q=1, we have the MA(1) model shown in Figure 18.8(b). Here the wtnodes are\nhidden common causes, which induces dependencies between adjacent time steps. This models\nshort-range correlation. If p=q=1, we get the ARMA(1,1) model shown in Figure 18.8(c), which\ncaptures correlation at short and long time scales.\nIt turns out that ARMA models can be represented as SSMs, as explained in (Aoki 1987; Harvey\n1990; West and Harrison 1997; Durbin and Koopman 2001; Petris et al. 2009; Prado and West2010). However, the structural approach to time series is often easier to understand than theARMA approach. In addition, it allows the parameters to evolve over time, which makes themodels more adaptive to non-stationarity.\n18.3 Inference in LG-SSM\nIn this section, we discuss exact inference in LG-SSM models. We \ufb01rst consider the online case,which is analogous to the forwards algorithm for HMMs. We then consider the offline case,which is analogous to the forwards-backwards algorithm for HMMs.\n18.3.1 The Kalman \ufb01ltering algorithm\nTheKalman \ufb01lter is an algorithm for exact Bayesian \ufb01ltering for linear-Gaussian state space\nmodels. We will represent the marginal posterior at time tby\np(zt|y1:t,u1:t)=N(zt|\u03bct,\u03a3t) (18.24)\nSince everything is Gaussian, we can perform the prediction and update steps in closed form,as we explain below. The resulting algorithm is the Gaussian analog of the HMM \ufb01lter inSection 17.4.2.", "671": "18.3. InferenceinLG-SSM 641\n18.3.1.1 Prediction step\nThe prediction step is straightforward to derive:\np(zt|y1:t\u22121,u1:t)=/integraldisplay\nN(zt|Atzt\u22121+Btut,Qt)N(zt\u22121|\u03bct\u22121,\u03a3t\u22121)dzt\u22121(18.25)\n=N(zt|\u03bct|t\u22121,\u03a3t|t\u22121) (18.26)\n\u03bct|t\u22121/definesAt\u03bct\u22121+Btut (18.27)\n\u03a3t|t\u22121/definesAt\u03a3t\u22121AT\nt+Qt (18.28)\n18.3.1.2 Measurement step\nThe measurement step can be computed using Bayes rule, as follows\np(zt|yt,y1:t\u22121,u1:t)\u221dp(yt|zt,ut)p(zt|y1:t\u22121,u1:t) (18.29)\nIn Section 18.3.1.6, we show that this is given by\np(zt|y1:t,ut)=N (zt|\u03bct,\u03a3t) (18.30)\n\u03bct=\u03bct|t\u22121+Ktrt (18.31)\n\u03a3t=(I\u2212KtCt)\u03a3t|t\u22121 (18.32)\nwherertis theresidual orinnovation, given by the difference between our predicted observa-\ntion and the actual observation:\nrt/definesyt\u2212\u02c6yt (18.33)\n\u02c6yt/definesE[yt|y1:t\u22121,u1:t]=Ct\u03bct|t\u22121+Dtut (18.34)\nandKtis theKalman gain matrix, given by\nKt/defines\u03a3t|t\u22121CTtS\u22121\nt (18.35)\nwhere\nSt/definescov[rt|y1:t\u22121,u1:t] (18.36)\n=E/bracketleftbig\n(Ctzt+\u03b4t\u2212\u02c6yt)(Ctzt+\u03b4t\u2212\u02c6yt)T|y1:t\u22121,u1:t/bracketrightbig\n(18.37)\n=Ct\u03a3t|t\u22121CTt+Rt (18.38)\nwhere\u03b4t\u223cN(0,Rt)is an observation noise term which is independent of all other noise\nsources. Note that by using the matrix inversion lemma, the Kalman gain matrix can also be\nwritten as\nKt=\u03a3t|t\u22121CT(C\u03a3t|t\u22121CT+R)\u22121=(\u03a3\u22121\nt|t\u22121+CTRC)\u22121CTR\u22121(18.39)\nWe now have all the quantities we need to implement the algorithm; see kalmanFilter for\nsome Matlab code.\nLet us try to make sense of these equations. In particular, consider the equation for the\nmean update: \u03bct=\u03bct|t\u22121+Ktrt. This says that the new mean is the old mean plus a", "672": "642 Chapter18. Statespacemodels\ncorrection factor, which is Kttimes the error signal rt. The amount of weight placed on the\nerror signal depends on the Kalman gain matrix. If Ct=I, thenKt=\u03a3t|t\u22121S\u22121\nt, which\nis the ratio between the covariance of the prior (from the dynamic model) and the covariance\nof the measurement error. If we have a strong prior and/or very noisy sensors, |Kt|will be\nsmall, and we will place little weight on the correction term. Conversely, if we have a weak priorand/or high precision sensors, then |K\nt|will be large, and we will place a lot of weight on the\ncorrection term.\n18.3.1.3 Marginal likelihood\nAs a byproduct of the algorithm, we can also compute the log-likelihood of the sequence using\nlogp(y1:T|u1:T)=/summationdisplay\ntlogp(yt|y1:t\u22121,u1:t) (18.40)\nwhere\np(yt|y1:t\u22121,u1:t)=N(yt|Ct\u03bct|t\u22121,St) (18.41)\n18.3.1.4 Posterior predictive\nThe one-step-ahead posterior predictive density for the observations can be computed as follows\np(yt|y1:t\u22121,u1:t)=/integraldisplay\nN(yt|Czt,R)N(zt|\u03bct|t\u22121,\u03a3t|t\u22121)dzt (18.42)\n=N(yt|C\u03bct|t\u22121,C\u03a3t|t\u22121CT+R) (18.43)\nThis is useful for time series forecasting.\n18.3.1.5 Computational issues\nThere are two dominant costs in the Kalman \ufb01lter: the matrix inversion to compute the Kalmangain matrix, K\nt, which takes O(|yt|3)time; and the matrix-matrix multiply to compute \u03a3t,\nwhich takes O(|zt|2)time. In some applications (e.g., robotic mapping), we have |zt|/greatermuch|yt|,s o\nthe latter cost dominates. However, in such cases, we can sometimes use sparse approximations(see (Thrun et al. 2006)).\nIn cases where |y\nt|/greatermuch|zt|, we can precompute Kt, since, suprisingly, it does not depend on\nthe actual observations y1:t(an unusual property that is speci\ufb01c to linear Gaussian systems).\nThe iterative equations for updating \u03a3tare called the Ricatti equations, and for time invariant\nsystems (i.e., where \u03b8t=\u03b8), they converge to a \ufb01xed point. This steady state solution can then\nbe used instead of using a time-speci\ufb01c gain matrix.\nIn practice, more sophisticated implementations of the Kalman \ufb01lter should be used, for rea-\nsons of numerical stability. One approach is the information \ufb01lter , which recursively updates\nthe canonical parameters of the Gaussian, \u039bt=\u03a3\u22121\ntand\u03b7t=\u039bt\u03bct, instead of the moment\nparameters. Another approach is the square root \ufb01lter, which works with the Cholesky de-\ncomposition or the UtDtUtdecomposition of \u03a3t. This is much more numerically stable than\ndirectly updating \u03a3t. Further details can be found at http://www .cs.unc.edu/~welch/kal\nman/and in various books, such as (Simon 2006).", "673": "18.3. InferenceinLG-SSM 643\n18.3.1.6 Derivation *\nWe now derive the Kalman \ufb01lter equations. For notational simplicity, we will ignore the input\ntermsu1:t. From Bayes rule for Gaussians (Equation 4.125), we have that the posterior precision\nis given by\n\u03a3\u22121\nt=\u03a3\u22121\nt|t\u22121+CT\ntR\u22121\ntCt (18.44)\nFrom the matrix inversion lemma (Equation 4.106) we can rewrite this as\n\u03a3t=\u03a3t|t\u22121\u2212\u03a3t|t\u22121CTt(Rt+Ct\u03a3t|t\u22121CTt)\u22121Ct\u03a3t|t\u22121 (18.45)\n=(I\u2212KtCt)\u03a3t|t\u22121 (18.46)\nFrom Bayes rule for Gaussians (Equation 4.125), the posterior mean is given by\n\u03bct=\u03a3tCtR\u22121\ntyt+\u03a3t\u03a3\u22121\nt|t\u22121\u03bct|t\u22121 (18.47)\nWe will now massage this into the form stated earlier. Applying the second matrix inversion\nlemma (Equation 4.107) to the \ufb01rst term of Equation 18.47 we have\n\u03a3tCtR\u22121\ntyt=(\u03a3\u22121\nt|t\u22121+CT\ntR\u22121\ntCt)\u22121CtR\u22121\ntyt (18.48)\n=\u03a3t|t\u22121CTt(Rt+Ct\u03a3t|t\u22121CTt)\u22121yt=Ktyt (18.49)\nNow applying the matrix inversion lemma (Equation 4.106) to the second term of Equation 18.47\nwe have\n\u03a3t\u03a3\u22121\nt|t\u22121\u03bct|t\u22121 (18.50)\n=(\u03a3\u22121\nt|t\u22121+CT\ntR\u22121\ntCt)\u22121\u03a3\u22121\nt|t\u22121\u03bct|t\u22121 (18.51)\n=/bracketleftbig\n\u03a3t|t\u22121\u2212\u03a3t|t\u22121CT(Rt+CTt\u03a3t|t\u22121CTt)Ct\u03a3t|t\u22121/bracketrightbig\n\u03a3\u22121\nt|t\u22121\u03bct|t\u22121 (18.52)\n=(\u03a3t|t\u22121\u2212KtCTt\u03a3t|t\u22121)\u03a3\u22121\nt|t\u22121\u03bct|t\u22121 (18.53)\n=\u03bct|t\u22121\u2212KtCTt\u03bct|t\u22121 (18.54)\nPutting the two together we get\n\u03bct=\u03bct|t\u22121+Kt(yt\u2212Ct\u03bct|t\u22121) (18.55)\n18.3.2 The Kalman smoothing algorithm\nIn Section 18.3.1, we described the Kalman \ufb01lter, which sequentially computes p(zt|y1:t)for each\nt. This is useful for online inference problems, such as tracking. However, in an offline setting,\nwe can wait until all the data has arrived, and then compute p(zt|y1:T). By conditioning\non past and future data, our uncertainty will be signi\ufb01cantly reduced. This is illustrated inFigure 18.1(c), where we see that the posterior covariance ellipsoids are smaller for the smoothedtrajectory than for the \ufb01ltered trajectory. (The ellipsoids are larger at the beginning and end ofthe trajectory, since states near the boundary do not have as many useful neighbors from whichto borrow information.)", "674": "644 Chapter18. Statespacemodels\nWe now explain how to compute the smoothed estimates, using an algorithm called the\nRTS smoother, named after its inventors, Rauch, Tung and Striebel (Rauch et al. 1965). It is\nalso known as the Kalman smoothing algorithm. The algorithm is analogous to the forwards-\nbackwards algorithm for HMMs, although there are some small differences which we discussbelow.\n18.3.2.1 Algorithm\nKalman \ufb01ltering can be regarded as message passing on a graph, from left to right. When themessages have reached the end of the graph, we have successfully computed p(z\nT|y1:T).N o w\nwe work backwards, from right to left, sending information from the future back to the past,and them combining the two information sources. The question is: how do we compute thesebackwards equations? We \ufb01rst give the equations, then the derivation.\nWe have\np(z\nt|y1:T)=N (\u03bct|T,\u03a3t|T) (18.56)\n\u03bct|T=\u03bct|t+Jt(\u03bct+1|T\u2212\u03bct+1|t) (18.57)\n\u03a3t|T=\u03a3t|t+Jt(\u03a3t+1|T\u2212\u03a3t+1|t)JT\nt (18.58)\nJt/defines\u03a3t|tATt+1\u03a3\u22121\nt+1|t(18.59)\nwhereJtis the backwards Kalman gain matrix. The algorithm can be initialized from \u03bcT|T\nand\u03a3T|Tfrom the Kalman \ufb01lter. Note that this backwards pass does not need access to the\ndata, that is, it does not need y1:T. This allows us to \u201cthrow away\u201d potentially high dimensional\nobservation vectors, and just keep the \ufb01ltered belief states, which usually requires less memory.\n18.3.2.2 Derivation *\nWe now derive the Kalman smoother, following the presentation of (Jordan 2007, sec 15.7).\nThe key idea is to leverage the Markov property, which says that ztis independent of future\ndata,yt+1:T, as long as zt+1is known. Of course, zt+1is not known, but we have a distribution\nover it. So we condition on zt+1and then integrate it out, as follows.\np(zt|y1:T)=/integraldisplay\np(zt|y1:T,zt+1)p(zt+1|y1:T)dzt+1 (18.60)\n=/integraldisplay\np(zt|y1:t,\u0018\u0018\u0018yt+1:T,zt+1)p(zt+1|y1:T)dzt+1 (18.61)\nBy induction, assume we have already computed the smoothed distribution for t+1:\np(zt+1|y1:T)=N(zt+1|\u03bct+1|T,\u03a3t+1|T) (18.62)\nThe question is: how do we perform the integration?\nFirst, we compute the \ufb01ltered two-slice distribution p(zt,zt+1|y1:t)as follows:\np(zt,zt+1|y1:t)=N/parenleftbigg/parenleftbiggzt\nzt+1/parenrightbigg\n|/parenleftbigg\u03bct|t\n\u03bct+1|t/parenrightbigg/parenleftbigg\n\u03a3t|t\u03a3t|tAT\nt+1\nAt+1\u03a3t|t\u03a3t+1|t/parenrightbigg/parenrightbigg\n(18.63)", "675": "18.3. InferenceinLG-SSM 645\nNow we use Gaussian conditioning to compute p(zt|zt+1,y1:t)as follows:\np(zt|zt+1,y1:t)=N (zt|\u03bct|t+Jt(zt+1\u2212\u03bct+1|t),\u03a3t|t\u2212Jt\u03a3t+1|tJT\nt) (18.64)\nWe can compute the smoothed distribution for tusing the rules of iterated expectation and\niterated covariance. First, the mean:\n\u03bct|T=E/bracketleftbig\nE[zt|zt+1,y1:T]/vextendsingle/vextendsingley1:T/bracketrightbig\n(18.65)\n=E/bracketleftbig\nE[zt|zt+1,y1:t]/vextendsingle/vextendsingley\n1:T/bracketrightbig\n(18.66)\n=E/bracketleftBig\n\u03bct|t+Jt(zt+1\u2212\u03bct+1|t)/vextendsingle/vextendsingley\n1:T/bracketrightBig\n(18.67)\n=\u03bct|t+Jt(\u03bct+1|T\u2212\u03bct+1|t) (18.68)\nNow the covariance:\n\u03a3t|T=c o v/bracketleftbig\nE[zt|zt+1,y1:T]/vextendsingle/vextendsingley\n1:T/bracketrightbig\n+E/bracketleftbig\ncov[zt|zt+1,y1:T]/vextendsingle/vextendsingley\n1:T/bracketrightbig\n(18.69)\n=c o v [ E[zt|zt+1,y1:t]|y1:T]+E/bracketleftbig\ncov[zt|zt+1,y1:t]/vextendsingle/vextendsingley\n1:T/bracketrightbig\n(18.70)\n=c o v/bracketleftBig\n\u03bct|t+Jt(zt+1\u2212\u03bct+1|t)|y1:T/bracketrightBig\n+E/bracketleftbig\n\u03a3t|t\u2212Jt\u03a3t+1|tJT\nt|y1:T/bracketrightbig\n(18.71)\n=Jtcov/bracketleftBig\nzt+1\u2212\u03bct+1|t|y1:T/bracketrightBig\nJTt+\u03a3t|t\u2212Jt\u03a3t+1|tJTt(18.72)\n=Jt\u03a3t+1|TJTt+\u03a3t|t\u2212Jt\u03a3t+1|tJTt(18.73)\n=\u03a3t|t+Jt(\u03a3t+1|T\u2212\u03a3t+1|t)JTt(18.74)\nThe algorithm can be initialized from \u03bcT|Tand\u03a3T|Tfrom the last step of the \ufb01ltering algo-\nrithm.\n18.3.2.3 Comparison to the forwards-backwards algorithm for HMMs *\nNote that in both the forwards and backwards passes for LDS, we always worked with normalized\ndistributions, either conditioned on the past data or conditioned on all the data. Furthermore,the backwards pass depends on the results of the forwards pass. This is different from the usualpresentation of forwards-backwards for HMMs, where the backwards pass can be computedindependently of the forwards pass (see Section 17.4.3).\nIt turns out that we can rewrite the Kalman smoother in a modi\ufb01ed form which makes it\nmore similar to forwards-backwards for HMMs. In particular, we have\np(z\nt|y1:T)=/integraldisplay\np(zt|y1:t,zt+1)p(zt+1|y1:T)dzt+1 (18.75)\n=/integraldisplay\np(zt,zt+1|y1:t)p(zt+1|y1:T)\np(zt+1|y1:t)dzt+1 (18.76)\nNow\np(zt+1|y1:T)=p(yt+1:T|zt+1,\b\by1:t)p(zt+1|y1:t)\np(yt+1:T|y1:t)(18.77)\nso\np(zt+1|y1:T)\np(zt+1|y1:t)=p(zt+1|y1:t)p(yt+1:T|zt+1)\np(zt+1|y1:t)p(yt+1:T|y1:t)\u221dp(yt+1:T|zt+1) (18.78)", "676": "646 Chapter18. Statespacemodels\nwhich is the conditional likelihood of the future data. This backwards message can be computed\nindependently of the forwards message. However, this approach has several disadvantages: (1)it needs access to the original observation sequence; (2) the backwards message is a likelihood,not a posterior, so it need not to integrate to 1 over z\nt\u2013 in fact, it may not always be possible\nto represent p(yt+1:T|zt+1)as a Gaussian with positive de\ufb01nite covariance (this problem does\nnot arise in discrete state-spaces, as used in HMMs); (3) when exact inference is not possible, itmakes more sense to try to approximate the smoothed distribution rather than the backwardslikelihood term (see Section 22.5).\nThere is yet another variant, known as two-\ufb01lter smoothing, whereby we compute p(z\nt|y1:t)\nin the forwards pass as usual, and the \ufb01ltered posterior p(zt|yt+1:T)in the backwards pass.\nThese can then be easily combined to compute p(zt|y1:T). See (Kitagawa 2004; Briers et al.\n2010) for details.\n18.4 Learning for LG-SSM\nIn this section, we brie\ufb02y discuss how to estimate the parameters of an LG-SSM. In the controltheory community, this is known as systems identi\ufb01cation (Ljung 1987).\nWhen using SSMs for time series forecasting, and also in some physical state estimation\nproblems, the observation matrix Cand the transition matrix Aare both known and \ufb01xed, by\nde\ufb01nition of the model. In such cases, all that needs to be learned are the noise covariances Q\nandR. (The initial state estimate \u03bc\n0is often less important, since it will get \u201cwashed away\u201d by\nthe data after a few time steps. This can be encouraged by setting the initial state covarianceto be large, representing a weak prior.) Although we can estimate QandRoffline, using the\nmethods described below, it is also possible to derive a recursive procedure to exactly computethe posterior p(z\nt,R,Q|y1:t), which has the form of a Normal-inverse-Wishart; see (West and\nHarrison 1997; Prado and West 2010) for details.\n18.4.1 Identi\ufb01ability and numerical stability\nIn the more general setting, where the hidden states have no pre-speci\ufb01ed meaning, we need tolearnAandC. However, in this case we can set Q=Iwithout loss of generality, since an\narbitrary noise covariance can be modeled by appropriately modifying A. Also, by analogy with\nfactor analysis, we can require Rto be diagonal without loss of generality. Doing this reduces\nthe number of free parameters and improves numerical stability.\nAnother constraint that is useful to impose is on the eigenvalues of the dynamics matrix A.\nTo see why this is important, consider the case of no system noise. In this case, the hiddenstate at time tis given by\nz\nt=Atz1=U\u039btU\u22121z1 (18.79)\nwhereUis the matrix of eigenvectors for A, and\u039b=d i a g (\u03bbi)contains the eigenvalues. If\nany\u03bbi>1, then for large t,ztwill blow up in magnitude. Consequently, to ensure stability, it\nis useful to require that all the eigenvalues are less than 1 (Siddiqi et al. 2007). Of course, if allthe eigenvalues are less than 1, then E[z\nt]=0for larget, so the state will return to the origin.\nFortunately, when we add noise, the state become non-zero, so the model does not degenerate.", "677": "18.5. Approximateonlineinferencefornon-linear,non-GaussianSSMs 647\nBelow we discuss how to estimate the parameters. However, for simplicity of presentation, we\ndo not impose any of the constraints mentioned above.\n18.4.2 Training with fully observed data\nIf we observe the hidden state sequences, we can \ufb01t the model by computing the MLEs (or even\nthe full posteriors) for the parameters by solving a multivariate linear regression problem forz\nt\u22121\u2192ztand forzt\u2192yt. That is, we can estimate Aby solving the least squares problem\nJ(A)=/summationtext2\nt=1(zt\u2212Azt\u22121)2, and similarly for C. We can estimate the system noise covariance\nQfrom the residuals in predicting ztfromzt\u22121, and estimate the observation noise covariance\nRfrom the residuals in predicting ytfromzt.\n18.4.3 EM for LG-SSM\nIf we only observe the output sequence, we can compute ML or MAP estimates of the parameters\nusing EM. The method is conceptually quite similar to the Baum-Welch algorithm for HMMs(Section 17.5), except we use Kalman smoothing instead of forwards-backwards in the E step,and use different calculations in the M step. We leave the details to Exercise 18.1.\n18.4.4 Subspace methods\nEM does not always give satisfactory results, because it is sensitive to the initial parameterestimates. One way to avoid this is to use a different approach known as a subspace method\n(Overschee and Moor 1996; Katayama 2005).\nTo understand this approach, let us initially assume there is no observation noise and no\nsystem noise. In this case, we have z\nt=Azt\u22121andyt=Czt, and hence yt=CAt\u22121z1.\nConsequently all the observations must be generated from a dim(zt)-dimensional linear mani-\nfold or subspace. We can identify this subspace using PCA (see the above references for details).Once we have an estimate of the z\nt\u2019s, we can \ufb01t the model as if it were fully observed. We can\neither use these estimates in their own right, or use them to initialize EM.\n18.4.5 Bayesian methods for \u201c\ufb01tting\u201d LG-SSMs\nThere are various offline Bayesian alternatives to the EM algorithm, including variational BayesEM (Beal 2003; Barber and Chiappa 2007) and blocked Gibbs sampling (Carter and Kohn 1994;Cappe et al. 2005; Fruhwirth-Schnatter 2007). The Bayesian approach can also be used toperform online learning, as we discussed in Section 18.2.3. Unfortunately, once we add the SSMparameters to the state space, the model is generally no longer linear Gaussian. Consequentlywe must use some of the approximate online inference methods to be discussed below.\n18.5 Approximate online inference for non-linear, non-Gaussian SSMs\nIn Section 18.3.1, we discussed how to perform exact online inference for LG-SSMs. However,many models are non linear. For example, most moving objects do not move in straight lines.And even if they did, if we assume the parameters of the model are unknown and add them", "678": "648 Chapter18. Statespacemodels\nto the state space, the model becomes nonlinear. Furthermore, non-Gaussian noise is also very\ncommon, e.g., due to outliers, or when inferring parameters for GLMs instead of just linearregression. For these more general models, we need to use approximate inference.\nThe approximate inference algorithms we discuss below approximate the posterior by a Gaus-\nsian. In general, if Y=f(X),w h e r eXhas a Gaussian distribution and fis a non-linear\nfunction, there are two main ways to approximate p(Y)by a Gaussian. The \ufb01rst is to use a\n\ufb01rst-order approximation of f. The second is to use the exact f, but to project f(X)onto the\nspace of Gaussians by moment matching. We discuss each of these methods in turn. (See alsoSection 23.5, where we discuss particle \ufb01ltering, which is a stochastic algorithm for approximateonline inference, which uses a non-parametric approximation to the posterior, which is oftenmore accurate but slower to compute.)\n18.5.1 Extended Kalman \ufb01lter (EKF)\nIn this section, we focus on non-linear models, but we assume the noise is Gaussian. That is,we consider models of the form\nz\nt=g(ut,zt\u22121)+N(0,Qt) (18.80)\nyt=h(zt)+N(0,Rt) (18.81)\nwhere the transition model gand the observation model hare nonlinear but differentiable\nfunctions. Furthermore, we focus on the case where we approximate the posterior by a singleGaussian. (The simplest way to handle more general posteriors (e.g., multi-modal, discrete, etc).is to use particle \ufb01ltering, which we discuss in Section 23.5.)\nTheextended Kalman \ufb01lter orEKFcan be applied to nonlinear Gaussian dynamical systems\nof this form. The basic idea is to linearize gandhabout the previous state estimate using\na \ufb01rst order Taylor series expansion, and then to apply the standard Kalman \ufb01lter equations.(The noise variance in the equations (Q andR) is not changed, i.e., the additional error due to\nlinearization is not modeled.) Thus we approximate the stationary non-linear dynamical systemwith a non-stationary linear dynamical system.\nThe intuition behind the approach is shown in Figure 18.9, which shows what happens when\nwe pass a Gaussian distribution p(x), shown on the bottom right, through a nonlinear function\ny=g(x), shown on the top right. The resulting distribution (approximated by Monte Carlo) is\nshown in the shaded gray area in the top left corner. The best Gaussian approximation to this,computed from E[g(x)]andvar[g(x)]by Monte Carlo, is shown by the solid black line. The\nEKF approximates this Gaussian as follows: it linearizes the gfunction at the current mode, \u03bc,\nand then passes the Gaussian distribution p(x)through this linearized function. In this example,\nthe result is quite a good approximation to the \ufb01rst and second moments of p(y), for much less\ncost than an MC approximation.\nIn more detail, the method works as follows. We approximate the measurement model using\np(y\nt|zt)\u2248N(yt|h(\u03bct|t\u22121)+Ht(yt\u2212\u03bct|t\u22121),Rt) (18.82)\nwhereHtis the Jacobian matrix of hevaluated at the prior mode:\nHij/defines\u2202hi(z)\n\u2202zj(18.83)\nHt/definesH|z=\u03bct|t\u22121(18.84)", "679": "18.5. Approximateonlineinferencefornon-linear,non-GaussianSSMs 649\np(y)yp(y)\nGaussian of p(y)Mean of p(y)EKF GaussianMean of EKF\nxy=g(x)Function g(x)Taylor approx.\nMean \u03bc\ng(\u03bc)\nxp(x)p(x)Mean \u03bc\nFigure 18.9 Nonlinear transformation of a Gaussian random variable. The prior p(x)is shown on the\nbottom right. The function y=g(x)is shown on the top right. The transformed distribution p(y)is\nshown in the top left. A linear function induces a Gaussian distribution, but a non-linear function induces\na complex distribution. The solid line is the best Gaussian approximation to this; the dotted line is the EKFapproximation to this. Source: Figure 3.4 of (Thrun et al. 2006). Used with kind permission of SebastianThrun.\nSimilarly, we approximate the system model using\np(zt|zt\u22121,ut)\u2248N(zt|g(ut,\u03bct\u22121)+Gt(zt\u22121\u2212\u03bct\u22121),Qt) (18.85)\nwhere\nGij(u)/defines\u2202gi(u,z)\n\u2202zj(18.86)\nGt/definesG(ut)|z=\u03bct\u22121(18.87)\nsoGis the Jacobian matrix of gevaluated at the prior mode.\nGiven this, we can then apply the Kalman \ufb01lter to compute the posterior as follows:\n\u03bct|t\u22121=g(ut,\u03bct\u22121) (18.88)\nVt|t\u22121=GtVt\u22121GT\nt+Qt (18.89)\nKt=Vt|t\u22121HTt(HtVt|t\u22121HTt+Rt)\u22121(18.90)\n\u03bct=\u03bct|t\u22121+Kt(yt\u2212h(\u03bct|t\u22121)) (18.91)\nVt=(I\u2212KtHt)Vt|t\u22121 (18.92)", "680": "650 Chapter18. Statespacemodels\nmean covariance sigma \npoints Actual (sampling)  Sigma-Point Linearized (EKF) \n()=yf x () ii\u03c7\u03d2=f\ntransformed \nsigma points S-P \ncovariance S-P \nmean \nyf x P P==( )    yAAT\nx\ntrue \nmean \ntrue \ncovariance fx()AAT\nxP\nFigure 18.10 An example of the unscented transform in two dimensions. Source: (Wan and der Merwe\n2001). Used with kind permission of Eric Wan.\nWe see that the only difference from the regular Kalman \ufb01lter is that, when we compute the\nstate prediction, we use g(ut,\u03bct\u22121)instead of At\u03bct\u22121+Btut, and when we compute the\nmeasurement update we use h(\u03bct|t\u22121)instead of Ct\u03bct|t\u22121.\nIt is possible to improve performance by repeatedly re-linearizing the equations around \u03bct\ninstead of \u03bct|t\u22121; this is called the iterated EKF , and yields better results, although it is of\ncourse slower.\nThere are two cases when the EKF works poorly. The \ufb01rst is when the prior covariance is\nlarge. In this case, the prior distribution is broad, so we end up sending a lot of probability\nmass through different parts of the function that are far from the mean, where the function has\nbeen linearized. The other setting where the EKF works poorly is when the function is highly\nnonlinear near the current mean. In Section 18.5.2, we will discuss an algorithm called the UKF\nwhich works better than the EKF in both of these settings.\n18.5.2 Unscented Kalman \ufb01lter (UKF)\nTheunscented Kalman \ufb01lter (UKF) is a better version of the EKF (Julier and Uhlmann 1997).\n(Apparently it is so-called because it \u201cdoesn\u2019t stink\u201d!) The key intuition is this: it is easier\nto approximate a Gaussian than to approximate a function. So instead of performing a linear\napproximation to the function, and passing a Gaussian through it, instead pass a deterministically\nchosen set of points, known as sigma points , through the function, and \ufb01t a Gaussian to the\nresulting transformed points. This is known as the unscented transform , and is sketched in\nFigure 18.10. (We explain this \ufb01gure in detail below.)", "681": "18.5. Approximateonlineinferencefornon-linear,non-GaussianSSMs 651\nThe UKF basically uses the unscented transform twice, once to approximate passing through\nthe system model g, and once to approximate passing through the measurement model h.W e\ngive the details below. Note that the UKF and EKF both perform O(d3)operations per time step\nwheredis the size of the latent state-space. However, the UKF is accurate to at least second\norder, whereas the EKF is only a \ufb01rst order approximation (although both the EKF and UKF can\nbe extended to capture higher order terms). Furthermore, the unscented transform does notrequire the analytic evaluation of any derivatives or Jacobians (a so-called derivative free \ufb01lter),\nmaking it simpler to implement and more widely applicable.\n18.5.2.1 The unscented transform\nBeforeexplainingtheUKF,we\ufb01rstexplaintheunscentedtransform. Assume p(x)=N(x|\u03bc,\u03a3),\nand consider estimating p(y),w h e r ey=f(x)for some nonlinear function f. The unscented\ntransform does this as follows. First we create a set of 2d+1sigma points x\ni, given by\nx=/parenleftBig\n\u03bc,{\u03bc+(/radicalbig\n(d+\u03bb)\u03a3):i}d\ni=1,{\u03bc\u2212(/radicalbig\n(d+\u03bb)\u03a3):i}di=1/parenrightBig\n(18.93)\nwhere\u03bb=\u03b12(d+\u03ba)\u2212dis a scaling parameter to be speci\ufb01ed below, and the notation M:i\nmeans the i\u2019th column of matrix M.\nThese sigma points are propagated through the nonlinear function to yield yi=f(xi), and\nthe mean and covariance for yis computed as follows:\n\u03bcy=2d/summationdisplay\ni=0wi\nmyi (18.94)\n\u03a3y=2d/summationdisplay\ni=0wi\nc(yi\u2212\u03bcy)(yi\u2212\u03bcy)T(18.95)\nwhere the w\u2019s are weighting terms, given by\nwi\nm=\u03bb\nd+\u03bb(18.96)\nwi\nc=\u03bb\nd+\u03bb+(1\u2212\u03b12+\u03b2) (18.97)\nwi\nm=wi\nc=1\n2(d+\u03bb)(18.98)\nSee Figure 18.10 for an illustration.\nIn general, the optimal values of \u03b1,\u03b2and\u03baare problem dependent, but when d=1, they\nare\u03b1=1,\u03b2=0,\u03ba=2. Thus in the 1d case, \u03bb=2, so the 3 sigma points are \u03bc,\u03bc+\u221a\n3\u03c3\nand\u03bc\u2212\u221a\n3\u03c3.\n18.5.2.2 The UKF algorithm\nThe UKF algorithm is simply two applications of the unscented tranform, one to compute\np(zt|y1:t\u22121,u1:t)and the other to compute p(zt|y1:t,u1:t). We give the details below.", "682": "652 Chapter18. Statespacemodels\nThe \ufb01rst step is to approximate the predictive density p(zt|y1:t\u22121,u1:t)\u2248N(zt|\u03bct,\u03a3t)by\npassing the old belief state N(zt\u22121|\u03bct\u22121,\u03a3t\u22121)through the system model gas follows:\nz0\nt\u22121=/parenleftBig\n\u03bct\u22121,{\u03bct\u22121+\u03b3(/radicalbig\n\u03a3t\u22121):i}di=1,{\u03bct\u22121\u2212\u03b3(/radicalbig\n\u03a3t\u22121):i}di=1/parenrightBig\n(18.99)\nz\u2217it=g(ut,z0it\u22121) (18.100)\n\u03bct=2d/summationdisplay\ni=0wi\nmz\u2217it(18.101)\n\u03a3t=2d/summationdisplay\ni=0wi\nc(z\u2217it\u2212\u03bct)(z\u2217it\u2212\u03bct)+Qt (18.102)\nwhere\u03b3=\u221a\nd+\u03bb.\nThe second step is to approximate the likelihood p(yt|zt)\u2248N(yt|\u02c6yt,St)by passing the\npriorN(zt|\u03bct,\u03a3t)through the observation model h:\nz0t=/parenleftbigg\n\u03bct,{\u03bct+\u03b3(/radicalBig\n\u03a3t):i}di=1,{\u03bct\u2212\u03b3(/radicalBig\n\u03a3t):i}di=1/parenrightbigg\n(18.103)\ny\u2217it=h(z0it) (18.104)\n\u02c6yt=2d/summationdisplay\ni=0wi\nmy\u2217it(18.105)\nSt=2d/summationdisplay\ni=0wi\nc(y\u2217it\u2212\u02c6yt)(y\u2217it\u2212\u02c6yt)T+Rt (18.106)\nFinally, we use Bayes rule for Gaussians to get the posterior p(zt|y1:t,u1:t)\u2248N(zt|\u03bct,\u03a3t):\n\u03a3z,y\nt=2d/summationdisplay\ni=0wi\nc(z\u2217i\nt\u2212\u03bct)(y\u2217it\u2212\u02c6yt)T(18.107)\nKt=\u03a3z,y\ntS\u22121\nt (18.108)\n\u03bct=\u03bct+Kt(yt\u2212\u02c6yt) (18.109)\n\u03a3t=\u03a3t\u2212KtStKT\nt (18.110)\n18.5.3 Assumed density \ufb01ltering (ADF)\nIn this section, we discuss inference where we perform an exact update step, but then approx-\nimate the posterior by a distribution of a certain convenient form, such as a Gaussian. Moreprecisely, let the unknowns that we want to infer be denoted by \u03b8\nt. Suppose that Qi sas e to f\ntractable distributions, e.g., Gaussians with a diagonal covariance matrix, or a product of discretedistributions. Suppose that we have an approximate prior q\nt\u22121(\u03b8t\u22121)\u2248p(\u03b8t\u22121|y1:t\u22121),w h e r e\nqt\u22121\u2208Q. We can update this with the new measurement to get the approximate posterior\n\u02c6p(\u03b8t)=1\nZtp(yt|\u03b8t)qt|t\u22121(\u03b8t) (18.111)", "683": "18.5. Approximateonlineinferencefornon-linear,non-GaussianSSMs 653\nqt\u22121qt|t\u22121\u02c6pt\nqtqt+1|t\u02c6pt+1\nqt+1PredictUpdateProject\nPredictUpdate Project\n(a)xt\u22121 xtyt\u22121 ytst\u22121 st\u03b8t\u22121 \u03b8t\n(b)\nFigure 18.11 (a) Illustration of the predict-update-project cycle of assumed density \ufb01ltering. (b) A dynam-\nical logistic regression model. Compare to Figure 18.4(a).\nwhere\nZt=/integraldisplay\np(yt|\u03b8t)qt|t\u22121(\u03b8t)d\u03b8t (18.112)\nis the normalization constant and\nqt|t\u22121(\u03b8t)=/integraldisplay\np(\u03b8t|\u03b8t\u22121)qt\u22121(\u03b8t\u22121)d\u03b8t\u22121 (18.113)\nis the one step ahead predictive distribution. If the prior is from a suitably restricted family, this\none-step update process is usually tractable. However, we often \ufb01nd that the resulting posterioris no longer in our tractable family, \u02c6p(\u03b8\nt)/negationslash\u2208Q. So after updating we seek the best tractable\napproximation by computing\nq(\u03b8t) = argmin\nq\u2208QKL(\u02c6p(\u03b8t)||q(\u03b8t)) (18.114)\nThis minimizes the the Kullback-Leibler divergence (Section 2.8.2) from the approximation q(\u03b8t)\nto the \u201cexact\u201d posterior \u02c6p(\u03b8t), and can be thought of as projecting \u02c6ponto the space of tractable\ndistributions. The whole algorithm consists of predict-update-project cycles. This is known as\nassumed density \ufb01ltering orADF(Maybeck 1979). See Figure 18.11(a) for a sketch.\nIfqis in the exponential family, one can show that this KL minimization can be done by\nmoment matching. We give some examples of this below.\n18.5.3.1 Boyen-Koller algorithm for online inference in DBNs\nIf we are performing inference in a discrete-state dynamic Bayes net (Section 17.6.7), where \u03b8tj\nis thej\u2019th hidden variable at time t, then the exact posterior p(\u03b8t)becomes intractable to\ncompute because of the entanglement problem. Suppose we use a fully factored approximation\nof the form q(\u03b8t)=/producttextD\nj=1Cat(\u03b8t,j|\u03c0t,j),w h e r e\u03c0tjk=q(\u03b8t,j=k)is the probability variable\njis in state k, andDis the number of variables. In this case, the moment matching operation\nbecomes\n\u03c0tjk=\u02c6p(\u03b8t,j=k) (18.115)", "684": "654 Chapter18. Statespacemodels\nThis can be computed by performing a predict-update step using the factored prior, and then\ncomputing the posterior marginals. This is known as the Boyen-Koller algorithm, named after\nthe authors of (Boyen and Koller 1998), who demonstrated that the error incurred by this seriesof repeated approximations remains bounded (under certain assumptions about the stochasticityof the system).\n18.5.3.2 Gaussian approximation for online inference in GLMs\nNow suppose q(\u03b8\nt)=/producttextD\nj=1N(\u03b8t,j|\u03bct,j,\u03c4t,j),w h e r e\u03c4t,jis the variance. Then the optimal\nparameters of the tractable approximation to the posterior are\n\u03bct,j=E\u02c6p[\u03b8t,j],\u03c4t,j=v a r\u02c6p[\u03b8t,j] (18.116)\nThis method can be used to do online inference for the parameters of many statistical models.\nFor example, theTrueSkill system, used in Microsoft\u2019s Xbox to rank players over time, uses thisform of approximation (Herbrich et al. 2007). We can also apply this method to simpler models,such as GLM, which have the advantage that the posterior is log-concave. Below we explain howto do this for binary logistic regression, following the presentation of (Zoeter 2007).\nThe model has the form\np(y\nt|xt,\u03b8t)=B e r ( yt|sigm(xT\nt\u03b8t)) (18.117)\np(\u03b8t|\u03b8t\u22121)=N (\u03b8t|\u03b8t\u22121,\u03c32I) (18.118)\nwhere\u03c32is some process noise which allows the parameters to change slowly over time. (This\ncan be set to 0, as in the recursive least squares method (Section 18.2.3), if desired.) We will\nassumeqt\u22121(\u03b8t\u22121)=/producttext\njN(\u03b8t\u22121,j|\u03bct\u22121,j,\u03c4t\u22121,j)is the tractable prior. We can compute the\none-step-ahead predictive density qt|t\u22121(\u03b8t)using the standard linear-Gaussian update. So now\nwe concentrate on the measurement update step.\nDe\ufb01ne the deterministic quantity st=\u03b8T\ntxt, as shown in Figure 18.11(b). If qt|t\u22121(\u03b8t)=/producttext\njN(\u03b8t,j|\u03bct|t\u22121,j,\u03c4t|t\u22121,j), then we can compute the predictive distribution for stas follows:\nqt|t\u22121(st)=N (st|mt|t\u22121,vt|t\u22121) (18.119)\nmt|t\u22121=/summationdisplay\njxt,j\u03bct|t\u22121,j (18.120)\nvt|t\u22121=/summationdisplay\njx2\nt,j\u03c4t|t\u22121,j (18.121)\nThe posterior for stis given by\nqt(st)=N (st|mt,vt) (18.122)\nmt=/integraldisplay\nst1\nZtp(yt|st)qt|t\u22121(st)dst (18.123)\nvt=/integraldisplay\ns2t1\nZtp(yt|st)qt|t\u22121(st)dst\u2212m2t(18.124)\nZt=/integraldisplay\np(yt|st)qt|t\u22121(st)dst (18.125)", "685": "18.6. Hybriddiscrete/continuousSSMs 655\nwherep(yt|st)=B e r ( yt|st). These integrals are one dimensional, and so can be computed\nusing Gaussian quadrature (see (Zoeter 2007) for details). This is the same as one step of the\nUKF algorithm.\nHaving inferred q(st), we need to compute q(\u03b8|st). This can be done as follows. De\ufb01ne \u03b4m\nas the change in the mean of stand\u03b4vas the change in the variance:\nmt=mt|t\u22121+\u03b4m,vt=vt|t\u22121+\u03b4v (18.126)\nThen one can show that the new factored posterior over the model parameters is given by\nq(\u03b8t,j)=N (\u03b8t,j|\u03bct,j,\u03c4t,j) (18.127)\n\u03bct,j=\u03bct|t\u22121,j+aj\u03b4m (18.128)\n\u03c4t,j=\u03c4t|t\u22121,j+a2\nj\u03b4v (18.129)\naj/definesxt,j\u03c4t|t\u22121,j/summationtext\nj/primex2\nt,j/prime\u03c42\nt|t\u22121,j(18.130)\nThus we see that the parameters which correspond to inputs with larger magnitude (big |xt,j|)\nor larger uncertainty (big \u03c4t|t\u22121,j) get updated most, which makes intuitive sense.\nIn (Opper 1998) a version of this algorithm is derived using a probit likelihood (see Section 9.4).\nInthiscase, themeasurementupdatecanbedoneinclosedform, withouttheneedfornumerical\nintegration. In either case, the algorithm only takes O(D)operations per time step, so it can\nbe applied to models with large numbers of parameters. And since it is an online algorithm,it can also handle massive datasets. For example (Zhang et al. 2010) use a version of thisalgorithm to \ufb01t a multi-class classi\ufb01er online to very large datasets. They beat alternative (nonBayesian) online learning algorithms, and sometimes even outperform state of the art batch(offline) learning methods such as SVMs (described in Section 14.5).\n18.6 Hybrid discrete/continuous SSMs\nMany systems contain both discrete and continuous hidden variables; these are known as hybrid\nsystems. For example, the discrete variables may indicate whether a measurement sensor isfaulty or not, or which \u201cregime\u201d the system is in. We will see some other examples below.\nA special case of a hybrid system is when we combine an HMM and an LG-SSM. This is\ncalled aswitching linear dynamical system (SLDS), a jump Markov linear system (JMLS),\nor aswitching state space model (SSSM). More precisely, we have a discrete latent variable,\nq\nt\u2208{1,...,K}, a continuous latent variable, zt\u2208RL, an continuous observed response\nyt\u2208RDand an optional continuous observed input or control ut\u2208RU. We then assume that\nthe continuous variables have linear Gaussian CPDs, conditional on the discrete states:\np(qt=k|qt\u22121=j,\u03b8)=A ij (18.131)\np(zt|zt\u22121,qt=k,ut,\u03b8)=N (zt|Akzt\u22121+Bkut,Qk) (18.132)\np(yt|zt,qt=k,ut,\u03b8)=N (yt|Ckzt+Dkut,Rk) (18.133)\nSee Figure 18.12(a) for the DGM representation.", "686": "656 Chapter18. Statespacemodels\nyt\u22121 ytzt\u22121 ztqt\u22121 qtut\u22121 ut\n(a) (b)\nFigure 18.12 A switching linear dynamical system. (a) Squares represent discrete nodes, circles represent\ncontinuous nodes. (b) Illustration of how the number of modes in the belief state grows exponentially over\ntime. We assume there are two binary states.\n18.6.1 Inference\nUnfortunately inference (i.e., state estimation) in hybrid models, including the switching LG-\nSSM model, is intractable. To see why, suppose qtis binary, but that only the dynamics\nAdepend on qt, not the observation matrix. Our initial belief state will be a mixture of\n2 Gaussians, corresponding to p(z1|y1,q1=1 )andp(z1|y1,q1=2 ). The one-step-ahead\npredictive density will be a mixture of 4 Gaussians p(z2|y1,q1=1,q2=1 ),p(z2|y1,q1=\n1,q2=2 ),p(z2|y1,q1=2,q2=1 ), andp(z2|y1,q1=2,q2=2 ), obtained by passing each of\nthe prior modes through the 2 possible transition models. The belief state at step 2 will also bea mixture of 4 Gaussians, obtained by updating each of the above distributions with y\n2.A ts t e p\n3, the belief state will be a mixture of 8 Gaussians. And so on. So we see there is an exponentialexplosion in the number of modes (see Figure 18.12(b)).\nVarious approximate inference methods have been proposed for this model, such as the\nfollowing:\n\u2022 Prune off low probability trajectories in the discrete tree; this is the basis of multiple\nhypothesis tracking (Bar-Shalom and Fortmann 1988; Bar-Shalom and Li 1993).\n\u2022 Use Monte Carlo. Essentially we just sample discrete trajectories, and apply an analytical\n\ufb01lter to the continuous variables conditional on a trajectory. See Section 23.6 for details.\n\u2022 Use ADF, where we approximate the exponentially large mixture of Gaussians with a smaller\nmixture of Gaussians. See Section 18.6.1.1 for details.\n18.6.1.1 A Gaussian sum \ufb01lter for switching SSMs\nAGaussian sum \ufb01lter (Sorenson and Alspach 1971) approximates the belief state at each step\nby a mixture of KGaussians. This can be implemented by running KKalman \ufb01lters in", "687": "18.6. Hybriddiscrete/continuousSSMs 657\nb1\nt\u22121\nb2\nt\u22121\u0000\u0000\u0000 \u0012\n@\n@@ R\n\u0000\u0000\u0000 \u0012\n@\n@@ R\nFilter 1\nFilter 2\nFilter 1\nFilter 2\n-\n-\n-\n-b1,1\nt\nb1,2\nt\nb2,1\nt\nb2,2\nt@\n@@ R\nB\nB\nB\nB\nB\nBB N\u0002\u0002\u0002\u0002\u0002\u0002\u0002 \u000e\n\u0000\u0000\u0000 \u0012\nMerge\nMerge\n-\n-b1\nt\nb2\nt\n(a)\nb1\nt\u22121\nb2\nt\u22121\n-\n-\nMerge\n-\n-\u02dcb1\nt\u22121\n\u02dcb2\nt\u22121\n-\n-\nFilter 1\nFilter 2\n-\n-b1\nt\nb2\nt\n(b)\nFigure 18.13 ADF for a switching linear dynamical system. (a) GPB2 method. (b) IMM method. See text\nfor details.\nparallel. This is particularly well suited to switching SSMs. We now describe one version of this\nalgorithm, known as the \u201csecond order generalized pseudo Bayes \ufb01lter \u201d (GPB2) (Bar-Shalom\nand Fortmann 1988). We assume that the prior belief state bt\u22121is a mixture of KGaussians,\none per discrete state:\nbi\nt\u22121/definesp(zt\u22121,qt\u22121=i|y1:t\u22121)=\u03c0t\u22121,iN(zt\u22121|\u03bct\u22121,i,\u03a3t\u22121,i) (18.134)\nWe then pass this through the Kdifferent linear models to get\nbij\nt/definesp(zt,qt\u22121=i,qt=j|y1:t)=\u03c0tijN(zt|\u03bct,ij,\u03a3t,ij) (18.135)\nwhere\u03c0tij=\u03c0t\u22121,ip(qt=j|qt\u22121=i). Finally, for each value of j, we collapse the KGaussian\nmixtures down to a single mixture to give\nbj\nt/definesp(zt,qt=j|y1:t)=\u03c0tjN(zt|\u03bct,j,\u03a3t,j) (18.136)", "688": "658 Chapter18. Statespacemodels\nSee Figure 18.13(a) for a sketch.\nThe optimal way to approximate a mixture of Gaussians with a single Gaussian is given by\nq=a r gm i n qKL(q||p),w h e r ep(z)=/summationtext\nk\u03c0kN(z|\u03bck,\u03a3k)andq(z)=N(z|\u03bc,\u03a3). This can\nbe solved by moment matching, that is,\n\u03bc=E[z]=/summationdisplay\nk\u03c0k\u03bck (18.137)\n\u03a3=c o v [z]=/summationdisplay\nk\u03c0k/parenleftbig\n\u03a3k+(\u03bck\u2212\u03bc)(\u03bck\u2212\u03bc)T/parenrightbig\n(18.138)\nIn the graphical model literature, this is called weak marginalization (Lauritzen 1992), since it\npreserves the \ufb01rst two moments. Applying these equations to our model, we can go from bij\ntto\nbjtas follows (where we drop the tsubscript for brevity):\n\u03c0j=/summationdisplay\ni\u03c0ij (18.139)\n\u03c0j|i=\u03c0ij/summationtext\nj/prime\u03c0ij/prime(18.140)\n\u03bcj=/summationdisplay\ni\u03c0j|i\u03bcij (18.141)\n\u03a3j=/summationdisplay\ni\u03c0j|i/parenleftbig\n\u03a3ij+(\u03bcij\u2212\u03bcj)(\u03bcij\u2212\u03bcj)T/parenrightbig\n(18.142)\nThis algorithm requires running K2\ufb01lters at each step. A cheaper alternative is to represent\nthe belief state by a single Gaussian, marginalizing over the discrete switch at each step. This\nis a straightforward application of ADF. An offline extension to this method, called expectation\ncorrection, is described in (Barber 2006; Mesot and Barber 2009).\nAnother heuristic approach, known as interactive multiple models orIMM(Bar-Shalom and\nFortmann 1988), can be obtained by \ufb01rst collapsing the prior to a single Gaussian (by momentmatching), and then updating it using Kdifferent Kalman \ufb01lters, one per value of q\nt. See\nFigure 18.13(b) for a sketch.\n18.6.2 Application: data association and multi-target tracking\nSuppose we are tracking Kobjects, such as airplanes, and at time t, we observe K/primedetection\nevents, e.g., \u201cblips\u201d on a radar screen. We can have K/prime<Kdue to occlusion or missed\ndetections. We can have K/prime>Kdue to clutter or false alarms. Or we can have K/prime=K.I n\nany case, we need to \ufb01gure out the correspondence between the K/primedetections ytkand theK\nobjectsztj. This is called the problem of data association, and it arises in many application\ndomains.\nFigure 18.14 gives an example in which we are tracking K=2objects. At each time step, qt\nis the unknown mapping which speci\ufb01es which objects caused which observations. It speci\ufb01esthe \u201cwiring diagram\u201d for time slice t. The standard way to solve this problem is to compute\na weight which measures the \u201ccompatibility\u201d between object jand measurement k, typically\nbased on how close kis to where the model thinks jshould be (the so-called nearest neighbor\ndata association heuristic). This gives us a K\u00d7K\n/primeweight matrix. We can make this into a", "689": "18.6. Hybriddiscrete/continuousSSMs 659\nzt\u22121,1\nzt\u22121,2zt,1\nzt,2zt+1,1\nzt+1,2\nyt\u22121,1\nyt\u22121,2\nyt\u22121,3yt,1yt+1,1\nyt+1,2\nqt\u22121 qt qt+1\nFigure 18.14 A model for tracking two objects in the presence of data-assocation ambiguity. We observe\n3, 1 and 2 detections in the \ufb01rst three time steps.\nsquare matrix of size N\u00d7N,w h e r eN = max(K,K/prime), by adding dummy background objects,\nwhich can explain all the false alarms, and adding dummy observations, which can explain all\nthe missed detections. We can then compute the maximal weight bipartite matching using theHungarian algorithm, which takes O(N\n3)time (see e.g., (Burkard et al. 2009)). Conditional\non this, we can perform a Kalman \ufb01lter update, where objects that are assigned to dummyobservations do not perform a measurement update.\nAn extension of this method, to handle a variable and/or unknown number of objects, is\nknown as multi-target tracking. This requires dealing with a variable-sized state space. There\nare many ways to do this, but perhaps the simplest and most robust methods are based onsequential Monte Carlo (e.g., (Ristic et al. 2004)) or MCMC (e.g., (Khan et al. 2006; Oh et al.2009)).\n18.6.3 Application: fault diagnosis\nConsider the model in Figure 18.15(a). This represents an industrial plant consisting of varioustanks of liquid, interconnected by pipes. In this example, we just have two tanks, for simplicity.We want to estimate the pressure inside each tank, based on a noisy measurement of the \ufb02owinto and out of each tank. However, the measurement devices can sometimes fail. Furthermore,pipes can burst or get blocked; we call this a \u201cresistance failure\u201d. This model is widely used asa benchmark in the fault diagnosis community (Mosterman and Biswas 1999).\nWe can create a probabilistic model of the system as shown in Figure 18.15(b). The square\nnodes represent discrete variables, such as measurement failures and resistance failures. Theremaining variables are continuous. A variety of approximate inference algorithms can be appliedto this model. See (Koller and Lerner 2001) for one approach, based on Rao-Blackwellized particle\ufb01ltering (which is explained in Section 23.6).", "690": "660 Chapter18. Statespacemodels\n(a)RF1\n1 RF1\n2\nR1\n1 R1\n2\nMF1\n1 MF1\n2\nF1\n1 M1\n1 F1\n2 M1\n2\nP1\n1 P1\n2\nMF12\n1 MF12\n2\nF12\n1 M12\n1 F12\n2 M12\n2\nR12\n1 R12\n2\nRF12\n1 RF12\n2\nP2\n1 P2\n2\nF2\n1 M2\n1 F2\n2 M2\n2\nMF2\n1 MF2\n2\nR2\n1 R2\n2\nRF2\n1 RF2\n2\n(b)\nFigure 18.15 (a) The two-tank system. The goal is to infer when pipes are blocked or have burst, or\nsensors have broken, from (noisy) observations of the \ufb02ow out of tank 1, F1o, out of tank 2, F2o,o r\nbetween tanks 1 and 2, F12.R1ois a hidden variable representing the resistance of the pipe out of\ntank 1,P1is a hidden variable representing the pressure in tank 1, etc. Source: Figure 11 of (Koller and\nLerner 2001) . Used with kind permission of Daphne Koller. (b) Dynamic Bayes net representation of the\ntwo-tank system. Discrete nodes are squares, continuous nodes are circles. Abbreviations: R = resistance,P = pressure, F = \ufb02ow, M = measurement, RF = resistance failure, MF = measurement failure. Based onFigure 12 of (Koller and Lerner 2001).\n18.6.4 Application: econometric forecasting\nThe switching LG-SSM model is widely used in econometric forecasting, where it is called\naregime switching model. For example, we can combine two linear trend models (see Sec-\ntion 18.2.4.2), one in which bt>0re\ufb02ects a growing economy, and one in which bt<0re\ufb02ects\na shrinking economy. See (West and Harrison 1997) for further details.\nExercises\nExercise 18.1 Derivation of EM for LG-SSM\nDerive the E and M steps for computing a (locally optimal) MLE for an LG-SSM model. Hint: the results\nare in (Ghahramani and Hinton 1996b); your task is to derive these results.\nExercise 18.2 Seasonal LG-SSM model in standard form\nWrite the seasonal model in Figure 18.7(a) as an LG-SSM. De\ufb01ne the matrices A,C,QandR.", "691": "19Undirected graphical models (Markov\nrandom \ufb01elds)\n19.1 Introduction\nIn Chapter 10, we discussed directed graphical models (DGMs), commonly known as Bayes nets.\nHowever, for some domains, being forced to choose a direction for the edges, as required bya DGM, is rather awkward. For example, consider modeling an image. We might suppose thatthe intensity values of neighboring pixels are correlated. We can create a DAG model with a 2dlattice topology as shown in Figure 19.1(a). This is known as a causal MRF or aMarkov mesh\n(Abend et al. 1965). However, its conditional independence properties are rather unnatural. Inparticular, the Markov blanket (de\ufb01ned in Section 10.5) of the node X\n8in the middle is the other\ncolored nodes (3, 4, 7, 9, 12 and 13) rather than just its 4 nearest neighbors as one might expect.\nAn alternative is to use an undirected graphical model (UGM), also called a Markov random\n\ufb01eld(MRF)o r Markov network. These do not require us to specify edge orientations, and are\nmuch more natural for some problems such as image analysis and spatial statistics. For example,an undirected 2d lattice is shown in Figure 19.1(b); now the Markov blanket of each node is justits nearest neighbors, as we show in Section 19.2.\nRoughly speaking, the main advantages of UGMs over DGMs are: (1) they are symmetric and\ntherefore more \u201cnatural\u201d for certain domains, such as spatial or relational data; and (2) discrimi-nativel UGMs (aka conditional random \ufb01elds, or CRFs), which de\ufb01ne conditional densities of theformp(y|x), work better than discriminative DGMs, for reasons we explain in Section 19.6.1. The\nmain disadvantages of UGMs compared to DGMs are: (1) the parameters are less interpretableand less modular, for reasons we explain in Section 19.3; and (2) parameter estimation is com-putationally more expensive, for reasons we explain in Section 19.5. See (Domke et al. 2008) foran empirical comparison of the two approaches for an image processing task.\n19.2 Conditional independence properties of UGMs\n19.2.1 Key properties\nUGMs de\ufb01ne CI relationships via simple graph separation as follows: for sets of nodes A,B,\nandC,w es a y xA\u22a5GxB|xCiffCseparates AfromBin the graph G. This means that,\nwhen we remove all the nodes in C, if there are no paths connecting any node in Ato any\nnode inB, then the CI property holds. This is called the global Markov property for UGMs.\nFor example, in Figure 19.2(b), we have that {1,2}\u22a5{6,7}|{3,4,5}.", "692": "662 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nX1 X2 X3 X4 X5\nX6 X7 X8 X9 X10\nX11 X12 X13 X14 X15\nX16 X17 X18 X19 X20\n(a)X1 X2 X3 X4 X5\nX6 X7 X8 X9 X10\nX11 X12 X13 X14 X15\nX16 X17 X18 X19 X20\n(b)\nFigure 19.1 (a) A 2d lattice represented as a DAG. The dotted red node X8is independent of all other\nnodes (black) given its Markov blanket, which include its parents (blue), children (green) and co-parents\n(orange). (b) The same model represented as a UGM. The red node X8is independent of the other black\nnodes given its neighbors (blue nodes).\n12\n354\n67\n(a)12\n354\n67\n(b)\nFigure 19.2 (a) A DGM. (b) Its moralized version, represented as a UGM.\nThe set of nodes that renders a node tconditionally independent of all the other nodes in\nthe graph is called t\u2019sMarkov blanket; we will denote this by mb(t). Formally, the Markov\nblanket satis\ufb01es the following property:\nt\u22a5V\\cl(t)|mb(t) (19.1)\nwherecl(t)/definesmb(t)\u222a{t}is theclosureof nodet. One can show that, in a UGM, a node\u2019s\nMarkov blanket is its set of immediate neighbors. This is called the undirected local Markov\nproperty. For example, in Figure 19.2(b), we have mb(5) = {2,3,4,6}.\nFrom the local Markov property, we can easily see that two nodes are conditionally indepen-\ndent given the rest if there is no direct edge between them. This is called the pairwise Markov\nproperty. In symbols, this is written as\ns\u22a5t|V \\{s,t}\u21d0 \u21d2G st=0 (19.2)\nUsingthethreeMarkovpropertieswehavediscussed, wecanderivethefollowingCIproperties\n(amongst others) from the UGM in Figure 19.2(b):\n\u2022Pairwise 1\u22a57|rest\n\u2022Local1\u22a5rest|2,3", "693": "19.2. ConditionalindependencepropertiesofUGMs 663\nGLP\np(x)>0\nFigure 19.3 Relationship between Markov properties of UGMs.\n12\n354\n(a)12\n354\n(b)\nFigure 19.4 (a) The ancestral graph induced by the DAG in Figure 19.2(a) wrt U={2,4,5}. (b) The\nmoralized version of (a).\n\u2022Global1,2\u22a56,7|3,4,5\nIt is obvious that global Markov implies local Markov which implies pairwise Markov. What is\nless obvious, but nevertheless true (assuming p(x)>0for allx, i.e., that pis a positive density),\nis that pairwise implies global, and hence that all these Markov properties are the same, as\nillustrated in Figure 19.3 (see e.g., (Koller and Friedman 2009, p119) for a proof).1The importance\nof this result is that it is usually easier to empirically assess pairwise conditional independence;such pairwise CI statements can be used to construct a graph from which global CI statementscan be extracted.\n19.2.2 An undirected alternative to d-separation\nWe have seen that determinining CI relationships in UGMs is much easier than in DGMs, becausewe do not have to worry about the directionality of the edges. In this section, we show how todetermine CI relationships for a DGM using a UGM.\nIt is tempting to simply convert the DGM to a UGM by dropping the orientation of the edges,\nbut this is clearly incorrect, since a v-structure A\u2192B\u2190Chas quite different CI properties\nthan the corresponding undirected chain A\u2212B\u2212C. The latter graph incorrectly states that\nA\u22a5C|B. To avoid such incorrect CI statements, we can add edges between the \u201cunmarried\u201d\nparentsAandC, and then drop the arrows from the edges, forming (in this case) a fully\nconnected undirected graph. This process is called moralization. Figure 19.2(b) gives a larger\n1. The restriction to positive densities arises because deterministic constraints can result in independencies present in\nthe distribution that are not explicitly represented in the graph. See e.g., (Koller and Friedman 2009, p120) for some\nexamples. Distributions with non-graphical CI properties are said to be unfaithful to the graph, so I(p)/negationslash=I(G).", "694": "664 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\n3UREDELOLVWLF\u00030RGHOV\n*UDSKLFDO\u00030RGHOV\n'LUHFWHG 8QGLUHFWHG &KRUGDO\nFigure 19.5 DGMs and UGMs can perfectly represent different sets of distributions. Some distributions\ncan be perfectly represented by either DGMs or UGMs; the corresponding graph must be chordal.\nexample of moralization: we interconnect 2 and 3, since they have a common child 5, and we\ninterconnect 4, 5 and 6, since they have a common child 7.\nUnfortunately, moralization loses some CI information, and therefore we cannot use the\nmoralized UGM to determine CI properties of the DGM. For example, in Figure 19.2(a), usingd-separation, we see that 4\u22a55|2. Adding a moralization arc 4\u22125would lose this fact (see\nFigure 19.2(b)). However, notice that the 4-5 moralization edge, due to the common child 7,is not needed if we do not observe 7 or any of its descendants. This suggests the followingapproach to determining if A\u22a5B|C. First we form the ancestral graph of DAGGwith respect\ntoU=A\u222aB\u222aC. This means we remove all nodes from Gthat are not in Uor are not\nancestors of U. We then moralize this ancestral graph, and apply the simple graph separation\nrules for UGMs. For example, in Figure 19.4(a), we show the ancestral graph for Figure 19.2(a)usingU={2,4,5}. In Figure 19.4(b), we show the moralized version of this graph. It is clear\nthat we now correctly conclude that 4\u22a55|2.\n19.2.3 Comparing directed and undirected graphical models\nWhich model has more \u201cexpressive power\u201d, a DGM or a UGM? To formalize this question, recallthat we say that Gis an I-map of a distribution pifI(G)\u2286I(p). Now de\ufb01ne Gto be\nperfect map ofpifI(G)=I(p), in other words, the graph can represent all (and only) the CI\nproperties of the distribution. It turns out that DGMs and UGMs are perfect maps for differentsets of distributions (see Figure 19.5). In this sense, neither is more powerful than the other asa representation language.\nAs an example of some CI relationships that can be perfectly modeled by a DGM but not a\nUGM, consider a v-structure A\u2192C\u2190B. This asserts that A\u22a5B, andA/negationslash\u22a5B|C.I fw ed r o p\nthe arrows, we get A\u2212C\u2212B, which asserts A\u22a5B|CandA/negationslash\u22a5B, which is incorrect. In fact,\nthere is no UGM that can precisely represent all and only the two CI statements encoded by a v-structure. In general, CI properties in UGMs are monotonic, in the following sense: if A\u22a5B|C,\nthenA\u22a5B|(C\u222aD). But in DGMs, CI properties can be non-monotonic, since conditioning", "695": "19.3. ParameterizationofMRFs 665\nC\n(a) (b) (c)ADB B DA\nCB DA\nC\nFigure 19.6 A UGM and two failed attempts to represent it as a DGM. Source: Figure 3.10 of (Koller and\nFriedman 2009). Used with kind permission of Daphne Koller.\non extra variables can eliminate conditional independencies due to explaining away.\nAs an example of some CI relationships that can be perfectly modeled by a UGM but not a\nDGM, consider the 4-cycle shown in Figure 19.6(a). One attempt to model this with a DGM is\nshown in Figure 19.6(b). This correctly asserts that A\u22a5C|B,D. However, it incorrectly asserts\nthatB\u22a5D|A. Figure 19.6(c) is another incorrect DGM: it correctly encodes A\u22a5C|B,D, but\nincorrectly encodes B\u22a5D. In fact there is no DGM that can precisely represent all and only\nthe CI statements encoded by this UGM.\nSome distributions can be perfectly modeled by either a DGM or a UGM; the resulting graphs\nare called decomposable orchordal. Roughly speaking, this means the following: if we collapse\ntogether all the variables in each maximal clique, to make \u201cmega-variables\u201d, the resulting graphwill be a tree. Of course, if the graph is already a tree (which includes chains as a special case),it will be chordal. See Section 20.4.1 for further details.\n19.3 Parameterization of MRFs\nAlthough the CI properties of UGM are simpler and more natural than for DGMs, representingthe joint distribution for a UGM is less natural than for a DGM, as we see below.\n19.3.1 The Hammersley-Clifford theorem\nSince there is no topological ordering associated with an undirected graph, we can\u2019t use the chainrule to represent p(y). So instead of associating CPDs with each node, we associate potential\nfunctionso rfactors with each maximal clique in the graph. We will denote the potentialfunction for clique cby\u03c8\nc(yc|\u03b8c). A potential function can be any non-negative function of\nits arguments. The joint distribution is then de\ufb01ned to be proportional to the product of cliquepotentials. Rather surprisingly, one can show that any positive distribution whose CI propertiescan be represented by a UGM can be represented in this way. We state this result more formallybelow.", "696": "666 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nTheorem 19.3.1 (Hammersley-Clifford). A positive distribution p(y)>0satis\ufb01es the CI prop-\nerties of an undirected graph Giffpcan be represented as a product of factors, one per maximal\nclique,i.e.,\np(y|\u03b8)=1\nZ(\u03b8)/productdisplay\nc\u2208C\u03c8c(yc|\u03b8c) (19.3)\nwhereCisthesetofallthe(maximal)cliquesof G,andZ(\u03b8)isthepartition function givenby\nZ(\u03b8)/defines/summationdisplay\nx/productdisplay\nc\u2208C\u03c8c(yc|\u03b8c) (19.4)\nNotethatthepartitionfunctioniswhatensurestheoveralldistributionsumsto1.2\nThe proof was never published, but can be found in e.g., (Koller and Friedman 2009).\nFor example, consider the MRF in Figure 10.1(b). If psatis\ufb01es the CI properties of this graph\nthen we can write pas follows:\np(y|\u03b8)=1\nZ(\u03b8)\u03c8123(y1,y2,y3)\u03c8234(y2,y3,y4)\u03c835(y3,y5) (19.5)\nwhere\nZ=/summationdisplay\ny\u03c8123(y1,y2,y3)\u03c8234(y2,y3,y4)\u03c835(y3,y5) (19.6)\nThere is a deep connection between UGMs and statistical physics. In particular, there is a\nmodel known as the Gibbs distribution, which can be written as follows:\np(y|\u03b8)=1\nZ(\u03b8)exp(\u2212/summationdisplay\ncE(yc|\u03b8c)) (19.7)\nwhereE(yc)>0is the energy associated with the variables in clique c. We can convert this to\na UGM by de\ufb01ning\n\u03c8c(yc|\u03b8c)=e x p (\u2212E(yc|\u03b8c)) (19.8)\nWe see that high probability states correspond to low energy con\ufb01gurations. Models of this formare known as energy based models, and are commonly used in physics and biochemistry, as\nwell as some branches of machine learning (LeCun et al. 2006).\nNote that we are free to restrict the parameterization to the edges of the graph, rather than\nthe maximal cliques. This is called a pairwise MRF. In Figure 10.1(b), we get\np(y|\u03b8)\u221d\u03c8\n12(y1,y2)\u03c813(y1,y3)\u03c823(y2,y3)\u03c824(y2,y4)\u03c834(y3,y4)\u03c835(y3,y5)(19.9)\n\u221d/productdisplay\ns\u223ct\u03c8st(ys,yt) (19.10)\nThis form is widely used due to its simplicity, although it is not as general.\n2. The partition function is denoted by Zbecause of the German word zustandssumme, which means \u201csum over states\u201d.\nThis re\ufb02ects the fact that a lot of pioneering working in statistical physics was done by Germans.", "697": "19.3. ParameterizationofMRFs 667\n19.3.2 Representing potential functions\nIf the variables are discrete, we can represent the potential or energy functions as tables of\n(non-negative) numbers, just as we did with CPTs. However, the potentials are not probabilities.Rather, they represent the relative \u201ccompatibility\u201d between the different assignments to thepotential. We will see some examples of this below.\nA more general approach is to de\ufb01ne the log potentials as a linear function of the parameters:\nlog\u03c8\nc(yc)/defines\u03c6c(yc)T\u03b8c (19.11)\nwhere\u03c6c(xc)is a feature vector derived from the values of the variables yc. The resulting log\nprobability has the form\nlogp(y|\u03b8)=/summationdisplay\nc\u03c6c(yc)T\u03b8c\u2212Z(\u03b8) (19.12)\nThis is also known as a maximum entropy or alog-linear model.\nFor example, consider a pairwise MRF, where for each edge, we associate a feature vector of\nlengthK2as follows:\n\u03c6st(ys,yt)=[...,I(ys=j,yt=k),...] (19.13)\nIf we have a weight for each feature, we can convert this into a K\u00d7Kpotential function as\nfollows:\n\u03c8st(ys=j,yt=k)=e x p ( [ \u03b8T\nst\u03c6st]jk)=e x p (\u03b8st(j,k)) (19.14)\nSo we see that we can easily represent tabular potentials using a log-linear form. But the\nlog-linear form is more general.\nTo see why this is useful, suppose we are interested in making a probabilistic model of English\nspelling. Since certain letter combinations occur together quite frequently (e.g., \u201cing\u201d), we willneed higher order factors to capture this. Suppose we limit ourselves to letter trigrams. Atabular potential still has 26\n3=1 7,576parameters in it. However, most of these triples will\nnever occur.\nAn alternative approach is to de\ufb01ne indicator functions that look for certain \u201cspecial\u201d triples,\nsuch as \u201cing\u201d, \u201cqu-\u201d, etc. Then we can de\ufb01ne the potential on each trigram as follows:\n\u03c8(yt\u22121,yt,yt+1)=e x p (/summationdisplay\nk\u03b8k\u03c6k(yt\u22121,yt,yt+1)) (19.15)\nwherekindexes the different features, corresponding to \u201cing\u201d, \u201cqu-\u201d, etc., and \u03c6kis the corre-\nsponding binary feature function. By tying the parameters across locations, we can de\ufb01ne the\nprobability of a word of any length using\np(y|\u03b8)\u221dexp(/summationdisplay\nt/summationdisplay\nk\u03b8k\u03c6k(yt\u22121,yt,yt+1)) (19.16)\nThis raises the question of where these feature functions come from. In many applications,they are created by hand to re\ufb02ect domain knowledge (we will see examples later), but it is alsopossible to learn them from data, as we discuss in Section 19.5.6.", "698": "668 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\n19.4 Examples of MRFs\nIn this section, we show how several popular probability models can be conveniently expressed\nas UGMs.\n19.4.1 Ising model\nTheIsing model is an example of an MRF that arose from statistical physics.3It was originally\nused for modeling the behavior of magnets. In particular, let ys\u2208{ \u22121,+1}represent the spin\nof an atom, which can either be spin down or up. In some magnets, called ferro-magnets,\nneighboring spins tend to line up in the same direction, whereas in other kinds of magnets,calledanti-ferromagnets, the spins \u201cwant\u201d to be different from their neighbors.\nWe can model this as an MRF as follows. We create a graph in the form of a 2D or 3D lattice,\nand connect neighboring variables, as in Figure 19.1(b). We then de\ufb01ne the following pairwiseclique potential:\n\u03c8\nst(ys,yt)=/parenleftbiggewste\u2212wst\ne\u2212wstewst/parenrightbigg\n(19.17)\nHerewstis the coupling strength between nodes sandt. If two nodes are not connected in\nthe graph, we set wst=0. We assume that the weight matrix Wis symmetric, so wst=wts.\nOften we assume all edges have the same strength, so wst=J(assuming wst/negationslash=0).\nIf all the weights are positive, J>0, then neighboring spins are likely to be in the same\nstate; this can be used to model ferromagnets, and is an example of an associative Markov\nnetwork. If the weights are sufficiently strong, the corresponding probability distribution willhave two modes, corresponding to the all +1\u2019s state and the all -1\u2019s state. These are called theground states of the system.\nIf all of the weights are negative, J<0, then the spins want to be different from their\nneighbors; this can be used to model an anti-ferromagnet, and results in a frustrated system,\nin which not all the constraints can be satis\ufb01ed at the same time. The corresponding probabilitydistribution will have multiple modes. Interestingly, computing the partition function Z(J)can\nbe done in polynomial time for associative Markov networks, but is NP-hard in general (Cipra2000).\nThere is an interesting analogy between Ising models and Gaussian graphical models. First,\nassuming y\nt\u2208{ \u22121,+1}, we can write the unnormalized log probability of an Ising model as\nfollows:\nlog \u02dcp(y)=\u2212/summationdisplay\ns\u223ctyswstyt=\u22121\n2yTWy (19.18)\n(The factor of1\n2arises because we sum each edge twice.) If wst=J>0, we get a low energy\n(and hence high probability) if neighboring states agree.\nSometimes there is an external \ufb01eld, which is an energy term which is added to each spin.\nThis can be modelled using a local energy term of the form \u2212bTy,w h e r ebis sometimes called\n3. Ernst Ising was a German-American physicist, 1900\u20131998.", "699": "19.4. ExamplesofMRFs 669\nabias term. The modi\ufb01ed distribution is given by\nlog \u02dcp(y)=/summationdisplay\ns\u223ctwstysyt+/summationdisplay\nsbsys=1\n2yTWy+bTy (19.19)\nwhere\u03b8=(W,b).\nIf we de\ufb01ne \u03bc/defines\u22121\n2\u03a3\u22121b,\u03a3\u22121=\u2212W, andc/defines1\n2\u03bcT\u03a3\u22121\u03bc, we can rewrite this in a form\nthat looks similar to a Gaussian:\n\u02dcp(y)\u221dexp(\u22121\n2(y\u2212\u03bc)T\u03a3\u22121(y\u2212\u03bc)+c) (19.20)\nOne very important difference is that, in the case of Gaussians, the normalization constant,\nZ=|2\u03c0\u03a3|, requires the computation of a matrix determinant, which can be computed in\nO(D3)time, whereas in the case of the Ising model, the normalization constant requires\nsumming over all 2Dbit vectors; this is equivalent to computing the matrix permanent, which\nis NP-hard in general (Jerrum et al. 2004).\n19.4.2 Hop\ufb01eld networks\nAHop\ufb01eld network (Hop\ufb01eld 1982) is a fully connected Ising model with a symmetric weight\nmatrix,W=WT. These weights, plus the bias terms b, can be learned from training data\nusing (approximate) maximum likelihood, as described in Section 19.5.4\nThe main application of Hop\ufb01eld networks is as an associative memory orcontent ad-\ndressable memory. The idea is this: suppose we train on a set of fully observed bit vectors,corresponding to patterns we want to memorize. Then, at test time, we present a partial patternto the network. We would like to estimate the missing variables; this is called pattern com-\npletion. See Figure 19.7 for an example. This can be thought of as retrieving an example frommemory based on a piece of the example itself, hence the term \u201cassociative memory\u201d.\nSince exact inference is intractable in this model, it is standard to use a coordinate descent\nalgorithm known as iterative conditional modes (ICM), which just sets each node to its most\nlikely (lowest energy) state, given all its neighbors. The full conditional can be shown to be\np(y\ns=1|y\u2212s,\u03b8) = sigm(wT\ns,:y\u2212s+bs) (19.21)\nPicking the most probable state amounts to using the rule y\u2217\ns=1if/summationtext\ntwstyt>bsand using\ny\u2217\ns=0otherwise. (Much better inference algorithms will be discussed later in this book.)\nSince inference is deterministic, it is also possible to interpret this model as a recurrent\nneural network. (This is quite different from the feedforward neural nets studied in Section 16.5;they are univariate conditional density models of the form p(y|x,\u03b8)which can only be used for\nsupervised learning.) See Hertz et al. (1991) for further details on Hop\ufb01eld networks.\nABoltzmann machine generalizes the Hop\ufb01eld / Ising model by including some hidden\nnodes, which makes the model representationally more powerful. Inference in such modelsoften uses Gibbs sampling, which is a stochastic version of ICM (see Section 24.2 for details).\n4. ML estimation works much better than the outer product rule proposed in in (Hop\ufb01eld 1982), because it not only\nlowers the energy of the observed patterns, but it also raises the energy of the non-observed patterns, in order to make\nthe distribution sum to one (Hillar et al. 2012).", "700": "670 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nFigure 19.7 Examples of how an associative memory can reconstruct images. These are binary images\nof size50\u00d750pixels. Top: training images. Row 2: partially visible test images. Row 3: estimate after\n5 iterations. Bottom: \ufb01nal state estimate. Based on Figure 2.1 of Hertz et al. (1991). Figure generated by\nhopfieldDemo .\n(a)\n (b)\n (c)\nFigure 19.8 Visualizing a sample from a 10-state Potts model of size 128\u00d7128for different association\nstrengths: (a) J=1.42, (b)J=1.44, (c)J=1.46. The regions are labeled according to size: blue is\nlargest, red is smallest. Used with kind permission of Erik Sudderth. See gibbsDemoIsing for Matlab\ncode to produce a similar plot for the Ising model.\nHowever, we could equally well apply Gibbs to a Hop\ufb01eld net and ICM to a Boltzmann machine:\nthe inference algorithm is not part of the model de\ufb01nition. See Section 27.7 for further details\non Boltzmann machines.", "701": "19.4. ExamplesofMRFs 671\nxs xtys yt\nFigure 19.9 A grid-structured MRF with local evidence nodes.\n19.4.3 Potts model\nIt is easy to generalize the Ising model to multiple discrete states, yt\u2208{1,2,...,K}.I t i s\ncommon to use a potential function of the following form:\n\u03c8st(ys,yt)=\u239b\n\u239deJ00\n0eJ0\n00 eJ\u239e\u23a0 (19.22)\nThis is called the Potts model.\n5IfJ>0, then neighboring nodes are encouraged to have the\nsame label. Some samples from this model are shown in Figure 19.8. We see that for J>1.44,\nlarge clusters occur, for J<1.44, many small clusters occur, and at the critical value of\nK=1.44, there is a mix of small and large clusters. This rapid change in behavior as we vary\na parameter of the system is called a phase transition, and has been widely studied in the\nphysics community. An analogous phenomenon occurs in the Ising model; see (MacKay 2003,\nch 31) for details.\nThe Potts model can be used as a prior for image segmentation, since it says that neighboring\npixels are likely to have the same discrete label and hence belong to the same segment. We cancombine this prior with a likelihood term as follows:\np(y,x|\u03b8)=p(y|J)/productdisplay\ntp(xt|yt,\u03b8)=/bracketleftBigg\n1\nZ(J)/productdisplay\ns\u223ct\u03c8(ys,yt;J)/bracketrightBigg/productdisplay\ntp(xt|yt,\u03b8) (19.23)\nwherep(xt|yt=k,\u03b8)is the probability of observing pixel xtgiven that the corresponding\nsegment belongs to class k. This observation model can be modeled using a Gaussian or a\nnon-parametric density. (Note that we label the hidden nodes ytand the observed nodes xt,t o\nbe compatible with Section 19.6.)\nThe corresponding graphical model is a mix of undirected and directed edges, as shown in\nFigure 19.9. The undirected 2d lattice represents the prior p(y); in addition, there are directed\nedge from each ytto its corresponding xt, representing the local evidence. Technically speak-\ning, this combination of an undirected and directed graph is called a chain graph. However,\n5. Renfrey Potts was an Australian mathematician, 1925\u20132005.", "702": "672 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nsince the xtnodes are observed, they can be \u201cabsorbed\u201d into the model, thus leaving behind an\nundirected \u201cbackbone\u201d.\nThis model is a 2d analog of an HMM, and could be called a partially observed MRF.A s\nin an HMM, the goal is to perform posterior inference, i.e., to compute (some function of)\np(y|x,\u03b8). Unfortunately, the 2d case is provably much harder than the 1d case, and we must\nresort to approximate methods, as we discuss in later chapters.\nAlthough the Potts prior is adequate for regularizing supervised learning problems, it is not\nsufficiently accurate to perform image segmentation in an unsupervised way, since the segmentsproduced by this model do not accurately represent the kinds of segments one sees in naturalimages (Morris et al. 1996).\n6For the unsupervised case, one needs to use more sophisticated\npriors, such as the truncated Gaussian process prior of (Sudderth and Jordan 2008).\n19.4.4 Gaussian MRFs\nAn undirected GGM, also called a Gaussian MRF (see e.g., (Rue and Held 2005)), is a pairwise\nMRF of the following form:\np(y|\u03b8)\u221d/productdisplay\ns\u223ct\u03c8st(ys,yt)/productdisplay\nt\u03c8t(yt) (19.24)\n\u03c8st(ys,yt)=e x p ( \u22121\n2ys\u039bstyt) (19.25)\n\u03c8t(yt)=e x p ( \u22121\n2\u039btty2\nt+\u03b7tyt) (19.26)\n(Note that we could easily absorb the node potentials \u03c8tinto the edge potentials, but we have\nkept them separate for clarity.) The joint distribution can be written as follows:\np(y|\u03b8)\u221dexp[\u03b7Ty\u22121\n2yT\u039by] (19.27)\nWe recognize this as a multivariate Gaussian written in information form where\u039b=\u03a3\u22121and\n\u03b7=\u039b\u03bc.\nIf\u039bst=0, then there is no pairwise term connecting sandt, so by the factorization theorem\n(Theorem 2.2.1), we conclude that\nys\u22a5yt|y\u2212(st)\u21d0\u21d2\u039bst=0 (19.28)\nThe zero entries in \u039bare called structural zeros, since they represent the absent edges in the\ngraph. Thus undirected GGMs correspond to sparse precision matrices, a fact which we exploitin Section 26.7.2 to efficiently learn the structure of the graph.\n19.4.4.1 Comparing Gaussian DGMs and UGMs *\nIn Section 10.2.5, we saw that directed GGMs correspond to sparse regression matrices, and hencesparse Cholesky factorizations of covariance matrices, whereas undirected GGMs correspond to\n6. An in\ufb02uential paper (Geman and Geman 1984), which introduced the idea of a Gibbs sampler (Section 24.2), proposed\nusing the Potts model as a prior for image segmentation, but the results in their paper are misleading because they did\nnot run their Gibbs sampler for long enough. See Figure 24.10 for a vivid illustration of this point.", "703": "19.4. ExamplesofMRFs 673\nFigure 19.10 A VAR(2) process represented as a dynamic chain graph. Source: (Dahlhaus and Eichler\n2000). Used with kind permission of Rainer Dahlhaus and Oxford University Press.\nsparse precision matrices. The advantage of the DAG formulation is that we can make the\nregression weights W, and hence \u03a3, be conditional on covariate information (Pourahmadi 2004),\nwithout worrying about positive de\ufb01nite constraints. The disadavantage of the DAG formulation\nis its dependence on the order, although in certain domains, such as time series, there is already\na natural ordering of the variables.\nIt is actually possible to combine both representations, resulting in a Gaussian chain graph.\nFor example, consider a a discrete-time, second-order Markov chain in which the states are\ncontinuous, yt\u2208RD. The transition function can be represented as a (vector-valued) linear-\nGaussian CPD:\np(yt|yt\u22121,yt\u22122,\u03b8)=N(yt|A1yt\u22121+A2yt\u22122,\u03a3) (19.29)\nThis is called vector auto-regressive orVARprocess of order 2. Such models are widely used\nin econometrics for time-series forecasting.\nThe time series aspect is most naturally modeled using a DGM. However, if \u03a3\u22121is sparse,\nthen the correlation amongst the components within a time slice is most naturally modeled\nusing a UGM. For example, suppose we have\nA1=\u239b\n\u239c\u239c\u239c\u239c\u239d3\n501\n500\n03\n50\u22121\n50\n2\n51\n33\n500\n000 \u22121\n21\n5\n001\n502\n5\u239e\n\u239f\u239f\u239f\u239f\u23a0,A2=\u239b\n\u239c\u239c\u239c\u239c\u239d00\u22121\n500\n0 0000\n0 0000\n001\n501\n3\n0 000 \u22121\n5\u239e\n\u239f\u239f\u239f\u239f\u23a0(19.30)\nand\n\u03a3=\u239b\n\u239c\u239c\u239c\u239c\u239d11\n21\n300\n1\n21\u22121\n300\n1\n3\u22121\n310 0\n00 01 0\n00 00 1\u239e\n\u239f\u239f\u239f\u239f\u23a0,\u03a3\u22121=\u239b\n\u239c\u239c\u239c\u239c\u239d2.13\u22121.47\u22121.200\n\u22121.47 2.13 1.20 0\n\u22121.21.21.80 0\n00 0 1 0\n00 0 0 1\u239e\n\u239f\u239f\u239f\u239f\u23a0(19.31)", "704": "674 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nx1 x2 x3\n(a)\nx1 x2 x3w1 w2\n(b)\nFigure 19.11 (a) A bi-directed graph. (b) The equivalent DAG. Here the wnodes are latent confounders.\nBased on Figures 5.12-5.13 of (Choi 2011). Used with kind permission of Myung Choi.\nThe resulting graphical model is illustrated in Figure 19.10. Zeros in the transition matrices A1\nandA2correspond to absent directed arcs from yt\u22121andyt\u22122intoyt. Zeros in the precision\nmatrix\u03a3\u22121correspond to absent undirected arcs between nodes in yt.\nSometimes we have a sparse covariance matrix rather than a sparse precision matrix. This can\nbe represented using a bi-directed graph, where each edge has arrows in both directions, as in\nFigure 19.11(a). Here nodes that are not connected are unconditionally independent. For example\nin Figure 19.11(a) we see that Y1\u22a5Y3. In the Gaussian case, this means \u03a31,3=\u03a33,1=0.( A\ngraph representing a sparse covariance matrix is called a covariance graph.) By contrast, if\nthis were an undirected model, we would have that Y1\u22a5Y3|Y2, and\u039b1,3=\u039b3,1=0,w h e r e\n\u039b=\u03a3\u22121.\nA bidirected graph can be converted to a DAG with latent variables, where each bidirected\nedge is replaced with a hidden variable representing a hidden common cause, or confounder,\nas illustrated in Figure 19.11(b). The relevant CI properties can then be determined using d-separation.\nWe can combine bidirected and directed edges to get a directed mixed graphical model.\nThis is useful for representing a variety of models, such as ARMA models (Section 18.2.4.4),structural equation models (Section 26.5.5), etc.\n19.4.5 Markov logic networks *\nIn Section 10.2.2, we saw how we could \u201cunroll\u201d Markov models and HMMs for an arbitrarynumber of time steps in order to model variable-length sequences. Similarly, in Section 19.4.1,we saw how we could expand a lattice UGM to model images of any size. What about morecomplex domains, where we have a variable number of objects and relationships between them?Creating models for such scenarios is often done using \ufb01rst-order logic (see e.g., (Russell and\nNorvig 2010)). For example, consider the sentences \u201cSmoking causes cancer\u201d and \u201cIf two peopleare friends, and one smokes, then so does the other\u201d. We can write these sentences in \ufb01rst-order", "705": "19.4. ExamplesofMRFs 675\nFriends(A,A) Smokes(A) Smokes(B) Friends(B,B)Friends(A,B)\nFriends(B,A)Cancer(A) Cancer(B)\nFigure 19.12 An example of a ground Markov logic network represented as a pairwise MRF for 2 people.\nBased on Figure 2.1 from (Domingos and Lowd 2009). Used with kind permission of Pedro Domingos.\nlogic as follows:\n\u2200x.Sm(x)=\u21d2Ca(x) (19.32)\n\u2200x.\u2200y.Fr(x,y)\u2227Sm(x)=\u21d2Sm(y) (19.33)\nwhereSmandCaare predicates, and Fris a relation.7\nOf course, such rules are not always true. Indeed, this brittleness is the main reason why\nlogical approaches to AI are no longer widely used, at least not in their pure form. There\nhave been a variety of attempts to combine \ufb01rst order logic with probability theory, an areaknown as statistical relational AI orprobabilistic relational modeling (Kersting et al. 2011).\nOne simple approach is to take logical rules and attach weights (known as certainty factors)t o\nthem, and then to interpret them as conditional probability distributions. For example, we mightsayp(Ca(x)=1|Sm(x)=1 )=0 .9. Unfortunately, the rule does not say what to predict if\nSm(x)=0. Furthermore, combining CPDs in this way is not guaranteed to de\ufb01ne a consistent\njoint distribution, because the resulting graph may not be a DAG.\nAn alternative approach is to treat these rules as a way of de\ufb01ning potential functions in an\nunrolled UGM. The result is known as a Markov logic network (Domingos and Lowd 2009).\nTo specify the network, we \ufb01rst rewrite all the rules in conjunctive normal form (CNF), also\nknown as clausal form. In this case, we get\n\u00acSm(x)\u2228Ca(x) (19.34)\n\u00acFr(x,y)\u2228\u00acSm(x)\u2228Sm(y) (19.35)\nThe \ufb01rst clause can be read as \u201cEither xdoes not smoke or he has cancer\u201d, which is logically\nequivalent to Equation 19.32. (Note that in a clause, any unbound variable, such as x, is assumed\nto be universally quanti\ufb01ed.)\n7. A predicate is just a function of one argument, known as an object, that evaluates to true or false, depending on\nwhether the property holds or not of that object. A (logical) relation is just a function of two or more arguments (objects)\nthat evaluates to true or false, depending on whether the relationship holds between that set of objects or not.", "706": "676 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nInference in \ufb01rst-order logic is only semi-decidable, so it is common to use a restricted subset.\nA common approach (as used in Prolog) is to restrict the language to Horn clauses, which are\nclauses that contain at most one positive literal. Essentially this means the model is a series of\nif-then rules, where the right hand side of the rules (the \u201cthen\u201d part, or consequence) has onlya single term.\nOnce we have encoded our knowledge base as a set of clauses, we can attach weights to\neach one; these weights are the parameter of the model, and they de\ufb01ne the clique potentialsas follows:\n\u03c8\nc(xc)=e x p (wc\u03c6c(xc)) (19.36)\nwhere\u03c6c(xc)is a logical expression which evaluates clause capplied to the variables xc, and\nwcis the weight we attach to this clause. Roughly speaking, the weight of a clause speci\ufb01es\nthe probability of a world in which this clause is satsi\ufb01ed relative to a world in which it is notsatis\ufb01ed.\nNow suppose there are two objects (people) in the world, Anna and Bob, which we will denote\nbyconstant symbols AandB. We can make a ground network from the above clauses by\ncreating binary random variables S\nx,Cx, andFx,yforx,y\u2208{A,B}, and then \u201cwiring these\nup\u201d according to the clauses above. The result is the UGM in Figure 19.12 with 8 binary nodes.Note that we have not encoded the fact that Fris a symmetric relation, so Fr(A,B)and\nFr(B,A)might have different values. Similarly, we have the \u201cdegenerate\u201d nodes Fr(A,A)and\nFr(B,B), since we did not enforce x/negationslash=yin Equation 19.33. (If we add such constraints,\nthen the model compiler, which generates the ground network, could avoid creating redundantnodes.)\nIn summary, we can think of MLNs as a convenient way of specifying a UGM template, that\ncan get unrolled to handle data of arbitrary size. There are several other ways to de\ufb01ne relationalprobabilistic models; see e.g., (Koller and Friedman 2009; Kersting et al. 2011) for details. In somecases, there is uncertainty about the number or existence of objects or relations (the so-calledopen universe problem). Section 18.6.2 gives a concrete example in the context of multi-object\ntracking. See e.g., (Russell and Norvig 2010; Kersting et al. 2011) and references therein for furtherdetails.\n19.5 Learning\nIn this section, we discuss how to perform ML and MAP parameter estimation for MRFs. We willsee that this is quite computationally expensive. For this reason, it is rare to perform Bayesianinference for the parameters of MRFs (although see (Qi et al. 2005)).\n19.5.1 Training maxent models using gradient methods\nConsider an MRF in log-linear form:\np(y|\u03b8)=1\nZ(\u03b8)exp/parenleftBigg/summationdisplay\nc\u03b8T\nc\u03c6c(y)/parenrightBigg\n(19.37)", "707": "19.5. Learning 677\nwherecindexes the cliques. The scaled log-likelihood is given by\n/lscript(\u03b8)/defines1\nN/summationdisplay\nilogp(yi|\u03b8)=1\nN/summationdisplay\ni/bracketleftBigg/summationdisplay\nc\u03b8T\nc\u03c6c(yi)\u2212logZ(\u03b8)/bracketrightBigg\n(19.38)\nSince MRFs are in the exponential family, we know that this function is convex in \u03b8(see\nSection 9.2.3), so it has a unique global maximum which we can \ufb01nd using gradient-based\noptimizers. In particular, the derivative for the weights of a particular clique, c, is given by\n\u2202/lscript\n\u2202\u03b8c=1\nN/summationdisplay\ni/bracketleftbigg\n\u03c6c(yi)\u2212\u2202\n\u2202\u03b8clogZ(\u03b8)/bracketrightbigg\n(19.39)\nExercise 19.1 asks you to show that the derivative of the log partition function wrt \u03b8cis the\nexpectation of the c\u2019th feature under the model, i.e.,\n\u2202logZ(\u03b8)\n\u2202\u03b8c=E[\u03c6c(y)|\u03b8]=/summationdisplay\ny\u03c6c(y)p(y|\u03b8) (19.40)\nHence the gradient of the log likelihood is\n\u2202/lscript\n\u2202\u03b8c=/bracketleftBigg\n1\nN/summationdisplay\ni\u03c6c(yi)/bracketrightBigg\n\u2212E[\u03c6c(y)] (19.41)\nIn the \ufb01rst term, we \ufb01x yto its observed values; this is sometimes called the clamped term.\nIn the second term, yis free; this is sometimes called the unclamped term orcontrastive\nterm. Note that computing the unclamped term requires inference in the model, and this must\nbe done once per gradient step. This makes UGM training much slower than DGM training.\nThe gradient of the log likelihood can be rewritten as the expected feature vector according\nto the empirical distribution minus the model\u2019s expectation of the feature vector:\n\u2202/lscript\n\u2202\u03b8c=Epemp[\u03c6c(y)]\u2212Ep(\u00b7|\u03b8)[\u03c6c(y)] (19.42)\nAt the optimum, the gradient will be zero, so the empirical distribution of the features willmatch the model\u2019s predictions:\nE\npemp[\u03c6c(y)] =Ep(\u00b7|\u03b8)[\u03c6c(y)] (19.43)\nThis is called moment matching. This observation motivates a different optimization algorithm\nwhich we discuss in Section 19.5.7.\n19.5.2 Training partially observed maxent models\nSuppose we have missing data and/or hidden variables in our model. In general, we canrepresent such models as follows:\np(y,h|\u03b8)=1\nZ(\u03b8)exp(/summationdisplay\nc\u03b8T\nc\u03c6c(h,y)) (19.44)", "708": "678 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nThe log likelihood has the form\n/lscript(\u03b8)=1\nN/summationdisplay\nilog/parenleftBigg/summationdisplay\nhip(yi,hi|\u03b8)/parenrightBigg\n=1\nN/summationdisplay\nilog/parenleftBigg\n1\nZ(\u03b8)/summationdisplay\nhi\u02dcp(yi,hi|\u03b8)/parenrightBigg\n(19.45)\nwhere\n\u02dcp(y,h|\u03b8)/definesexp/parenleftBigg/summationdisplay\nc\u03b8T\nc\u03c6c(h,y)/parenrightBigg\n(19.46)\nis the unnormalized distribution. The term/summationtext\nhi\u02dcp(yi,hi|\u03b8)is the same as the partition function\nfor the whole model, except that yis \ufb01xed at yi. Hence the gradient is just the expected features\nwhere we clamp yi, but average over h:\n\u2202\n\u2202\u03b8clog/parenleftBigg/summationdisplay\nhi\u02dcp(yi,hi|\u03b8)/parenrightBigg\n=E[\u03c6c(h,yi)|\u03b8] (19.47)\nSo the overall gradient is given by\n\u2202/lscript\n\u2202\u03b8c=1\nN/summationdisplay\ni{E[\u03c6c(h,yi)|\u03b8]\u2212E[\u03c6c(h,y)|\u03b8]} (19.48)\nThe \ufb01rst set of expectations are computed by \u201cclamping\u201d the visible nodes to their observed\nvalues, and the second set are computed by letting the visible nodes be free. In both cases, wemarginalize over h\ni.\nAn alternative approach is to use generalized EM, where we use gradient methods in the M\nstep. See (Koller and Friedman 2009, p956) for details.\n19.5.3 Approximate methods for computing the MLEs of MRFs\nWhen \ufb01tting a UGM there is (in general) no closed form solution for the ML or the MAP estimateof the parameters, so we need to use gradient-based optimizers. This gradient requires inference.In models where inference is intractable, learning also becomes intractable. This has motivatedvarious computationally faster alternatives to ML/MAP estimation, which we list in Table 19.1. Wedicsuss some of these alternatives below, and defer others to later sections.\n19.5.4 Pseudo likelihood\nOne alternative to MLE is to maximize the pseudo likelihood (Besag 1975), de\ufb01ned as follows:\n/lscriptPL(\u03b8)/defines/summationdisplay\nyD/summationdisplay\nd=1pemp(ylogp(yd|y\u2212d)=1\nNN/summationdisplay\ni=1D/summationdisplay\nd=1logp(yid|yi,\u2212d,\u03b8) (19.49)\nThat is, we optimize the product of the full conditionals, also known as the composite likeli-\nhood(Lindsay 1988), Compare this to the objective for maximum likelihood:\n/lscriptML(\u03b8)=/summationdisplay\ny,xpemp(ylogp(y|\u03b8)=N/summationdisplay\ni=1logp(yi|\u03b8) (19.50)", "709": "19.5. Learning 679\nMethod Restriction Exact MLE? Section\nClosed form Only Chordal MRF Exact Section 19.5.7.4\nIPF Only Tabular / Gaussian MRF Exact Section 19.5.7\nGradient-based optimization Low tree width Exact Section 19.5.1\nMax-margin training Only CRFs N/A Section 19.7\nPseudo-likelihood No hidden variables Approximate Section 19.5.4\nStochastic ML - Exact (up to MC error) Section 19.5.5\nContrastive divergence - Approximate Section 27.7.2.4\nMinimum probability \ufb02ow Can integrate out the hiddens Approximate Sohl-Dickstein et al. (2011)\nTable 19.1 Some methods that can be used to compute approximate ML/ MAP parameter estimates for\nMRFs/ CRFs. Low tree-width means that, in order for the method to be efficient, the graph must \u201ctree-like\u201d;\nsee Section 20.5 for details.\n(a) (b)\nFigure 19.13 (a) A small 2d lattice. (b) The representation used by pseudo likelihood. Solid nodes are\nobserved neighbors. Based on Figure 2.2 of (Carbonetto 2003).\nIn the case of Gaussian MRFs, PL is equivalent to ML (Besag 1975), but this is not true in general\n(Liang and Jordan 2008).\nThe PL approach is illustrated in Figure 19.13 for a 2d grid. We learn to predict each node,\ngiven all of its neighbors. This objective is generally fast to compute since each full conditionalp(y\nid|yi,\u2212d,\u03b8)only requires summing over the states of a single node, yid, in order to compute\nthe local normalization constant. The PL approach is similar to \ufb01tting each full conditionalseparately (which is the method used to train dependency networks, discussed in Section 26.2.2),except that the parameters are tied between adjacent nodes.\nOne problem with PL is that it is hard to apply to models with hidden variables (Parise and\nWelling 2005). Another more subtle problem is that each node assumes that its neighbors haveknown values. If node t\u2208nbr(s)is a perfect predictor for node s, thenswill learn to rely\ncompletely on node t, even at the expense of ignoring other potentially useful information, such\nas its local evidence.\nHowever, experiments in (Parise and Welling 2005; Hoe\ufb02ing and Tibshirani 2009) suggest that\nPL works as well as exact ML for fully observed Ising models, and of course PL is muchfaster.\n19.5.5 Stochastic maximum likelihood\nRecall that the gradient of the log-likelihood for a fully observed MRF is given by\n\u2207\u03b8/lscript(\u03b8)=1\nN/summationdisplay\ni[\u03c6(yi)\u2212E[\u03c6(y)]] (19.51)", "710": "680 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nThe gradient for a partially observed MRF is similar. In both cases, we can approximate the\nmodel expectations using Monte Carlo sampling. We can combine this with stochastic gradientdescent (Section 8.5.2), which takes samples from the empirical distribution. Pseudocode for theresulting method is shown in Algorithm 3.\nAlgorithm 19.1: Stochastic maximum likelihood for \ufb01tting an MRF\n1Initialize weights \u03b8randomly;\n2k=0,\u03b7=1;\n3foreachepoch do\n4foreachminibatchofsize Bdo\n5 foreachsample s=1:Sdo\n6 Sampleys,k\u223cp(y|\u03b8k);\n7 \u02c6E(\u03c6(y)) =1\nS/summationtextS\ns=1\u03c6(ys,k);\n8 foreachtrainingcase iinminibatch do\n9 gik=\u03c6(yi)\u2212\u02c6E(\u03c6(y));\n10 gk=1\nB/summationtext\ni\u2208Bgik;\n11 \u03b8k+1=\u03b8k\u2212\u03b7gk;\n12 k=k+1;\n13 Decrease step size \u03b7;\nTypically we use MCMC to generate the samples. Of course, running MCMC to convergence\nat each step of the inner loop would be extremely slow. Fortunately, it was shown in (Younes\n1989) that we can start the MCMC chain at its previous value, and just take a few steps. Inotherwords, we sample y\ns,kby initializing the MCMC chain at ys,k\u22121, and then run for a few\niterations. This is valid since p(y|\u03b8k)is likely to be close to p(y|\u03b8k\u22121), since we only changed\nthe parameters a small amount. We call this algorithm stochastic maximum likelihood or\nSML. (There is a closely related algorithm called persistent contrastive divergence which wediscuss in Section 27.7.2.5.)\n19.5.6 Feature induction for maxent models *\nMRFs require a good set of features. One unsupervised way to learn such features, known asfeature induction, is to start with a base set of features, and then to continually create newfeature combinations out of old ones, greedily adding the best ones to the model. This approachwas \ufb01rst proposed in (Pietra et al. 1997; Zhu et al. 1997), and was later extended to the CRF casein (McCallum 2003).\nTo illustrate the basic idea, we present an example from (Pietra et al. 1997), which described\nhow to build unconditional probabilistic models to represent English spelling. Initially the modelhas no features, which represents the uniform distribution. The algorithm starts by choosing toadd the feature\n\u03c6\n1(y)=/summationdisplay\ntI(yt\u2208{a,...,z}) (19.52)", "711": "19.5. Learning 681\nwhich checks if any letter is lower case or not. After the feature is added, the parameters are\n(re)-\ufb01t by maximum likelihood. For this feature, it turns out that \u02c6\u03b81=1.944, which means that\na word with a lowercase letter in any position is about e1.944\u22487times more likely than the\nsame word without a lowercase letter in that position. Some samples from this model, generatedusing (annealed) Gibbs sampling (Section 24.2), are shown below.\n8\nm, r, xevo, ijjiir, b, to, jz, gsr, wq, vf, x, ga, msmGh, pcp, d, oziVlal,\nhzagh, yzop, io, advzmxnv, ijv_bolft, x, emx, kayerf, mlj, rawzyb, jp, ag,ctdnnnbg, wgdw, t, kguv, cy, spxcq, uzflbbf, dxtkkn, cxwx, jpd, ztzh, lv,zhpkvnu, l^, r, qee, nynrx, atze4n, ik, se, w, lrh, hp+, yrqyka\u2019h, zcngotcnx,igcump, zjcjs, lqpWiqu, cefmfhc, o, lb, fdcY, tzby, yopxmvk, by, fz\u201e t, govyccm,ijyiduwfzo, 6xr, duh, ejv, pk, pjw, l, fl, w\nThe second feature added by the algorithm checks if two adjacent characters are lower case:\n\u03c62(y)=/summationdisplay\ns\u223ctI(ys\u2208{a,...,z},yt\u2208{a,...,z}) (19.53)\nNow the model has the form\np(y)=1\nZexp(\u03b81\u03c61(y)+\u03b82\u03c62(y)) (19.54)\nContinuing in this way, the algorithm adds features for the strings s>anding>,w h e r e>\nrepresents the end of word, and for various regular expressions such as [0-9], etc. Some\nsamples from the model with 1000 features, generated using (annealed) Gibbs sampling, are\nshown below.\nwas, reaser, in, there, to, will, \u201e was, by, homes, thing, be, reloverated,\nther, which, conists, at, fores, anditing, with, Mr., proveral, the, \u201e ***,on\u2019t, prolling, prothere, \u201e mento, at, yaou, 1, chestraing, for, have, to,intrally, of, qut, ., best, compers, ***, cluseliment, uster, of, is, deveral,this, thise, of, offect, inatever, thifer, constranded, stater, vill, in, thase,in, youse, menttering, and, ., of, in, verate, of, to\nThis approach of feature learning can be thought of as a form of graphical model structure\nlearning (Chapter 26), except it is more \ufb01ne-grained: we add features that are useful, regardless\nof the resulting graph structure. However, the resulting graphs can become densely connected,which makes inference (and hence parameter estimation) intractable.\n19.5.7 Iterative proportional \ufb01tting (IPF) *\nConsider a pairwise MRF where the potentials are represented as tables, with one parameter pervariable setting. We can represent this in log-linear form using\n\u03c8\nst(ys,yt)=e x p/parenleftBig\n\u03b8T\nst[I(ys=1,yt=1 ),...,I(ys=K,yt=K)]/parenrightBig\n(19.55)\nand similarly for \u03c8t(yt). Thus the feature vectors are just indicator functions.\n8. We thank John Lafferty for sharing this example.", "712": "682 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nFromEquation19.43, wehavethat, atthemaximumofthelikelihood, theempiricalexpectation\nof the features equals the model\u2019s expectation:\nEpemp[I(ys=j,yt=k)] = Ep(\u00b7|\u03b8)[I(ys=j,yt=k)] (19.56)\npemp(ys=j,yt=k)=p( ys=j,yt=k|\u03b8) (19.57)\nwherepempis the empirical probability:\npemp(ys=j,yt=k)=Nst,jk\nN=/summationtextN\nn=1I(yns=j,ynt=k)\nN(19.58)\nFor a general graph, the condition that must hold at the optimum is\npemp(yc)=p(yc|\u03b8) (19.59)\nFor a special family of graphs known as decomposable graphs (de\ufb01ned in Section 20.4.1), one\ncan show that p(yc|\u03b8)=\u03c8 c(yc). However, even if the graph is not decomposable, we can\nimagine trying to enforce this condition. This suggests an iterative coordinate ascent schemewhere at each step we compute\n\u03c8\nt+1\nc(yc)=\u03c8t\nc(yc)\u00d7pemp(yc)\np(yc|\u03c8t)(19.60)\nwhere the multiplication is elementwise. This is known as iterative proportional \ufb01tting orIPF\n(Fienberg 1970; Bishop et al. 1975). See Algorithm 7 for the pseudocode.\nAlgorithm 19.2: Iterative Proportional Fitting algorithm for tabular MRFs\n1Initialize\u03c8c=1forc=1:C;\n2repeat\n3forc=1:Cdo\n4 pc=p(yc|\u03c8);\n5 \u02c6pc=pemp(yc);\n6 \u03c8c=\u03c8c\u2217\u02c6pc\npc;\n7untilconverged ;\n19.5.7.1 Example\nLet us consider a simple example from http://en .wikipedia .org/wiki/Iterative_propo\nrtional_fitting . We have two binary variables, Y1andY2,w h e r eYn1=1if mannis left\nhanded, and Yn1=0otherwise; similarly, Yn2=1if woman nis left handed, and Yn2=0\notherwise. We can summarize the data using the following 2\u00d72contingency table:\nright-handed left-handed Total\nmale 43 9 52\nfemale 44 4 48\nTotal 87 13 100", "713": "19.5. Learning 683\nSuppose we want to \ufb01t a disconnected graphical model containing nodes Y1andY2but with\nno edge between them. That is, we want to \ufb01nd vectors \u03c81and\u03c82such that M/defines\u03c81\u03c8T\n2\u2248C,\nwhereMare the model\u2019s expected counts, and Care the empirical counts. By moment\nmatching, we \ufb01nd that the row and column sums of the model must exactly match the row\nand column sums of the data. One possible solution is to use \u03c81=[ 0.5200,0.4800]and\n\u03c82=[ 8 7,13]. Below we show the model\u2019s predictions, M=\u03c81\u03c8T\n2.\nright-handed left-handed Total\nmale 45.24 6.76 52\nfemale 41.76 6.24 48\nTotal 87 13 100\nIt is easy to see that this matches the required constraints. See IPFdemo2x2 for some Matlab\ncode that computes these numbers. This method is easily to generalized to arbitrary graphs.\n19.5.7.2 Speed of IPF\nIPF is a \ufb01xed point algorithm for enforcing the moment matching constraints and is guaranteed\nto converge to the global optimum (Bishop et al. 1975). The number of iterations depends on theform of the model. If the graph is decomposable, then IPF converges in a single iteration, but ingeneral, IPF may require many iterations.\nIt is clear that the dominant cost of IPF is computing the required marginals under the model.\nEfficient methods, such as the junction tree algorithm (Section 20.4), can be used, resulting insomething called efficient IPF (Jirousek and Preucil 1995).\nNevertheless, coordinate descent can be slow. An alternative method is to update all the\nparameters at once, by simply following the gradient of the likelihood. This gradient approachhas the further signi\ufb01cant advantage that it works for models in which the clique potentials maynot be fully parameterized, i.e., the features may not consist of all possible indicators for eachclique, but instead can be arbitrary. Although it is possible to adapt IPF to this setting of generalfeatures, resulting in a method known as iterative scaling, in practice the gradient method is\nmuch faster (Malouf 2002; Minka 2003).\n19.5.7.3 Generalizations of IPF\nWe can use IPF to \ufb01t Gaussian graphical models: instead of working with empirical counts, wework with empirical means and covariances (Speed and Kiiveri 1986). It is also possible to createa Bayesian IPF algorithm for sampling from the posterior of the model\u2019s parameters (see e.g.,(Dobra and Massam 2010)).\n19.5.7.4 IPF for decomposable graphical models\nThere is a special family of undirected graphical models known as decomposable graphicalmodels. This is formally de\ufb01ned in Section 20.4.1, but the basic idea is that it contains graphswhich are \u201ctree-like\u201d. Such graphs can be represented by UGMs or DGMs without any loss ofinformation.\nIn the case of decomposable graphical models, IPF converges in one iteration. In fact, the", "714": "684 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nMLE has a closed form solution (Lauritzen 1996). In particular, for tabular potentials we have\n\u02c6\u03c8c(yc=k)=/summationtextN\ni=1I(yi,c=k)\nN(19.61)\nand for Gaussian potentials, we have\n\u02c6\u03bcc=/summationtextNi=1yic\nN,\u02c6\u03a3c=/summationtext\ni(yic\u2212\u02c6\u03bcc)(xic\u2212\u02c6\u03bcc)T\nN(19.62)\nBy using conjugate priors, we can also easily compute the full posterior over the model pa-\nrameters in the decomposable case, just as we did in the DGM case. See (Lauritzen 1996) fordetails.\n19.6 Conditional random \ufb01elds (CRFs)\nAconditional random \ufb01eld orCRF(Lafferty et al. 2001), sometimes a discriminative random\n\ufb01eld(Kumar and Hebert 2003), is just a version of an MRF where all the clique potentials are\nconditioned on input features:\np(y|x,w)=1\nZ(x,w)/productdisplay\nc\u03c8c(yc|x,w) (19.63)\nA CRF can be thought of as a structured output extension of logistic regression. We will usually\nassume a log-linear representation of the potentials:\n\u03c8c(yc|x,w)=e x p (wT\nc\u03c6(x,yc)) (19.64)\nwhere\u03c6(x,yc)is a feature vector derived from the global inputs xand the local set of labels\nyc. We will give some examples below which will make this notation clearer.\nThe advantage of a CRF over an MRF is analogous to the advantage of a discriminative\nclassi\ufb01er over a generative classi\ufb01er (see Section 8.6), namely, we don\u2019t need to \u201cwaste resources\u201dmodeling things that we always observe. Instead we can focus our attention on modeling whatwe care about, namely the distribution of labels given the data.\nAnother important advantage of CRFs is that we can make the potentials (or factors) of the\nmodel be data-dependent. For example, in image processing applications, we may \u201cturn off\u201d thelabel smoothing between two neighboring nodes sandtif there is an observed discontinuity in\nthe image intensity between pixels sandt. Similarly, in natural language processing problems,\nwe can make the latent labels depend on global properties of the sentence, such as whichlanguage it is written in. It is hard to incorporate global features into generative models.\nThe disadvantage of CRFs over MRFs is that they require labeled training data, and they\nare slower to train, as we explain in Section 19.6.3. This is analogous to the strengths andweaknesses of logistic regression vs naive Bayes, discussed in Section 8.6.\n19.6.1 Chain-structured CRFs, MEMMs and the label-bias problem\nThe most widely used kind of CRF uses a chain-structured graph to model correlation amongstneighboring labels. Such models are useful for a variety of sequence labeling tasks (see Sec-tion 19.6.2).", "715": "19.6. Conditionalrandom\ufb01elds(CRFs) 685\nxt\u22121 xt xt+1yt\u22121 yt yt+1\n(a)xt\u22121 xt xt+1yt\u22121ytyt+1xg\n(b)xt\u22121 xt xt+1yt\u22121ytyt+1xg\n(c)\nFigure 19.14 Various models for sequential data. (a) A generative directed HMM. (b) A discriminative\ndirected MEMM. (c) A discriminative undirected CRF.\nTraditionally, HMMs (discussed in detail in Chapter 17) have been used for such tasks. These\nare joint density models of the form\np(x,y|w)=T/productdisplay\nt=1p(yt|yt\u22121,w)p(xt|yt,w) (19.65)\nwhere we have dropped the initial p(y1)term for simplicity. See Figure 19.14(a). If we observe\nbothxtandytfor allt, it is very easy to train such models, using techniques described in\nSection 17.5.1.\nAn HMM requires specifying a generative observation model, p(xt|yt,w), which can be\ndifficult. Furthemore, each xtis required to be local, since it is hard to de\ufb01ne a generative\nmodel for the whole stream of observations, x=x1:T.\nAn obvious way to make a discriminative version of an HMM is to \u201creverse the arrows\u201d from\nyttoxt, as in Figure 19.14(b). This de\ufb01nes a directed discriminative model of the form\np(y|x,w)=/productdisplay\ntp(yt|yt\u22121,x,w) (19.66)\nwherex=(x1:T,xg),xgare global features, and xtare features speci\ufb01c to node t. (This\npartition into local and global is not necessary, but helps when comparing to HMMs.) This is\ncalled amaximum entropy Markov model orMEMM(McCallum et al. 2000; Kakade et al.\n2002).\nAn MEMM is simply a Markov chain in which the state transition probabilities are conditioned\non the input features. (It is therefore a special case of an input-output HMM, discussed inSection 17.6.3.) This seems like the natural generalization of logistic regression to the structured-output setting, but it suffers from a subtle problem known (rather obscurely) as the label bias\nproblem (Lafferty et al. 2001). The problem is that local features at time tdo not in\ufb02uence states\nprior to time t. This follows by examining the DAG, which shows that x\ntis d-separated from\nyt\u22121(and all earlier time points) by the v-structure at yt, which is a hidden child, thus blocking\nthe information \ufb02ow.\nTo understand what this means in practice, consider the part of speech (POS) tagging task.\nSuppose we see the word \u201cbanks\u201d; this could be a verb (as in \u201che banks at BoA\u201d), or a noun (asin \u201cthe river banks were over\ufb02owing\u201d). Locally the POS tag for the word is ambiguous. However,", "716": "686 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\n(a)\n (b)\n (c)\n (d)\n (e)\nFigure 19.15 Example of handwritten letter recognition. In the word \u2019brace\u2019, the \u2019r\u2019 and the \u2019c\u2019 look very\nsimilar, but can be disambiguated using context. Source: (Taskar et al. 2003) . Used with kind permission\nof Ben Taskar.\nsuppose that later in the sentence, we see the word \u201c\ufb01shing\u201d; this gives us enough context to\ninfer that the sense of \u201cbanks\u201d is \u201criver banks\u201d. However, in an MEMM (unlike in an HMM and\nCRF), the \u201c\ufb01shing\u201d evidence will not \ufb02ow backwards, so we will not be able to disambiguate\n\u201cbanks\u201d.\nNow consider a chain-structured CRF. This model has the form\np(y|x,w)=1\nZ(x,w)T/productdisplay\nt=1\u03c8(yt|x,w)T\u22121/productdisplay\nt=1\u03c8(yt,yt+1|x,w) (19.67)\nFrom the graph in Figure 19.14(c), we see that the label bias problem no longer exists, since yt\ndoes not block the information from xtfrom reaching other yt/primenodes.\nThe label bias problem in MEMMs occurs because directed models are locally normalized ,\nmeaning each CPD sums to 1. By contrast, MRFs and CRFs are globally normalized , which\nmeans that local factors do not need to sum to 1, since the partition function Z, which sums over\nall joint con\ufb01gurations, will ensure the model de\ufb01nes a valid distribution. However, this solution\ncomes at a price: we do not get a valid probability distribution over yuntil we have seen\nthe whole sentence, since only then can we normalize over all con\ufb01gurations. Consequently,\nCRFs are not as useful as DGMs (whether discriminative or generative) for online or real-time\ninference. Furthermore, the fact that Zdepends on all the nodes, and hence all their parameters,\nmakes CRFs much slower to train than DGMs, as we will see in Section 19.6.3.\n19.6.2 Applications of CRFs\nCRFs have been applied to many interesting problems; we give a representative sample below.\nThese applications illustrate several useful modeling tricks, and will also provide motivation for\nsome of the inference techniques we will discuss in Chapter 20.\n19.6.2.1 Handwriting recognition\nA natural application of CRFs is to classify hand-written digit strings, as illustrated in Figure 19.15.\nThe key observation is that locally a letter may be ambiguous, but by depending on the (un-\nknown) labels of one\u2019s neighbors, it is possible to use context to reduce the error rate. Note\nthat the node potential, \u03c8t(yt|xt), is often taken to be a probabilistic discriminative classi\ufb01er,", "717": "19.6. Conditionalrandom\ufb01elds(CRFs) 687\nits withdrawal from the UAL Airways rose after announcing  British dealADJ N V IN V PRP N IN N N DTBIO O O BIO I\nPOSNP I B\nBegin noun phrase\nWithin noun phraseNot a noun phraseNounAdjectiveBIONADJVerbPrepositionPossesive pronounDeterminer (e.g., a, an, the)VINPRPDTKEY\nFigure 19.16 A CRF for joint POS tagging and NP segmentation. Source: Figure 4.E.1 of (Koller and\nFriedman 2009). Used with kind permission of Daphne Koller.\nsuch as a neural network or RVM, that is trained on isolated letters, and the edge potentials,\n\u03c8st(ys,yt), are often taken to be a language bigram model. Later we will discuss how to train\nall the potentials jointly.\n19.6.2.2 Noun phrase chunking\nOne common NLP task is noun phrase chunking, which refers to the task of segmenting a\nsentence into its distinct noun phrases (NPs). This is a simple example of a technique known asshallow parsing.\nIn more detail, we tag each word in the sentence with B (meaning beginning of a new NP), I\n(meaning inside a NP), or O (meaning outside an NP). This is called BIOnotation. For example,\nin the following sentence, the NPs are marked with brackets:\nB I O O OB IO B I I\n(British Airways) rose after announcing (its withdrawl) from (the UAI deal)\n(We need the B symbol so that we can distinguish II, meaning two words within a single NP,\nfrom BB, meaning two separate NPs.)\nA standard approach to this problem would \ufb01rst convert the string of words into a string of\nPOS tags, and then convert the POS tags to a string of BIOs. However, such a pipeline method\ncan propagate errors. A more robust approach is to build a joint probabilistic model of the\nformp(NP\n1:T,POS1:T|words1:T). One way to do this is to use the CRF in Figure 19.16. The\nconnections between adjacent labels encode the probability of transitioning between the B, Iand O states, and can enforce constraints such as the fact that B must preceed I. The featuresare usually hand engineered and include things like: does this word begin with a capital letter, isthis word followed by a full stop, is this word a noun, etc. Typically there are \u223c1,000\u221210,000\nfeatures per node.\nThe number of features has minimal impact on the inference time, since the features are\nobserved and do not need to be summed over. (There is a small increase in the cost of", "718": "688 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\n Mrs. Green spoke today in New Y ork\n()Green chairs the \ufb01nance committe eB-PER I-PER OTH OTH OTH B-LOC I-LOC B-PER OTH OTH OTH OTH\nKEY\nBegin person name\nWithin person nameBegin location nameB-PERI-PERB-LOCWithin location nameNot an entitiyI-LOCOTH\nFigure 19.17 A skip-chain CRF for named entity recognition. Source: Figure 4.E.1 of (Koller and Friedman\n2009). Used with kind permission of Daphne Koller.\nevaluating potential functions with many features, but this is usually negligible; if not, one can\nuse/lscript1regularization to prune out irrelevant features.) However, the graph structure can have a\ndramatic effect on inference time. The model in Figure 19.16 is tractable, since it is essentially a\u201cfat chain\u201d, so we can use the forwards-backwards algorithm (Section 17.4.3) for exact inferenceinO(T|POS|\n2|NP|2)time, where|POS|is the number of POS tags, and |NP|is the number\nof NP tags. However, the seemingly similar graph in Figure 19.17, to be explained below, iscomputationally intractable.\n19.6.2.3 Named entity recognition\nA task that is related to NP chunking is named entity extraction. Instead of just segmenting\nout noun phrases, we can segment out phrases to do with people and locations. Similartechniques are used to automatically populate your calendar from your email messages; this iscalledinformation extraction.\nA simple approach to this is to use a chain-structured CRF, but to expand the state space\nfrom BIO to B-Per, I-Per, B-Loc, I-Loc, and Other. However, sometimes it is ambiguous whethera word is a person, location, or something else. (Proper nouns are particularly difficult to dealwith because they belong to an open class, that is, there is an unbounded number of possible\nnames, unlike the set of nouns and verbs, which is large but essentially \ufb01xed.) We can get betterperformance by considering long-range correlations between words. For example, we might adda link between all occurrences of the same word, and force the word to have the same tag ineach occurence. (The same technique can also be helpful for resolving the identity of pronouns.)This is known as a skip-chain CRF. See Figure 19.17 for an illustration.\nWe see that the graph structure itself changes depending on the input, which is an additional\nadvantage of CRFs over generative models. Unfortunately, inference in this model is gener-ally more expensive than in a simple chain with local connections, for reasons explained inSection 20.5.", "719": "19.6. Conditionalrandom\ufb01elds(CRFs) 689\nFigure 19.18 Illustration of a simple parse tree based on a context free grammar in Chomsky normal\nform. The feature vector \u03c6(x,y)=\u03a8 (x,y)counts the number of times each production rule was used.\nSource: Figure 5.2 of (Altun et al. 2006) . Used with kind permission of Yasemin Altun.\n19.6.2.4 Natural language parsing\nA generalization of chain-structured models for language is to use probabilistic grammars. In\nparticular, a probabilistic context free grammar orPCFGis a set of re-write or production\nrules of the form \u03c3\u2192\u03c3/prime\u03c3/prime/primeor\u03c3\u2192x,w h e r e\u03c3,\u03c3/prime,\u03c3/prime/prime\u2208\u03a3are non-terminals (analogous to\nparts of speech), and x\u2208Xare terminals, i.e., words. See Figure 19.18 for an example. Each\nsuch rule has an associated probability. The resulting model de\ufb01nes a probability distribution\nover sequences of words. We can compute the probability of observing a particular sequence\nx=x1...xTby summing over all trees that generate it. This can be done in O(T3)time\nusing the inside-outside algorithm ; see e.g., (Jurafsky and Martin 2008; Manning and Schuetze\n1999) for details.\nPCFGs are generative models. It is possible to make discriminative versions which encode\nthe probability of a labeled tree, y, given a sequence of words, x, by using a CRF of the form\np(y|x)\u221dexp(wT\u03c6(x,y)). For example, we might de\ufb01ne \u03c6(x,y)to count the number of\ntimes each production rule was used (which is analogous to the number of state transitions in\na chain-structured model). See e.g., (Taskar et al. 2004) for details.\n19.6.2.5 Hierarchical classi\ufb01cation\nSuppose we are performing multi-class classi\ufb01cation, where we have a label taxonomy , which\ngroups the classes into a hierarchy. We can encode the position of ywithin this hierarchy by\nde\ufb01ning a binary vector \u03c6(y), where we turn on the bit for component yand for all its children.\nThis can be combined with input features \u03c6(x)using a tensor product, \u03c6(x,y)=\u03c6(x)\u2297\u03c6(y).\nSee Figure 19.19 for an example.\nThis method is widely used for text classi\ufb01cation, where manually constructed taxnomies\n(such as the Open Directory Project at www.dmoz.org) are quite common. The bene\ufb01t is that\ninformation can be shared between the parameters for nearby categories, enabling generalization\nacross classes.", "720": "690 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\n/angbracketleftw,\u03a8(x,2)/angbracketright=/angbracketleftw2,x/angbracketright+/angbracketleftw6,x/angbracketright+/angbracketleftw9,x/angbracketright\nFigure 19.19 Illustration of a simple label taxonomy, and how it can be used to compute a distributed\nrepresentation for the label for class 2. In this \ufb01gure, \u03c6(x)=x,\u03c6(y=2 )=\u039b ( 2 ) ,\u03c6(x,y)is denoted\nby\u03a8(x,2), andwT\u03c6(x,y)is denoted by /angbracketleftw,\u03a8(x,2)/angbracketright. Source: Figure 5.1 of (Altun et al. 2006) . Used\nwith kind permission of Yasemin Altun.\n19.6.2.6 Protein side-chain prediction\nAn interesting analog to the skip-chain model arises in the problem of predicting the structure\nof protein side chains. Each residue in the side chain has 4 dihedral angles, which are usually\ndiscretized into 3 values called rotamers. The goal is to predict this discrete sequence of angles,\ny, from the discrete sequence of amino acids, x.\nWe can de\ufb01ne an energy function E(x,y), where we include various pairwise interaction\nterms between nearby residues (elements of the yvector). This energy is usually de\ufb01ned as a\nweighted sum of individual energy terms, E(x,y|w)=/summationtextD\nj=1\u03b8jEj(x,y), where the Ejare\nenergy contribution due to various electrostatic charges, hydrogen bonding potentials, etc, and\nware the parameters of the model. See (Yanover et al. 2007) for details.\nGiven the model, we can compute the most probable side chain con\ufb01guration using y\u2217=\nargminE(x,y|w). In general, this problem is NP-hard, depending on the nature of the graph\ninduced by the Ejterms, due to long-range connections between the variables. Nevertheless,\nsome special cases can be efficiently handled, using methods discussed in Section 22.6.\n19.6.2.7 Stereo vision\nLow-level vision problems are problems where the input is an image (or set of images), and\nthe output is a processed version of the image. In such cases, it is common to use 2d lattice-\nstructured models; the models are similar to Figure 19.9, except that the features can be global,\nand are not generated by the model. We will assume a pairwise CRF.\nA classic low-level vision problem is dense stereo reconstruction , where the goal is to\nestimate the depth of every pixel given two images taken from slightly different angles. In this\nsection (based on (Sudderth and Freeman 2008)), we give a sketch of how a simple CRF can be\nused to solve this task. See e.g., (Sun et al. 2003) for a more sophisticated model.\nBy using some standard preprocessing techniques, one can convert depth estimation into a", "721": "19.6. Conditionalrandom\ufb01elds(CRFs) 691\nproblem of estimating the disparity ysbetween the pixel at location (is,js)in the left image\nand the corresponding pixel at location (is+ys,js)in the right image. We typically assume\nthat corresponding pixels have similar intensity, so we de\ufb01ne a local node potential of the form\n\u03c8s(ys|x)\u221dexp/braceleftbigg\n\u22121\n2\u03c32(xL(is,js)\u2212xR(is+ys,js))2/bracerightbigg\n(19.68)\nwherexLis the left image and xRis the right image. This equation can be generalized to model\nthe intensity of small windows around each location. In highly textured regions, it is usually\npossible to \ufb01nd the corresponding patch using cross correlation, but in regions of low texture,there will be considerable ambiguity about the correct value of y\ns.\nWe can easily add a Gaussian prior on the edges of the MRF that encodes the assumption\nthat neighboring disparities ys,ytshould be similar, as follows:\n\u03c8st(ys,yt)\u221dexp/parenleftbigg\n\u22121\n2\u03b32(ys\u2212yt)2/parenrightbigg\n(19.69)\nThe resulting model is a Gaussian CRF.\nHowever, using Gaussian edge-potentials will oversmooth the estimate, since this prior fails\nto account for the occasional large changes in disparity that occur between neighboring pixelswhich are on different sides of an occlusion boundary. One gets much better results using atruncated Gaussian potential of the form\n\u03c8\nst(ys,yt)\u221dexp/braceleftbigg\n\u22121\n2\u03b32min/parenleftbig\n(ys\u2212yt)2,\u03b42\n0/parenrightbig/bracerightbigg\n(19.70)\nwhere\u03b3encodes the expected smoothness, and \u03b40encodes the maximum penalty that will\nbe imposed if disparities are signi\ufb01cantly different. This is called a discontinuity preserving\npotential; note that such penalties are not convex. The local evidence potential can be maderobust in a similar way, in order to handle outliers due to specularities, occlusions, etc.\nFigure 19.20 illustrates the difference between these two forms of prior. On the top left is an\nimage from the standard Middlebury stereo benchmark dataset (Scharstein and Szeliski 2002).On the bottom left is the corresponding true disparity values. The remaining columns representthe estimated disparity after 0, 1 and an \u201cin\ufb01nite\u201d number of rounds of loopy belief propagation(see Section 22.2), where by \u201cin\ufb01nite\u201d we mean the results at convergence. The top row showsthe results using a Gaussian edge potential, and the bottom row shows the results using thetruncated potential. The latter is clearly better.\nUnfortunately, performing inference with real-valued variables is computationally difficult,\nunless the model is jointly Gaussian. Consequently, it is common to discretize the variables.(For example, Figure 19.20(bottom) used 50 states.) The edge potentials still have the form givenin Equation 19.69. The resulting model is called a metric CRF, since the potentials form a\nmetric.\n9Inference in metric CRFs is more efficient than in CRFs where the discrete labels\nhave no natural ordering, as we explain in Section 22.6.3.3. See Section 22.6.4 for a comparisonof various approximate inference methods applied to low-level CRFs, and see (Blake et al. 2011;Prince 2012) for more details on probabilistic models for computer vision.\n9. A function fis said to be a metricif it satis\ufb01es the following three properties: Re\ufb02exivity: f(a,b)=0 iffa=b;\nSymmetry: f(a,b)=f (b,a); and Triangle inequality: f(a,b)+f(b,c)\u2265f(a,c).I f fsatis\ufb01es only the \ufb01rst two\nproperties, it is called a semi-metric.", "722": "692 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\n/g2/g3 /g5/g10/g13/g7/g1/g4/g8/g11/g9/g6/g10/g8/g12/g8/g7/g11 \u0001\nFigure 19.20 Illustration of belief propagation for stereo depth estimation. Left column: image and true\ndisparities. Remaining columns: initial estimate, estimate after 1 iteration, and estimate at convergence.\nTop row: Gaussian edge potentials. Bottom row: robust edge potentials. Source: Figure 4 of (Sudderth and\nFreeman 2008). Used with kind permission of Erik Sudderth.\n19.6.3 CRF training\nWe can modify the gradient based optimization of MRFs described in Section 19.5.1 to the CRF\ncase in a straightforward way. In particular, the scaled log-likelihood becomes\n/lscript(w)/defines1\nN/summationdisplay\nilogp(yi|xi,w)=1\nN/summationdisplay\ni/bracketleftBigg/summationdisplay\ncwT\nc\u03c6c(yi,xi)\u2212logZ(w,xi)/bracketrightBigg\n(19.71)\nand the gradient becomes\n\u2202/lscript\n\u2202wc=1\nN/summationdisplay\ni/bracketleftbigg\n\u03c6c(yi,xi)\u2212\u2202\n\u2202wclogZ(w,xi)/bracketrightbigg\n(19.72)\n=1\nN/summationdisplay\ni[\u03c6c(yi,xi)\u2212E[\u03c6c(y,xi)]] (19.73)\nNote that we now have to perform inference for every single training case inside each gradient\nstep, which is O(N)times slower than the MRF case. This is because the partition function\ndepends on the inputs xi.\nIn most applications of CRFs (and some applications of MRFs), the size of the graph structure\ncan vary. Hence we need to use parameter tying to ensure we can de\ufb01ne a distribution of\narbitrary size. In the pairwise case, we can write the model as follows:\np(y|x,w)=1\nZ(w,x)exp/parenleftbig\nwT\u03c6(y,x)/parenrightbig\n(19.74)", "723": "19.7. StructuralSVMs 693\nwherew=[wn,we]are the node and edge parameters, and\n\u03c6(y,x)/defines[/summationdisplay\nt\u03c6t(yt,x),/summationdisplay\ns\u223ct\u03c6st(ys,yt,x)] (19.75)\nare the summed node and edge features (these are the sufficient statistics). The gradient\nexpression is easily modi\ufb01ed to handle this case.\nIn practice, it is important to use a prior/ regularization to prevent over\ufb01tting. If we use a\nGaussian prior, the new objective becomes\n/lscript/prime(w)/defines1\nN/summationdisplay\nilogp(yi|xi,w)\u2212\u03bb||w||2\n2 (19.76)\nIt is simple to modify the gradient expression.\nAlternatively, we can use /lscript1regularization. For example, we could use /lscript1for the edge weights\nweto learn a sparse graph structure, and /lscript2for the node weights wn, as in (Schmidt et al.\n2008). In other words, the objective becomes\n/lscript/prime(w)/defines1\nN/summationdisplay\nilogp(yi|xi,w)\u2212\u03bb1||we||1\u2212\u03bb2||wn||22(19.77)\nUnfortunately, the optimization algorithms are more complicated when we use /lscript1(see Sec-\ntion 13.4), although the problem is still convex.\nTo handle large datasets, we can use stochastic gradient descent (SGD), as described in\nSection 8.5.2.\nIt is possible (and useful) to de\ufb01ne CRFs with hidden variables, for example to allow for an\nunknown alignment between the visible features and the hidden labels (see e.g., (Schnitzspan\net al. 2010)). In this case, the objective function is no longer convex. Nevertheless, we can \ufb01nda locally optimal ML or MAP parameter estimate using EM and/ or gradient methods.\n19.7 Structural SVMs\nWe have seen that training a CRF requires inference, in order to compute the expected sufficientstatistics needed to evaluate the gradient. For certain models, computing a joint MAP estimateof the states is provably simpler than computing marginals, as we discuss in Section 22.6. In thissection, we discuss a way to train structured output classi\ufb01ers that that leverages the existence offast MAP solvers. (To avoid confusion with MAP estimation of parameters, we will often refer toMAP estimation of states as decoding.) These methods are known as structural support vector\nmachines orSSVMs(Tsochantaridis et al. 2005). (There is also a very similar class of methods\nknown as max margin Markov networks orM3nets(Taskar et al. 2003); see Section 19.7.2 for\na discussion of the differences.)\n19.7.1 SSVMs: a probabilistic view\nIn this book, we have mostly concentrated on \ufb01tting models using MAP parameter estimation,i.e., by minimizing functions of the form\nR\nMAP(w)=\u2212logp(w)\u2212N/summationdisplay\ni=1logp(yi|xi,w) (19.78)", "724": "694 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nHowever, at test time, we pick the label so as to minimize the posterior expected loss (de\ufb01ned\nin Section 5.7):\n\u02c6y(x|w) = argmin\n\u02c6y/summationdisplay\nyL(\u02c6y,y)p(y|x,w) (19.79)\nwhereL(y\u2217,\u02c6y)is the loss we incur when we estimate \u02c6ybut the truth is y\u2217. It therefore seems\nreasonable to take the loss function into account when performing parameter estimation.10So,\nfollowing (Yuille and He 2011), let us instead minimized the posterior expected loss on thetraining set:\nR\nEL(w)/defines\u2212logp(w)+N/summationdisplay\ni=1log/bracketleftBigg/summationdisplay\nyL(yi,y)p(y|xi,w)/bracketrightBigg\n(19.80)\nIn the special case of 0-1 loss, L(yi,y)=1\u2212\u03b4y,yi, this reduces to RMAP.\nWe will assume that we can write our model in the following form:\np(y|x,w)=exp(wT\u03c6(x,y))\nZ(x,w)(19.81)\np(w)=exp(\u2212E(w))\nZ(19.82)\nwhereZ(x,w)=/summationtext\nyexp(wT\u03c6(x,y)). Also, let us de\ufb01ne L(yi,y)=e x p \u02dcL(yi,y). With\nthis, we can rewrite our objective as follows:\nREL(w)=\u2212 logp(w)+/summationdisplay\nilog/bracketleftBigg/summationdisplay\nyexp\u02dcL(yi,y)exp(wT\u03c6(x,y))\nZ(x,w)/bracketrightBigg\n(19.83)\n=E(w)+/summationdisplay\ni\u2212logZ(xi,w)+log/summationdisplay\nyexp/parenleftBig\n\u02dcL(vyi,y)+wT\u03c6(xi,y)/parenrightBig\n(19.84)\nWe will now consider various bounds in order to simplify this objective. First note that for\nany function f(y)we have\nmax\ny\u2208Yf(y)\u2264log/summationdisplay\ny\u2208Yexp[f(y)]\u2264log/bracketleftbigg\n|Y|exp/parenleftbigg\nmax\nyf(y)/parenrightbigg/bracketrightbigg\n=l o g|Y|+max\nyf(y)(19.85)\nFor example, suppose Y={0,1,2}andf(y)=y. Then we have\n2 = log[exp(2)] \u2264log[exp(0)+exp(1)+exp(2)] \u2264log[3\u00d7exp(2)] = log(3)+2 (19.86)\nWe can ignore the log|Y|term, which is independent of y, and treat maxy\u2208Yf(y)as both a\nlower and upper bound. Hence we see that\nREL(w)\u223cE(w)+N/summationdisplay\ni=1/bracketleftbigg\nmax\ny/braceleftBig\n\u02dcL(yi,y)+wT\u03c6(xi,y)/bracerightBig\n\u2212max\nywT\u03c6(xi,y)/bracketrightbigg\n(19.87)\n10. Note that this violates the fundamental Bayesian distinction between inference and decision making. However,\nperforming these tasks separately will only result in an optimal decision if we can compute the exact posterior. In most\ncases, this is intractable, so we need to perform loss-calibrated inference (Lacoste-Julien et al. 2011). In this section,\nwe just perform loss-calibrated MAP parameter estimation, which is computationally simpler. (See also (Stoyanov et al.2011).)", "725": "19.7. StructuralSVMs 695\nwherex\u223cymeansc1+x\u2264y+c2for some constants c1,c2. Unfortunately, this objective\nis not convex in w. However, we can devise a convex upper bound by exploiting the following\nlooser lower bound on the log-sum-exp function:\nf(y/prime)\u2264log/summationdisplay\nyexp[f(y)] (19.88)\nfor anyy/prime\u2208Y. Applying this equation to our earlier example, for f(y)=yandy/prime=1,w eg e t\n1 = log[exp(1)] \u2264log[exp(0)+exp(1)+exp(2)] . And applying this bound to RELwe get\nREL(w)\u2264E(w)+N/summationdisplay\ni=1/bracketleftbigg\nmax\ny/braceleftBig\n\u02dcL(yi,y)+wT\u03c6(xi,y)/bracerightBig\n\u2212wT\u03c6(xi,yi)/bracketrightbigg\n(19.89)\nIf we setE(w)=\u22121\n2C||w||2\n2(corresponding to a spherical Gaussian prior), we get\nRSSVM(w)/defines1\n2||w||2+CN/summationdisplay\ni=1/bracketleftbigg\nmax\ny/braceleftBig\n\u02dcL(yi,y)+wT\u03c6(xi,y)/bracerightBig\n\u2212wT\u03c6(xi,yi)/bracketrightbigg\n(19.90)\nThis is the same objective as used in the SSVM approach of (Tsochantaridis et al. 2005).\nIn the special case that Y={\u22121,+1}L(y\u2217,y)=1\u2212\u03b4y,y\u2217, and\u03c6(x,y)=1\n2yx, this\ncriterion reduces to the following (by considering the two cases that y=yiandy/negationslash=yi):\nRSVM(w)/defines1\n2||w||2+CN/summationdisplay\ni=1/bracketleftbig\nmax{0,1\u2212yiwTxi}/bracketrightbig\n(19.91)\nwhich is the standard binary SVM objective (see Equation 14.57).\nSo we see that the SSVM criterion can be seen as optimizing an upper bound on the Bayesian\nobjective, a result \ufb01rst shown in (Yuille and He 2011). This bound will be tight (and hence\nthe approximation will be a good one) when ||w||is large, since in that case, p(y|x,w)will\nconcentrate its mass on argmaxyp(y|x,w). Unfortunately, a large ||w||corresponds to a\nmodel that is likely to over\ufb01t, so it is unlikely that we will be working in this regime (because wewill tune the strength of the regularizer to avoid this situation). An alternative justi\ufb01cation for theSVM criterion is that it focusses effort on \ufb01tting parameters that affect the decision boundary.This is a better use of computational resources than \ufb01tting the full distribution, especially whenthe model is wrong.\n19.7.2 SSVMs: a non-probabilistic view\nWe now present SSVMs in a more traditional (non-probabilistic) way, following (Tsochantaridiset al. 2005). The resulting objective will be the same as the one above. However, this derivationwill set the stage for the algorithms we discuss below.\nLetf(x;w) = argmax\ny\u2208YwT\u03c6(x,y)be the prediction function. We can obtain zero loss\non the training set using this predictor if\n\u2200i.max\ny\u2208Y\\yiwT\u03c6(xi,y)\u2264wT\u03c6(xi,yi) (19.92)", "726": "696 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nEach one of these nonlinear inequalities can be equivalently replaced by |Y|\u22121linear inequal-\nities, resulting in a total of N|Y|\u2212Nlinear constraints of the following form:\n\u2200i.\u2200y\u2208Y\\yi.wT\u03c6(xi,yi)\u2212wT\u03c6(xi,y)\u22650 (19.93)\nFor brevity, we introduce the notation\n\u03b4i(y)/defines\u03c6(xi,yi)\u2212\u03c6(xi,y) (19.94)\nso we can rewrite these constraints as wT\u03b4i(y)\u22650.\nIf we can achieve zero loss, there will typically be multiple solution vectors w. We pick the\none that maximizes the margin, de\ufb01ned as\n\u03b3/definesmin\nif(x,yi;w)\u2212max\ny/prime\u2208Y\\yf(x,y/prime;w) (19.95)\nSince the margin can be made arbitrarily large by rescaling w, we \ufb01x its norm to be 1, resulting\nin the optimization problem\nmax\n\u03b3,w:||w||=1s.t.\u2200i.\u2200y\u2208Y\\yi.wT\u03b4i(y)\u2265\u03b3 (19.96)\nEquivalently, we can write\nmin\nw1\n2||w||2s.t.\u2200i.\u2200y\u2208Y\\yi.wT\u03b4i(y)\u22651 (19.97)\nTo allow for the case where zero loss cannot be achieved (equivalent to the data being inseparable\nin the case of binary classi\ufb01cation), we relax the constraints by introducing slack terms \u03bei, one\nper data case. This yields\nmin\nw,\u03be1\n2||w||2+CN/summationdisplay\ni=1\u03beis.t.\u2200i.\u2200y\u2208Y\\yi.wT\u03b4i(y)\u22651\u2212\u03bei,\u03bei\u22650 (19.98)\nIn the case of structured outputs, we don\u2019t want to treat all constraint violations equally. For\nexample, in a segmentation problem, getting one position wrong should be punished less thangetting many positions wrong. One way to achieve this is to divide the slack variable by the sizeof the loss (this is called slack re-scaling). This yields\nmin\nw,\u03be1\n2||w||2+CN/summationdisplay\ni=1\u03beis.t.\u2200i.\u2200y\u2208Y\\yi.wT\u03b4i(y)\u22651\u2212\u03bei\nL(yi,y),\u03bei\u22650(19.99)\nAlternatively, we can de\ufb01ne the margin to be proportional to the loss (this is called margin\nre-rescaling). This yields\nmin\nw,\u03be1\n2||w||2+CN/summationdisplay\ni=1\u03beis.t.\u2200i.\u2200y\u2208Y\\yi.wT\u03b4i(y)\u2265L(yi,y)\u2212\u03bei,\u03bei\u22650(19.100)\n(In fact, we can write \u2200y\u2208Yinstead of\u2200y\u2208Y\\yi, since if y=yi, thenwT\u03b4i(y)=0and\n\u03bei=0. By using the simpler notation, which doesn\u2019t exclude yi, we add an extra but redundant\nconstraint.) This latter approach is used in M3nets.", "727": "19.7. StructuralSVMs 697\nFor future reference, note that we can solve for the \u03be\u2217\niterms as follows:\n\u03be\u2217\ni(w)=m a x{0,max\ny(L(yi,y)\u2212wT\u03b4i))}=m a x\ny(L(yi,y)\u2212wT\u03b4i)) (19.101)\nSubstituting in, and dropping the constraints, we get the following equivalent problem:\nmin\nw1\n2||w||2+C/summationdisplay\nimax\ny/braceleftbig\nL(yi,y)+wT\u03c6(xi,y)/bracerightbig\n\u2212wT\u03c6(xi,yi) (19.102)\n19.7.2.1 Empirical risk minimization\nLet us pause and consider whether the above objective is reasonable. Recall that in the frequen-\ntist approach to machine learning (Section 6.5), the goal is to minimize the regularized empiricalrisk, de\ufb01ned by\nR(w)+C\nNN/summationdisplay\ni=1L(yi,f(xi,w)) (19.103)\nwhereR(w)is the regularizer, and f(xi,w) = argmaxywT\u03c6(xi,y)=\u02c6yiis the prediction.\nSince this objective is hard to optimize, because the loss is not differentiable, we will constructa convex upper bound instead.\nWe can show that\nR(w)+C\nN/summationdisplay\nimax\ny(L(yi,y)\u2212wT\u03b4i)) (19.104)\nis such a convex upper bound. To see this, note that\nL(yi,f(xi,w))\u2264L(yi,f(xi,w))\u2212wT\u03c6(xi,yi)+wT\u03c6(xi,\u02c6yi) (19.105)\n\u2264max\nyL(yi,y)\u2212wT\u03c6(xi,yi)+wT\u03c6(xi,y) (19.106)\nUsing this bound and R(w)=1\n2||w||2yields Equation 19.102.\n19.7.2.2 Computational issues\nAlthough the above objectives are simple quadratic programs (QP), they have O(N|Y|)con-\nstraints. This is intractable, since Yis usually exponentially large. In the case of the margin\nrescaling formulation, it is possible to reduce the exponential number of constraints to a poly-nomial number, provided the loss function and the feature vector decompose according to agraphical model. This is the approach used in M3nets (Taskar et al. 2003).\nAn alternative approach is to work directly with the exponentially sized QP. This allows for\nthe use of more general loss functions. There are several possible methods to make this feasible.One is to use cutting plane methods. Another is to use stochastic subgradient methods. Wediscuss both of these below.", "728": "698 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nFi C llh S f h l lh I\nFigure 19.21 Illustration of the cutting plane algorithm in 2d. We start with the estimate w=w0=0.\n(a) We add the \ufb01rst constraint; the shaded region is the new feasible set. The new minimum norm solution\nisw1. (b) We add another constraint; the dark shaded region is the new feasible set. (c) We add a third\nconstraint. Source: Figure 5.3 of (Altun et al. 2006) . Used with kind permission of Yasemin Altun.\n19.7.3 Cutting plane methods for \ufb01tting SSVMs\nIn this section, we discuss an efficient algorithm for \ufb01tting SSVMs due to (Joachims et al. 2009).\nThis method can handle general loss functions, and is implemented in the popular SVMstruct\npackage11. The method is based on the cutting plane method from convex optimization (Kelley\n1960).\nThe basic idea is as follows. We start with an initial guess wand no constraints. At each\niteration, we then do the following: for each example i, we \ufb01nd the \u201cmost violated\u201d constraint\ninvolving xiand\u02c6yi. If the loss-augmented margin violation exceeds the current value of \u03beiby\nmore than /epsilon1, we add \u02c6yito the working set of constraints for this training case, Wi, and then\nsolve the resulting new QP to \ufb01nd the new w,\u03be. See Figure 19.21 for a sketch, and Algorithm 11\nfor the pseudo code. (Since at each step we only add one new constraint, we can warm-start\nthe QP solver.) We can can easily modify the algorithm to optimize the slack rescaling version\nby replacing the expression L(yi,y)\u2212wT\u03b4i(\u02c6yi)withL(yi,y)(1\u2212wT\u03b4i(\u02c6yi)).\nThe key to the efficiency of this method is that only polynomially many constraints need to\nbe added, and as soon as they are, the exponential number of other constraints are guaranteed\nto also be satis\ufb01ed to within a tolerance of /epsilon1(see (Tsochantaridis et al. 2005) for the proof).\n19.7.3.1 Loss-augmented decoding\nThe other key to efficiency is the ability to \ufb01nd the most violated constraint in line 5 of the\nalgorithm, i.e., to compute\nargmax\ny\u2208YL(yi,y)\u2212wT\u03b4i(y) = argmax\ny\u2208YL(yi,y)+wT\u03c6(xi,y) (19.107)\n11.http://svmlight .joachims .org/svm_struct .html", "729": "19.7. StructuralSVMs 699\nAlgorithm 19.3: Cutting plane algorithm for SSVMs (margin rescaling, N-slack version)\n1InputD={(x1,y1),...,(xN,yn)},C,/epsilon1;\n2Wi=\u2205,\u03bei=0fori=1:N;\n3repeat\n4fori=1:Ndo\n5 \u02c6yi=a r g m a x\u02c6yi\u2208YL(yi,y)\u2212wT\u03b4i(\u02c6yi);\n6 ifL(yi,y)\u2212wT\u03b4i(\u02c6yi)>\u03bei+/epsilon1then\n7Wi=Wi\u222a{\u02c6yi};\n8 (w,\u03be) = argminw,\u03be\u226501\n2||w||2\n2+C/summationtextN\ni=1\u03bei;\n9 s.t.\u2200i=1:N,\u2200y/prime\u2208Wi:wT\u03b4i(\u02c6yi)\u2265L(yi,y/prime)\u2212\u03bei;\n10untilnoWihaschanged ;\n11Return (w ,\u03be)\nWe call this process loss-augmented decoding . (In (Joachims et al. 2009), this procedure is\ncalled the separation oracle .) If the loss function has an additive decomposition of the same\nform as the features, then we can fold the loss into the weight vector, i.e., we can \ufb01nd a new\nset of parameters w/primesuch that (w/prime)T\u03b4i(y)=wT\u03b4i(y). We can then use a standard decoding\nalgorithm, such as Viterbi, on the model p(y|x,w/prime).\nInthespecialcaseof0-1loss, theoptimumwilleitherbethebestsolution, argmaxywT\u03c6(xi,y),\nwith a value of of 0\u2212wT\u03b4i(\u02c6y), or it will be the second best solution, i.e.,\n\u02dcy=a r g m a x\ny/negationslash=\u02c6ywT\u03c6(xi,y) (19.108)\nwhich achieves an overall value of 1\u2212wT\u03b4i(\u02dcy). For chain structured CRFs, we can use the\nViterbi algorithm to do decoding; the second best path will differ from the best path in a singleposition, which can be obtained by changing the variable whose max marginal is closest to itsdecision boundary to its second best value. We can generalize this (with a bit more work) to\ufb01nd theN-best list (Schwarz and Chow 1990; Nilsson and Goldberger 2001).\nFor Hamming loss, L(y\n\u2217,y)=/summationtext\ntI(y\u2217\nt/negationslash=yt), and for the F1 score (de\ufb01ned in Section 5.7.2.3),\nwe can devise a dynamic programming algorithm to compute Equation 19.107. See (Altun et al.2006) for details. Other models and loss function combinations will require different methods.\n19.7.3.2 A linear time algorithm\nAlthough the above algorithm takes polynomial time, we can do better, and devise an algorithmthat runs in lineartime, assuming we use a linear kernel (i.e., we work with the original features\n\u03c6(x,y)and do not apply the kernel trick). The basic idea, as explained in (Joachims et al.\n2009), is to have a single slack variable, \u03be, instead of N, but to use|Y|\nNconstraints, instead of", "730": "700 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\njustN|Y|. Speci\ufb01cally, we optimize the following (assuming the margin rescaling formulation):\nmin\nw,\u03be\u226501\n2||w||2\n2+C\u03be\ns.t.\u2200(y1,...,yN)\u2208YN:1\nNwTN/summationdisplay\ni=1\u03b4i(yi)\u22651\nNN/summationdisplay\ni=1L(yi,yi)\u2212\u03be(19.109)\nCompare this to the original version, which was\nmin\nw,\u03be\u226501\n2||w||22+C\nN\u03bes.t.\u2200i=1:N,\u2200y\u2208Y:wT\u03b4i(y)\u2265L(yi,yi)\u2212\u03bei(19.110)\nOne can show that any solution w\u2217of Equation 19.109 is also a solution of Equation 19.110 and\nvice versa, with \u03be\u2217=1\nN\u03be\u2217\ni.\nAlgorithm 19.4: Cutting plane algorithm for SSVMs (margin rescaling, 1-slack version)\n1InputD={(x1,y1),...,(xN,yn)},C,/epsilon1;\n2W=\u2205;\n3repeat\n4(w,\u03be) = argminw,\u03be\u226501\n2||w||2\n2+C/summationtextN\ni=1\u03be;\n5 s.t.\u2200(y1,...,yN)\u2208W:1\nNwT/summationtextNi=1\u03b4i(yi)\u22651\nN/summationtextNi=1L(yi,yi)\u2212\u03be;\n6fori=1:Ndo\n7 \u02c6yi=a r g m a x\u02c6yi\u2208YL(yi,\u02c6yi)+wT\u03c6(xi,\u02c6yi)\n8W=W\u222a{(\u02c6y1,...,\u02c6yN)};\n9until1\nN/summationtextNi=1L(yi,\u02c6yi)\u22121\nNwT/summationtextNi=1\u03b4i(\u02c6yi)\u2264\u03be+/epsilon1;\n10Return (w ,\u03be)\nWe can optimize Equation 19.109 using the cutting plane algorithm in Algorithm 10. (This\nis what is implemented in SVMstruct.) The inner QP in line 4 can be solved in O(N)time\nusing the method of (Joachims 2006). In line 7 we make Ncalls to the loss-augmented decoder.\nFinally, it can be shown that the number of iterations is a constant independent on N. Thus\nthe overall running time is linear.\n19.7.4 Online algorithms for \ufb01tting SSVMs\nAlthough the cutting plane algorithm can be made to run in time linear in the number of data\npoints, that can still be slow if we have a large dataset. In such cases, it is preferable to useonline learning. We brie\ufb02y mention a few possible algorithms below.\n19.7.4.1 The structured perceptron algorithm\nA very simple algorithm for \ufb01tting SSVMs is the structured perceptron algorithm (Collins\n2002). This method is an extension of the regular perceptron algorithm of Section 8.5.4. At each", "731": "19.7. StructuralSVMs 701\nstep, we compute \u02c6y=a r g m a x p(y|x)(e.g., using the Viterbi algorithm) for the current training\nsamplex.I f\u02c6y=y, we do nothing, otherwise we update the weight vector using\nwk+1=wk+\u03c6(y,x)\u2212\u03c6(\u02c6y,x) (19.111)\nTo get good performance, it is necessary to average the parameters over the last few updates\n(see Section 8.5.2 for details), rather than using the most recent value.\n19.7.4.2 Stochastic subgradient descent\nThe disadvantage of the structured perceptron algorithm is that it implicitly assumes 0-1 loss,and it does not enforce any kind of margin. An alternative approach is to perform stochasticsubgradient descent. A speci\ufb01c instance of this the Pegasos algorithm (Shalev-Shwartz et al.\n2007), which stands for \u201cprimal estimated sub-gradient solver for SVM\u201d. Pegasos was designedfor binary SVMs, but can be extended to SSVMS.\nLet us start by considering the objective function:\nf(w)=\nN/summationdisplay\ni=1max\n\u02c6yi/bracketleftbig\nL(yi,\u02c6yi)+wT\u03c6(xi,\u02c6yi)/bracketrightbig\n\u2212wT\u03c6(xi,yi)+\u03bb||w||2(19.112)\nLetting\u02c6yibe the argmax of this max. Then the subgradient of this objective function is\ng(w)=N/summationdisplay\ni=1\u03c6(xi,\u02c6yi)\u2212\u03c6(xi,yi)+2\u03bbw (19.113)\nIn stochastic subgradient descent, we approximate this gradient with a single term, i, and then\nperform an update:\nwk+1=wk\u2212\u03b7kgi(wk)=wk\u2212\u03b7k[\u03c6(xi,\u02c6yi)\u2212\u03c6(xi,yi)+(2/N)\u03bbw] (19.114)\nwhere\u03b7kis the step size parameter, which should satisfy the Robbins-Monro conditions (Sec-\ntion 8.5.2.1). (Notice that the perceptron algorithm is just a special case where \u03bb=0and\n\u03b7k=1.) To ensure that whas unit norm, we can project it onto the /lscript2ball after each update.\n19.7.5 Latent structural SVMs\nIn many applications of interest, we have latent or hidden variables h. For example, in object\ndetections problems, we may be told that the image contains an object, so y=1, but we may\nnot know where it is. The location of the object, or its pose, can be considered a hidden variable.Or in machine translation, we may know the source text x(say English) and the target text y\n(say French), but we typically do not know the alignment between the words.\nWe will extend our model as follows, to get a latent CRF:\np(y,h|x,w)=exp(w\nT\u03c6(x,y,h))\nZ(x,w)(19.115)\nZ(x,w)=/summationdisplay\ny,hexp(wT\u03c6(x,y,h)) (19.116)", "732": "702 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nIn addition, we introduce the loss function L(y\u2217,y,h); this measures the loss when the \u201caction\u201d\nthat we take is to predict yusing latent variables h. We could just use L(y\u2217,y)as before, since\nhis usually a nuisance variable and not of direct interest. However, hcan sometimes play a\nuseful role in de\ufb01ning a loss function.12\nGiven the loss function, we de\ufb01ne our objective as\nREL(w)=\u2212 logp(w)+/summationdisplay\nilog\u23a1\n\u23a3/summationdisplay\ny,hexp\u02dcL(yi,y,h)exp(wT\u03c6(x,y,h))\nZ(x,w)\u23a4\u23a6(19.117)\nUsing the same loose lower bound as before, we get\nR\nEL(w)\u2264E(w)+N/summationdisplay\ni=1max\ny,h/braceleftBig\n\u02dcL(yi,y,h)+wT\u03c6(xi,y,h)/bracerightBig\n\u2212N/summationdisplay\ni=1max\nhwT\u03c6(xi,yi,h) (19.118)\nIf we set E(w)=\u22121\n2C||w||2\n2, we get the same objective as is optimized in latent SVMs (Yu\nand Joachims 2009).\nUnfortunately, this objective is no longer convex. However, it is a difference of convex\nfunctions, and hence can be solved efficiently using the CCCPorconcave-convex procedure\n(Yuille and Rangarajan 2003). This is a method for minimizing functions of the form f(w)\u2212\ng(w),w h e r efandgare convex. The method alternates between \ufb01nding a linear upper bound\nuon\u2212g, and then minimizing the convex function f(w)+u(w); see Algorithm 6 for the\npseudocode. CCCP is guaranteed to decrease the objective at every iteration, and to converge to\na local minimum or a saddle point.\nAlgorithm 19.5: Concave-Convex Procedure (CCCP)\n1Sett=0and initialize w0;\n2repeat\n3Find hyperplane vtsuch that\u2212g(w)\u2264\u2212g(wt)+(w\u2212wt)Tvtfor allw;\n4Solvewt+1=a r g m i nwf(w)+wTvt;\n5Sett=t+1\n6untilconverged ;\nWhen applied to latent SSVMs, CCCP is very similar to (hard) EM. In the \u201cE step\u201d, we compute\n12. For example, consider the problem of learning to classify a set of documents as relevant or not to a query. That\nis, givenndocuments x1,...,x nfor a single query q, we want to produce a labeling yj\u2208{ \u22121,+1}, representing\nwhether document jis relevant to qor not. Suppose our goal is to maximize the precision at k, which is a metric widely\nused in ranking (see Section 9.7.4). We will introduce a latent variable for each document hjrepresenting its degree\nof relevance. This corresponds to a latent total ordering, that has to be consistent with the observed partial ordering\ny. Given this, we can de\ufb01ne the following loss function: L(y,\u02c6y,\u02c6h)=m i n {1,n(y)\nk}\u22121\nk/summationtextk\nj=1I(yhj=1 ),where\nn(y)is the total number of relevant documents. This loss is essentially just 1 minus the precision@k, except we replace\n1 withn(y)/kso that the loss will have a minimum of zero. See (Yu and Joachims 2009) for details.", "733": "19.7. StructuralSVMs 703\nthe linear upper bound by setting vt=\u2212C/summationtextN\ni=1\u03c6(xi,yi,h\u2217\ni),w h e r e\nhi=a r g m a x\nhwT\nt\u03c6(xi,yi,h) (19.119)\nIn the \u201cM step\u201d, we estimate wusing techniques for solving fully visible SSVMs. Speci\ufb01cally, we\nminimize\n1\n2||w||2+CN/summationdisplay\ni=1max\ny,h/braceleftbig\nL(yi,y,h)+wT\u03c6(xi,y,h)/bracerightbig\n\u2212CN/summationdisplay\ni=1wT\u03c6(xi,yi,h\u2217\ni)(19.120)\nExercises\nExercise 19.1 Derivative of the log partition function\nDerive Equation 19.40.\nExercise 19.2 CI properties of Gaussian graphical models\n(Source: Jordan.)\nIn this question, we study the relationship between sparse matrices and sparse graphs for Gaussian\ngraphical models. Consider a multivariate Gaussian N(x|\u03bc,\u03a3)in 3 dimensions. Suppose \u03bc=( 0,0,0)T\nthroughout.\nRecall that for jointly Gaussian random variables, we know that XiandXjare independent iff they are\nuncorrelated, ie. \u03a3ij=0. (This is not true in general, or even if XiandXjare Gaussian but not jointly\nGaussian.) Also, Xiis conditionally independent of Xjgiven all the other variables iff \u03a3\u22121\nij=0.\na. Suppose\n\u03a3=\u239b\n\u239d0.75 0.50 .25\n0.51.00.5\n0.25 0.50 .75\u239e\u23a0\nAre there any marginal independencies amongst X\n1,X2andX3? What about conditional indepen-\ndencies? Hint: compute \u03a3\u22121and expand out xT\u03a3\u22121x: which pairwise terms xixjare missing? Draw\nan undirected graphical model that captures as many of these independence statements (marginal and\nconditional) as possible, but does not make any false independence assertions.\nb. Suppose\n\u03a3=\u239b\n\u239d210\n121012\u239e\n\u23a0\nAre there any marginal independencies amongst X\n1,X2andX3? Are there any conditional inde-\npendencies amongst X1,X2andX3? Draw an undirected graphical model that captures as many of\nthese independence statements (marginal and conditional) as possible, but does not make any false\nindependence assertions.\nc. Now suppose the distribution on Xcan be represented by the following DAG:\nX1\u2192X2\u2192X3\nLet the CPDs be as follows:\nP(X1)=N(X1;0,1),P(X2|x1)=N(X2;x1,1),P(X3|x2)=N(X3;x2,1) (19.121)\nMultiply these 3 CPDs together and complete the square (Bishop p101) to \ufb01nd the corresponding joint\ndistribution N(X1:3|\u03bc,\u03a3). (You may \ufb01nd it easier to solve for \u03a3\u22121rather than \u03a3.)", "734": "704 Chapter19. Undirectedgraphicalmodels(Markovrandom\ufb01elds)\nd. For the DAG model in the previous question: Are there any marginal independencies amongst X1,X2\nandX3? What about conditional independencies? Draw an undirected graphical model that captures\nas many of these independence statements as possible, but does not make any false independence\nassertions (either marginal or conditional).\nExercise 19.3 Independencies in Gaussian graphical models\n(Source: MacKay.)\na. Consider the DAG X1\u2190X2\u2192X3. Assume that all the CPDs are linear-Gaussian. Which of the\nfollowing matrices couldbe the covariance matrix?\nA=\u239b\n\u239d931\n393139\u239e\n\u23a0,B=\u239b\u239d8\u221231\n\u221239\u22123\n1\u221238\u239e\u23a0,C=\u239b\u239d930\n393039\u239e\n\u23a0,D=\u239b\u239d9\u221230\n\u221231 0\u22123\n0\u221239\u239e\u23a0(19.122)\nb. Which of the above matrices could be inverse covariance matrix?\nc. Consider the DAG X1\u2192X2\u2190X3. Assume that all the CPDs are linear-Gaussian. Which of the\nabove matrices could be the covariance matrix?\nd. Which of the above matrices could be the inverse covariance matrix?\ne. Let three variables x\n1,x2,x4have covariance matrix \u03a3(1:3)and precision matrix \u03a9(1:3)=\u03a3\u22121\n(1:3)as\nfollows\n\u03a3(1:3)=\u239b\u239d10.50\n0.510 .5\n00.51\u239e\u23a0,\u03a9\n(1:3)=\u239b\u239d1.5\u221210.5\n\u221212\u22121\n0.5\u221211.5\u239e\n\u23a0 (19.123)\nNow focus on x\n1andx2. Which of the following statements about their covariance matrix \u03a3(1:2)and\nprecision matrix \u03a9(1:2)are true?\nA:\u03a3(1:2)=/parenleftbigg10.5\n0.51/parenrightbigg\n,B:\u03a9(1:2)=/parenleftbigg1.5\u22121\n\u221212/parenrightbigg\n(19.124)\nExercise 19.4 Cost of training MRFs and CRFs\n(Source: Koller.) Consider the process of gradient-ascent training for a log-linear model with kfeatures,\ngiven a data set with Ntraining instances. Assume for simplicity that the cost of computing a single\nfeature over a single instance in our data set is constant, as is the cost of computing the expected value\nof each feature once we compute a marginal over the variables in its scope. Assume that it takes ctime\nto compute all the marginals for each data case. Also, assume that we need riterations for the gradient\nprocess to converge.\n\u2022 Using this notation, what is the time required to train an MRF in big-O notation?\n\u2022 Using this notation, what is the time required to train a CRF in big-O notation?\nExercise 19.5 Full conditional in an Ising model\nConsider an Ising model\np(x1,...,x n|\u03b8)=1\nZ(\u03b8)/productdisplay\n<ij>exp(J ijxixj)n/productdisplay\ni=1exp(hixi) (19.125)\nwhere<i j>denotes all unique pairs (i.e., all edges), Jij\u2208Ris the coupling strength (weight) on edge\ni\u2212j,hi\u2208Ris the local evidence (bias term), and \u03b8=(J,h)are all the parameters.", "735": "19.7. StructuralSVMs 705\nIfxi\u2208{0,1}, derive an expression for the full conditional\np(xi=1|x\u2212i,\u03b8)=p(xi=1|xnbi,\u03b8) (19.126)\nwherex\u2212iare all nodes except i, andnbiare the neighbors of iin the graph. Hint: you answer should\nuse the sigmoid/ logistic function \u03c3(z)=1/(1 +e\u2212z). Now suppose xi\u2208{ \u22121,+1}. Derive a related\nexpression for p(xi|x\u2212i,\u03b8)in this case. (This result can be used when applying Gibbs sampling to the\nmodel.)", "736": "", "737": "20 Exact inference for graphical models\n20.1 Introduction\nIn Section 17.4.3, we discussed the forwards-backwards algorithm, which can exactly compute the\nposterior marginals p(xt|v,\u03b8)in any chain-structured graphical model, where xare the hidden\nvariables (assumed discrete) and vare the visible variables. This algorithm can be modi\ufb01ed\nto compute the posterior mode and posterior samples. A similar algorithm for linear-Gaussianchains, known as the Kalman smoother, was discussed in Section 18.3.2. Our goal in this chapteris to generalize these exact inference algorithms to arbitrary graphs. The resulting methods applyto both directed and undirected graphical models. We will describe a variety of algorithms, butwe omit their derivations for brevity. See e.g., (Darwiche 2009; Koller and Friedman 2009) for adetailed exposition of exact inference techniques for discrete directed graphical models.\n20.2 Belief propagation for trees\nIn this section, we generalize the forwards-backwards algorithm from chains to trees. Theresulting algorithm is known as belief propagation (BP) (Pearl 1988), or the sum-product\nalgorithm.\n20.2.1 Serial protocol\nWe initially assume (for notational simplicity) that the model is a pairwise MRF (or CRF), i.e.,\np(x|v)=1\nZ(v)/productdisplay\ns\u2208V\u03c8s(xs)/productdisplay\n(s,t)\u2208E\u03c8s,t(xs,xt) (20.1)\nwhere\u03c8sis the local evidence for node s, and\u03c8stis the potential for edge s\u2212t. We will\nconsider the case of models with higher order cliques (such as directed trees) later on.\nOne way to implement BP for undirected trees is as follows. Pick an arbitrary node and call it\nthe root,r. Now orient all edges away from r(intuitively, we can imagine \u201cpicking up the graph\u201d\nat noderand letting all the edges \u201cdangle\u201d down). This gives us a well-de\ufb01ned notion of parent\nand child. Now we send messages up from the leaves to the root (the collect evidence phase)\nand then back down from the root (the distribute evidence phase), in a manner analogous to\nforwards-backwards on chains.", "738": "708 Chapter20. Exactinferenceforgraphicalmodels\ns1s2s\nu1u2utroot\nv\u2212\nst\n(a)s1s2s\nu1u2utrootv+\nst\n(b)\nFigure 20.1 Message passing on a tree. (a) Collect-to-root phase. (b) Distribute-from-root phase.\nTo explain the process in more detail, consider the example in Figure 20.1. Suppose we want\nto compute the belief state at node t. We will initially condition the belief only on evidence that\nis at or below tin the graph, i.e., we want to compute bel\u2212\nt(xt)/definesp(xt|v\u2212\nt). We will call this a\n\u201cbottom-up belief state\u201d. Suppose, by induction, that we have computed \u201cmessages\u201d from t\u2019s two\nchildren, summarizing what they think tshould know about the evidence in their subtrees, i.e.,\nwe have computed m\u2212\ns\u2192t(xt)=p(xt|v\u2212\nst),w h e r ev\u2212\nstis all the evidence on the downstream\nside of the s\u2212tedge (see Figure 20.1(a)), and similarly we have computed mu\u2192t(xt). Then we\ncan compute the bottom-up belief state at tas follows:\nbel\u2212\nt(xt)/definesp(xt|v\u2212\nt)=1\nZt\u03c8t(xt)/productdisplay\nc\u2208ch(t)m\u2212\nc\u2192t(xt) (20.2)\nwhere\u03c8t(xt)\u221dp(xt|vt)is the local evidence for node t, andZtis the local normalization\nconstant. In words, we multiply all the incoming messages from our children, as well as the\nincoming message from our local evidence, and then normalize.\nWe have explained how to compute the bottom-up belief states from the bottom-up messages.\nHow do we compute the messages themselves? Consider computing m\u2212\ns\u2192t(xt),w h e r esis one\noft\u2019s children. Assume, by recursion, that we have computed bel\u2212\ns(xs)=p(xs|v\u2212\ns). Then we\ncan compute the message as follows:\nm\u2212\ns\u2192t(xt)=/summationdisplay\nxs\u03c8st(xs,xt)bel\u2212\ns(xs) (20.3)\nEssentially we convert beliefs about xsinto beliefs about xtby using the edge potential \u03c8st.\nWe continue in this way up the tree until we reach the root. Once at the root, we have \u201cseen\u201d\nall the evidence in the tree, so we can compute our local belief state at the root using\nbelr(xr)/definesp(xr|v)=p(xt|v\u2212\nr)\u221d\u03c8r(xr)/productdisplay\nc\u2208ch(r)m\u2212\nc\u2192r(xr) (20.4)\nThis completes the end of the upwards pass, which is analogous to the forwards pass in an\nHMM. As a \u201cside effect\u201d, we can compute the probability of the evidence by collecting the", "739": "20.2. Beliefpropagationfortrees 709\nnormalization constants:\np(v)=/productdisplay\ntZt (20.5)\nWe can now pass messages down from the root. For example, consider node s, with parent t,\nas shown in Figure 20.1(b). To compute the belief state for s, we need to combine the bottom-up\nbelief for stogether with a top-down message from t, which summarizes all the information in\nthe rest of the graph, m+\nt\u2192s(xs)/definesp(xt|v+\nst),w h e r ev+\nstis all the evidence on the upstream\n(root) side of the s\u2212tedge, as shown in Figure 20.1(b). We then have\nbels(xs)/definesp(xs|v)\u221dbel\u2212\ns(xs)/productdisplay\nt\u2208pa(s)m+\nt\u2192s(xt) (20.6)\nHow do we compute these downward messages? For example, consider the message from t\ntos. Suppose t\u2019s parent is r, andt\u2019s children are sandu, as shown in Figure 20.1(b). We want\nto include in m+t\u2192sall the information that thas received, except for the information that s\nsent it:\nm+t\u2192s(xs)/definesp(xs|v+\nst)=/summationdisplay\nxt\u03c8st(xs,xt)belt(xt)\nm\u2212s\u2192t(xt)(20.7)\nRather than dividing out the message sent up to t, we can plug in the equation of beltto get\nm+t\u2192s(xs)=/summationdisplay\nxt\u03c8st(xs,xt)\u03c8t(xt)/productdisplay\nc\u2208ch(t),c/negationslash=sm\u2212c\u2192t(xt)/productdisplay\np\u2208pa(t)m+p\u2192t(xt) (20.8)\nIn other words, we multiply together all the messages coming into tfrom all nodes except for\nthe recipient s, combine together, and then pass through the edge potential \u03c8st. In the case of\na chain,tonly has one child sand one parent p, so the above simpli\ufb01es to\nm+t\u2192s(xs)=/summationdisplay\nxt\u03c8st(xs,xt)\u03c8t(xt)m+p\u2192t(xt) (20.9)\nThe version of BP in which we use division is called belief updating, and the version in\nwhich we multiply all-but-one of the messages is called sum-product. The belief updating\nversion is analogous to how we formulated the Kalman smoother in Section 18.3.2: the top-\ndown messages depend on the bottom-up messages. This means they can be interpreted asconditional posterior probabilities. The sum-product version is analogous to how we formulatedthe backwards algorithm in Section 17.4.3: the top-down messages are completely independentof the bottom-up messages, which means they can only be interpreted as conditional likelihoods.See Section 18.3.2.3 for a more detailed discussion of this subtle difference.\n20.2.2 Parallel protocol\nSo far, we have presented a serial version of the algorithm, in which we send messages upto the root and back. This is the optimal approach for a tree, and is a natural extension offorwards-backwards on chains. However, as a prelude to handling general graphs with loops, wenow consider a parallel version of BP. This gives equivalent results to the serial version but isless efficient when implemented on a serial machine.", "740": "710 Chapter20. Exactinferenceforgraphicalmodels\nThe basic idea is that all nodes receive messages from their neighbors in parallel, they then\nupdates their belief states, and \ufb01nally they send new messages back out to their neighbors.\nThis process repeats until convergence. This kind of computing architecture is called a systolic\narray, due to its resemblance to a beating heart.\nMore precisely, we initialize all messages to the all 1\u2019s vector. Then, in parallel, each node\nabsorbs messages from all its neighbors using\nbels(xs)\u221d\u03c8s(xs)/productdisplay\nt\u2208nbrsmt\u2192s(xs) (20.10)\nThen, in parallel, each node sends messages to each of its neighbors:\nms\u2192t(xt)=/summationdisplay\nxs\u239b\n\u239d\u03c8s(xs)\u03c8st(xs,xt)/productdisplay\nu\u2208nbrs\\tmu\u2192s(xs)\u239e\u23a0 (20.11)\nThem\ns\u2192tmessage is computed by multiplying together all incoming messages, except the one\nsent by the recipient, and then passing through the \u03c8stpotential.\nAt iteration Tof the algorithm, bels(xs)represents the posterior belief of xsconditioned on\nthe evidence that is Tsteps away in the graph. After D(G)steps, where D(G)is thediameter\nof the graph (the largest distance between any two pairs of nodes), every node has obtained\ninformation from all the other nodes. Its local belief state is then the correct posterior marginal.Since the diameter of a tree is at most |V|\u22121, the algorithm converges in a linear number of\nsteps.\nWe can actually derive the up-down version of the algorithm by imposing the condition that\na node can only send a message once it has received messages from all its other neighbors.This means we must start with the leaf nodes, which only have one neighbor. The messagesthen propagate up to the root and back. We can also update the nodes in a random order.The only requirement is that each node get updated D(G)times. This is just enough time for\ninformation to spread throughout the whole tree.\nSimilar parallel, distributed algorithms for solving linear systems of equations are discussed\nin (Bertsekas 1997). In particular, the Gauss-Seidel algorithm is analogous to the serial up-downversion of BP, and the Jacobi algorithm is analogous to the parallel version of BP.\n20.2.3 Gaussian BP *\nNow consider the case where p(x|v)is jointly Gaussian, so it can be represented as a Gaussian\npairwise MRF, as in Section 19.4.4. We now present the belief propagation algorithm for thisclass of models, follow the presentation of (Bickson 2009) (see also (Malioutov et al. 2006)). Wewill assume the following node and edge potentials:\n\u03c8\nt(xt)=e x p (\u22121\n2Attx2\nt+btxt) (20.12)\n\u03c8st(xs,xt)=e x p (\u22121\n2xsAstxt) (20.13)\nso the overall model has the form\np(x|v)\u221dexp(\u22121\n2xTAx+bTx) (20.14)", "741": "20.2. Beliefpropagationfortrees 711\nThis is the information form of the MVN (see Exercise 9.2), where Ais the precision matrix.\nNote that by completing the square, the local evidence can be rewritten as a Gaussian:\n\u03c8t(xt)\u221dN(bt/Att,A\u22121\ntt)/definesN(mt,/lscript\u22121\nt) (20.15)\nBelow we describe how to use BP to compute the posterior node marginals,\np(xt|v)=N(\u03bct,\u03bb\u22121\nt) (20.16)\nIf the graph is a tree, the method is exact. If the graph is loopy, the posterior means may still\nbe exact, but the posterior variances are often too small (Weiss and Freeman 1999).\nAlthough the precision matrix Ais often sparse, computing the posterior mean requires\ninverting it, since \u03bc=A\u22121b. BP provides a way to exploit graph structure to perform this\ncomputation in O(D)time instead of O(D3). This is related to various methods from linear\nalgebra, as discussed in (Bickson 2009).\nSince the model is jointly Gaussian, all marginals and all messages will be Gaussian. The\nkey operations we need are to multiply together two Gaussian factors, and to marginalize out avariable from a joint Gaussian factor.\nFor multiplication, we can use the fact that the product of two Gaussians is Gaussian:\nN(x|\u03bc\n1,\u03bb\u22121\n1)\u00d7N(x|\u03bc2,\u03bb\u22121\n2)=CN(x|\u03bc,\u03bb\u22121) (20.17)\n\u03bb=\u03bb1+\u03bb2 (20.18)\n\u03bc=\u03bb\u22121(\u03bc1\u03bb1+\u03bc2\u03bb2) (20.19)\nwhere\nC=/radicalbigg\n\u03bb\n\u03bb1\u03bb2exp/parenleftbigg1\n2(\u03bb1\u03bc2\n1(\u03bb\u22121\u03bb1\u22121)+\u03bb2\u03bc22(\u03bb\u22121\u03bb2\u22121)+2\u03bb\u22121\u03bb1\u03bb2\u03bc1\u03bc2)/parenrightbigg\n(20.20)\nSee Exercise 20.2 for the proof.\nFor marginalization, we have the following result:\n/integraldisplay\nexp(\u2212ax2+bx)dx=/radicalbig\n\u03c0/aexp(b2/4a) (20.21)\nwhich follows from the normalization constant of a Gaussian (Exercise 2.11).\nWe now have all the pieces we need. In particular, let the message ms\u2192t(xt)be a Gaussian\nwith mean \u03bcstand precision \u03bbst. From Equation 20.10, the belief at node sis given by the\nproduct of incoming messages times the local evidence (Equation 20.15) and hence\nbels(xs)=\u03c8 s(xs)/productdisplay\nt\u2208nbr(s)mts(xs)=N(xs|\u03bcs,\u03bb\u22121\ns) (20.22)\n\u03bbs=/lscripts+/summationdisplay\nt\u2208nbr(s)\u03bbts (20.23)\n\u03bcs=\u03bb\u22121\ns\u239b\n\u239d/lscriptsms+/summationdisplay\nt\u2208nbr(s)\u03bbts\u03bcts\u239e\u23a0 (20.24)", "742": "712 Chapter20. Exactinferenceforgraphicalmodels\nTo compute the messages themselves, we use Equation 20.11, which is given by\nms\u2192t(xt)=/integraldisplay\nxs\u239b\n\u239d\u03c8st(xs,xt)\u03c8s(xs)/productdisplay\nu\u2208nbrs\\tmu\u2192s(xs)\u239e\u23a0dx\ns (20.25)\n=/integraldisplay\nxs\u03c8st(xs,xt)fs\\t(xs)dxs (20.26)\nwherefs\\t(xs)is the product of the local evidence and all incoming messages excluding the\nmessage from t:\nfs\\t(xs)/defines\u03c8s(xs)/productdisplay\nu\u2208nbrs\\tmu\u2192s(xs) (20.27)\n=N(xs|\u03bcs\\t,\u03bb\u22121\ns\\t) (20.28)\n\u03bbs\\t/defines/lscripts+/summationdisplay\nu\u2208nbr(s)\\t\u03bbus (20.29)\n\u03bcs\\t/defines\u03bb\u22121\ns\\t\u239b\u239d/lscript\nsms+/summationdisplay\nu\u2208nbr(s)\\t\u03bbus\u03bcus\u239e\u23a0 (20.30)\nReturning to Equation 20.26 we have\nm\ns\u2192t(xt)=/integraldisplay\nxsexp(\u2212xsAstxt)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c8st(xs,xt)exp(\u2212\u03bbs\\t/2(xs\u2212\u03bcs\\t)2)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\nfs\\t(xs)dxs (20.31)\n=/integraldisplay\nxsexp/parenleftbig\n(\u2212\u03bbs\\tx2\ns/2)+(\u03bbs\\t\u03bcs\\t\u2212Astxt)xs/parenrightbig\ndxs+const (20.32)\n\u221dexp/parenleftbig\n(\u03bbs\\t\u03bcs\\t\u2212Astxt)2/(2\u03bbs\\t)/parenrightbig\n(20.33)\n\u221dN(\u03bcst,\u03bb\u22121\nst) (20.34)\n\u03bbst=A2st/\u03bbs\\t (20.35)\n\u03bcst=Ast\u03bcs\\t/\u03bbst (20.36)\nOne can generalize these equations to the case where each node is a vector, and the messages\nbecome small MVNs instead of scalar Gaussians (Alag and Agogino 1996). If we apply the\nresulting algorithm to a linear dynamical system, we recover the Kalman smoothing algorithmof Section 18.3.2.\nTo perform message passing in models with non-Gaussian potentials, one can use sampling\nmethods to approximate the relevant integrals. This is called non-parametric BP (Sudderth\net al. 2003; Isard 2003; Sudderth et al. 2010).\n20.2.4 Other BP variants *\nIn this section, we brie\ufb02y discuss several variants of the main algorithm.", "743": "713\nxt xt+1\nvt vt+1\u03c8t,t+1\n\u03c8t \u03c8t+1ft \u03b2t+1\nFigure 20.2 Illustration of how to compute the two-slice distribution for an HMM. The \u03c8tand\u03c8t+1\nterms are the local evidence messages from the visible nodes vt,vt+1to the hidde nodes xt,xt+1\nrespectively; ftis the forwards message from xt\u22121and\u03b2t+1is the backwards message from xt+2.\n20.2.4.1 Max-product algorithm\nIt is possible to devise a max-product version of the BP algorithm, by replacing the/summationtextoperator\nwith themaxoperator. We can then compute the local MAP marginal of each node. However,\nif there are ties, this might not be globally consistent, as discussed in Section 17.4.4. Fortunately,\nwe can generalize the Viterbi algorithm to trees, where we use max and argmax in the collect-to-root phase, and perform traceback in the distribute-from-root phase. See (Dawid 1992) fordetails.\n20.2.4.2 Sampling from a tree\nIt is possible to draw samples from a tree structured model by generalizing the forwards \ufb01ltering/ backwards sampling algorithm discussed in Section 17.4.5. See (Dawid 1992) for details.\n20.2.4.3 Computing posteriors on sets of variables\nIn Section 17.4.3.2, we explained how to compute the \u201ctwo-slice\u201d distribution \u03be\nt,t+1(i,j)=\np(xt=i,xt+1=j|v)in an HMM, namely by using\n\u03bet,t+1(i,j)=\u03b1 t(i)\u03c8t+1(j)\u03b2t+1(j)\u03c8t,t+1(i,j) (20.37)\nSince\u03b1t(i)\u221d\u03c8t(i)ft(i),w h e r eft=p(xt|v1:t\u22121)is the forwards message, we can think of\nthis as sending messages ftand\u03c8tintoxt,\u03b2t+1and\u03c6t+1intoxt+1, and then combining\nthem with the \u03a8matrix, as shown in Figure 20.2. This is like treating xtandxt+1as a single\n\u201cmega node\u201d, and then multiplying all the incoming messages as well as all the local factors(here,\u03c8\nt,t+1).20.2. Beliefpropagationfortrees", "744": "714 Chapter20. Exactinferenceforgraphicalmodels\nCoherence\nDi\ufb03culty\nGradeIntelligence\nSAT\nLetter\nHappyJob\n(a)Coherence\nDi\ufb03culty\nGradeIntelligence\nSAT\nLetter\nHappyJob\n(b)\nFigure 20.3 Left: The \u201cstudent\u201d DGM. Right: the equivalent UGM. We add moralization arcs D-I, G-J and\nL-S. Based on Figure 9.8 of (Koller and Friedman 2009).\n20.3 The variable elimination algorithm\nWe have seen how to use BP to compute exact marginals on chains and trees. In this section,\nwe discuss an algorithm to compute p(xq|xv)for any kind of graph.\nWe will explain the algorithm by example. Consider the DGM in Figure 20.3(a). This model,\nfrom (Koller and Friedman 2009), is a hypothetical model relating various variables pertaining toa typical student. The corresponding joint has the following form:\nP(C,D,I,G,S,L,J,H ) (20.38)\n=P(C)P(D|C)P(I)P(G|I,D)P(S|I)P(L|G)P(J|L,S)P(H|G,J) (20.39)\nNote that the forms of the CPDs do not matter, since all our calculations will be symbolic.However, for illustration purposes, we will assume all variables are binary.\nBefore proceeding, we convert our model to undirected form. This is not required, but it\nmakes for a more uni\ufb01ed presentation, since the resulting method can then be applied to bothDGMs and UGMs (and, as we will see in Section 20.3.1, to a variety of other problems thathave nothing to do with graphical models). Since the computational complexity of inference inDGMs and UGMs is, generally speaking, the same, nothing is lost in this transformation from acomputational point of view.\n1\nTo convert the DGM to a UGM, we simply de\ufb01ne a potential or factor for every CPD, yielding\np(C,D,I,G,S,L,J,H ) (20.40)\n=\u03c8C(C)\u03c8D(D,C)\u03c8I(I)\u03c8G(G,I,D)\u03c8S(S,I)\u03c8L(L,G)\u03c8J(J,L,S)\u03c8H(H,G,J)(20.41)\n1. There are a few \u201ctricks\u201d one can exploit in the directed case that cannot easily be exploited in the undirected case.\nOne important example is barren node removal. To explain this, consider a naive Bayes classi\ufb01er, as in Figure 10.2.\nSuppose we want to infer yand we observe x1andx2, but not x3andx4. It is clear that we can safely remove\nx3andx4, since/summationtext\nx3p(x3|y)=1, and similarly for x4. In general, once we have removed hidden leaves, we can\napply this process recursively. Since potential functions do not necessary sum to one, we cannot use this trick in theundirected case. See (Koller and Friedman 2009) for a variety of other speedup tricks.", "745": "20.3. Thevariableeliminationalgorithm 715\nSince all the potentials are locally normalized, since they are CPDs, there is no need for a\nglobal normalization constant, so Z=1. The corresponding undirected graph is shown in\nFigure 20.3(b). Note that it has more edges than the DAG. In particular, any \u201cunmarried\u201d nodes\nthat share a child must get \u201cmarried\u201d, by adding an edge between them; this process is knownasmoralization. Only then can the arrows be dropped. In this example, we added D-I, G-J, and\nL-S moralization arcs. The reason this operation is required is to ensure that the CI propertiesof the UGM match those of the DGM, as explained in Section 19.2.2. It also ensures there is aclique that can \u201cstore\u201d the CPDs of each family.\nNow suppose we want to compute p(J=1 ), the marginal probability that a person will get a\njob. Since we have 8 binary variables, we could simply enumerate over all possible assignmentsto all the variables (except for J), adding up the probability of each joint instantiation:\np(J)=/summationdisplay\nL/summationdisplay\nS/summationdisplay\nG/summationdisplay\nH/summationdisplay\nI/summationdisplay\nD/summationdisplay\nCp(C,D,I,G,S,L,J,H ) (20.42)\nHowever, this would take O(27)time. We can be smarter by pushing sums inside products.\nThis is the key idea behind the variable elimination algorithm (Zhang and Poole 1996), also\ncalledbucket elimination (Dechter 1996), or, in the context of genetic pedigree trees, the\npeeling algorithm (Cannings et al. 1978). In our example, we get\np(J)=/summationdisplay\nL,S,G,H,I,D,Cp(C,D,I,G,S,L,J,H )\n=/summationdisplay\nL,S,G,H,I,D,C\u03c8C(C)\u03c8D(D,C)\u03c8I(I)\u03c8G(G,I,D)\u03c8S(S,I)\u03c8L(L,G)\n\u00d7\u03c8J(J,L,S)\u03c8H(H,G,J)\n=/summationdisplay\nL,S\u03c8J(J,L,S)/summationdisplay\nG\u03c8L(L,G)/summationdisplay\nH\u03c8H(H,G,J)/summationdisplay\nI\u03c8S(S,I)\u03c8I(I)\n\u00d7/summationdisplay\nD\u03c8G(G,I,D)/summationdisplay\nC\u03c8C(C)\u03c8D(D,C)\nWe now evaluate this expression, working right to left as shown in Table 20.1. First we multiplytogether all the terms in the scope of the/summationtext\nCoperator to create the temporary factor\n\u03c4/prime\n1(C,D)=\u03c8C(C)\u03c8D(D,C) (20.43)\nThen we marginalize out Cto get the new factor\n\u03c41(D)=/summationdisplay\nC\u03c4/prime\n1(C,D) (20.44)\nNext we multiply together all the terms in the scope of the/summationtext\nDoperator and then marginalize\nout to create\n\u03c4/prime\n2(G,I,D)=\u03c8 G(G,I,D)\u03c41(D) (20.45)\n\u03c42(G,I)=/summationdisplay\nD\u03c4/prime\n2(G,I,D) (20.46)", "746": "716 Chapter20. Exactinferenceforgraphicalmodels\n/summationdisplay\nL/summationdisplay\nS\u03c8J(J,L,S)/summationdisplay\nG\u03c8L(L,G)/summationdisplay\nH\u03c8H(H,G,J )/summationdisplay\nI\u03c8S(S,I)\u03c8I(I)/summationdisplay\nD\u03c8G(G,I,D)/summationdisplay\nC\u03c8C(C)\u03c8D(D,C)\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c41(D)\n/summationdisplay\nL/summationdisplay\nS\u03c8J(J,L,S)/summationdisplay\nG\u03c8L(L,G)/summationdisplay\nH\u03c8H(H,G,J )/summationdisplay\nI\u03c8S(S,I)\u03c8I(I)/summationdisplay\nD\u03c8G(G,I,D)\u03c41(D)\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c42(G,I)\n/summationdisplay\nL/summationdisplay\nS\u03c8J(J,L,S)/summationdisplay\nG\u03c8L(L,G)/summationdisplay\nH\u03c8H(H,G,J )/summationdisplay\nI\u03c8S(S,I)\u03c8I(I)\u03c42(G,I)\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c43(G,S)\n/summationdisplay\nL/summationdisplay\nS\u03c8J(J,L,S)/summationdisplay\nG\u03c8L(L,G)/summationdisplay\nH\u03c8H(H,G,J )\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c44(G,J)\u03c43(G,S)\n/summationdisplay\nL/summationdisplay\nS\u03c8J(J,L,S)/summationdisplay\nG\u03c8L(L,G)\u03c4 4(G,J)\u03c43(G,S)\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c45(J,L,S)\n/summationdisplay\nL/summationdisplay\nS\u03c8J(J,L,S)\u03c45(J,L,S)\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c46(J,L)\n/summationdisplay\nL\u03c46(J,L)\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c47(J)\nTable 20.1 Eliminating variables from Figure 20.3 in the order C,D,I,H,G,S,L to compute P(J).\nNext we multiply together all the terms in the scope of the/summationtext\nIoperator and then marginalize\nout to create\n\u03c4/prime\n3(G,I,S)=\u03c8 S(S,I)\u03c8I(I)\u03c42(G,I) (20.47)\n\u03c43(G,S)=/summationdisplay\nI\u03c4/prime\n3(G,I,S) (20.48)\nAnd so on.\nThe above technique can be used to compute any marginal of interest, such as p(J)o r\np(J,H). To compute a conditional, we can take a ratio of two marginals, where the visible\nvariables have been clamped to their known values (and hence don\u2019t need to be summed over).\nFor example,\np(J=j|I=1,H=0 )=p(J=j,I=1,H=0 )/summationtext\nj/primep(J=j/prime,I=1,H=0 )(20.49)\nIn general, we can write\np(xq|xv)=p(xq,xv)\np(xv)=/summationtext\nxhp(xh,xq,xv)/summationtext\nxh/summationtext\nx/primeqp(xh,x/primeq,xv)(20.50)", "747": "20.3. Thevariableeliminationalgorithm 717\nThe normalization constant in the denominator, p(xv), is called the probability of the evi-\ndence.\nSeevariableElimination for a simple Matlab implementation of this algorithm, which\nworks for arbitrary graphs, and arbitrary discrete factors. But before you go too crazy, please\nread Section 20.3.2, which points out that VE can be exponentially slow in the worst case.\n20.3.1 The generalized distributive law *\nAbstractly, VE can be thought of as computing the following expression:\np(xq|xv)\u221d/summationdisplay\nx/productdisplay\nc\u03c8c(xc) (20.51)\nIt is understood that the visible variables xvare clamped, and not summed over. VE uses\nnon-serial dynamic programming (Bertele and Brioschi 1972), caching intermediate results to\navoid redundant computation.\nHowever, there are other tasks we might like to solve for any given graphical model. For\nexample, we might want the MAP estimate:\nx\u2217=a r g m a x\nx/productdisplay\nc\u03c8c(xc) (20.52)\nFortunately, essentially the same algorithm can also be used to solve this task: we just replacesum with max. (We also need a traceback step, which actually recovers the argmax, as opposed\nto just the value of max; these details are explained in Section 17.4.4.)\nIn general, VE can be applied to any commutative semi-ring. This is a set K, together with\ntwo binary operations called \u201c+\u201d and \u201c\u00d7 \u201d, which satisfy the following three axioms:\n1. The operation \u201c+\u201d is associative and commutative, and there is an additive identity element\ncalled \u201c0\u201d such that k+0=kfor allk\u2208K.\n2. The operation \u201c\u00d7 \u201d is associative and commutative, and there is a multiplicative identity\nelement called \u201c1\u201d such that k\u00d71=kfor allk\u2208K.\n3. Thedistributive law holds, i.e.,\n(a\u00d7b)+(a\u00d7c)=a\u00d7(b+c) (20.53)\nfor all triples (a,b,c)fromK.\nThisframeworkcoversanextremelywiderangeofimportantapplications, includingconstraint\nsatisfaction problems (Bistarelli et al. 1997; Dechter 2003), the fast Fourier transform (Aji andMcEliece 2000), etc. See Table 20.2 for some examples.\n20.3.2 Computational complexity of VE\nThe running time of VE is clearly exponential in the size of the largest factor, since we have sumover all of the corresponding variables. Some of the factors come from the original model (andare thus unavoidable), but new factors are created in the process of summing out. For example,", "748": "718 Chapter20. Exactinferenceforgraphicalmodels\nDomain + \u00d7 Name\n[0,\u221e)( + ,0) (\u00d7,1)sum-product\n[0,\u221e)( m a x ,0) (\u00d7,1)max-product\n(\u2212\u221e,\u221e]( m i n ,\u221e)( + ,0)min-sum\n{T,F}(\u2228,F)(\u2227,T)Boolean satis\ufb01ability\nTable 20.2 Some commutative semirings.\n/summationdisplay\nD/summationdisplay\nC\u03c8D(D,C)/summationdisplay\nH/summationdisplay\nL/summationdisplay\nS\u03c8J(J,L,S)/summationdisplay\nI\u03c8I(I)\u03c8S(S,I)/summationdisplay\nG\u03c8G(G,I,D)\u03c8L(L,)\u03c8H(H,G,J )\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c41(I,D,L,J,H )\n/summationdisplay\nD/summationdisplay\nC\u03c8D(D,C)/summationdisplay\nH/summationdisplay\nL/summationdisplay\nS\u03c8J(J,L,S)/summationdisplay\nI\u03c8I(I)\u03c8S(S,I)\u03c41(I,D,L,J,H )\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c42(D,L,S,J,H )\n/summationdisplay\nD/summationdisplay\nC\u03c8D(D,C)/summationdisplay\nH/summationdisplay\nL/summationdisplay\nS\u03c8J(J,L,S)\u03c42(D,L,S,J,H )\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c43(D,L,J,H )\n/summationdisplay\nD/summationdisplay\nC\u03c8D(D,C)/summationdisplay\nH/summationdisplay\nL\u03c43(D,L,J,H )\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c44(D,J,H )\n/summationdisplay\nD/summationdisplay\nC\u03c8D(D,C)/summationdisplay\nH\u03c44(D,J,H )\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c45(D,J)\n/summationdisplay\nD/summationdisplay\nC\u03c8D(D,C)\u03c45(D,J)\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c46(D,J)\n/summationdisplay\nD\u03c46(D,J)\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n\u03c47(J)\nTable 20.3 Eliminating variables from Figure 20.3 in the order G,I,S,L,H,C,D .\nin Equation 20.47, we created a factor involving G, I and S; but these nodes were not originally\npresent together in any factor.\nThe order in which we perform the summation is known as the elimination order. This\ncan have a large impact on the size of the intermediate factors that are created. For example,consider the ordering in Table 20.1: the largest created factor (beyond the original ones in themodel) has size 3, corresponding to \u03c4\n5(J,L,S). Now consider the ordering in Table 20.3: now\nthe largest factors are \u03c41(I,D,L,J,H )and\u03c42(D,L,S,J,H ), which are much bigger.\nWe can determine the size of the largest factor graphically, without worrying about the actual\nnumerical values of the factors. When we eliminate a variable Xt, we connect it to all variables", "749": "20.3. Thevariableeliminationalgorithm 719\nCoherence\nDi\ufb03culty\nGradeIntelligence\nSAT\nLetter\nHappyJob\n(a)Coherence\nDi\ufb03culty\nGradeIntelligence\nSAT\nLetter\nHappyJob\n(b)Coherence\nDi\ufb03culty\nGradeIntelligence\nSAT\nLetter\nHappyJob\n(c)\nFigure 20.4 Example of the elimination process, in the order C,D,I, etc. When we eliminate I(\ufb01gure\nc), we add a \ufb01ll-in edge between GandS, since they are not connected. Based on Figure 9.10 of (Koller\nand Friedman 2009).\nthat share a factor with Xt(to re\ufb02ect the new temporary factor \u03c4/prime\nt). The edges created by this\nprocess are called \ufb01ll-in edges. For example, Figure 20.4 shows the \ufb01ll-in edges introduced\nwhen we eliminate in the order C,D,I,... . The \ufb01rst two steps do not introduce any \ufb01ll-ins,\nbut when we eliminate I, we connect GandS, since they co-occur in Equation 20.48.\nLetG(\u227a)be the (undirected) graph induced by applying variable elimination to Gusing\nelimination ordering \u227a. The temporary factors generated by VE correspond to maximal cliques\nin the graph G(\u227a). For example, with ordering (C,D,I,H,G,S,L ), the maximal cliques are\nas follows:\n{C,D},{D,I,G},{G,L,S,J},{G,J,H},{G,I,S} (20.54)\nIt is clear that the time complexity of VE is\n/productdisplay\nc\u2208C(G(\u227a))K|c|(20.55)\nwhereCare the cliques that are created, |c|is the size of the clique c, and we assume for\nnotational simplicity that all the variables have Kstates each.\nLet us de\ufb01ne the induced width of a graph given elimination ordering \u227a, denoted w(\u227a),a s\nthe size of the largest factor (i.e., the largest clique in the induced graph ) minus 1. Then it is\neasy to see that the complexity of VE with ordering \u227aisO(Kw(\u227a)+1).\nObviously we would like to minimize the running time, and hence the induced width. Let us\nde\ufb01ne the treewidth of a graph as the minimal induced width.\nw/definesmin\n\u227amax\nc\u2208G(\u227a)|c|\u22121 (20.56)\nThen clearly the best possible running time for VE is O(DKw+1). Unfortunately, one can show\nthat for arbitrary graphs, \ufb01nding an elimination ordering \u227athat minimizes w(\u227a)is NP-hard\n(Arnborg et al. 1987). In practice greedy search techniques are used to \ufb01nd reasonable orderings(Kjaerulff 1990), although people have tried other heuristic methods for discrete optimization,", "750": "720 Chapter20. Exactinferenceforgraphicalmodels\nsuch as genetic algorithms (Larranaga et al. 1997). It is also possible to derive approximate\nalgorithms with provable performance guarantees (Amir 2010).\nIn some cases, the optimal elimination ordering is clear. For example, for chains, we should\nwork forwards or backwards in time. For trees, we should work from the leaves to the root.These orderings do not introduce any \ufb01ll-in edges, so w=1. Consequently, inference in chains\nand trees takes O(VK\n2)time. This is one reason why Markov chains and Markov trees are so\nwidely used.\nUnfortunately, for other graphs, the treewidth is large. For example, for an m\u00d7n2d lattice,\nthe treewidth is O(min{m,n}) (Lipton and Tarjan 1979). So VE on a 100\u00d7100Ising model\nwould take O(2100)time.\nOf course, just because VE is slow doesn\u2019t mean that there isn\u2019t some smarter algorithm out\nthere. We discuss this issue in Section 20.5.\n20.3.3 A weakness of VE\nThe main disadvantage of the variable elimination algorithm (apart from its exponential depen-dence on treewidth) is that it is inefficient if we want to compute multiple queries conditionedon the same evidence. For example, consider computing all the marginals in a chain-structuredgraphical model such as an HMM. We can easily compute the \ufb01nal marginal p(x\nT|v)by elimi-\nnating all the nodes x1toxT\u22121in order. This is equivalent to the forwards algorithm, and takes\nO(K2T)time. But now suppose we want to compute p(xT\u22121|v). We have to run VE again, at\na cost of O(K2T)time. So the total cost to compute all the marginals is O(K2T2). However,\nwe know that we can solve this problem in O(K2T)using forwards-backwards. The difference\nis that FB caches the messages computed on the forwards pass, so it can reuse them later.\nThe same argument holds for BP on trees. For example, consider the 4-node tree in Fig-\nure 20.5. We can compute p(x1|v)by eliminating x2:4; this is equivalent to sending messages\nup tox1(the messages correspond to the \u03c4factors created by VE). Similarly we can compute\np(x2|v),p(x3|v)and then p(x4|v). We see that some of the messages used to compute the\nmarginal on one node can be re-used to compute the marginals on the other nodes. By storingthe messages for later re-use, we can compute all the marginals in O(DK\n2)time. This is what\nthe up-down (collect-distribute) algorithm on trees does.\nThe question is: how can we combine the efficiency of BP on trees with the generality of VE?\nThe answer is given in Section 20.4.\n20.4 The junction tree algorithm *\nThejunction tree algorithm orJTAgeneralizes BP from trees to arbitrary graphs. We sketch\nthe basic idea below; for details, see e.g., (Koller and Friedman 2009).\n20.4.1 Creating a junction tree\nThe basic idea behind the JTA is this. We \ufb01rst run the VE algorithm \u201csymbolically\u201d, adding \ufb01ll-inedges as we go, according to a given elimination ordering. The resulting graph will be a chordal\ngraph, which means that every undirected cycle X\n1\u2212X2\u00b7\u00b7\u00b7Xk\u2212X1of length k\u22654has a", "751": "20.4. Thejunctiontreealgorithm* 721\n;\n\u0014\n;\n\u0015\n;\n\u0016;\n\u0017;\n\u0014\n;\n\u0015\n;\n\u0016;\n\u0017\n;\n\u0014\n;\n\u0015\n;\n\u0016;\n\u0017;\n\u0014\n;\n\u0015\n;\n\u0016;\n\u0017\u000bD\f \u000bE\f\nP\u0014\u0015\u000b[\u0015\f\nP\u0016\u0015\u000b[\u0015\f\nP\u0015\u0017\u000b[\u0017\fP\u0016\u0015\u000b[\u0015\fP\u0015\u0014\u000b[\u0014\f\nP\u0017\u0015\u000b[\u0015\fP\u0014\u0015\u000b[\u0015\f\nP\u0016\u0015\u000b[\u0015\fP\u0017\u0015\u000b[\u0017\f\nP\u0014\u0015\u000b[\u0015\fP\u0015\u0014\u000b[\u0014\f\nP\u0016\u0015\u000b[\u0015\fP\u0017\u0015\u000b[\u0015\f\nP\u0015\u0017\u000b[\u0017\f P\u0015\u0016\u000b[\u0016\f\nFigure 20.5 Sending multiple messages along a tree. (a) X1is root. (b) X2is root. (c) X4is root. (d) All\nof the messages needed to compute all singleton marginals. Based on Figure 4.3 of (Jordan 2007).\n\u0014\u0015\n\u0016\n\u0019\n\u0018\u0017\n(a)\u0014\u0015\n\u0016\n\u0019\n\u0018\u0017\n(b)\nFigure 20.6 Left: this graph is not triangulated, despite appearances, since it contains a chordless 5-cycle\n1-2-3-4-5-1. Right: one possible triangulation, by adding the 1-3 and 1-4 \ufb01ll-in edges. Based on (Armstrong\n2005, p46)", "752": "722 Chapter20. Exactinferenceforgraphicalmodels\nchord, i.e., an edge connects Xi,Xjfor all non-adjacent nodes i,jin the cycle.2\nHaving created a chordal graph, we can extract its maximal cliques. In general, \ufb01nding max\ncliques is computationally hard, but it turns out that it can be done efficiently from this special\nkind of graph. Figure 20.7(b) gives an example, where the max cliques are as follows:\n{C,D},{G,I,D},{G,S,I},{G,J,S,L},{H,G,J} (20.57)\nNote that if the original graphical model was already chordal, the elimination process would notadd any extra \ufb01ll-in edges (assuming the optimal elimination ordering was used). We call suchmodelsdecomposable, since they break into little pieces de\ufb01ned by the cliques.\nIt turns out that the cliques of a chordal graph can be arranged into a special kind of\ntree known as a junction tree. This enjoys the running intersection property (RIP), which\nmeans that any subset of nodes containing a given variable forms a connected component.Figure 20.7(c) gives an example of such a tree. We see that the node Ioccurs in two adjacent\ntree nodes, so they can share information about this variable. A similar situation holds for allthe other variables.\nOne can show that if a tree that satis\ufb01es the running intersection property, then applying\nBP to this tree (as we explain below) will return the exact values of p(x\nc|v)for each node c\nin the tree (i.e., clique in the induced graph). From this, we can easily extract the node andedge marginals, p(x\nt|v)andp(xs,xt|v)from the original model, by marginalizing the clique\ndistributions.3\n20.4.2 Message passing on a junction tree\nHaving constructed a junction tree, we can use it for inference. The process is very similarto belief propagation on a tree. As in Section 20.2, there are two versions: the sum-productform, also known as the Shafer-Shenoy algorithm, named after (Shafer and Shenoy 1990); and\nthe belief updating form (which involves division), also known as the Hugin(named after a\ncompany) or the Lauritzen-Spiegelhalter algorithm (named after (Lauritzen and Spiegelhalter\n1988)). See (Lepar and Shenoy 1998) for a detailed comparison of these methods. Below wesketch how the Hugin algorithm works.\nWe assume the original model has the following form:\np(x)=1\nZ/productdisplay\nc\u2208C(G)\u03c8c(xc) (20.58)\nwhereC(G)are the cliques of the original graph. On the other hand, the tree de\ufb01nes a\ndistribution of the following form:\np(x)=/producttext\nc\u2208C(T)\u03c8c(xc)\n/producttext\ns\u2208S(T)\u03c8s(xs)(20.59)\n2. The largest loop in a chordal graph is length 3. Consequently chordal graphs are sometimes called triangulated.\nHowever, it is not enough for the graph to look like it is made of little triangles. For example, Figure 20.6(a) is not\nchordal, even though it is made of little triangles, since it contains the chordless 5-cycle 1-2-3-4-5-1.\n3. If we want the joint distribution of some variables that are not in the same clique \u2014 a so-called out-of-clique\nquery\u2014 we can adapt the technique described in Section 20.2.4.3 as follows: create a mega node containing the query\nvariables and any other nuisance variables that lie on the path between them, multiply in messages onto the boundary\nof the mega node, and then marginalize out the internal nuisance variables. This internal marginalization may require\nthe use of BP or VE. See (Koller and Friedman 2009) for details.", "753": "20.4. Thejunctiontreealgorithm* 723\nGrade\nLetter\nJob\nHappyCoherence\nSAT\nG ,J ,S ,L H ,G ,J G, S, I G, I, D C, D\nG, J G, S G, I DIntelligence Dif\ufb01culty\nGrade\nLetter\nJob\nHappyCoherence\nSATIntelligence Dif\ufb01culty\n(a) (b)\n(c)\nFigure 20.7 (a) The student graph with \ufb01ll-in edges added. (b) The maximal cliques. (c) The junction\ntree. An edge between nodes sandtis labeled by the intersection of the sets on nodes sandt; this is\ncalled the separating set. From Figure 9.11 of (Koller and Friedman 2009). Used with kind permission of\nDaphne Koller.\nwhereC(T)are the nodes of the junction tree (which are the cliques of the chordal graph), and\nS(T)are the separators of the tree. To make these equal, we initialize by de\ufb01ning \u03c8s=1for\nall separators and \u03c8c=1for all cliques. Then, for each clique in the original model, c\u2208C(G),\nwe \ufb01nd a clique in the tree c/prime\u2208C(T)which contains it, c/prime\u2287c. We then multiply \u03c8conto\u03c8c/prime\nby computing \u03c8c/prime=\u03c8c/prime\u03c8c. After doing this for all the cliques in the original graph, we have\n/productdisplay\nc\u2208C(T)\u03c8c(xc)=/productdisplay\nc\u2208C(G)\u03c8c(xc) (20.60)\nAs in Section 20.2.1, we now send messages from the leaves to the root and back, as sketched\nin Figure 20.1. In the upwards pass, also known as the collect-to-root phase, node isends to\nits parent jthe following message:\nmi\u2192j(Sij)=/summationdisplay\nCi\\Sij\u03c8i(Ci) (20.61)\nThat is, we marginalize out the variables that node i\u201cknows about\u201d which are irrelevant to j,\nand then we send what is left over. Once a node has received messages from all its children, it\nupdates its belief state using\n\u03c8i(Ci)\u221d\u03c8i(Ci)/productdisplay\nj\u2208chimj\u2192i(Sij) (20.62)", "754": "724 Chapter20. Exactinferenceforgraphicalmodels\nAt the root, \u03c8r(Cr)represents p(xCr|v), which is the posterior over the nodes in clique\nCrconditioned on all the evidence. Its normalization constant is p(v)/Z0,w h e r eZ0is the\nnormalization constant for the unconditional prior, p(x).( W eh a v e Z0=1if the original model\nwas a DGM.)\nIn the downwards pass, also known as the distribute-from-root phase, node isends to its\nchildrenjthe following message:\nmi\u2192j(Sij)=/summationtext\nCi\\Sij\u03c8i(Ci)\nmj\u2192i(Sij)(20.63)\nWe divide out by what jsent toito avoid double counting the evidence. This requires that we\nstore the messages from the upwards pass. Once a node has received a top-down message from\nits parent, it can compute its \ufb01nal belief state using\n\u03c8j(Cj)\u221d\u03c8j(Cj)mi\u2192j(Sij) (20.64)\nAn equivalent way to present this algorithm is based on storing the messages inside the\nseparator potentials. So on the way up, sending from itojwe compute the separator potential\n\u03c8\u2217\nij(Sij)=/summationdisplay\nCi\\Sij\u03c8i(Ci) (20.65)\nand then update the recipient potential:\n\u03c8\u2217\nj(Cj)\u221d\u03c8j(Cj)\u03c8\u2217\nij(Sij)\n\u03c8ij(Sij)(20.66)\n(Recall that we initialize \u03c8ij(Sij)=1.) This is sometimes called passing a \ufb02ow fromitoj.\nOn the way down, from itoj, we compute the separator potential\n\u03c8\u2217\u2217\nij(Sij)=/summationdisplay\nCi\\Sij\u03c8\u2217\ni(Ci) (20.67)\nand then update the recipient potential:\n\u03c8\u2217\u2217\nj(Cj)\u221d\u03c8\u2217\nj(Cj)\u03c8\u2217\u2217\nij(Sij)\n\u03c8\u2217\nij(Sij)(20.68)\nThis process is known as junction tree calibration. See Figure 20.1 for an illustration. Its\ncorrectness follows from the fact that each edge partitions the evidence into two distinct groups,plus the fact that the tree satis\ufb01es RIP, which ensures that no information is lost by onlyperforming local computations.\n20.4.2.1 Example: jtree algorithm on a chain\nIt is interesting to see what happens if we apply this process to a chain structured graph such asan HMM. A detailed discussion can be found in (Smyth et al. 1997), but the basic idea is this. Thecliques are the edges, and the separators are the nodes, as shown in Figure 20.8. We initializethe potentials as follows: we set \u03c8\ns=1for all the separators, we set \u03c8c(xt\u22121,xt)=p(xt|xt\u22121)\nfor clique c=(Xt\u22121,Xt), and we set \u03c8c(xt,yt)=p(yt|xt)for clique c=(Xt,Yt).", "755": "20.4. Thejunctiontreealgorithm* 725\nX1X2\nX1X2\nX1Y1X2Y2X2X2X3\nX3\nX3Y3X3X3X4\nX4\nX4Y4\nFigure 20.8 The junction tree derived from an HMM/SSM of length T=4.\nNext we send messages from left to right. Consider clique (Xt\u22121,Xt)with potential\np(Xt|Xt\u22121). It receives a message from clique (Xt\u22122,Xt\u22121)via separator Xt\u22121of the form/summationtext\nxt\u22122p(Xt\u22122,Xt\u22121|v1:t\u22121)=p(Xt\u22121|v1:t\u22121). When combined with the clique potential,\nthis becomes the two-slice predictive density\np(Xt|Xt\u22121)p(Xt\u22121|v1:t\u22121)=p(Xt\u22121,Xt|v1:t\u22121 (20.69)\nThe clique (Xt\u22121,Xt)also receives a message from (Xt,Yt)via separator Xtof the form\np(yt|Xt), which corresponds to its local evidence. When combined with the updated clique\npotential, this becomes the two-slice \ufb01ltered posterior\np(Xt\u22121,Xt|v1:t\u22121)p(vt|Xt)=p(Xt\u22121,Xt|v1:t) (20.70)\nThus the messages in the forwards pass are the \ufb01ltered belief states \u03b1t, and the clique potentials\nare the two-slice distributions. In the backwards pass, the messages are the update factors\u03b3t\n\u03b1t,\nwhere\u03b3t(k)=p(xt=k|v1:T)and\u03b1t(k)=p(xt=k|v1:t). By multiplying by this message,\nwe \u201cswap out\u201d the old \u03b1tmessage and \u201cswap in\u201d the new \u03b3tmessage. We see that the backwards\npass involves working with posterior beliefs, not conditional likelihoods. See Section 18.3.2.3 for\nfurther discussion of this difference.\n20.4.3 Computational complexity of JTA\nIf all nodes are discrete with Kstates each, it is clear that the JTA takes O(|C|Kw+1)time\nand space, where |C|is the number of cliques and wis the treewidth of the graph, i.e., the\nsize of the largest clique minus 1. Unfortunately, choosing a triangulation so as to minimize thetreewidth is NP-hard, as explained in Section 20.3.2.\nThe JTA can be modi\ufb01ed to handle the case of Gaussian graphical models. The graph-theoretic\nsteps remain unchanged. Only the message computation differs. We just need to de\ufb01ne howto multiply, divide, and marginalize Gaussian potential functions. This is most easily done ininformation form. See e.g., (Lauritzen 1992; Murphy 1998; Cemgil 2001) for the details. Thealgorithm takes O(|C|w\n3)time and O(|C|w2)space. When applied to a chain structured graph,\nthe algorithm is equivalent to the Kalman smoother in Section 18.3.2.", "756": "726 Chapter20. Exactinferenceforgraphicalmodels\nQ1 Qn Q4 Q3 Q2\nC1\nA1 X Am\u20132 A2Cm Cm\u20131 C3 C2\n. . .. . .\nFigure 20.9 Encoding a 3-SAT problem on nvariables and mclauses as a DGM. The Qsvariables are\nbinary random variables. The Ctvariables are deterministic functions of the Qs\u2019s, and compute the truth\nvalue of each clause. The Atnodes are a chain of AND gates, to ensure that the CPT for the \ufb01nal xnode\nhas bounded size. The double rings denote nodes with deterministic CPDs. Source: Figure 9.1 of (Koller\nand Friedman 2009). Used with kind permission of Daphne Koller.\n20.4.4 JTA generalizations *\nWe have seen how to use the JTA algorithm to compute posterior marginals in a graphical model.\nThere are several possible generalizations of this algorithm, some of which we mention below.All of these exploit graph decomposition in some form or other. They only differ in terms ofhow they de\ufb01ne/ compute messages and \u201cbeliefs\u201d. The key requirement is that the operatorswhich compute messages form a commutative semiring (see Section 20.3.1).\n\u2022 Computing the MAP estimate. We just replace the sum-product with max-product in the\ncollect phase, and use traceback in the distribute phase, as in the Viterbi algorithm (Sec-\ntion 17.4.4). See (Dawid 1992) for details.\n\u2022 Computing the N-most probable con\ufb01gurations (Nilsson 1998).\n\u2022 Computing posterior samples. The collect pass is the same as usual, but in the distribute\npass, we sample variables given the values higher up in the tree, thus generalizing forwards-\n\ufb01ltering backwards-sampling for HMMs described in Section 17.4.5. See (Dawid 1992) fordetails.\n\u2022 Solving constraint satisfaction problems (Dechter 2003).\n\u2022 Solving logical reasoning problems (Amir and McIlraith 2005).\n20.5 Computational intractability of exact inference in the worst case\nAs we saw in Sections 20.3.2 and 20.4.3, VE and JTA take time that is exponential in the treewidth\nof a graph. Since the treewidth can be O(number of nodes) in the worst case, this means thesealgorithms can be exponential in the problem size.\nOf course, just because VE and JTA are slow doesn\u2019t mean that there isn\u2019t some smarter algo-\nrithm out there. Unfortunately, this seems unlikely, since it is easy to show that exact inferenceis NP-hard (Dagum and Luby 1993). The proof is a simple reduction from the satis\ufb01ability prob-", "757": "20.5. Computationalintractabilityofexactinferenceintheworstcase 727\nMethod Restriction Section\nForwards-backwards Chains, D or LG Section 17.4.3\nBelief propagation Trees, D or LG Section 20.2\nVariable elimination Low treewidth, D or LG, single query Section 20.3\nJunction tree algorithm Low treewidth, D or LG Section 20.4\nLoopy belief propagation Approximate, D or LG Section 22.2\nConvex belief propagation Approximate, D or LG Section 22.4.2\nMean \ufb01eld Approximate, C-E Section 21.3\nGibbs sampling Approximate Section 24.2\nTable 20.4 Summary of some methods that can be used for inference in graphical models. \u201cD\u201d means\nthat all the hidden variables must be discrete. \u201cL-G\u201d means that all the factors must be linear-Gaussian.\nThe term \u201csingle query\u201d refers to the restriction that VE only computes one marginal p(xq|xv)at a time.\nSee Section 20.3.3 for a discussion of this point. \u201cC-E\u201d stands for \u201cconjugate exponential\u201d; this means thatvariational mean \ufb01eld only applies to models where the likelihood is in the exponential family, and theprior is conjugate. This includes the D and LG case, but many others as well, as we will see in Section 21.5.\nlem. In particular, note that we can encode any 3-SAT problem4as a DGM with deterministic\nlinks, as shown in Figure 20.9. We clamp the \ufb01nal node, x, to be on, and we arrange the CPTs\nso thatp(x=1 )>0iff there a satisfying assignment. Computing any posterior marginal\nrequires evaluating the normalization constant p(x=1 ), which represents the probability of the\nevidence, so inference in this model implicitly solves the SAT problem.\nIn fact, exact inference is #P-hard (Roth 1996), which is even harder than NP-hard. (See e.g.,\n(Arora and Barak 2009) for de\ufb01nitions of these terms.) The intuitive reason for this is that to\ncompute the normalizing constant Z,w eh a v et ocount how many satisfying assignments there\nare. By contrast, MAP estimation is provably easier for some model classes (Greig et al. 1989),since, intuitively speaking, it only requires \ufb01nding one satisfying assignment, not counting all ofthem.\n20.5.1 Approximate inference\nMany popular probabilistic models support efficient exact inference, since they are based onchains, trees or low treewidth graphs. But there are many other models for which exactinference is intractable. In fact, even simple two node models of the form \u03b8\u2192xmay not\nsupport exact inference if the prior on \u03b8is not conjugate to the likelihood p(x|\u03b8).\n5\nTherefore we will need to turn to approximate inference methods. See Table 20.4 for a\nsummary of coming attractions. For the most part, these methods do not come with anyguarantee as to their accuracy or running time. Theoretical computer scientists would thereforedescribe them as heuristics rather than approximation algorithms. In fact, one can prove that\n4. A 3-SAT problem is a logical expression of the form (Q1\u2227Q2\u2227\u00acQ3)\u2228(Q1\u2227\u00acQ4\u2227Q5)\u00b7\u00b7\u00b7, where the Qiare\nbinary variables, and each clause consists of the conjunction of three variables (or their negation). The goal is to \ufb01nd a\nsatisfying assignment, which is a set of values for the Qivariables such that the expression evaluates to true.\n5. For discrete random variables, conjugacy is not a concern, since discrete distributions are always closed underconditioning and marginalization. Consequently, graph-theoretic considerations are of more importance when discussing\ninference in models with discrete hidden states.", "758": "728 Chapter20. Exactinferenceforgraphicalmodels\nit is not possible to construct polynomial time approximation schemes for inference in general\ndiscrete GMs (Dagum and Luby 1993; Roth 1996). Fortunately, we will see that for many of theseheuristic methods often perform well in practice.\nExercises\nExercise 20.1 Variable elimination\nConsider the MRF in Figure 10.14(b).\na. Suppose we want to compute the partition function using the elimination ordering \u227a=( 1,2,3,4,5,6),\ni.e.,\n/summationdisplay\nx6/summationdisplay\nx5/summationdisplay\nx4/summationdisplay\nx3/summationdisplay\nx2/summationdisplay\nx1\u03c812(x1,x2)\u03c813(x1,x3)\u03c824(x2,x4)\u03c834(x3,x4)\u03c845(x4,x5)\u03c856(x5,x6)(20.71)\nIf we use the variable elimination algorithm, we will create new intermediate factors. What is the largest\nintermediate factor?\nb. Add an edge to the original MRF between every pair of variables that end up in the same factor. (These\nare called \ufb01ll in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in thisgraph?\nc. Now consider elimination ordering \u227a=( 4,1,2,3,5,6), i.e.,\n/summationdisplay\nx6/summationdisplay\nx5/summationdisplay\nx3/summationdisplay\nx2/summationdisplay\nx1/summationdisplay\nx4\u03c812(x1,x2)\u03c813(x1,x3)\u03c824(x2,x4)\u03c834(x3,x4)\u03c845(x4,x5)\u03c856(x5,x6)(20.72)\nIf we use the variable elimination algorithm, we will create new intermediate factors. What is the largestintermediate factor?\nd. Add an edge to the original MRF between every pair of variables that end up in the same factor. (These\nare called \ufb01ll in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in thisgraph?\nExercise 20.2 Gaussian times Gaussian is Gaussian\nProve Equation 20.17. Hint: use completing the square.\nExercise 20.3 Message passing on a tree\nConsider the DGM in Figure 20.10 which represents the following \ufb01ctitious biological model. Each G\ni\nrepresents the genotype of a person: Gi=1if they have a healthy gene and Gi=2if they have an\nunhealthy gene. G2andG3may inherit the unhealthy gene from their parent G1.Xi\u2208Ris a continuous\nmeasure of blood pressure, which is low if you are healthy and high if you are unhealthy. We de\ufb01ne the\nCPDs as follows\np(G1)=[ 0 .5,0.5] (20.73)\np(G2|G1)=/parenleftbigg0.90.1\n0.10.9/parenrightbigg\n(20.74)\np(G3|G1)=/parenleftbigg0.90.1\n0.10.9/parenrightbigg\n(20.75)\np(Xi|Gi=1 ) = N(Xi|\u03bc=5 0,\u03c32= 10) (20.76)\np(Xi|Gi=2 ) = N(Xi|\u03bc=6 0,\u03c32= 10) (20.77)\nThe meaning of the matrix for p(G2|G1)is thatp(G2=1|G1=1 )=0 .9,p(G2=1|G1=2 )=0 .1,\netc.", "759": "20.5. Computationalintractabilityofexactinferenceintheworstcase 729\nX1\nX2X3G1\nG2 G3\nFigure 20.10 A simple DAG representing inherited diseases.\na. Suppose you observe X2=5 0, andX1is unobserved. What is the posterior belief on G1, i.e.,\np(G1|X2= 50)?\nb. Now suppose you observe X2=5 0amdX3=5 0. What is p(G1|X2,X3)? Explain your answer\nintuitively.\nc. Now suppose X2=6 0,X3=6 0. What is p(G1|X2,X3)? Explain your answer intuitively.\nd. Now suppose X2=5 0,X3=6 0. What is p(G1|X1,X2)? Explain your answer intuitively.\nExercise 20.4 Inference in 2D lattice MRFs\nConsider an MRF with a 2D m\u00d7nlattice graph structure, so each hidden node, Xij, is connected to its\n4 nearest neighbors, as in an Ising model. In addition, each hidden node has its own local evidence, Yij.\nAssume all hidden nodes have K>2states. In general, exact inference in such models is intractable,\nbecause the maximum cliques of the corresponding triangulated graph have size O(max{m,n}). Suppose\nm/lessmuchni.e., the lattice is short and fat.\na. How can one efficiently perform exact inference (using a deterministic algorithm) in such models? (By\nexact inference, I mean computing marginal probabilities P(Xij|/vector y)exactly, where /vector yis all the evidence.)\nGive abriefdescription of your method.\nb. What is the asymptotic complexity (running time) of your algorithm?\nc. Now suppose the lattice is large and square, so m=n, but all hidden states are binary (ie K=2). In\nthis case, how can one efficiently exactly compute (using a deterministic algorithm) the MAP estimate\nargmax xP(x|y),w h e r ex is the joint assignment to all hidden nodes?", "760": "", "761": "21 Variational inference\n21.1 Introduction\nWe have now seen several algorithms for computing (functions of) a posterior distribution. For\ndiscrete graphical models, we can use the junction tree algorithm to perform exact inference,as explained in Section 20.4. However, this takes time exponential in the treewidth of thegraph, rendering exact inference often impractical. For the case of Gaussian graphical models,exact inference is cubic in the treewidth. However, even this can be too slow if we have manyvariables. In addition, the JTA does not work for continuous random variables outside of theGaussian case, nor for mixed discrete-continuous variables, outside of the conditionally Gaussiancase.\nFor some simple two node graphical models, of the form x\u2192D, we can compute the\nexact posterior p(x|D)in closed form, provided the prior p(x)is conjugate to the likelihood,\np(D|x)(which means the likelihood must be in the exponential family). See Chapter 5 for some\nexamples of this. (Note that in this chapter, xrepresent the unknown variables, whereas in\nChapter 5, we used \u03b8to represent the unknowns.)\nIn more general settings, we must use approximate inference methods. In Section 8.4.1, we\ndiscussed the Gaussian approximation, which is useful for inference in two node models of theformx\u2192D, where the prior is not conjugate. (For example, Section 8.4.3 applied the method\nto logistic regression.)\nThe Gaussian approximation is simple. However, some posteriors are not naturally modelled\nusing Gaussians. For example, when inferring multinomial parameters, a Dirichlet distribution isa better choice, and when inferring states in a discrete graphical model, a categorical distributionis a better choice.\nIn this chapter, we will study a more general class of deterministic approximate inference\nalgorithms based on variational inference (Jordan et al. 1998; Jaakkola and Jordan 2000; Jaakkola\n2001; Wainwright and Jordan 2008a). The basic idea is to pick an approximation q(x)to the\ndistribution from some tractable family, and then to try to make this approximation as closeas possible to the true posterior, p\n\u2217(x)/definesp(x|D). This reduces inference to an optimization\nproblem. By relaxing the constraints and/or approximating the objective, we can trade accuracyfor speed. The bottom line is that variational inference often gives us the speed bene\ufb01ts of MAPestimation but the statistical bene\ufb01ts of the Bayesian approach.", "762": "732 Chapter21. Variationalinference\n21.2 Variational inference\nSupposep\u2217(x)is our true but intractable distribution and q(x)is some approximation, chosen\nfrom some tractable family, such as a multivariate Gaussian or a factored distribution. We\nassumeqhas some free parameters which we want to optimize so as to make q\u201csimilar to\u201d p\u2217.\nAn obvious cost function to try to minimize is the KL divergence:\nKL(p\u2217||q)=/summationdisplay\nxp\u2217(x)logp\u2217(x)\nq(x)(21.1)\nHowever, this is hard to compute, since taking expectations wrt p\u2217is assumed to be intractable.\nA natural alternative is the reverse KL divergence:\nKL(q||p\u2217)=/summationdisplay\nxq(x)logq(x)\np\u2217(x)(21.2)\nThe main advantage of this objective is that computing expectations wrt qis tractable (by choos-\ning a suitable form for q). We discuss the statistical differences between these two objectives in\nSection 21.2.2.\nUnfortunately, Equation 21.2 is still not tractable as written, since even evaluating p\u2217(x)=\np(x|D)pointwise is hard, since it requires evaluating the intractable normalization constant\nZ=p(D). However, usually the unnormalized distribution \u02dcp(x)/definesp(x,D)=p\u2217(x)Zis\ntractable to compute. We therefore de\ufb01ne our new objective function as follows:\nJ(q)/definesKL(q||\u02dcp) (21.3)\nwhere we are slightly abusing notation, since \u02dcpis not a normalized distribution. Plugging in the\nde\ufb01nition of KL, we get\nJ(q)=/summationdisplay\nxq(x)logq(x)\n\u02dcp(x)(21.4)\n=/summationdisplay\nxq(x)logq(x)\nZp\u2217(x)(21.5)\n=/summationdisplay\nxq(x)logq(x)\np\u2217(x)\u2212logZ (21.6)\n=KL(q||p\u2217)\u2212logZ (21.7)\nSinceZis a constant, by minimizing J(q), we will force qto become close to p\u2217.\nSince KL divergence is always non-negative, we see that J(q)is an upper bound on the NLL\n(negative log likelihood):\nJ(q)=KL(q||p\u2217)\u2212logZ\u2265\u2212logZ=\u2212logp(D) (21.8)\nAlternatively, we can try to maximize the following quantity (in (Koller and Friedman 2009), this\nis referred to as the energy functional), which is a lower bound on the log likelihood of the\ndata:\nL(q)/defines\u2212J(q)=\u2212KL(q||p\u2217)+logZ\u2264logZ=l o gp(D) (21.9)\nSince this bound is tight when q=p\u2217, we see that variational inference is closely related to EM\n(see Section 11.4.7).", "763": "21.2. Variationalinference 733\n21.2.1 Alternative interpretations of the variational objective\nThere are several equivalent ways of writing this objective that provide different insights. One\nformulation is as follows:\nJ(q)=Eq[logq(x)]+Eq[\u2212log \u02dcp(x)] =\u2212H(q)+Eq[E(x)] (21.10)\nwhich is the expected energy (recall E(x)=\u2212log \u02dcp(x)) minus the entropy of the system. In\nstatistical physics, J(q)is called the variational free energy or theHelmholtz free energy.1\nAnother formulation of the objective is as follows:\nJ(q)=E q[logq(x)\u2212logp(x)p(D|x)] (21.11)\n=Eq[logq(x)\u2212logp(x)\u2212logp(D|x)] (21.12)\n=Eq[\u2212logp(D|x)]+KL(q(x)||p(x)) (21.13)\nThis is the expected NLL, plus a penalty term that measures how far the approximate posterioris from the exact prior.\nWe can also interpret the variational objective from the point of view of information theory\n(the so-called bits-back argument). See (Hinton and Camp 1993; Honkela and Valpola 2004), fordetails.\n21.2.2 Forward or reverse KL? *\nSince the KL divergence is not symmetric in its arguments, minimizing KL(q||p)wrtqwill give\ndifferent behavior than minimizing KL(p||q). Below we discuss these two different methods.\nFirst, consider the reverse KL, KL(q||p), also known as an I-projection orinformation\nprojection. By de\ufb01nition, we have\nKL(q||p)=/summationdisplay\nxq(x)lnq(x)\np(x)(21.14)\nThis is in\ufb01nite if p(x)=0andq(x)>0. Thus if p(x)=0we must ensure q(x)=0.W es a y\nthat the reverse KL is zero forcing forq. Henceqwill typically under-estimate the support of\np.\nNow consider the forwards KL, also known as an M-projection ormoment projection:\nKL(p||q)=/summationdisplay\nxp(x)lnp(x)\nq(x)(21.15)\nThis is in\ufb01nite if q(x)=0andp(x)>0.S oi fp(x)>0we must ensure q(x)>0.W es a y\nthat the forwards KL is zero avoiding forq. Henceqwill typically over-estimate the support of\np.\nThe difference between these methods is illustrated in Figure 21.1. We see that when the true\ndistribution is multimodal, using the forwards KL is a bad idea (assuming qis constrained to\nbe unimodal), since the resulting posterior mode/mean will be in a region of low density, rightbetween the two peaks. In such contexts, the reverse KL is not only more tractable to compute,but also more sensible statistically.", "764": "734 Chapter21. Variationalinference\n(a) (b) (c)\nFigure 21.1 Illustrating forwards vs reverse KL on a bimodal distribution. The blue curves are the contours\nof the true distribution p. The red curves are the contours of the unimodal approximation q. (a) Minimizing\nforwards KL: qtends to \u201ccover\u201d p. (b-c) Minimizing reverse KL: qlocks on to one of the two modes. Based\non Figure 10.3 of (Bishop 2006b). Figure generated by KLfwdReverseMixGauss .\n\u22121 \u22120.8 \u22120.6 \u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8 1\u22121\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.81\n(a)\u22121 \u22120.8 \u22120.6 \u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8 1\u22121\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.81\n(b)\nFigure 21.2 Illustrating forwards vs reverse KL on a symmetric Gaussian. The blue curves are the\ncontours of the true distribution p. The red curves are the contours of a factorized approximation q. (a)\nMinimizing KL(q||p). (b) Minimizing KL(p||q). Based on Figure 10.2 of (Bishop 2006b). Figure generated\nbyKLpqGauss .\nAnother example of the difference is shown in Figure 21.2, where the target distribution is\nan elongated 2d Gaussian and the approximating distribution is a product of two 1d Gaussians.\nThat is,p(x)=N(x|\u03bc,\u039b\u22121),w h e r e\n\u03bc=/parenleftbigg\u03bc1\n\u03bc2/parenrightbigg\n,\u039b=/parenleftbigg\u039b11\u039b12\n\u039b21\u039b22/parenrightbigg\n(21.16)\nIn Figure 21.2(a) we show the result of minimizing KL(q||p). In this simple example, one can\nshow that the solution has the form\nq(x)=N (x1|m1,\u039b\u22121\n11)N(x2|m2,\u039b\u22121\n22) (21.17)\nm1=\u03bc1\u2212\u039b\u22121\n11\u039b12(m2\u2212\u03bc2) (21.18)\nm2=\u03bc2\u2212\u039b\u22121\n22\u039b21(m1\u2212\u03bc1) (21.19)\n1. It is called \u201cfree\u201d because the variables xare free to vary, rather than being \ufb01xed. The variational free energy is a\nfunction of the distribution q, whereas the regular energy is a function of the state vector x.", "765": "21.3. Themean\ufb01eldmethod 735\nFigure 21.2(a) shows that we have correctly captured the mean, but the approximation is too\ncompact: its variance is controlled by the direction of smallest variance of p. In fact, it is\noften the case (although not always (Turner et al. 2008)) that minimizing KL(q||p),w h e r eqis\nfactorized, results in an approximation that is overcon\ufb01dent.\nIn Figure 21.2(b), we show the result of minimizing KL(p||q). As we show in Exercise 21.7,\nthe optimal solution when minimizing the forward KL wrt a factored approximation is to set q\nto be the product of marginals. Thus the solution has the form\nq(x)=N(x1|\u03bc1,\u039b\u22121\n11)N(x2|\u03bc2,\u039b\u22121\n22) (21.20)\nFigure 21.2(b) shows that this is too broad, since it is an over-estimate of the support of p.\nFor the rest of this chapter, and for most of the next, we will focus on minimizing KL(q||p).\nIn Section 22.5, when we discuss expectation proagation, we will discuss ways to locally optimizeKL(p||q).\nOne can create a family of divergence measures indexed by a parameter \u03b1\u2208Rby de\ufb01ning\nthealpha divergence as follows:\nD\n\u03b1(p||q)/defines4\n1\u2212\u03b12/parenleftbigg\n1\u2212/integraldisplay\np(x)(1+\u03b1)/2q(x)(1\u2212\u03b1)/2dx/parenrightbigg\n(21.21)\nThis measure satis\ufb01es D\u03b1(p||q)=0iffp=q, but is obviously not symmetric, and hence is\nnot a metric. KL(p||q)corresponds to the limit \u03b1\u21921, whereas KL(q||p)corresponds to the\nlimit\u03b1\u2192\u22121. When \u03b1=0, we get a symmetric divergence measure that is linearly related to\ntheHellinger distance, de\ufb01ned by\nDH(p||q)/defines/integraldisplay/parenleftBig\np(x)1\n2\u2212q(x)1\n2/parenrightBig2\ndx (21.22)\nNote that/radicalbig\nDH(p||q)is a valid distance metric, that is, it is symmetric, non-negative and\nsatis\ufb01es the triangle inequality. See (Minka 2005) for details.\n21.3 The mean \ufb01eld method\nOne of the most popular forms of variational inference is called the mean \ufb01eld approxima-\ntion (Opper and Saad 2001). In this approach, we assume the posterior is a fully factorizedapproximation of the form\nq(x)=/productdisplay\niqi(xi) (21.23)\nOur goal is to solve this optimization problem:\nmin\nq1,...,qDKL(q||p) (21.24)\nwhere we optimize over the parameters of each marginal distribution qi. In Section 21.3.1, we\nderive a coordinate descent method, where at each step we make the following update:\nlogqj(xj)=E\u2212qj[log \u02dcp(x)]+const (21.25)", "766": "736 Chapter21. Variationalinference\nModel Section\nIsing model Section 21.3.2\nFactorial HMM Section 21.4.1\nUnivariate Gaussian Section 21.5.1\nLinear regression Section 21.5.2Logistic regression Section 21.8.1.1Mixtures of Gaussians Section 21.6.1Latent Dirichlet allocation Section 27.3.6.3\nTable 21.1 Some models in this book for which we provide detailed derivations of the mean \ufb01eld inference\nalgorithm.\nwhere\u02dcp(x)=p(x,D)is the unnormalized posterior and the notation E\u2212qj[f(x)]means to\ntake the expectation over f(x)with respect to all the variables except for xj. For example, if\nwe have three variables, then\nE\u2212q2[f(x)] =/summationdisplay\nx1/summationdisplay\nx3q(x1)q3(x3)f(x1,x2,x3) (21.26)\nwhere sums get replaced by integrals where necessary.\nWhen updating qj, we only need to reason about the variables which share a factor with xj,\ni.e., the terms in j\u2019s Markov blanket (see Section 10.5.3); the other terms get absorbed into the\nconstant term. Since we are replacing the neighboring values by their mean value, the methodis known as mean \ufb01eld. This is very similar to Gibbs sampling (Section 24.2), except insteadof sending sampled values between neighboring nodes, we send mean values between nodes.This tends to be more efficient, since the mean can be used as a proxy for a large number ofsamples. (On the other hand, mean \ufb01eld messages are dense, whereas samples are sparse; thiscan make sampling more scalable to very large models.)\nOf course, updating one distribution at a time can be slow, since it is a form of coordinate\ndescent. Several methods have been proposed to speed up this basic approach, including usingpattern search (Honkela et al. 2003), and techniques based on parameter expansion (Qi andJaakkola 2008). However, we will not consider these methods in this chapter.\nIt is important to note that the mean \ufb01eld method can be used to infer discrete or continuous\nlatent quantities, using a variety of parametric forms for q\ni, as we will see below. This is\nin contrast to some of the other variational methods we will encounter later, which are morerestricted in their applicability. Table 21.1 lists some of the examples of mean \ufb01eld that we coverin this book.\n21.3.1 Derivation of the mean \ufb01eld update equations\nRecall that the goal of variational inference is to minimize the upper bound J(q)\u2265\u2212logp(D).\nEquivalently, we can try to maximize the lower bound\nL(q)/defines\u2212J(q)=/summationdisplay\nxq(x)log\u02dcp(x)\nq(x)\u2264logp(D) (21.27)\nWe will do this one term at a time.", "767": "21.3. Themean\ufb01eldmethod 737\nIf we write the objective singling out the terms that involve qj, and regarding all the other\nterms as constants, we get\nL(qj)=/summationdisplay\nx/productdisplay\niqi(xi)/bracketleftBigg\nlog \u02dcp(x)\u2212/summationdisplay\nklogqk(xk)/bracketrightBigg\n(21.28)\n=/summationdisplay\nxj/summationdisplay\nx\u2212jqj(xj)/productdisplay\ni/negationslash=jqi(xi)/bracketleftBigg\nlog \u02dcp(x)\u2212/summationdisplay\nklogqk(xk)/bracketrightBigg\n(21.29)\n=/summationdisplay\nxjqj(xj)/summationdisplay\nx\u2212j/productdisplay\ni/negationslash=jqi(xi)log \u02dcp(x)\n\u2212/summationdisplay\nxjqj(xj)/summationdisplay\nx\u2212j/productdisplay\ni/negationslash=jqi(xi)\u23a1\n\u23a3/summationdisplay\nk/negationslash=jlogqk(xk)+qj(xj)\u23a4\u23a6 (21.30)\n=/summationdisplay\nxjqj(xj)logfj(xj)\u2212/summationdisplay\nxjqj(xj)logqj(xj)+const (21.31)\nwhere\nlogfj(xj)/defines/summationdisplay\nx\u2212j/productdisplay\ni/negationslash=jqi(xi)log \u02dcp(x)=E\u2212qj[log \u02dcp(x)] (21.32)\nSo we average out all the hidden variables except for xj. Thus we can rewrite L(qj)as follows:\nL(qj)=\u2212KL(qj||fj) (21.33)\nWe can maximize Lby minimizing this KL, which we can do by setting qj=fj, as follows:\nqj(xj)=1\nZjexp/parenleftbig\nE\u2212qj[log \u02dcp(x)]/parenrightbig\n(21.34)\nWe can usually ignore the local normalization constant Zj, since we know qjmust be a\nnormalized distribution. Hence we usually work with the form\nlogqj(xj)=E\u2212qj[log \u02dcp(x)]+const (21.35)\nThe functional form of the qjdistributions will be determined by the type of variables xj,a s\nwell as the form of the model. (This is sometimes called free-form optimization.) If xjis a\ndiscrete random variable, then qjwill be a discrete distribution; if xjis a continuous random\nvariable, then qjwill be some kind of pdf. We will see examples of this below.\n21.3.2 Example: mean \ufb01eld for the Ising model\nConsider the image denoising example from Section 19.4.1, where xi\u2208{\u22121,+1}are the hidden\npixel values of the \u201cclean\u201d image. We have a joint model of the form\np(x,y)=p( x)p(y|x) (21.36)", "768": "738 Chapter21. Variationalinference\nwhere the prior has the form\np(x)=1\nZ0exp(\u2212E0(x)) (21.37)\nE0(x)=\u2212D/summationdisplay\ni=1/summationdisplay\nj\u2208nbriWijxixj (21.38)\nand the likelihood has the form\np(y|x)=/productdisplay\nip(yi|xi)=/summationdisplay\niexp(\u2212Li(xi)) (21.39)\nTherefore the posterior has the form\np(x|y)=1\nZexp(\u2212E(x)) (21.40)\nE(x)=E 0(x)\u2212/summationdisplay\niLi(xi) (21.41)\nWe will now approximate this by a fully factored approximation\nq(x)=/productdisplay\niq(xi,\u03bci) (21.42)\nwhere\u03bciis the mean value of node i. To derive the update for the variational parameter \u03bci,w e\n\ufb01rst write out log \u02dcp(x)=\u2212E(x), dropping terms that do not involve xi:\nlog \u02dcp(x)=xi/summationdisplay\nj\u2208nbriWijxj+Li(xi)+const (21.43)\nThis only depends on the states of the neighboring nodes. Now we take expectations of this wrt/producttext\nj/negationslash=iqj(xj)to get\nqi(xi)\u221dexp\u239b\n\u239dxi/summationdisplay\nj\u2208nbriWij\u03bcj+Li(xi)\u239e\u23a0 (21.44)\nThus we replace the states of the neighbors by their average values. Let\nm\ni=/summationdisplay\nj\u2208nbriWij\u03bcj (21.45)\nbe the mean \ufb01eld in\ufb02uence on node i. Also, let L+\ni/definesLi(+1)andL\u2212i/definesLi(\u22121). The\napproximate marginal posterior is given by\nqi(xi=1 ) =emi+L+\ni\nemi+L+i+e\u2212mi+L\u2212i=1\n1+e\u22122mi+L\u2212i\u2212L+i=s i g m ( 2 ai) (21.46)\nai/definesmi+0.5(L+\ni\u2212L\u2212i) (21.47)", "769": "21.4. Structuredmean\ufb01eld* 739\nsample 1, meanfieldH\n  \n\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.8\n(a)\nsample 3, meanfieldH\n  \n\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.8\n(b)\nmean after 15 sweeps of meanfieldH\n  \n\u22120.8\u22120.6\u22120.4\u22120.200.20.40.60.8\n(c)\nFigure 21.3 Example of image denoising using mean \ufb01eld (with parallel updates and a damping factor\nof 0.5). We use an Ising prior with Wij=1and a Gaussian noise model with \u03c3=2. We show\nthe results after 1, 3 and 15 iterations across the image. Compare to Figure 24.1. Figure generated by\nisingImageDenoiseDemo .\nSimilarly, we have qi(xi=\u22121) = sigm(\u22122ai). From this we can compute the new mean for\nsitei:\n\u03bci=Eqi[xi]=qi(xi=+ 1 )\u00b7(+1)+qi(xi=\u22121)\u00b7(\u22121) (21.48)\n=1\n1+e\u22122ai\u22121\n1+e2ai=eai\neai+e\u2212ai\u2212e\u2212ai\ne\u2212ai+eai= tanh(ai) (21.49)\nHence the update equation becomes\n\u03bci= tanh\u239b\n\u239d/summationdisplay\nj\u2208nbriWij\u03bcj+0.5(L+\ni\u2212L\u2212\ni)\u239e\n\u23a0 (21.50)\nSee also Exercise 21.6 for an alternative derivation of these equations.\nWe can turn the above equations in to a \ufb01xed point algorithm by writing\n\u03bct\ni= tanh\u239b\n\u239d/summationdisplay\nj\u2208nbriWij\u03bct\u22121\nj+0.5(L+\ni\u2212L\u2212\ni)\u239e\n\u23a0 (21.51)\nIt is usually better to use damped updates of the form\n\u03bct\ni=( 1\u2212\u03bb)\u03bct\u22121\ni+\u03bbtanh\u239b\n\u239d/summationdisplay\nj\u2208nbriWij\u03bct\u22121\nj+0.5(L+\ni\u2212L\u2212\ni)\u239e\n\u23a0 (21.52)\nfor0<\u03bb<1. We can update all the nodes in parallel, or update them asychronously.\nFigure 21.3 shows the method in action, applied to a 2d Ising model with homogeneous\nattractive potentials, Wij=1. We use parallel updates with a damping factor of \u03bb=0.5. (If we\ndon\u2019t use damping, we tend to get \u201ccheckerboard\u201d artefacts.)\n21.4 Structured mean \ufb01eld *\nAssuming that all the variables are independent in the posterior is a very strong assumption that\ncan lead to poor results. Sometimes we can exploit tractable substructure in our problem, so", "770": "740 Chapter21. Variationalinference\ny1y2y3x3,1x3,2x3,3x2,1x2,2x2,3x1,1x1,2x1,3\n(a)x3,1x3,2x3,3x2,1x2,2x2,3x1,1x1,2x1,3\n(b)x3,1x3,2x3,3x2,1x2,2x2,3x1,1x1,2x1,3\n(c)\nFigure 21.4 (a) A factorial HMM with 3 chains. (b) A fully factorized approximation. (c) A product-of-\nchains approximation. Based on Figure 2 of (Ghahramani and Jordan 1997).\nthat we can efficiently handle some kinds of dependencies. This is called the structured mean\n\ufb01eldapproach (Saul and Jordan 1995). The approach is the same as before, except we group sets\nof variables together, and we update them simultaneously. (This follows by simply treating all\nthe variables in the i\u2019th group as a single \u201cmega-variable\u201d, and then repeating the derivation in\nSection 21.3.1.) As long as we can perform efficient inference in each qi, the method is tractable\noverall. We give an example below. See (Bouchard-Cote and Jordan 2009) for some more recentwork in this area.\n21.4.1 Example: factorial HMM\nConsider the factorial HMM model (Ghahramani and Jordan 1997) introduced in Section 17.6.5.Suppose there are Mchains, each of length T, and suppose each hidden node has Kstates.\nThe model is de\ufb01ned as follows\np(x,y)=/productdisplay\nm/productdisplay\ntp(xtm|xt\u22121,m)p(yt|xtm) (21.53)\nwherep(xtm=k|xt\u22121,m=j)=A mjkis an entry in the transition matrix for chain m,\np(x1m=k|x0m)=p(x1m=k)=\u03c0mk, is the initial state distribution for chain m, and\np(yt|xt)=N/parenleftBigg\nyt|M/summationdisplay\nm=1Wmxtm,\u03a3/parenrightBigg\n(21.54)\nis the observation model, where xtmis a 1-of-K encoding of xtmandWmis aD\u00d7K\nmatrix (assuming yt\u2208RD). Figure 21.4(a) illustrates the model for the case where M=3.\nEven though each chain is a priori independent, they become coupled in the posterior dueto having an observed common child, y\nt. The junction tree algorithm applied to this graph\ntakesO(TMKM+1)time. Below we will derive a structured mean \ufb01eld algorithm that takes\nO(TMK2I)time, where Iis the number of mean \ufb01eld iterations (typically I\u223c10suffices for\ngood performance).", "771": "21.4. Structuredmean\ufb01eld* 741\nWe can write the exact posterior in the following form:\np(x|y)=1\nZexp(\u2212E(x,y)) (21.55)\nE(x,y)=1\n2T/summationdisplay\nt=1/parenleftBigg\nyt\u2212/summationdisplay\nmWmxtm/parenrightBiggT\n\u03a3\u22121/parenleftBigg\nyt\u2212/summationdisplay\nmWmxtm/parenrightBigg\n\u2212/summationdisplay\nmxT\n1m\u02dc\u03c0m\u2212T/summationdisplay\nt=2/summationdisplay\nmxTtm\u02dcAmxt\u22121,m (21.56)\nwhere\u02dcAm/defineslogAmand\u02dc\u03c0m/defineslog\u03c0m(both interpreted elementwise).\nWe can approximate the posterior as a product of marginals, as in Figure 21.4(b), but a better\napproximation is to use a product of chains, as in Figure 21.4(c). Each chain can be tractably\nupdated individually, using the forwards-backwards algorithm. More precisely, we assume\nq(x|y)=1\nZqM/productdisplay\nm=1q(x1m|\u03be1m)T/productdisplay\nt=2q(xtm|xt\u22121,m,\u03betm) (21.57)\nq(x1m|\u03be1m)=K/productdisplay\nk=1(\u03be1mk\u03c0mk)x1mk(21.58)\nq(xtm|xt\u22121,m,\u03betm)=K/productdisplay\nk=1\u239b\n\u239d\u03betmkK/productdisplay\nj=1(Amjk)xt\u22121,m,j\u239e\u23a0xtmk\n(21.59)\nWe see that the \u03betmkparameters play the role of an approximate local evidence, averaging out\nthe effects of the other chains. This is contrast to the exact local evidence, which couples all\nthe chains together.\nWe can rewrite the approximate posterior as q(x)=1\nZqexp(\u2212Eq(x)),w h e r e\nEq(x)=\u2212T/summationdisplay\nt=1M/summationdisplay\nm=1xT\ntm\u02dc\u03betm\u2212M/summationdisplay\nm=1xT1m\u02dc\u03c0m\u2212T/summationdisplay\nt=2M/summationdisplay\nm=1xTtm\u02dcAmxt\u22121,m (21.60)\nwhere\u02dc\u03betm=l o g\u03betm. We see that this has the same temporal factors as the exact posterior,\nbut the local evidence term is different. The objective function is given by\nKL(q||p)=E[E]\u2212E[Eq]\u2212logZq+logZ (21.61)\nwhere the expectations are taken wrt q. One can show (Exercise 21.8) that the update has the\nform\n\u03betm=e x p/parenleftbigg\nWT\nm\u03a3\u22121\u02dcytm\u22121\n2\u03b4m/parenrightbigg\n(21.62)\n\u03b4m/definesdiag(WT\nm\u03a3\u22121Wm) (21.63)\n\u02dcytm/definesyt\u2212M/summationdisplay\n/lscript/negationslash=mW/lscriptE[xt,/lscript] (21.64)", "772": "742 Chapter21. Variationalinference\nThe\u03betmparameter plays the role of the local evidence, averaging over the neighboring chains.\nHaving computed this for each chain, we can perform forwards-backwards in parallel, using\nthese approximate local evidence terms to compute q(xt,m|y1:T)for eachmandt.\nThe update cost is O(TMK2)for a full \u201csweep\u201d over all the variational parameters, since we\nhave to run forwards-backwards Mtimes, for each chain independently. This is the same cost\nas a fully factorized approximation, but is much more accurate.\n21.5 Variational Bayes\nSo far we have been concentrating on inferring latent variables ziassuming the parameters \u03b8\nof the model are known. Now suppose we want to infer the parameters themselves. If wemake a fully factorized (i.e., mean \ufb01eld) approximation, p(\u03b8|D)\u2248/producttext\nkq(\u03b8k), we get a method\nknown as variational Bayes orVB(Hinton and Camp 1993; MacKay 1995a; Attias 2000; Beal\nand Ghahramani 2006; Smidl and Quinn 2005).2We give some examples of VB below, assuming\nthat there are no latent variables. If we want to infer both latent variables and parameters, andwe make an approximation of the form p(\u03b8,z\n1:N|D)\u2248q(\u03b8)/producttext\niqi(zi), we get a method known\nas variational Bayes EM, which we described in Section 21.6.\n21.5.1 Example: VB for a univariate Gaussian\nFollowing (MacKay 2003, p429), let us consider how to apply VB to infer the posterior over theparameters for a 1d Gaussian, p(\u03bc,\u03bb|D),w h e r e\u03bb=1/\u03c3\n2is the precision. For convenience, we\nwill use a conjugate prior of the form\np(\u03bc,\u03bb)=N(\u03bc|\u03bc0,(\u03ba0\u03bb)\u22121)Ga(\u03bb|a0,b0) (21.65)\nHowever, we will use an approximate factored posterior of the form\nq(\u03bc,\u03bb)=q\u03bc(\u03bc)q\u03bb(\u03bb) (21.66)\nWe do not need to specify the forms for the distributions q\u03bcandq\u03bb; the optimal forms will \u201cfall\nout\u201d automatically during the derivation (and conveniently, they turn out to be Gaussian andGamma respectively).\nYou might wonder why we would want to do this, since we know how to compute the\nexact posterior for this model (Section 4.6.3.7). There are two reasons. First, it is a usefulpedagogical exercise, since we can compare the quality of our approximation to the exactposterior. Second, it is simple to modify the method to handle a semi-conjugate prior of theformp(\u03bc,\u03bb)=N(\u03bc|\u03bc\n0,\u03c40)Ga(\u03bb|a0,b0), for which exact inference is no longer possible.\n2. This method was originally called ensemble learning (MacKay 1995a), since we are using an ensemble of parameters\n(a distribution) instead of a point estimate. However, the term \u201censemble learning\u201d is also used to describe methods\nsuch as boosting, so we prefer the term VB.", "773": "21.5. VariationalBayes 743\n21.5.1.1 Target distribution\nThe unnormalized log posterior has the form\nlog \u02dcp(\u03bc,\u03bb)=l o g p(\u03bc,\u03bb,D)=l o gp(D|\u03bc,\u03bb)+logp(\u03bc|\u03bb)+logp(\u03bb) (21.67)\n=N\n2log\u03bb\u2212\u03bb\n2N/summationdisplay\ni=1(xi\u2212\u03bc)2\u2212\u03ba0\u03bb\n2(\u03bc\u2212\u03bc0)2\n+1\n2log(\u03ba0\u03bb)+(a0\u22121)log\u03bb\u2212b0\u03bb+const (21.68)\n21.5.1.2 Updating q\u03bc(\u03bc)\nThe optimal form for q\u03bc(\u03bc)is obtained by averaging over \u03bb:\nlogq\u03bc(\u03bc)=E q\u03bb[logp(D|\u03bc,\u03bb)+logp(\u03bc|\u03bb)]+const (21.69)\n=\u2212Eq\u03bb[\u03bb]\n2/braceleftBigg\n\u03ba0(\u03bc\u2212\u03bc0)2+N/summationdisplay\ni=1(xi\u2212\u03bc)2/bracerightBigg\n+const (21.70)\nBy completing the square one can show that q\u03bc(\u03bc)=N(\u03bc|\u03bcN,\u03ba\u22121\nN),w h e r e\n\u03bcN=\u03ba0\u03bc0+Nx\n\u03ba0+N,\u03baN=(\u03ba0+N)Eq\u03bb[\u03bb] (21.71)\nAt this stage we don\u2019t know what q\u03bb(\u03bb)is, and hence we cannot compute E[\u03bb], but we will\nderive this below.\n21.5.1.3 Updating q\u03bb(\u03bb)\nThe optimal form for q\u03bb(\u03bb)is given by\nlogq\u03bb(\u03bb)=E q\u03bc[logp(D|\u03bc,\u03bb)+logp(\u03bc|\u03bb)+logp(\u03bb)]+const (21.72)\n=(a0\u22121)log\u03bb\u2212b0\u03bb+1\n2log\u03bb+N\n2log\u03bb\n\u2212\u03bb\n2Eq\u03bc/bracketleftBigg\n\u03ba0(\u03bc\u2212\u03bc0)2+N/summationdisplay\ni=1(xi\u2212\u03bc)2/bracketrightBigg\n+const (21.73)\nWe recognize this as the log of a Gamma distribution, hence q\u03bb(\u03bb)=G a ( \u03bb|aN,bN),w h e r e\naN=a0+N+1\n2(21.74)\nbN=b0+1\n2Eq\u03bc/bracketleftBigg\n\u03ba0(\u03bc\u2212\u03bc0)2+N/summationdisplay\ni=1(xi\u2212\u03bc)2/bracketrightBigg\n(21.75)", "774": "744 Chapter21. Variationalinference\n21.5.1.4 Computing the expectations\nTo implement the updates, we have to specify how to compute the various expectations. Since\nq(\u03bc)=N(\u03bc|\u03bcN,\u03ba\u22121\nN),w eh a v e\nEq(\u03bc)[\u03bc]=\u03bc N (21.76)\nEq(\u03bc)/bracketleftbig\n\u03bc2/bracketrightbig\n=1\n\u03baN+\u03bc2\nN (21.77)\nSinceq(\u03bb)=G a ( \u03bb|aN,bN),w eh a v e\nEq(\u03bb)[\u03bb]=aN\nbN(21.78)\nWe can now give explicit forms for the update equations. For q(\u03bc)we have\n\u03bcN=\u03ba0\u03bc0+Nx\n\u03ba0+N(21.79)\n\u03baN=(\u03ba0+N)aN\nbN(21.80)\nand forq(\u03bb)we have\naN=a0+N+1\n2(21.81)\nbN=b0+\u03ba0(E/bracketleftbig\n\u03bc2/bracketrightbig\n+\u03bc20\u22122E[\u03bc]\u03bc0)+1\n2N/summationdisplay\ni=1/parenleftbig\nx2i+E/bracketleftbig\n\u03bc2/bracketrightbig\n\u22122E[\u03bc]xi/parenrightbig\n(21.82)\nWe see that \u03bcNandaNare in fact \ufb01xed constants, and only \u03baNandbNneed to be updated\niteratively. (In fact, one can solve for the \ufb01xed points of \u03baNandbNanalytically, but we don\u2019t\ndo this here in order to illustrate the iterative updating scheme.)\n21.5.1.5 Illustration\nFigure 21.5 gives an example of this method in action. The green contours represent the\nexact posterior, which is Gaussian-Gamma. The dotted red contours represent the variationalapproximation over several iterations. We see that the \ufb01nal approximation is reasonably close tothe exact solution. However, it is more \u201ccompact\u201d than the true distribution. It is often the casethat mean \ufb01eld inference underestimates the posterior uncertainty; See Section 21.2.2 for morediscussion of this point.\n21.5.1.6 Lower bound *\nIn VB, we are maximizing L(q), which is a lower bound on the log marginal likelihood:\nL(q)\u2264logp(D)=l o g/integraldisplay/integraldisplay\np(D|\u03bc,\u03bb)p(\u03bc,\u03bb)d\u03bcd\u03bb (21.83)\nIt is very useful to compute the lower bound itself, for three reasons. First, it can be used toassess convergence of the algorithm. Second, it can be used to assess the correctness of one\u2019s", "775": "21.5. VariationalBayes 745\n\u03bc\u03bb\n  \n\u22121 \u22120.5 0 0.5 100.20.40.60.811.21.41.61.82\nexact\nvb\n(a)\u03bc\u03bb\n  \n\u22121 \u22120.5 0 0.5 100.20.40.60.811.21.41.61.82\nexact\nvb\n(b)\n\u03bc\u03bb\n  \n\u22121 \u22120.5 0 0.5 100.20.40.60.811.21.41.61.82\nexact\nvb\n(c)\u03bc\u03bb\n  \n\u22121 \u22120.5 0 0.5 100.20.40.60.811.21.41.61.82\nexact\nvb\n(d)\nFigure 21.5 Factored variational approximation (red) to the Gaussian-Gamma distribution (green). (a)\nInitial guess. (b) After updating q\u03bc. (c) After updating q\u03bb. (d) At convergence (after 5 iterations). Based on\n10.4 of (Bishop 2006b). Figure generated by unigaussVbDemo .\ncode: as with EM, if the bound does not increase monotonically, there must be a bug. Third,\nthe bound can be used as an approximation to the marginal likelihood, which can be used forBayesian model selection.\nUnfortunately, computing this lower bound involves a fair amount of tedious algebra. We\nwork out the details for this example, but for other models, we will just state the results withoutproof, or even omit discussion of the bound altogether, for brevity.\nFor this model, L(q)can be computed as follows:\nL(q)=/integraldisplay/integraldisplay\nq(\u03bc,\u03bb)logp(D,\u03bc,\u03bb)\nq(\u03bc,\u03bb)d\u03bcd\u03bb (21.84)\n=E[logp(D|\u03bc,\u03bb)]+E[logp(\u03bc|\u03bb)]+ E[logp(\u03bb)]\n\u2212E[logq(\u03bc)]\u2212E[logq(\u03bb)] (21.85)\nwhere all expectations are wrt q(\u03bc,\u03bb). We recognize the last two terms as the entropy of a\nGaussian and the entropy of a Gamma distribution, which are given by\nH/parenleftbig\nN(\u03bcN,\u03ba\u22121\nN)/parenrightbig\n=\u22121\n2log\u03baN+1\n2(1+log(2\u03c0 )) (21.86)\nH(Ga(aN,bN)) = log\u0393(a N)\u2212(aN\u22121)\u03c8(aN)\u2212log(bN)+aN (21.87)", "776": "746 Chapter21. Variationalinference\nwhere\u03c8()is the digamma function.\nTo compute the other terms, we need the following facts:\nE[logx|x\u223cGa(a,b)] =\u03c8(a)\u2212log(b) (21.88)\nE[x|x\u223cGa(a,b)] =a\nb(21.89)\nE/bracketleftbig\nx|x\u223cN(\u03bc,\u03c32)/bracketrightbig\n=\u03bc (21.90)\nE/bracketleftbig\nx2|x\u223cN(\u03bc,\u03c32)/bracketrightbig\n=\u03bc+\u03c32(21.91)\nFor the expected log likelihood, one can show that\nEq(\u03bc,\u03bb)[logp(D|\u03bc,\u03bb)] (21.92)\n=\u2212N\n2log(2\u03c0)+N\n2Eq(\u03bb)[log\u03bb]\u2212E[\u03bb]q(\u03bb)\n2N/summationdisplay\ni=1Eq(\u03bc)/bracketleftbig\n(xi\u2212\u03bc)2/bracketrightbig\n=\u2212N\n2log(2\u03c0)+N\n2(\u03c8(aN)\u2212logbN) (21.93)\n\u2212NaN\n2bN/parenleftbigg\n\u02c6\u03c32+x2\u22122\u03bcNx+\u03bc2\nN+1\n\u03baN/parenrightbigg\n(21.94)\nwherexand\u02c6\u03c32are the empirical mean and variance.\nFor the expected log prior of \u03bb,w eh a v e\nEq(\u03bb)[logp(\u03bb) ]=( a0\u22121)E[log\u03bb]\u2212b0E[\u03bb]+a0logb0\u2212log\u0393(a 0) (21.95)\n=(a0\u22121)(\u03c8(aN)\u2212logbN)\u2212b0aN\nbN+a0logb0\u2212log\u0393(a 0)(21.96)\nFor the expected log prior of \u03bc, one can show that\nEq(\u03bc,\u03bb)[logp(\u03bc|\u03bb)] =1\n2log\u03ba0\n2\u03c0+1\n2E[log\u03bb]q(\u03bb)\u22121\n2Eq(\u03bc,\u03bb)/bracketleftbig\n(\u03bc\u2212\u03bc0)2\u03ba0\u03bb/bracketrightbig\n=1\n2log\u03ba0\n2\u03c0+1\n2(\u03c8(aN)\u2212logbN)\n\u2212\u03ba0\n2aN\nbN/bracketleftbigg1\n\u03baN+(\u03bcN\u2212\u03bc0)2/bracketrightbigg\n(21.97)\nPutting it altogether, one can show that\nL(q)=1\n2log1\n\u03baN+log\u0393(a N)\u2212aNlogbN+const (21.98)\nThis quantity monotonically increases after each VB update.\n21.5.2 Example: VB for linear regression\nIn Section 7.6.4, we discussed an empirical Bayes approach to setting the hyper-parameters for\nridge regression known as the evidence procedure. In particular, we assumed a likelihood ofthe form p(y|X,\u03b8)=N(Xw,\u03bb\n\u22121)and a prior of the form p(w)=N(w|0,\u03b1\u22121I). We then", "777": "21.5. VariationalBayes 747\ncomputed a type II estimate of \u03b1and\u03bb. The same approach was extended in Section 13.7 to\nhandle a prior of the form N(w|0,diag(\u03b1)\u22121), which allows one hyper-parameter per feature,\na technique known as automatic relevancy determination.\nIn this section, we derive a VB algorithm for this model. We follow the presentation of\n(Drugowitsch 2008).3Initially we will use the following prior:\np(w,\u03bb,\u03b1)=N (w|0,(\u03bb\u03b1)\u22121I)Ga(\u03bb|a\u03bb\n0,b\u03bb0)Ga(\u03b1|a\u03b10,b\u03b10) (21.99)\nWe choose to use the following factorized approximation to the posterior:\nq(w,\u03b1,\u03bb)=q(w,\u03bb)q(\u03b1) (21.100)\nGiven these assumptions, one can show (see (Drugowitsch 2008)) that the optimal form for the\nposterior is\nq(w,\u03b1,\u03bb)=N (w|wN,\u03bb\u22121VN)Ga(\u03bb|a\u03bb\nN,b\u03bbN)Ga(\u03b1|a\u03b1N,b\u03b1N) (21.101)\nwhere\nV\u22121\nN=A+XX(21.102)\nwN=VNXTy (21.103)\na\u03bbN=a\u03bb0+N\n2(21.104)\nb\u03bbN=b\u03bb0+1\n2(||y\u2212Xw||2+wT\nNAwN) (21.105)\na\u03b1N=a\u03b10+D\n2(21.106)\nb\u03b1N=b\u03b10+1\n2/parenleftbigga\u03bb\nN\nb\u03bbNwT\nNwN+tr(VN)/parenrightbigg\n(21.107)\nA=/angbracketleft\u03b1/angbracketrightI=a\u03b1N\nb\u03b1NI (21.108)\nThis method can be extended to the ARD case in a straightforward way, by using the following\npriors:\np(w)=N (0,diag(\u03b1)\u22121) (21.109)\np(\u03b1)=D/productdisplay\nj=1Ga(\u03b1j|a\u03b1\n0,b\u03b10) (21.110)\nThe posterior for wand\u03bbis computed as before, except we use A=d i a g (a\u03b1\nN/b\u03b1N\nj)instead of\n3. Note that Drugowitsch uses a0,b0as the hyper-parameters for p(\u03bb)andc0,d0as the hyper-parameters for p(\u03b1),\nwhereas (Bishop 2006b, Sec 10.3) uses a0,b0as the hyper-parameters for p(\u03b1)and treats \u03bbas \ufb01xed. To (hopefully)\navoid confusion, I use a\u03bb\n0,b\u03bb0as the hyper-parameters for p(\u03bb), anda\u03b10,b\u03b10as the hyper-parameters for p(\u03b1).", "778": "748 Chapter21. Variationalinference\na\u03b1\nN/b\u03b1NI. The posterior for \u03b1has the form\nq(\u03b1)=/productdisplay\njGa(\u03b1j|a\u03b1\nN,b\u03b1N\nj) (21.111)\na\u03b1N=a\u03b10+1\n2(21.112)\nb\u03b1N\nj=b\u03b10+1\n2/parenleftbigga\u03bb\nN\nb\u03bbNw2\nN,j+(VN)jj/parenrightbigg\n(21.113)\nThe algorithm alternates between updating q(w,\u03bb)andq(\u03b1). Oncewand\u03bbhave been\ninferred, the posterior predictive is a Student distribution, as shown in Equation 7.76. Speci\ufb01cally,\nfor a single data case, we have\np(y|x,D)=T (y|wT\nNx,b\u03bb\nN\na\u03bbN(1+xTVNx),2a\u03bb\nN) (21.114)\nThe exact marginal likelihood, which can be used for model selection, is given by\np(D)=/integraldisplay/integraldisplay/integraldisplay\np(y|X,w,\u03bb)p(w|\u03b1)p(\u03bb)dwd\u03b1d\u03bb (21.115)\nWe can compute a lower bound on logp(D)as follows:\nL(q)=\u2212N\n2log(2\u03c0)\u22121\n2N/summationdisplay\ni=1/parenleftbigga\u03bb\nN\nb\u03bbN(yi\u2212wT\nNxi)2+xT\niVNxi/parenrightbigg\n+1\n2log|VN|+D\n2\n\u2212log\u0393(a\u03bb0)+a\u03bb0logb\u03bb0\u2212b\u03bb0a\u03bb\nN\nb\u03bbN+log\u0393(a\u03bb\nN)\u2212a\u03bbNlogb\u03bbN+a\u03bbN\n\u2212log\u0393(a\u03b10)+a\u03b10logb\u03b10+log\u0393(a\u03b1N)\u2212a\u03b1Nlogb\u03b1N(21.116)\nIn the ARD case, the last line becomes\nD/summationdisplay\nj=1/bracketleftBig\n\u2212log\u0393(a\u03b10)+a\u03b10logb\u03b10+log\u0393(a\u03b1N)\u2212a\u03b1Nlogb\u03b1N\nj/bracketrightBig\n(21.117)\nFigure 21.6 compare VB and EB on a model selection problem for polynomial regression. We\nsee that VB gives similar results to EB, but the precise behavior depends on the sample size.WhenN=5, VB\u2019s estimate of the posterior over models is more diffuse than EB\u2019s, since VB\nmodels uncertainty in the hyper-parameters. When N=3 0, the posterior estimate of the hyper-\nparameters becomes more well-determined. Indeed, if we compute E[\u03b1|D]when we have an\nuninformative prior, a\n\u03b1\n0=b\u03b10=0,w eg e t\n\u03b1=a\u03b1\nN\nb\u03b1N=D/2\n1\n2(a\u03bb\nN\nb\u03bbNwT\nNwN+tr(VN))(21.118)", "779": "21.6. VariationalBayesEM 749\n1 2 300.20.40.60.81\nMP(M|D)N=5, method=VB\n(a)1 2 300.20.40.60.81\nMP(M|D)N=5, method=EB\n(b)\n1 2 300.20.40.60.81\nMP(M|D)N=30, method=VB\n(c)1 2 300.20.40.60.81\nMP(M|D)N=30, method=EB\n(d)\nFigure 21.6 We plot the posterior over models (polynomials of degree 1, 2 and 3) assuming a uniform\npriorp(m)\u221d1. We approximate the marginal likelihood using (a,c) VB and (b,d) EB. In (a-b), we use\nN=5data points (shown in Figure 5.7). In (c-d), we use N=3 0data points (shown in Figure 5.8). Figure\ngenerated by linregEbModelSelVsN .\nCompare this to Equation 13.167 for EB:\n\u02c6\u03b1=D\nE[wTw]=D\nwT\nNwN+tr(VN)(21.119)\nModulo the a\u03bb\nNandb\u03bbNterms, these are the same. In hindsight this is perhaps not that\nsurprising, since EB is trying to maximize logp(D), and VB is trying to maximize a lower\nbound on logp(D).\n21.6 Variational Bayes EM\nNow consider latent variable models of the form zi\u2192xi\u2190\u03b8. This includes mixtures models,\nPCA, HMMs, etc. There are now two kinds of unknowns: parameters, \u03b8, and latent variables, zi.\nAs we saw in Section 11.4, it is common to \ufb01t such models using EM, where in the E step we\ninfer the posterior over the latent variables, p(zi|xi,\u03b8), and in the M step, we compute a point\nestimate of the parameters, \u03b8. The justi\ufb01cation for this is two-fold. First, it results in simple\nalgorithms. Second, the posterior uncertainty in \u03b8is usually less than in zi, since the \u03b8are\ninformed by all Ndata cases, whereas ziis only informed by xi; this makes a MAP estimate of", "780": "750 Chapter21. Variationalinference\n\u03b8more reasonable than a MAP estimate of zi.\nHowever, VB provides a way to be \u201cmore Bayesian\u201d, by modeling uncertainty in the parameters\n\u03b8as well in the latent variables zi, at a computational cost that is essentially the same as EM.\nThis method is known as variational Bayes EM orVBEM. The basic idea is to use mean \ufb01eld,\nwhere the approximate posterior has the form\np(\u03b8,z1:N|D)\u2248q(\u03b8)q(z)=q(\u03b8)/productdisplay\niq(zi) (21.120)\nThe \ufb01rst factorization, between \u03b8andz, is a crucial assumption to make the algorithm tractable.\nThe second factorization follows from the model, since the latent variables are iid conditional\non\u03b8.\nIn VBEM, we alternate between updating q(zi|D)(the variational E step) and updating q(\u03b8|D)\n(the variational M step). We can recover standard EM from VBEM by approximating the param-eter posterior using a delta function, q(\u03b8|D)\u2248\u03b4\n\u02c6\u03b8(\u03b8).\nThe variational E step is similar to a standard E step, except instead of plugging in a MAP\nestimate of the parameters and computing p(zi|D,\u02c6\u03b8), we need to average over the parameters.\nRoughly speaking, this can be computed by plugging in the posterior mean of the parametersinstead of the MAP estimate, and then computing p(z\ni|D,\u03b8)using standard algorithms, such\nas forwards-backwards. Unfortunately, things are not quite this simple, but this is the basic idea.The details depend on the form of the model; we give some examples below.\nThe variational M step is similar to a standard M step, except instead of computing a point\nestimate of the parameters, we update the hyper-parameters, using the expected sufficient statis-tics. This process is usually very similar to MAP estimation in regular EM. Again, the details onhow to do this depend on the form of the model.\nThe principle advantage of VBEM over regular EM is that by marginalizing out the parameters,\nwe can compute a lower bound on the marginal likelihood, which can be used for modelselection. We will see an example of this in Section 21.6.1.6. VBEM is also \u201cegalitarian\u201d, sinceit treats parameters as \u201c\ufb01rst class citizens\u201d, just like any other unknown quantity, whereas EMmakes an arti\ufb01cial distinction between parameters and latent variables.\n21.6.1 Example: VBEM for mixtures of Gaussians *\nLet us consider how to \u201c\ufb01t\u201d a mixture of Gaussians using VBEM. (We use scare quotes since weare not estimating the model parameters, but inferring a posterior over them.) We will followthe presentation of (Bishop 2006b, Sec 10.2). Unfortunately, the details are rather complicated.Fortunately, as with EM, one gets used to it after a bit of practice. (As usual with math, simplyreading the equations won\u2019t help much, you should really try deriving these results yourself (ortry some of the exercises) if you want to learn this stuff in depth.)\n21.6.1.1 The variational posterior\nThe likelihood function is the usual one for Gaussian mixture models:\np(z,X|\u03b8)=/productdisplay\ni/productdisplay\nk\u03c0zik\nkN(xi|\u03bck,\u039b\u22121\nk)zik(21.121)\nwherezik=1if data point ibelongs to cluster k, andzik=0otherwise.", "781": "21.6. VariationalBayesEM 751\nWe will assume the following factored conjugate prior\np(\u03b8)=D i r (\u03c0|\u03b10)/productdisplay\nkN(\u03bck|m0,(\u03b20\u039bk)\u22121)Wi(\u039b k|L0,\u03bd0) (21.122)\nwhere\u039bkis the precision matrix for cluster k. The subscript 0 means these are parameters\nof the prior; we assume all the prior parameters are the same for all clusters. For the mixing\nweights, we usually use a symmetric prior, \u03b10=\u03b101.\nThe exact posterior p(z,\u03b8|D)is a mixture of KNdistributions, corresponding to all possible\nlabelings z. We will try to approximate the volume around one of these modes. We will use the\nstandard VB approximation to the posterior:\np(\u03b8,z1:N|D)\u2248q(\u03b8)/productdisplay\niq(zi) (21.123)\nAt this stage we have not speci\ufb01ed the forms of the qfunctions; these will be determined by\nthe form of the likelihood and prior. Below we will show that the optimal form is as follows:\nq(z,\u03b8)=q(z|\u03b8)q(\u03b8)=/bracketleftBigg/productdisplay\niCat(zi|ri)/bracketrightBigg\n(21.124)\n/bracketleftBigg\nDir(\u03c0|\u03b1)/productdisplay\nkN(\u03bck|mk,(\u03b2k\u039bk)\u22121)Wi(\u039b k|Lk,\u03bdk)/bracketrightBigg\n(21.125)\n(The lack of 0 subscript means these are parameters of the posterior, not the prior.) Below wewill derive the update equations for these variational parameters.\n21.6.1.2 Derivation of q(z)(variational E step)\nThe form for q(z)can be obtained by looking at the complete data log joint, ignoring terms that\ndo not involve z, and taking expectations of what\u2019s left over wrt all the hidden variables except\nforz.W eh a v e\nlogq(z)=E\nq(\u03b8)[logp(x,z,\u03b8)]+const (21.126)\n=/summationdisplay\ni/summationdisplay\niziklog\u03c1ik+const (21.127)\nwhere we de\ufb01ne\nlog\u03c1ik/definesEq(\u03b8)[log\u03c0k]+1\n2Eq(\u03b8)[log|\u039bk|]\u2212D\n2log(2\u03c0)\n\u22121\n2Eq(\u03b8)/bracketleftbig\n(xi\u2212\u03bck)T\u039bk(xi\u2212\u03bck)/bracketrightbig\n(21.128)\nUsing the fact that q(\u03c0)=D i r (\u03c0),w eh a v e\nlog\u02dc\u03c0k/definesE[log\u03c0k]=\u03c8(\u03b1k)\u2212\u03c8(/summationdisplay\nk/prime\u03b1k/prime) (21.129)", "782": "752 Chapter21. Variationalinference\nwhere\u03c8()is the digamma function. (See Exercise 21.5 for the detailed derivation.) Next, we use\nthe fact that\nq(\u03bck,\u039bk)=N(\u03bck|mk,(\u03b2k\u039bk)\u22121)Wi(\u039b k|Lk,\u03bdk) (21.130)\nto get\nlog\u02dc\u039bk/definesE[log|\u039bk|]=D/summationdisplay\nj=1\u03c8/parenleftbigg\u03bdk+1\u2212j\n2/parenrightbigg\n+Dlog2+log|\u039bk| (21.131)\nFinally, for the expected value of the quadratic form, we get\nE/bracketleftbig\n(xi\u2212\u03bck)T\u039bk(xi\u2212\u03bck)/bracketrightbig\n=D\u03b2\u22121\nk+\u03bdk(xi\u2212mk)T\u039bk(xi\u2212mk) (21.132)\nPutting it altogether, we get that the posterior responsibility of cluster kfor datapoint iis\nrik\u221d\u02dc\u03c0k\u02dc\u039b1\n2\nkexp/parenleftbigg\n\u2212D\n2\u03b2k\u2212\u03bdk\n2(xi\u2212mk)T\u039bk(xi\u2212mk)/parenrightbigg\n(21.133)\nCompare this to the expression used in regular EM:\nrEM\nik\u221d\u02c6\u03c0k|\u02c6\u039b|1\n2\nkexp/parenleftbigg\n\u22121\n2(xi\u2212\u02c6\u03bck)T\u02c6\u039bk(xi\u2212\u02c6\u03bck)/parenrightbigg\n(21.134)\nThe signi\ufb01cance of this difference is discussed further in Section 21.6.1.7.\n21.6.1.3 Derivation of q(\u03b8)(variational M step)\nUsing the mean \ufb01eld recipe, we have\nlogq(\u03b8)=l o g p(\u03c0)+/summationdisplay\nklogp(\u03bck,\u039bk)+/summationdisplay\niEq(z)[logp(zi|\u03c0)]\n+/summationdisplay\nk/summationdisplay\niEq(z)[zik]logN(xi|\u03bck,\u039b\u22121\nk)+const (21.135)\nWe see this factorizes into the form\nq(\u03b8)=q(\u03c0)/productdisplay\nkq(\u03bck,\u039bk) (21.136)\nFor the\u03c0term, we have\nlogq(\u03c0)=(\u03b10\u22121)/summationdisplay\nklog\u03c0k+/summationdisplay\nk/summationdisplay\niriklog\u03c0k+const (21.137)\nExponentiating, we recognize this as a Dirichlet distribution:\nq(\u03c0)=D i r ( \u03c0|\u03b1) (21.138)\n\u03b1k=\u03b10+Nk (21.139)\nNk=/summationdisplay\nirik (21.140)", "783": "21.6. VariationalBayesEM 753\n0 20 40 60 80 100\u22121100\u22121050\u22121000\u2212950\u2212900\u2212850\u2212800\u2212750\u2212700\u2212650\u2212600\niterlower bound on log marginal likelihoodvariational Bayes objective for GMM on old faithful data\nFigure 21.7 Lower bound vs iterations for the VB algorithm in Figure 21.8. The steep parts of the\ncurve correspond to places where the algorithm \ufb01gures out that it can increase the bound by \u201ckilling\noff\u201d unnecessary mixture components, as described in Section 21.6.1.6. The plateaus correspond to slowlymoving the clusters around. Figure generated by mixGaussVbDemoFaithful .\nFor the\u03bckand\u039bkterms, we have\nq(\u03bck,\u039bk)=N (\u03bck|mk,(\u03b2k\u039bk)\u22121)Wi(\u039b k|Lk,\u03bdk) (21.141)\n\u03b2k=\u03b20+Nk (21.142)\nmk=(\u03b20m0+Nkxk)/\u03b2k (21.143)\nL\u22121\nk=L\u22121\n0+NkSk+\u03b20Nk\n\u03b20+Nk(xk\u2212m0)(xk\u2212m0)T(21.144)\n\u03bdk=\u03bd0+Nk+1 (21.145)\nxk=1\nNk/summationdisplay\nirikxi (21.146)\nSk=1\nNk/summationdisplay\nirik(xi\u2212xk)(xi\u2212xk)T(21.147)\nThis is very similar to the M step for MAP estimation discussed in Section 11.4.2.8, except here\nwe are computing the parameters of the posterior over \u03b8, rather than MAP estimates of \u03b8.\n21.6.1.4 Lower bound on the marginal likelihood\nThe algorithm is trying to maximize the following lower bound\nL=/summationdisplay\nz/integraldisplay\nq(z,\u03b8)logp(x,z,\u03b8)\nq(z,\u03b8)d\u03b8\u2264logp(D) (21.148)\nThis quantity should increase monotonically with each iteration, as shown in Figure 21.7. Un-\nfortunately, deriving the bound is a bit messy, because we need to compute expectations of theunnormalized log posterior as well as entropies of the qdistribution. We leave the details (which\nare similar to Section 21.5.1.6) to Exercise 21.4.", "784": "754 Chapter21. Variationalinference\n21.6.1.5 Posterior predictive distribution\nWe showed that the approximate posterior has the form\nq(\u03b8)=D i r (\u03c0|\u03b1)/productdisplay\nkN(\u03bck|mk,(\u03b2k\u039bk)\u22121)Wi(\u039b k|Lk,\u03bdk) (21.149)\nConsequently the posterior predictive density can be approximated as follows, using the results\nfrom Section 4.6.3.6:\np(x|D)\u2248/summationdisplay\nz/integraldisplay\np(x|z,\u03b8)p(z|\u03b8)q(\u03b8)d\u03b8 (21.150)\n=/summationdisplay\nk/integraldisplay\n\u03c0kN(x|\u03bck,\u039b\u22121\nk)q(\u03b8)d\u03b8 (21.151)\n=/summationdisplay\nk\u03b1k/summationtext\nk/prime\u03b1k/primeT(x|mk,Mk,\u03bdk+1\u2212D) (21.152)\nMk=(\u03bdk+1\u2212D)\u03b2k\n1+\u03b2kLk (21.153)\nThis is just a weighted sum of Student distributions. If instead we used a plug-in approximation,we would get a weighted sum of Gaussian distributions.\n21.6.1.6 Model selection using VBEM\nThe simplest way to select Kwhen using VB is to \ufb01t several models, and then to use the\nvariational lower bound to the log marginal likelihood, L(K)\u2264logp(D|K), to approximate\np(K|D):\np(K|D)=e\nL(K)\n/summationtext\nK/primeeL(K/prime)(21.154)\nHowever, the lower bound needs to be modi\ufb01ed somewhat to take into account the lack ofidenti\ufb01ability of the parameters (Section 11.3.1). In particular, although VB will approximate thevolume occupied by the parameter posterior, it will only do so around one of the local modes.WithKcomponents, there are K!equivalent modes, which differ merely by permuting the\nlabels. Therefore we should use logp(D|K)\u2248L(K)+log(K!).\n21.6.1.7 Automatic sparsity inducing effects of VBEM\nAlthough VB provides a reasonable approximation to the marginal likelihood (better than BIC(Beal and Ghahramani 2006)), this method still requires \ufb01tting multiple models, one for eachvalue ofKbeing considered. A faster alternative is to \ufb01t a single model, where Kis set large,\nbut where \u03b1\n0is set very small, \u03b10/lessmuch1. From Figure 2.14(d), we see that the resulting prior for\nthe mixing weights \u03c0has \u201cspikes\u201d near the corners of the simplex, encouraging a sparse mixing\nweight vector.\nIn regular EM, the MAP estimate of the mixing weights will have the form \u02c6\u03c0k\u221d(\u03b1k\u22121),\nwhere\u03b1k=\u03b10+Nk. Unforuntately, this can be negative if \u03b10=0andNk=0(Figueiredo", "785": "21.6. VariationalBayesEM 755\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\u22122.5\u22122\u22121.5\u22121\u22120.500.511.52\n1\n2\n345\n6iter 1\n(a)\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\u22122.5\u22122\u22121.5\u22121\u22120.500.511.52\n45iter 94\n(b)\nFigure 21.8 We visualize the posterior mean parameters at various stages of the VBEM algorithm applied\nto a mixture of Gaussians model on the Old Faithful data. Shading intensity is proportional to the mixing\nweight. We initialize with K-means and use \u03b10=0.001as the Dirichlet hyper-parameter. Based on Figure\n10.6 of (Bishop 2006b). Figure generated by mixGaussVbDemoFaithful , based on code by Emtiyaz Khan.\n1 2 3 4 5 60102030405060708090iter 1\n(a)1 2 3 4 5 6020406080100120140160180iter 94\n(b)\nFigure 21.9 We visualize the posterior values of \u03b1kfor the model in Figure 21.8. We see that unnecessary\ncomponents get \u201ckilled off\u201d. Figure generated by mixGaussVbDemoFaithful .\nand Jain 2002). However, in VBEM, we use\n\u02dc\u03c0k=exp[\u03a8(\u03b1 k)]\nexp[\u03a8(/summationtext\nk/prime\u03b1k/prime)](21.155)\nNowexp(\u03a8(x))\u2248x\u22120.5forx>1.S oi f \u03b1k=0, when we compute \u02dc\u03c0k, it\u2019s like we substract\n0.5 from the posterior counts. This will hurt small clusters more than large clusters (like a\nregressive tax).4The effect is that clusters which have very few (weighted) members become\nmore and more empty over successive iterations, whereas the popular clusters get more andmore members. This is called the rich get richer phenomenon; we will encounter it again in\nSection 25.2, when we discuss Dirichlet process mixture models.\nThis automatic pruning method is demonstrated in Figure 21.8. We \ufb01t a mixture of 6 Gaussians\nto the Old Faithful dataset, but the data only really \u201cneeds\u201d 2 clusters, so the rest get \u201ckilled off\u201d.\n4. For more details, see (Liang et al. 2007).", "786": "756 Chapter21. Variationalinference\nIn this example, we used \u03b10=0.001; if we use a larger \u03b10, we do not get a sparsity effect.\nIn Figure 21.9, we plot q(\u03b1|D)at various iterations; we see that the unwanted components\nget extinguished. This provides an efficient alternative to performing a discrete search over the\nnumber of clusters.\n21.7 Variational message passing and VIBES\nWe have seen that mean \ufb01eld methods, at least of the fully-factorized variety, are all very similar:just compute each node\u2019s full conditional, and average out the neighbors. This is very similarto Gibbs sampling (Section 24.2), except the derivation of the equations is usually a bit morework. Fortunately it is possible to derive a general purpose set of update equations that work forany DGM for which all CPDs are in the exponential family, and for which all parent nodes haveconjugate distributions (Ghahramani and Beal 2001). (See (Wand et al. 2011) for a recent extensionto handle non-conjugate priors.) One can then sweep over the graph, updating nodes one at atime, in a manner similar to Gibbs sampling. This is known as variational message passing or\nVMP(Winn and Bishop 2005), and has been implemented in the open-source program VIBES\n5.\nThis is a VB analog to BUGS, which is a popular generic program for Gibbs sampling discussedin Section 24.2.6.\nVMP/ mean \ufb01eld is best-suited to inference where one or more of the hidden nodes are\ncontinuous (e.g., when performing \u201cBayesian learning\u201d). For models where all the hidden nodesare discrete, more accurate approximate inference algorithms can be used, as we discuss inChapter 22.\n21.8 Local variational bounds *\nSo far, we have been focusing on mean \ufb01eld inference, which is a form of variational inferencebased on minimizing KL(q||\u02dcp),w h e r eqis the approximate posterior, assumed to be factorized,\nand\u02dcpis the exact (but unnormalized) posterior. However, there is another kind of variational\ninference, where we replace a speci\ufb01c term in the joint distribution with a simpler function, tosimplify computation of the posterior. Such an approach is sometimes called a local variational\napproximation, since we are only modifying one piece of the model, unlike mean \ufb01eld, whichis a global approximation. In this section, we study several examples of this method.\n21.8.1 Motivating applications\nBefore we explain how to derive local variational bounds, we give some examples of where thisis useful.\n21.8.1.1 Variational logistic regression\nConsider the problem of how to approximate the parameter posterior for multiclass logisticregression model under a Gaussian prior. One approach is to use a Gaussian (Laplace) approx-imation, as discussed in Section 8.4.3. However, a variational approach can produce a more\n5. Available at http://vibes .sourceforge .net/.", "787": "21.8. Localvariationalbounds* 757\naccurate approximation to the posterior, since it has tunable parameters. Another advantage is\nthat the variational approach monotonically optimizes a lower bound on the likelihood of thedata, as we will see.\nTo see why we need a bound, note that the likelihood can be written as follows:\np(y|X,w)= N/productdisplay\ni=1exp/bracketleftbig\nyT\ni\u03b7i\u2212lse(\u03b7i)/bracketrightbig\n(21.156)\nwhere\u03b7i=Xiwi=[xT\niw1,...,xTiwM],w h e r eM=C\u22121(since we set wC=0for\nidenti\ufb01ability), and where we de\ufb01ne the log-sum-exp orlsefunction as follows:\nlse(\u03b7i)/defineslog/parenleftBigg\n1+M/summationdisplay\nm=1e\u03b7im/parenrightBigg\n(21.157)\nThe main problem is that this likelihood is not conjugate to the Gaussian prior. Below we discuss\nhow to compute \u201cGaussian-like\u201d lower bounds to this likelihood, which give rise to approximateGaussian posteriors.\n21.8.1.2 Multi-task learning\nOne important application of Bayesian inference for logistic regression is where we have multiplerelated classi\ufb01ers we want to \ufb01t. In this case, we want to share information between theparameters for each classi\ufb01er; this requires that we maintain a posterior distibution over theparameters, so we have a measure of con\ufb01dence as well as an estimate of the value. We canembed the above variational method inside of a larger hierarchical model in order to performsuch multi-task learning, as described in e.g., (Braun and McAuliffe 2010).\n21.8.1.3 Discrete factor analysis\nAnother situation where variational bounds are useful arises when we \ufb01t a factor analysismodel to discrete data. This model is just like multinomial logistic regression, except the inputvariables are hidden factors. We need to perform inference on the hidden variables as well asthe regression weights. For simplicity, we might perform point estimation of the weights, andjust integrate out the hidden variables. We can do this using variational EM, where we use thevariational bound in the E step. See Section 12.4 for details.\n21.8.1.4 Correlated topic model\nA topic model is a latent variable model for text documents and other forms of discrete data; seeSection 27.3 for details. Often we assume the distribution over topics has a Dirichlet prior, buta more powerful model, known as the correlated topic model, uses a Gaussian prior, which canmodel correlations more easily (see Section 27.4.1 for details). Unfortunately, this also involvesthe lse function. However, we can use our variational bounds in the context of a variational EMalgorithm, as we will see later.", "788": "758 Chapter21. Variationalinference\n21.8.2 Bohning\u2019s quadratic bound to the log-sum-exp function\nAll of the above examples require dealing with multiplying a Gaussian prior by a multinomial\nlikelihood; this is difficult because of the log-sum-exp (lse) term. In this section, we derive a wayto derive a \u201cGaussian-like\u201d lower bound on this likelihood.\nConsider a Taylor series expansion of the lse function around \u03c8\ni\u2208RM:\nlse(\u03b7i) = lse(\u03c8i)+(\u03b7i\u2212\u03c8i)Tg(\u03c8i)+1\n2(\u03b7i\u2212\u03c8i)TH(\u03c8i)(\u03b7i\u2212\u03c8i) (21.158)\ng(\u03c8i)=e x p [ \u03c8i\u2212lse(\u03c8i)] =S(\u03c8i) (21.159)\nH(\u03c8i) = diag(g (\u03c8i))\u2212g(\u03c8i)g(\u03c8i)T(21.160)\nwheregandHare the gradient and Hessian of lse, and \u03c8i\u2208RMis chosen such that equality\nholds. An upper bound to lse can be found by replacing the Hessian matrix H(\u03c8i)with a\nmatrixAisuch that Ai\u227aH(\u03c8i). (Bohning 1992) showed that this can be achieved if we use\nthe matrix Ai=1\n2/bracketleftBig\nIM\u22121\nM+11M1T\nM/bracketrightBig\n. (Recall that M+1=Cis the number of classes.)\nNote that Aiis independent of \u03c8i; however, we still write it as Ai(rather than dropping the\nisubscript), since other bounds that we consider below will have a data-dependent curvature\nterm. The upper bound on lse therefore becomes\nlse(\u03b7i)\u22641\n2\u03b7T\niAi\u03b7i\u2212bTi\u03b7i+ci (21.161)\nAi=1\n2/bracketleftbigg\nIM\u22121\nM+11M1TM/bracketrightbigg\n(21.162)\nbi=Ai\u03c8i\u2212g(\u03c8i) (21.163)\nci=1\n2\u03c8T\niAi\u03c8i\u2212g(\u03c8i)T\u03c8i+lse(\u03c8i) (21.164)\nwhere\u03c8i\u2208RMis a vector of variational parameters.\nWe can use the above result to get the following lower bound on the softmax likelihood:\nlogp(yi=c|xi,w)\u2265/bracketleftbigg\nyT\niXiw\u22121\n2wTXiAiXiw+bT\niXiw\u2212ci/bracketrightbigg\nc(21.165)\nTo simplify notation, de\ufb01ne the pseudo-measurement\n\u02dcyi/definesA\u22121\ni(bi+yi) (21.166)\nThen we can get a \u201cGaussianized\u201d version of the observation model:\np(yi|xi,w)\u2265f(xi,\u03c8i)N(\u02dcyi|Xiw,A\u22121\ni) (21.167)\nwheref(xi,\u03c8i)is some function that does not depend on w. Given this, it is easy to compute\nthe posterior q(w)=N(mN,VN), using Bayes rule for Gaussians. Below we will explain how\nto update the variational parameters \u03c8i.", "789": "21.8. Localvariationalbounds* 759\n21.8.2.1 Applying Bohning\u2019s bound to multinomial logistic regression\nLet us see how to apply this bound to multinomial logistic regression. From Equation 21.13, we\ncan de\ufb01ne the goal of variational inference as maximizing\nL(q)/defines\u2212KL(q(w)||p(w|D))+Eq/bracketleftBiggN/summationdisplay\ni=1logp(yi|xi,w)/bracketrightBigg\n(21.168)\n=\u2212KL(q(w)||p(w|D))+Eq/bracketleftBiggN/summationdisplay\ni=1yT\ni\u03b7i\u2212lse(\u03b7i)/bracketrightBigg\n(21.169)\n=\u2212KL(q(w)||p(w|D))+N/summationdisplay\ni=1yT\niEq[\u03b7i]\u2212N/summationdisplay\ni=1Eq[lse(\u03b7i)] (21.170)\nwhereq(w)=N(w|mN,VN)is the approximate posterior. The \ufb01rst term is just the KL\ndivergence between two Gaussians, which is given by\n\u2212KL(N(m0,V0)||N(mN,VN)) =\u22121\n2/bracketleftbig\ntr(VNV\u22121\n0)\u2212log|VNV\u22121\n0|\n+(mN\u2212m0)TV\u22121\n0(mN\u2212m0)\u2212DM/bracketrightbig\n(21.171)\nwhereDMis the dimensionality of the Gaussian, and we assume a prior of the form p(w)=\nN(m0,V0), where typically \u03bc0=0DM, andV0is block diagonal. The second term is simply\nN/summationdisplay\ni=1yT\niEq[\u03b7i]=N/summationdisplay\ni=1yT\ni\u02dcmi (21.172)\nwhere\u02dcmi/definesXimN. The \ufb01nal term can be lower bounded by taking expectations of our\nquadratic upper bound on lse as follows:\n\u2212N/summationdisplay\ni=1Eq[lse(\u03b7i)]\u2265\u22121\n2tr(Ai\u02dcVi)\u22121\n2\u02dcmiAi\u02dcmi+bT\ni\u02dcmi\u2212ci (21.173)\nwhere\u02dcVi/definesXiVNXT\ni. Putting it altogether, we have\nLQJ(q)\u2265\u22121\n2/bracketleftbig\ntr(VNV\u22121\n0)\u2212log|VNV\u22121\n0|+(mN\u2212m0)TV\u22121\n0(mN\u2212m0)/bracketrightbig\n\u22121\n2DM+N/summationdisplay\ni=1yT\ni\u02dcmi\u22121\n2tr(Ai\u02dcVi)\u22121\n2\u02dcmiAi\u02dcmi+bT\ni\u02dcmi\u2212ci(21.174)\nThis lower bound combines Jensen\u2019s inequality (as in mean \ufb01eld inference), plus the quadratic\nlower bound due to the lse term, so we write it as LQJ.\nWe will use coordinate ascent to optimize this lower bound. That is, we update the variational\nposterior parameters VNandmN, and then the variational likelihood parameters \u03c8i. We leave", "790": "760 Chapter21. Variationalinference\nthe detailed derivation as an exercise, and just state the results. We have\nVN=/parenleftBigg\nV0+N/summationdisplay\ni=1XT\niAiXi/parenrightBigg\u22121\n(21.175)\nmN=Vn/parenleftBigg\nV\u22121\n0m0+N/summationdisplay\ni=1XTi(yi+bi)/parenrightBigg\n(21.176)\n\u03c8i=\u02dcmi=XimN (21.177)\nWe can exploit the fact that Aiis a constant matrix, plus the fact that Xihas block structure,\nto simplify the \ufb01rst two terms as follows:\nVN=/parenleftBigg\nV0+A\u2297N/summationdisplay\ni=1xixTi/parenrightBigg\u22121\n(21.178)\nmN=Vn/parenleftBigg\nV\u22121\n0m0+N/summationdisplay\ni=1(yi+bi)\u2297xi/parenrightBigg\n(21.179)\nwhere\u2297denotes the kronecker product. See Algorithm 15 for some pseudocode, and http:\n//www.cs.ubc.ca/~emtiyaz/software/catLGM .htmlfor some Matlab code.\nAlgorithm 21.1: Variational inference for multi-class logistic regression using Bohning\u2019s\nbound\n1Input:yi\u2208{1,...,C},xi\u2208RD,i=1:N, priorm0,V0;\n2De\ufb01neM:=C\u22121; dummy encode yi\u2208{0,1}M; de\ufb01neXi=blockdiag( xT\ni);\n3De\ufb01ney:= [y1;...;yN],X:= [X1;...;XN]andA:=1\n2/bracketleftBig\nIM\u22121\nM+11M1T\nM/bracketrightBig\n;\n4VN:=/parenleftbig\nV\u22121\n0+/summationtextn\ni=1XT\niAXi/parenrightbig\u22121;\n5InitializemN:=m0;\n6repeat\n7\u03c8:=XmN;\n8\u03a8:=reshape(m,M,N );\n9G:= exp(\u03a8\u2212lse(\u03a8));\n10B:=A\u03a8\u2212G;\n11b:= (B);\n12mN:=VN/parenleftbig\nV\u22121\n0m0+XT(y+b)/parenrightbig\n;\n13Compute the lower bound LQJusing Equation 21.174;\n14untilconverged ;\n15ReturnmNandVN;\n21.8.3 Bounds for the sigmoid function\nIn many models, we just have binary data. In this case, we have yi\u2208{0,1},M=1and\n\u03b7i=wTxiwherew\u2208RDis a weight vector (not matrix). In this case, the Bohning bound", "791": "21.8. Localvariationalbounds* 761\n\u22126 \u22124 \u22122 0 2 4 600.10.20.30.40.50.60.70.80.91Bohning bound, \u03c7=\u22122.5\n(a)\u22126 \u22124 \u22122 0 2 4 600.10.20.30.40.50.60.70.80.91JJ bound, \u03c7=2.5\n(b)\nFigure 21.10 Quadratic lower bounds on the sigmoid (logistic) function. In solid red, we plot sigm(x) vs\nx. In dotted blue, we plot the lower bound L(x,\u03be)vsxfor\u03be=2.5. (a) Bohning bound. This is tight at\n\u2212\u03be=2.5. (b) JJ bound. This is tight at \u03be=\u00b12.5. Figure generated by sigmoidLowerBounds .\nbecomes\nlog(1+e\u03b7)\u22641\n2a\u03b72\u2212b\u03b7+c (21.180)\na=1\n4(21.181)\nb=A\u03c8\u2212(1+e\u2212\u03c8)\u22121(21.182)\nc=1\n2A\u03c82\u2212(1+e\u2212\u03c8)\u22121\u03c8+log(1+ e\u03c8) (21.183)\nIt is possible to derive an alternative quadratic bound for this case, as shown in (Jaakkola and\nJordan 1996b, 2000). This has the following form\nlog(1+e\u03b7)\u2264\u03bb(\u03be)(\u03b72\u2212\u03be2)+1\n2(\u03b7\u2212\u03be)+log(1+e\u03be) (21.184)\n\u03bb(\u03be)/defines1\n4\u03betanh(\u03be/2) =1\n2\u03be/bracketleftbigg\nsigm(\u03be)\u22121\n2/bracketrightbigg\n(21.185)\nWe shall refer to this as the JJ bound, after its inventors, (Jaakkola and Jordan 1996b, 2000).\nTo facilitate comparison with Bohning\u2019s bound, let us rewrite the JJ bound as a quadratic form\nas follows\nlog(1+e\u03b7)\u22641\n2a(\u03be)\u03b72\u2212b(\u03be)\u03b7+c(\u03be) (21.186)\na(\u03be)=2\u03bb(\u03be) (21.187)\nb(\u03be)=\u22121\n2(21.188)\nc(\u03be)=\u2212\u03bb( \u03be)\u03be2\u22121\n2\u03be+log(1+ e\u03be) (21.189)\nThe JJ bound has an adaptive curvature term, since adepends on \u03be. In addition, it is tight at\ntwo points, as is evident from Figure 21.10(b). By contrast, the Bohning bound is a constantcurvature bound, and is only tight at one point, as is evident from Figure 21.10(a).", "792": "762 Chapter21. Variationalinference\nIf we wish to use the JJ bound for binary logistic regression, we can make some small\nmodi\ufb01cations to Algorithm 15. First, we use the new de\ufb01nitions for ai,biandci. The fact that\naiis not constant when using the JJ bound, unlike when using the Bohning bound, means we\ncannot compute VNoutside of the main loop, making the method a constant factor slower.\nNext we note that Xi=xT\ni, so the updates for the posterior become\nV\u22121\nN=V\u22121\n0+2N/summationdisplay\ni=1\u03bb(\u03bei)xixT\ni (21.190)\nmN=VN/parenleftBigg\nV\u22121\n0m0+N/summationdisplay\ni=1(yi\u22121\n2)xi/parenrightBigg\n(21.191)\nFinally, to compute the update for \u03bei, we isolate the terms in LQJthat depend on \u03beito get\nL(\u03be)=N/summationdisplay\ni=1/braceleftbig\nlnsigm(\u03be i)\u2212\u03bei/2\u2212\u03bb(\u03bei)(xTiEq/bracketleftbig\nwwT/bracketrightbig\nxi\u2212\u03be2\ni)/bracerightbig\n+const (21.192)\nOptimizing this wrt \u03beigives the equation\n0=\u03bb/prime(\u03bei)(xTiEq/bracketleftbig\nwwT/bracketrightbig\nxi\u2212\u03be2\ni) (21.193)\nNow\u03bb/prime(\u03bei)is monotonic for \u03bei\u22650, and we do not need to consider negative values of \u03beiby\nsymmetry of the bound around \u03bei=0(see Figure 21.10). Hence the only way to make the above\nexpression 0 is if we have (xT\niE/bracketleftbig\nwwT/bracketrightbig\nxi\u2212\u03be2\ni)=0. Hence the update becomes\n(\u03benew\ni)2=xT\ni(VN+mNmTN)xi (21.194)\nAlthough the JJ bound is tighter than the Bohning bound, sometimes it is not tight enough\nin order to estimate the posterior covariance accurately. A more accurate approach, which uses\na piecewise quadratic upper bound to lse, is described in (Marlin et al. 2011). By increasing thenumber of pieces, the bound can be made arbitrarily tight.\n21.8.4 Other bounds and approximations to the log-sum-exp function *\nThere are several other bounds and approximations to the multiclass lse function which wecan use, which we brie\ufb02y summarize below. Note, however, that all of these require numericaloptimization methods to compute m\nNandVN, making them more complicated to implement.\n21.8.4.1 Product of sigmoids\nThe approach in (Bouchard 2007) exploits the fact that\nlog/parenleftBiggK/summationdisplay\nk=1e\u03b7k/parenrightBigg\n\u2264\u03b1+K/summationdisplay\nk=1log(1+e\u03b7k\u2212\u03b1) (21.195)\nIt then applies the JJ bound to the term on the right.", "793": "21.8. Localvariationalbounds* 763\n21.8.4.2 Jensen\u2019s inequality\nThe approach in (Blei and Lafferty 2006a, 2007) uses Jensen\u2019s inequality as follows:\nEq[lse(\u03b7i)] = Eq/bracketleftBigg\nlog/parenleftBigg\n1+M/summationdisplay\nc=1exp(xT\niwc)/parenrightBigg/bracketrightBigg\n(21.196)\n\u2264log/parenleftBigg\n1+M/summationdisplay\nc=1Eq/bracketleftbig\nexp(xTiwc)/bracketrightbig/parenrightBigg\n(21.197)\n\u2264log/parenleftBigg\n1+M/summationdisplay\nc=1exp(xTimN,c+1\n2xTiVN,ccxi)/parenrightBigg\n(21.198)\nwhere the last term follows from the mean of a log-normal distribution, which is e\u03bc+\u03c32/2.\n21.8.4.3 Multivariate delta method\nThe approach in (Ahmed and Xing 2007; Braun and McAuliffe 2010) uses the multivariate delta\nmethod, which is a way to approximate moments of a function using a Taylor series expansion.\nIn more detail, let f(w)be the function of interest. Using a second-order approximation around\nmwe have\nf(w)\u2248f(m)+(w\u2212m)Tg(w\u2212m)+1\n2(w\u2212m)TH(w\u2212m) (21.199)\nwheregandHare the gradient and Hessian evaluated at m.I fq(w)=N(w|m,V),w eh a v e\nEq[f(w)]\u2248f(m)+1\n2tr[HV] (21.200)\nIf we use f(w)=l s e (Xiw),w eg e t\nEq[lse(X iw)]\u2248lse(Xim)+1\n2tr[XiHXT\niV] (21.201)\nwheregandHfor the lse function are de\ufb01ned in Equations 21.159 and 21.160.\n21.8.5 Variational inference based on upper bounds\nSo far, we have been concentrating on lower bounds. However, sometimes we need to use an\nupper bound. For example, (Saul et al. 1996) derives a mean \ufb01eld algorithm for sigmoid beliefnets, which are DGMs in which each CPD is a logistic regression function (Neal 1992). Unlike thecase of Ising models, the resulting MRF is not pairwise, but contains higher order interactions.This makes the standard mean \ufb01eld updates intractable. In particular, they turn out to involvecomputing an expression which requires evaluating\nE/bracketleftBig\nlog(1+e\n\u2212/summationtext\nj\u2208paiwijxj)/bracketrightBig\n=E/bracketleftbig\n\u2212logsigm(wT\nixpa(i))/bracketrightbig\n(21.202)\n(Notice the minus sign in front.) (Saul et al. 1996) show how to derive an upper bound on thesigmoid function so as to make this update tractable, resulting in a monotonically convergentinference procedure.", "794": "764 Chapter21. Variationalinference\nExercises\nExercise 21.1 Laplace approximation to p(\u03bc,log\u03c3|D)for a univariate Gaussian\nComputeaLaplaceapproximationof p(\u03bc,log\u03c3|D)foraGaussian, usinganuninformativeprior p(\u03bc,log\u03c3)\u221d\n1.\nExercise 21.2 Laplace approximation to normal-gamma\nConsider estimating \u03bcand/lscript=l o g\u03c3for a Gaussian using an uniformative normal-Gamma prior. The log\nposterior is\nlogp(\u03bc,/lscript|D)=\u2212nlog\u03c3\u22121\n2\u03c32[ns2+n(y\u2212\u03bc)2] (21.203)\na. Show that the \ufb01rst derivatives are\n\u2202\n\u2202\u03bclogp(\u03bc,/lscript|D)=n(y\u2212\u03bc)\n\u03c32(21.204)\n\u2202\n\u2202/lscriptlogp(\u03bc,/lscript|D)=\u2212n +ns2+n(y\u2212\u03bc)2\n\u03c32(21.205)\nb. Show that the Hessian matrix is given by\nH=/parenleftBigg\n\u22022\n\u2202\u03bc2logp(\u03bc,/lscript|D)\u22022\n\u2202\u03bc\u2202/lscriptlogp(\u03bc,/lscript|D)\n\u22022\n\u2202/lscript2logp(\u03bc,/lscript|D)\u22022\n\u2202/lscript2logp(\u03bc,/lscript|D)/parenrightBigg\n(21.206)\n=/parenleftbigg\u2212n\n\u03c32 \u22122ny\u2212\u03bc\n\u03c32\n\u22122ny\u2212\u03bc\n\u03c32\u22122\n\u03c32(ns2+n(y\u2212\u03bc)2)/parenrightbigg\n(21.207)\nc. Use this to derive a Laplace approximation to the posterior p(\u03bc,/lscript|D).\nExercise 21.3 Variational lower bound for VB for univariate Gaussian\nFill in the details of the derivation in Section 21.5.1.6.\nExercise 21.4 Variational lower bound for VB for GMMs\nConsider VBEM for GMMs as in Section 21.6.1.4. Show that the lower bound has the following form\nL=E[lnp(x|z,\u03bc,\u039b)]+ E[lnp(z|\u03c0)]+E[lnp(\u03c0)]+E[lnp(\u03bc,\u039b)]\n\u2212E[lnq(z)]\u2212E[lnq(\u03c0)]\u2212E[lnq(\u03bc,\u039b)] (21.208)", "795": "21.8. Localvariationalbounds* 765\nwhere\nE[lnp(x|z,\u03bc,\u039b)] =1\n2/summationdisplay\nkNk/braceleftBig\nln\u02dc\u039bk\u2212D\u03b2\u22121\nk\u2212\u03bdktr(SkLk)\n\u2212\u03bdk(xk\u2212mk)TLk(xk\u2212mk)\u2212Dln(2\u03c0)/bracerightBig\n(21.209)\nE[lnp(z|\u03c0)] =/summationdisplay\ni/summationdisplay\nkrikln\u02dc\u03c0k (21.210)\nE[lnp(\u03c0)] = ln Cdir(\u03b10)+(\u03b10\u22121)/summationdisplay\nkln\u02dc\u03c0k (21.211)\nE[lnp(\u03bc,\u039b)] =1\n2/summationdisplay\nk/braceleftbigg\nDln(\u03b20/2\u03c0)+ln\u02dc\u039bk\u2212D\u03b20\n\u03b2k\n\u2212\u03b20\u03bdk(mk\u2212m0)TLk(mk\u2212m0)\n+lnCWi(L0,\u03bd0)+\u03bd0\u2212D\u22121\n2ln\u02dc\u039bk\u22121\n2\u03bdktr(L\u22121\n0Lk)/bracerightbigg\n(21.212)\nE[lnq(z)] =/summationdisplay\ni/summationdisplay\nkriklnrik (21.213)\nE[lnq(\u03c0)] =/summationdisplay\nk(\u03b1k\u22121)ln\u02dc\u03c0k+lnCdir(\u03b1) (21.214)\nE[lnq(\u03bc,\u039b)] =/summationdisplay\nk/braceleftbigg1\n2ln\u02dc\u039bk+D\n2ln/parenleftbigg\u03b2k\n2\u03c0/parenrightbigg\n\u2212D\n2\u2212H(q(\u039bk))/bracerightbigg\n(21.215)\nwhere the normalization constant for the Dirichlet and Wishart is given by\nCdir(\u03b1)/defines\u0393(/summationtext\nk\u03b1k)/producttext\nk\u0393(\u03b1k)(21.216)\nCWi(L,\u03bd)/defines|L|\u2212\u03bd/2/parenleftBig\n2\u03bdD/2\u0393D(\u03bd/2)/parenrightBig\u22121\n(21.217)\n\u0393D(\u03b1)/defines\u03c0D(D\u22121)/4D/productdisplay\nj=1\u0393(\u03b1+(1\u2212j)/2) (21.218)\nwhere\u0393D(\u03bd)is the multivariate Gamma function. Finally, the entropy of the Wishart is given by\nH(Wi(L,\u03bd)) = \u2212lnCWi(L,\u03bd)\u2212\u03bd\u2212D\u22121\n2E[ln|\u039b|]+\u03bdD\n2(21.219)\nwhere E[ln|\u039b|]is given in Equation 21.131.\nExercise 21.5 Derivation of E[log\u03c0k]under a Dirichlet distribution\nShow that\nexp(E[log\u03c0k]) =exp(\u03a8(\u03b1 k))\nexp(\u03a8(/summationtext\nk/prime\u03b1k/prime))(21.220)\nwhere\u03c0\u223cDir(\u03b1).\nExercise 21.6 Alternative derivation of the mean \ufb01eld updates for the Ising model\nDerive Equation 21.50 by directly optimizing the variational free energy one term at a time.", "796": "766 Chapter21. Variationalinference\nExercise 21.7 Forwards vs reverse KL divergence\n(Source: Exercise 33.7 of (MacKay 2003).) Consider a factored approximation q(x,y)=q(x)q(y)to a joint\ndistribution p(x,y). Show that to minimize the forwards KL KL(p||q)we should set q(x)=p(x)and\nq(y)=p(y), i.e., the optimal approximation is a product of marginals\nNow consider the following joint distribution, where the rows represent yand the columns x.\nx\n1234\n11/8 1/8 0 0\n21/8 1/8 0 0\n30 0 1/4 0\n40 0 0 1/4\nShow that the reverse KL KL(q||p)for thisphas three distinct minima. Identify those minima and\nevaluate KL(q||p)at each of them. What is the value of KL(q||p)if we setq(x,y)=p(x)p(y)?\nExercise 21.8 Derivation of the structured mean \ufb01eld updates for FHMM\nDerive the updates in Section 21.4.1.\nExercise 21.9 Variational EM for binary FA with sigmoid link\nConsider the binary FA model:\np(xi|zi,\u03b8)=D/productdisplay\nj=1Ber(xij|sigm(wT\njzi+\u03b2j)) =D/productdisplay\nj=1Ber(xij|sigm(\u03b7 ij)) (21.221)\n\u03b7i=\u02dcW\u02dczi (21.222)\n\u02dc zi/defines(zi;1) (21.223)\n\u02dcW/defines(W,\u03b2) (21.224)\np(zi)=N (0,I) (21.225)\nDerive an EM algorithm to \ufb01t this model, using the Jaakkola-Jordan bound. Hint: the answer is in (Tipping\n1998), but the exercise asks you to derive these equations.\nExercise 21.10 VB for binary FA with probit link\nIn Section 11.4.6, we showed how to use EM to \ufb01t probit regression, using a model of the form p(yi=\n1|zi)=I(zi>0),w h e r e zi\u223cN(wTxi,1)is latent. Now consider the case where the inputs xiare\nalso unknown, as in binary factor analysis. Show how to \ufb01t this model using variational Bayes, making an\napproximation to the posterior of the form q(x,z,W)=/producttextN\ni=1q(xi)q(zi)/producttextLl=1q(wl). Hint:q(xi)and\nq(wi)will be Gaussian, and q(zi)will be a truncated univariate Gaussian.", "797": "22 More variational inference\n22.1 Introduction\nIn Chapter 21, we discussed mean \ufb01eld inference, which approximates the posterior by a product\nof marginal distributions. This allows us to use different parametric forms for each variable,which is particularly useful when performing Bayesian inference for the parameters of statisticalmodels (such as the mean and variance of a Gaussian or GMM, or the regression weights in aGLM), as we saw when we discussed variational Bayes and VB-EM.\nIn this chapter, we discuss a slightly different kind of variational inference. The basic idea is\nto minimize J(q)=KL(q||\u02dcp),w h e r e\u02dcpis the exact but unnormalized posterior as before, but\nwhere we no longer require qto be factorized. In fact, we do not even require qto be a globally\nvalid joint distribution. Instead, we only require that qis locally consistent, meaning that the\njoint distribution of two adjacent nodes agrees with the corresponding marginals (we will de\ufb01nethis more precisely below).\nIn addition to this new kind of inference, we will discuss approximate methods for MAP\nstate estimation in discrete graphical models. It turns out that algorithms for solving the MAPproblem are very similar to some approximate methods for computing marginals, as we will see.\n22.2 Loopy belief propagation: algorithmic issues\nThere is a very simple approximate inference algorithm for discrete (or Gaussian) graphicalmodels known as loopy belief propagation orLBP. The basic idea is extremely simple: we\napply the belief propagation algorithm of Section 20.2 to the graph, even if it has loops (i.e.,even if it is not a tree). This method is simple and efficient, and often works well in practice,outperforming mean \ufb01eld (Weiss 2001). In this section, we discuss the algorithm in more detail.In the next section, we analyse this algorithm in terms of variational inference.\n22.2.1 A brief history\nWhen applied to loopy graphs, BP is not guaranteed to give correct results, and may not evenconverge. Indeed, Judea Pearl, who invented belief propagation for trees, wrote the followingabout loopy BP in 1988:\nWhen loops are present, the network is no longer singly connected and local propagation", "798": "768 Chapter22. Morevariationalinference\nschemes will invariably run into trouble ...If we ignore the existence of loops and\npermit the nodes to continue communicating with each other as if the network were\nsingly connected, messages may circulate inde\ufb01nitely around the loops and the processmay not converge to a stable equilibrium ...Such oscillations do not normally occur in\nprobabilistic networks ...which tend to bring all messages to some stable equilibrium as\ntime goes on. However, this asymptotic equilibrium is not coherent, in the sense that itdoes not represent the posterior probabilities of all nodes of the network \u2014 (Pearl 1988,p.195)\nDespite these reservations, Pearl advocated the use of belief propagation in loopy networks asan approximation scheme (J. Pearl, personal communication) and exercise 4.7in (Pearl 1988)\ninvestigates the quality of the approximation when it is applied to a particular loopy beliefnetwork.\nHowever, the main impetus behind the interest in BP arose when McEliece et al. (1998) showed\nthat a popular algorithm for error correcting codes known as turbo codes (Berrou et al. 1993)could be viewed as an instance of BP applied to a certain kind of graph. This was an importantobservation since turbo codes have gotten very close to the theoretical lower bound on codingefficiency proved by Shannon. (Another approach, known as low density parity check or LDPCcodes, has achieved comparable performance; it also uses LBP for decoding \u2014 see Figure 22.1for an example.) In (Murphy et al. 1999), LBP was experimentally shown to also work well forinference in other kinds of graphical models beyond the error-correcting code context, and sincethen, the method has been widely used in many different applications.\n22.2.2 LBP on pairwise models\nWe now discuss how to apply LBP to an undirected graphical model with pairwise factors (wediscuss the directed case, which can involve higher order factors, in the next section). Themethod is simple: just continually apply Equations 20.11 and 20.10 until convergence. SeeAlgorithm 8 for the pseudocode, and beliefPropagation for some Matlab code. We will\ndiscuss issues such as convergence and accuracy of this method shortly.\nAlgorithm 22.1: Loopy belief propagation for a pairwise MRF\n1Input: node potentials \u03c8s(xs), edge potentials \u03c8st(xs,xt);\n2Initialize messages ms\u2192t(xt)=1for all edges s\u2212t;\n3Initialize beliefs bels(xs)=1for all nodes s;\n4repeat\n5Send message on each edge\nms\u2192t(xt)=/summationtext\nxs/parenleftBig\n\u03c8s(xs)\u03c8st(xs,xt)/producttext\nu\u2208nbrs\\tmu\u2192s(xs)/parenrightBig\n;\n6Update belief of each node bels(xs)\u221d\u03c8s(xs)/producttext\nt\u2208nbrsmt\u2192s(xs);\n7untilbeliefsdon\u2019tchangesigni\ufb01cantly ;\n8Return marginal beliefs bels(xs);", "799": "22.2. Loopybeliefpropagation: algorithmicissues 769\nFigure 22.1 (a) A simple factor graph representation of a (2,3) low-density parity check code (factor graphs\nare de\ufb01ned in Section 22.2.3.1). Each message bit (hollow round circle) is connected to two parity factors\n(solid black squares), and each parity factor is connected to three bits. Each parity factor has the form\u03c8\nstu(xs,xt,xu)=I(xs\u2297xt\u2297xu=1 ),w h e r e\u2297is the xor operator. The local evidence factors for\neach hidden node are not shown. (b) A larger example of a random LDPC code. We see that this graph is\u201clocally tree-like\u201d, meaning there are no short cycles; rather, each cycle has length \u223clogm,w h e r e mis the\nnumber of nodes. This gives us a hint as to why loopy BP works so well on such graphs. (Note, however,that some error correcting code graphs have short loops, so this is not the full explanation.) Source:Figure 2.9 from (Wainwright and Jordan 2008b). Used with kind permission of Martin Wainwright.\n22.2.3 LBP on a factor graph\nTo handle models with higher-order clique potentials (which includes directed models where\nsome nodes have more than one parent), it is useful to use a representation known as a factorgraph. We explain this representation below, and then describe how to apply LBP to suchmodels.\n22.2.3.1 Factor graphs\nAfactor graph (Kschischang et al. 2001; Frey 2003) is a graphical representation that uni\ufb01es\ndirected and undirected models, and which simpli\ufb01es certain message passing algorithms. Moreprecisely, a factor graph is an undirected bipartite graph with two kinds of nodes. Round nodesrepresent variables, square nodes represent factors, and there is an edge from each variable toevery factor that mentions it. For example, consider the MRF in Figure 22.2(a). If we assumeone potential per maximal clique, we get the factor graph in Figure 22.2(b), which represents thefunction\nf(x\n1,x2,x3,x4)=f124(x1,x2,x4)f234(x2,x3,x4) (22.1)\nIf we assume one potential per edge. we get the factor graph in Figure 22.2(c), which representsthe function\nf(x\n1,x2,x3,x4)=f14(x1,x4)f12(x1,x2)f34(x3,x4)f23(x2,x3)f24(x2,x4) (22.2)", "800": "770 Chapter22. Morevariationalinference\n1\n2\n34\n(a)34 21\n(b)34 21\n(c)\nFigure 22.2 (a) A simple UGM. (b) A factor graph representation assuming one potential per maximal\nclique. (c) A factor graph representation assuming one potential per edge.\nx4x5x3x1x2\n(a)x4p(x4|x3)x3\np(x5|x3)\nx5p(x3|x1,x2)x1 x2p(x1) p(x2)\n(b)\nFigure 22.3 (a) A simple DGM. (b) Its corresponding factor graph. Based on Figure 5 of (Yedidia et al.\n2001)..\nWe can also convert a DGM to a factor graph: just create one factor per CPD, and connect that\nfactor to all the variables that use that CPD. For example, Figure 22.3 represents the following\nfactorization:\nf(x1,x2,x3,x4,x5)=f1(x1)f2(x2)f123(x1,x2,x3)f34(x3,x4)f35(x3,x5) (22.3)\nwhere we de\ufb01ne f123(x1,x2,x3)=p(x3|x1,x2), etc. If each node has at most one parent (and\nhence the graph is a chain or simple tree), then there will be one factor per edge (root nodescan have their prior CPDs absorvbed into their children\u2019s factors). Such models are equivalentto pairwise MRFs.", "801": "22.2. Loopybeliefpropagation: algorithmicissues 771\nFigure 22.4 Message passing on a bipartite factor graph. Square nodes represent factors, and circles\nrepresent variables. Source: Figure 6 of (Kschischang et al. 2001). Used with kind permission of Brendan\nFrey.\n22.2.3.2 BP on a factor graph\nWe now derive a version of BP that sends messages on a factor graph, as proposed in (Kschis-\nchang et al. 2001). Speci\ufb01cally, we now have two kinds of messages: variables to factors\nmx\u2192f(x)=/productdisplay\nh\u2208nbr(x)\\{f}mh\u2192x(x) (22.4)\nand factors to variables:\nmf\u2192x(x)=/summationdisplay\nyf(x,y)/productdisplay\ny\u2208nbr(f)\\{x}my\u2192f(y) (22.5)\nHerenbr(x)are all the factors that are connected to variable x, andnbr(f)are all the variables\nthat are connected to factor f. These messages are illustrated in Figure 22.4. At convergence,\nwe can compute the \ufb01nal beliefs as a product of incoming messages:\nbel(x)\u221d/productdisplay\nf\u2208nbr(x)mf\u2192x(x) (22.6)\nIn the following sections, we will focus on LBP for pairwise models, rather than for factor\ngraphs, but this is just for notational simplicity.\n22.2.4 Convergence\nLBP does not always converge, and even when it does, it may converge to the wrong answers.\nThis raises several questions: how can we predict when convergence will occur? what can we do\nto increase the probability of convergence? what can we do to increase the rate of convergence?\nWe brie\ufb02y discuss these issues below. We then discuss the issue of accuracy of the results at\nconvergence.", "802": "772 Chapter22. Morevariationalinference\nSynchronous Asynchronous No smoothing True01.0\n0.90.80.70.60.50.40.30.20.1\n0\n10 20 30 40 50 60 70 80 90 100\nTime (seconds)% of messages converged1.00.90.80.70.60.50.40.30.20.1\n0\nTime (seconds)P(X10=0 )1.00.90.80.70.60.50.40.30.20.1\n0\n0 0.1 0.2 0.3 0.4 0.5\nTime (seconds)P(X115=0 )\n1.00.90.80.70.60.50.40.30.20.1\n0\nTime (seconds)P(X61=0 )1.00.90.80.70.60.50.40.30.20.1\n0\nTime (seconds)P(X17=0 )1.00.90.80.70.60.50.40.30.20.1\n0\nTime (seconds)P(X7=0 )0 0.1 0.2 0.3 0.4 0.5\n0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5(a)\n(d)(b)\n(e)(c)\n(f)\nFigure 22.5 Illustration of the behavior of loopy belief propagation on an 11\u00d711Ising grid with\nrandom potentials, wij\u223cUnif(\u2212C,C ),w h e r eC=1 1. For larger C, inference becomes harder. (a)\nPercentage of messasges that have converged vs time for 3 different update schedules: Dotted = damped\nsychronous (few nodes converge), dashed = undamped asychnronous (half the nodes converge), solid =damped asychnronous (all nodes converge). (b-f) Marginal beliefs of certain nodes vs time. Solid straightline = truth, dashed = sychronous, solid = damped asychronous. Source: Figure 11.C.1 of (Koller andFriedman 2009). Used with kind permission of Daphne Koller.\n22.2.4.1 When will LBP converge?\nThe details of the analysis of when LBP will converge are beyond the scope of this chapter, but\nwe brie\ufb02y sketch the basic idea. The key analysis tool is the computation tree, which visualizes\nthe messages that are passed as the algorithm proceeds. Figure 22.6 gives a simple example.In the \ufb01rst iteration, node 1 receives messages from nodes 2 and 3. In the second iteration, itreceives one message from node 3 (via node 2), one from node 2 (via node 3), and two messagesfrom node 4 (via nodes 2 and 3). And so on.\nThe key insight is that Titerations of LBP is equivalent to exact computation in a computation\ntree of height T+1. If the strengths of the connections on the edges is sufficiently weak, then\nthe in\ufb02uence of the leaves on the root will diminish over time, and convergence will occur. See(Wainwright and Jordan 2008b) and references therein for more information.", "803": "22.2. Loopybeliefpropagation: algorithmicissues 773\nFigure 22.6 (a) A simple loopy graph. (b) The computation tree, rooted at node 1, after 4 rounds of\nmessage passing. Nodes 2 and 3 occur more often in the tree because they have higher degree than nodes\n1 and 2. Source: Figure 8.2 of (Wainwright and Jordan 2008b). Used with kind permission of MartinWainwright.\n22.2.4.2 Making LBP converge\nAlthough the theoretical convergence analysis is very interesting, in practice, when faced with a\nmodel where LBP is not converging, what should we do?\nOne simple way to reduce the chance of oscillation is to use damping. That is, instead of\nsending the message Mk\nts, we send a damped message of the form\n\u02dcMk\nts(xs)=\u03bbMts(xs)+(1\u2212\u03bb)\u02dcMk\u22121\nts(xs) (22.7)\nwhere0\u2264\u03bb\u22641is the damping factor Clearly if \u03bb=1this reduces to the standard scheme,\nbut for\u03bb<1, this partial updating scheme can help improve convergence. Using a value such\nas\u03bb\u223c0.5is standard practice. The bene\ufb01ts of this approach are shown in Figure 22.5, where\nwe see that damped updating results in convergence much more often than undamped updating.\nIt is possible to devise methods, known as double loop algorithms, which are guaranteed to\nconverge to a local minimum of the same objective that LBP is minimizing (Yuille 2001; Wellingand Teh 2001). Unfortunately, these methods are rather slow and complicated, and the accuracyof the resulting marginals is usually not much greater than with standard LBP. (Indeed, oscillatingmarginals is sometimes a sign that the LBP approximation itself is a poor one.) Consequently,these techniques are not very widely used. In Section 22.4.2, we will see a different convergentversion of BP that is widely used.\n22.2.4.3 Increasing the convergence rate: message scheduling\nEven if LBP converges, it may take a long time. The standard approach when implementingLBP is to perform synchronous updates, where all nodes absorb messages in parallel, and then\nsend out messages in parallel. That is, the new messages at iteration k+1are computed in\nparallel using\nm\nk+1=(f1(mk),...,f E(mk)) (22.8)\nwhereEis the number of edges, and fst(m)is the function that computes the message for\nedges\u2192tgiven all the old messages. This is analogous to the Jacobi method for solving linear", "804": "774 Chapter22. Morevariationalinference\nsystems of equations. It is well known (Bertsekas 1997) that the Gauss-Seidel method, which\nperforms asynchronous updates in a \ufb01xed round-robin fashion, converges faster when solving\nlinear systems of equations. We can apply the same idea to LBP, using updates of the form\nmk+1\ni=fi/parenleftbig\n{mk+1\nj:j<i},{mk\nj:j>i}/parenrightbig\n(22.9)\nwhere the message for edge iis computed using new messages (iteration k+1) from edges\nearlier in the ordering, and using old messages (iteration k) from edges later in the ordering.\nThis raises the question of what order to update the messages in. One simple idea is to use\na \ufb01xed or random order. The bene\ufb01ts of this approach are shown in Figure 22.5, where we see\nthat (damped) asynchronous updating results in convergence much more often than synchronousupdating.\nA smarter approach is to pick a set of spanning trees, and then to perform an up-down\nsweep on one tree at a time, keeping all the other messages \ufb01xed. This is known as tree\nreparameterization (TRP) (Wainwright et al. 2001), which should not be confused with the more\nsophisticated tree-reweighted BP (often abbreviated to TRW) to be discussed in Section 22.4.2.1.\nHowever, we can do even better by using an adaptive ordering. The intuition is that we should\nfocus our computational efforts on those variables that are most uncertain. (Elidan et al. 2006)proposed a technique known as residual belief propagation, in which messages are scheduled\nto be sent according to the norm of the difference from their previous value. That is, we de\ufb01nethe residual of new message m\nstat iteration kto be\nr(s,t,k)=||logmst\u2212logmk\nst||\u221e=m a x\ni|logmst(i)\nmk\nst(i)| (22.10)\nWe can store messages in a priority queue, and always send the one with highest residual. When\na message is sent from stot, all of the other messages that depend on mst(i.e., messages of\nthe form mtuwhereu\u2208nbr(t)\\s) need to be recomputed; their residual is recomputed, and\nthey are added back to the queue. In (Elidan et al. 2006), they showed (experimentally) that thismethod converges more often, and much faster, than using sychronous updating, asynchronousupdating with a \ufb01xed order, and the TRP approach.\nA re\ufb01nement of residual BP was presented in (Sutton and McCallum 2007). In this paper, they\nuse an upper bound on the residual of a message instead of the actual residual. This meansthat messages are only computed if they are going to be sent; they are not just computed forthe purposes of evaluating the residual. This was observed to be about \ufb01ve times faster thanresidual BP, although the quality of the \ufb01nal results is similar.\n22.2.5 Accuracy of LBP\nFor a graph with a single loop, one can show that the max-product version of LBP will \ufb01nd thecorrect MAP estimate, if it converges (Weiss 2000). For more general graphs, one can boundthe error in the approximate marginals computed by LBP, as shown in (Wainwright et al. 2003;Vinyals et al. 2010). Much stronger results are available in the case of Gaussian models (Weissand Freeman 2001a; Johnson et al. 2006; Bickson 2009). In particular, in the Gaussian case, ifthe method converges, the means are exact, although the variances are not (typically the beliefsare over con\ufb01dent).", "805": "22.2. Loopybeliefpropagation: algorithmicissues 775\n22.2.6 Other speedup tricks for LBP *\nThere are several tricks one can use to make BP run faster. We discuss some of them below.\n22.2.6.1 Fast message computation for large state spaces\nThe cost of computing each message in BP (whether in a tree or a loopy graph) is O(Kf),\nwhereKis the number of states, and fis the size of the largest factor (f =2for pairwise\nUGMs). In many vision problems (e.g., image denoising), Kis quite large (say 256), because\nit represents the discretization of some underlying continuous space, so O(K2)per message\nis too expensive. Fortunately, for certain kinds of pairwise potential functions of the form\n\u03c8st(xs,xt)=\u03c8(xs\u2212xt), one can compute the sum-product messages in O(KlogK)time\nusing the fast Fourier transform or FFT, as explained in (Felzenszwalb and Huttenlocher 2006).The key insight is that message computation is just convolution:\nM\nk\nst(xt)=/summationdisplay\nxs\u03c8(xs\u2212xt)h(xs) (22.11)\nwhereh(xs)=\u03c8s(xs)/producttext\nv\u2208nbr(s)\\tMk\u22121\nvs(xs). If the potential function \u03c8(z)is a Gaussian-like\npotential, we can compute the convolution in O(K)time by sequentially convolving with a\nsmall number of box \ufb01lters (Felzenszwalb and Huttenlocher 2006).\nFor the max-product case, a technique called the distance transform can be used to compute\nmessages in O(K)time. However, this only works if \u03c8(z)=e x p (\u2212E(z))and where E(z)\nhas one the following forms: quadratic, E(z)=z2; truncated linear, E(z)=m i n ( c1|z|,c2);o r\nPotts model, E(z)=cI(z/negationslash=0 ). See (Felzenszwalb and Huttenlocher 2006) for details.\n22.2.6.2 Multi-scale methods\nA method which is speci\ufb01c to 2d lattice structures, which commonly arise in computer vision,is based on multi-grid techniques. Such methods are widely used in numerical linear algebra,where one of the core problems is the fast solution of linear systems of equations; this isequivalent to MAP estimation in a Gaussian MRF. In the computer vision context, (Felzenszwalband Huttenlocher 2006) suggested using the following heuristic to signi\ufb01cantly speedup BP:construct a coarse-to-\ufb01ne grid, compute messages at the coarse level, and use this to initializemessages at the level below; when we reach the bottom level, just a few iterations of standard BPare required, since long-range communication has already been achieved via the initializationprocess.\nThe beliefs at the coarse level are computed over a small number of large blocks. The local\nevidence is computed from the average log-probability each possible block label assigns to allthe pixels in the block. The pairwise potential is based on the discrepancy between labels ofneighboring blocks, taking into account their size. We can then run LBP at the coarse level,and then use this to initialize the messages one level down. Note that the modelis still a\n\ufb02at grid; however, the initialization process exploits the multi-scale nature of the problem. See\n(Felzenszwalb and Huttenlocher 2006) for details.", "806": "776 Chapter22. Morevariationalinference\n22.2.6.3 Cascades\nAnother trick for handling high-dimensional state-spaces, that can also be used with exact\ninference (e.g., for chain-structured CRFs), is to prune out improbable states based on a com-putationally cheap \ufb01ltering step. In fact, one can create a hierarchy of models which tradeoffspeed and accuracy. This is called a computational cascade. In the case of chains, one can\nguarantee that the cascade will never \ufb01lter out the true MAP solution (Weiss et al. 2010).\n22.3 Loopy belief propagation: theoretical issues *\nWe now attempt to understand the LBP algorithm from a variational point of view. Our presen-tation is closely based on an excellent 300-page review article (Wainwright and Jordan 2008a).This paper is sometimes called \u201cthe monster\u201d (by its own authors!) in view of its length andtechnical difficulty. This section just sketches some of the main results.\nTo simplify the presentation, we focus on the special case of pairwise UGMs with discrete\nvariables and tabular potentials. Many of the results generalize to UGMs with higher-order cliquepotentials (which includes DGMs), but this makes the notation more complex (see (Koller andFriedman 2009) for details of the general case).\n22.3.1 UGMs represented in exponential family form\nWe assume the distribution has the following form:\np(x|\u03b8,G)=1\nZ(\u03b8)exp\u23a7\n\u23a8\n\u23a9/summationdisplay\ns\u2208V\u03b8s(xs)+/summationdisplay\n(s,t)\u2208E\u03b8st(xs,xt)\u23ab\n\u23ac\n\u23ad(22.12)\nwhere graph Ghas nodesVand edgesE. (Henceforth we will drop the explicit conditioning\non\u03b8andGfor brevity, since we assume both are known and \ufb01xed.) We can rewrite this in\nexponential family form as follows:\np(x|\u03b8)=1\nZ(\u03b8)exp(\u2212E(x)) (22.13)\nE(x)/defines\u2212\u03b8T\u03c6(x) (22.14)\nwhere\u03b8=({\u03b8s;j},{\u03b8s,t;j,k})are all the node and edge parameters (the canonical parameters),\nand\u03c6(x)=({I(xs=j)},{I(xs=j,xt=k)})are all the node and edge indicator functions\n(the sufficient statistics). Note: we use s,t\u2208Vto index nodes and j,k\u2208Xto index states.\nThe mean of the sufficient statistics are known as the mean parameters of the model, and are\ngiven by\n\u03bc=E[\u03c6(x)] = ({p( xs=j)}s,{p(xs=j,xt=k)}s/negationslash=t)=({\u03bcs;j}s,{\u03bcst;jk}s/negationslash=t)(22.15)\nThis is a vector of length d=|X||V|+|X|2|E|, containing the node and edge marginals.\nIt completely characterizes the distribution p(x|\u03b8), so we sometimes treat \u03bcas a distribution\nitself.\nEquation 22.12 is called the standard overcomplete representation. It is called \u201covercom-\nplete\u201d because it ignores the sum-to-one constraints. In some cases, it is convenient to remove", "807": "22.3. Loopybeliefpropagation: theoreticalissues* 777\nthis redundancy. For example, consider an Ising model where Xs\u2208{0,1}. The model can be\nwritten as\np(x)=1\nZ(\u03b8)exp\u23a7\n\u23a8\n\u23a9/summationdisplay\ns\u2208V\u03b8sxs+/summationdisplay\n(s,t)\u2208E\u03b8stxsxt\u23ab\n\u23ac\n\u23ad(22.16)\nHence we can use the following minimal parameterization\n\u03c6(x)=(xs,s\u2208V;xsxt,(s,t)\u2208E)\u2208Rd(22.17)\nwhered=|V|+|E|. The corresponding mean parameters are \u03bcs=p(xs=1 )and\u03bcst=\np(xs=1,xt=1 ).\n22.3.2 The marginal polytope\nThe space of allowable \u03bcvectors is called the marginal polytope, and is denoted M(G),w h e r e\nGis the structure of the graph de\ufb01ning the UGM. This is de\ufb01ned to be the set of all mean\nparameters for the given model that can be generated from a valid probability distribution:\nM(G)/defines{\u03bc\u2208Rd:\u2203ps.t.\u03bc=/summationdisplay\nx\u03c6(x)p(x)for some p(x)\u22650,/summationdisplay\nxp(x)=1}(22.18)\nFor example, consider an Ising model. If we have just two nodes connected as X1\u2212X2,\none can show that we have the following minimal set of constraints: 0\u2264\u03bc12,0\u2264\u03bc12\u2264\u03bc1,\n0\u2264\u03bc12\u2264\u03bc2, and1+\u03bc12\u2212\u03bc1\u2212\u03bc2\u22650. We can write these in matrix-vector form as\n\u239b\n\u239c\u239c\u239d001\n10\u22121\n01\u22121\n\u22121\u221211\u239e\n\u239f\u239f\u23a0\u239b\n\u239d\u03bc1\n\u03bc2\n\u03bc12\u239e\u23a0\u2265\u239b\n\u239c\u239c\u239d0\n00\n\u22121\u239e\n\u239f\u239f\u23a0(22.19)\nThese four constraints de\ufb01ne a series of half-planes, whose intersection de\ufb01nes a polytope,\nas shown in Figure 22.7(a).\nSince M(G)is obtained by taking a convex combination of the \u03c6(x)vectors, it can also be\nwritten as the convex hull of the feature set:\nM(G)=conv{\u03c6\n1(x),...,\u03c6 d(x)} (22.20)\nFor example, for a 2 node MRF X1\u2212X2with binary states, we have\nM(G)=conv{(0,0,0),(1,0,0),(0,1,0),(1,1,1)} (22.21)\nThese are the four black dots in Figure 22.7(a). We see that the convex hull de\ufb01nes the same\nvolume as the intersection of half-spaces.\nThe marginal polytope will play a crucial role in the approximate inference algorithms we\ndiscuss in the rest of this chapter.", "808": "778 Chapter22. Morevariationalinference\n(a) (b) (c)\nFigure 22.7 (a) Illustration of the marginal polytope for an Ising model with two variables. (b) Cartoon\nillustration of the set MF(G), which is a nonconvex inner bound on the marginal polytope M(G).MF(G)\nis used by mean \ufb01eld. (c) Cartoon illustration of the relationship between M(G)andL(G), which is used\nby loopy BP. The set L(G)is always an outer bound on M(G), and the inclusion M(G)\u2282L(G)is strict\nwhenever Ghas loops. Both sets are polytopes, which can be de\ufb01ned as an intersection of half-planes\n(de\ufb01ned by facets), or as the convex hull of the vertices. L(G)actually has fewer facets than M(G), despite\nthe picture. In fact, L(G)hasO(|X||V|+|X|2|E|)facets, where |X|is the number of states per variable,\n|V|is the number of variables, and |E|is the number of edges. By contrast, M(G)hasO(|X||V|)facets.\nOn the other hand, L(G)has more vertices than M(G), despite the picture, since L(G)contains all the\nbinary vector extreme points \u03bc\u2208M(G), plus additional fractional extreme points. Source: Figures 3.6,\n5.4 and 4.2 of (Wainwright and Jordan 2008a). Used with kind permission of Martin Wainwright.\n22.3.3 Exact inference as a variational optimization problem\nRecall from Section 21.2 that the goal of variational inference is to \ufb01nd the distribution qthat\nmaximizes the energy functional\nL(q)=\u2212KL(q||p)+log Z=Eq[log \u02dcp(x)]+H(q)\u2264logZ (22.22)\nwhere\u02dcp(x)=Zp(x)is the unnormalized posterior. If we write log \u02dcp(x)=\u03b8T\u03c6(x), and we\nletq=p, then the exact energy functional becomes\nmax\n\u03bc\u2208M(G)\u03b8T\u03bc+H(\u03bc) (22.23)\nwhere\u03bc=Ep[\u03c6(x)]is a joint distribution over all state con\ufb01gurations x(so it is valid to write\nH(\u03bc)). Since the KL divergence is zero when p=q, we know that\nmax\n\u03bc\u2208M(G)\u03b8T\u03bc+H(\u03bc)=l o gZ(\u03b8) (22.24)\nThis is a way to cast exact inference as a variational optimization problem.\nEquation 22.24 seems easy to optimize: the objective is concave, since it is the sum of a linear\nfunction and a concave function (see Figure 2.21 to see why entropy is concave); furthermore, we\nare maximizing this over a convex set. However, the marginal polytope M(G)has exponentially\nmany facets. In some cases, there is structure to this polytope that can be exploited by dynamicprogramming (as we saw in Chapter 20), but in general, exact inference takes exponential time.Most of the existing deterministic approximate inference schemes that have been proposed inthe literature can be seen as different approximations to the marginal polytope, as we explainbelow.", "809": "22.3. Loopybeliefpropagation: theoreticalissues* 779\n22.3.4 Mean \ufb01eld as a variational optimization problem\nWe discussed mean \ufb01eld at length in Chapter 21. Let us re-interpret mean \ufb01eld inference in\nour new more abstract framework. This will help us compare it to other approximate methodswhich we discuss below.\nFirst, letFbe an edge subgraph of the original graph G, and letI(F)\u2286Ibe the subset of\nsufficient statistics associated with the cliques of F.L e t\u03a9be the set of canonical parameters\nfor the full model, and de\ufb01ne the canonical parameter space for the submodel as follows:\n\u03a9(F)/defines{\u03b8\u2208\u03a9:\u03b8\n\u03b1=0\u2200\u03b1\u2208I\\I(F)} (22.25)\nIn other words, we require that the natural parameters associated with the sufficient statistics\u03b1outside of our chosen class to be zero. For example, in the case of a fully factorized\napproximation, F\n0, we remove all edges from the graph, giving\n\u03a9(F0)/defines{\u03b8\u2208\u03a9:\u03b8st=0\u2200(s,t)\u2208E} (22.26)\nIn the case of structured mean \ufb01eld (Section 21.4), we set \u03b8st=0for edges which are not in\nour tractable subgraph.\nNext, we de\ufb01ne the mean parameter space of the restricted model as follows:\nMF(G)/defines{\u03bc\u2208Rd:\u03bc=E\u03b8[\u03c6(x)]for some \u03b8\u2208\u03a9(F)} (22.27)\nThis is called an inner approximation to the marginal polytope, since MF(G)\u2286M(G). See\nFigure 22.7(b) for a sketch. Note that MF(G)is a non-convex polytope, which results in multiple\nlocal optima. By contrast, some of the approximations we will consider later will be convex.\nWe de\ufb01ne the entropy of our approximation H(\u03bc(F))as the entropy of the distribution\n\u03bcde\ufb01ned on submodel F. Then we de\ufb01ne the mean \ufb01eld energy functional optimization\nproblem as follows:\nmax\n\u03bc\u2208MF(G)\u03b8T\u03bc+H(\u03bc)\u2264logZ(\u03b8) (22.28)\nIn the case of the fully factorized mean \ufb01eld approximation for pairwise UGMs, we can writethis objective as follows:\nmax\n\u03bc\u2208Pd/summationdisplay\ns\u2208V/summationdisplay\nxs\u03b8s(xs)\u03bcs(xs)+/summationdisplay\n(s,t)\u2208E/summationdisplay\nxs,xt\u03b8st(xs,xt)\u03bcs(xs)\u03bct(xt)+/summationdisplay\ns\u2208VH(\u03bcs)(22.29)\nwhere\u03bcs\u2208P, andPis the probability simplex over X.\nMean \ufb01eld involves a concave objective being maximized over a non-convex set. It is typically\noptimized using coordinate ascent, since it is easy to optimize a scalar concave function over P\nfor each\u03bcs. For example, for a pairwise UGM we get\n\u03bcs(xs)\u221dexp(\u03b8s(xs))exp\u239b\n\u239d/summationdisplay\nt\u2208nbr(s)/summationdisplay\nxt\u03bct(xt)\u03b8st(xs,xt)\u239e\u23a0 (22.30)\n22.3.5 LBP as a variational optimization problem\nIn this section, we explain how LBP can be viewed as a variational inference problem.", "810": "780 Chapter22. Morevariationalinference\nFigure 22.8 (a) Illustration of pairwise UGM on binary nodes, together with a set of pseudo marginals\nthat are not globally consistent. (b) A slice of the marginal polytope illustrating the set of feasible edge\nmarginals, assuming the node marginals are clamped at \u03bc1=\u03bc2=\u03bc3=0.5. Source: Figure 4.1 of\n(Wainwright and Jordan 2008a). Used with kind permission of Martin Wainwright.\n22.3.5.1 An outer approximation to the marginal polytope\nIf we want to consider all possible probability distributions which are Markov wrt our model, we\nneed to consider all vectors \u03bc\u2208M(G). Since the set M(G)is exponentially large, it is usually\ninfeasible to optimize over. A standard strategy in combinatorial optimization is to relax the\nconstraints. In this case, instead of requiring probability vector \u03bcto live in M(G), we consider\na vector\u03c4that only satis\ufb01es the following local consistency constraints:\n/summationdisplay\nxs\u03c4s(xs)=1 (22.31)\n/summationdisplay\nxt\u03c4st(xs,xt)=\u03c4s(xs) (22.32)\nThe \ufb01rst constraint is called the normalization constraint, and the second is called the marginal-\nization constraint. We then de\ufb01ne the set\nL(G)/defines{\u03c4\u22650:( 2 2.31)holds\u2200s\u2208Vand(22.32)holds\u2200(s,t)\u2208E} (22.33)\nThe set L(G)is also a polytope, but it only has O(|V|+|E|)constraints. It is a convex outer\napproximation onM(G), as shown in Figure 22.7(c).\nWe call the terms \u03c4s,\u03c4st\u2208L(G)pseudo marginals , since they may not correspond to\nmarginals of any valid probability distribution. As an example of this, consider Figure 22.8(a).\nThe picture shows a set of pseudo node and edge marginals, which satisfy the local consistency\nrequirements. However, they are not globally consistent. To see why, note that \u03c412implies\np(X1=X2)=0.8,\u03c423impliesp(X2=X3)=0.8, but\u03c413impliesp(X1=X3)=0.2, which\nis not possible (see (Wainwright and Jordan 2008b, p81) for a formal proof). Indeed, Figure 22.8(b)\nshows that L(G)contains points that are not in M(G).\nWe claim that M(G)\u2286L(G), with equality iff Gis a tree. To see this, \ufb01rst consider", "811": "22.3. Loopybeliefpropagation: theoreticalissues* 781\nan element \u03bc\u2208M(G). Any such vector must satisfy the normalization and marginalization\nconstraints, hence M(G)\u2286L(G).\nNow consider the converse. Suppose Tis a tree, and let \u03bc\u2208L(T). By de\ufb01nition, this satis\ufb01es\nthe normalization and marginalization constraints. However, any tree can be represented in the\nform\np\u03bc(x)=/productdisplay\ns\u2208V\u03bcs(xs)/productdisplay\n(s,t)\u2208E\u03bcst(xs,xt)\n\u03bcs(xs)\u03bct(xt)(22.34)\nHence satsifying normalization and local consistency is enough to de\ufb01ne a valid distribution forany tree. Hence \u03bc\u2208M(T)as well.\nIn contrast, if the graph has loops, we have that M(G)/negationslash=L(G). See Figure 22.8(b) for an\nexample of this fact.\n22.3.5.2 The entropy approximation\nFrom Equation 22.34, we can write the exact entropy of any tree structured distribution \u03bc\u2208\nM(T)as follows:\nH(\u03bc)=/summationdisplay\ns\u2208VHs(\u03bcs)\u2212/summationdisplay\n(s,t)\u2208EIst(\u03bcst) (22.35)\nHs(\u03bcs)=\u2212/summationdisplay\nxs\u2208Xs\u03bcs(xs)log\u03bcs(xs) (22.36)\nIst(\u03bcst)=/summationdisplay\n(xs,xt)\u2208Xs\u00d7Xt\u03bcst(xs,xt)log\u03bcst(xs,xt)\n\u03bcs(xs)\u03bct(xt)(22.37)\nNote that we can rewrite the mutual information term in the form Ist(\u03bcst)=Hs(\u03bcs)+Ht(\u03bct)\u2212\nHst(\u03bcst), and hence we get the following alternative but equivalent expression:\nH(\u03bc)=\u2212/summationdisplay\ns\u2208V(ds\u22121)Hs(\u03bcs)+/summationdisplay\n(s,t)\u2208EHst(\u03bcst) (22.38)\nwheredsis the degree (number of neighbors) for node s.\nTheBethe1approximation to the entropy is simply the use of Equation 22.35 even when we\ndon\u2019t have a tree:\nHBethe(\u03c4)=/summationdisplay\ns\u2208VHs(\u03c4s)\u2212/summationdisplay\n(s,t)\u2208EIst(\u03c4st) (22.39)\nWe de\ufb01ne the Bethe free energy as\nFBethe(\u03c4)/defines\u2212/bracketleftBig\n\u03b8T\u03c4+HBethe(\u03c4)/bracketrightBig\n(22.40)\nWe de\ufb01ne the Bethe energy functional as the negative of the Bethe free energy.\n1. Hans Bethe was a German-American physicist, 1906\u20132005.", "812": "782 Chapter22. Morevariationalinference\n22.3.5.3 The LBP objective\nCombining the outer approximation L(G)with the Bethe approximation to the entropy, we get\nthe following Bethe variational problem (BVP):\nmin\n\u03c4\u2208L(G)FBethe(\u03c4)= m a x\n\u03c4\u2208L(G)\u03b8T\u03c4+HBethe(\u03c4) (22.41)\nThe space we are optimizing over is a convex set, but the objective itself is not concave (since\nHBetheis not concave). Thus there can be multiple local optima of the BVP.\nThe value obtained by the BVP is an approximation to logZ(\u03b8). In the case of trees, the\napproximation is exact, and in the case of models with attractive potentials, the approximationturns out to be an upper bound (Sudderth et al. 2008).\n22.3.5.4 Message passing and Lagrange multipliers\nIn this subsection, we will show that any \ufb01xed point of the LBP algorithm de\ufb01nes a stationarypoint of the above constrained objective. Let us de\ufb01ne the normalization constraint at C\nss(\u03c4)/defines\n1\u2212/summationtext\nxs\u03c4s(xs), and the marginalization constraint as Cts(xs;\u03c4)/defines\u03c4s(xs)\u2212/summationtext\nxt\u03c4st(xs,xt)\nfor each edge t\u2192s. We can now write the Lagrangian as\nL(\u03c4,\u03bb;\u03b8)/defines\u03b8T\u03c4+HBethe(\u03c4)+/summationdisplay\ns\u03bbssCss(\u03c4)\n+/summationdisplay\ns,t/bracketleftBigg/summationdisplay\nxs\u03bbts(xs)Cts(xs;\u03c4)+/summationdisplay\nxt\u03bbst(xt)Cst(xt;\u03c4)/bracketrightBigg\n(22.42)\n(The constraint that \u03c4\u22650is not explicitly enforced, but one can show that it will hold at the\noptimum since \u03b8>0.) Some simple algebra then shows that \u2207\u03c4L=0yields\nlog\u03c4s(xs)=\u03bb ss+\u03b8s(xs)+/summationdisplay\nt\u2208nbr(s)\u03bbts(xs) (22.43)\nlog\u03c4st(xs,xt)\n\u02dc\u03c4s(xs)\u02dc\u03c4t(xt)=\u03b8st(xs,xt)\u2212\u03bbts(xs)\u2212\u03bbst(xt) (22.44)\nwhere we have de\ufb01ned \u02dc\u03c4s(xs)/defines/summationtext\nxt\u03c4(xs,xt). Using the fact that the marginalization con-\nstraint implies \u02dc\u03c4s(xs)=\u03c4s(xs),w eg e t\nlog\u03c4st(xs,xt)=\u03bb ss+\u03bbtt+\u03b8st(xs,xt)+\u03b8s(xs)+\u03b8t(xt)\n+/summationdisplay\nu\u2208nbr(s)\\t\u03bbus(xs)+/summationdisplay\nu\u2208nbr(t)\\s\u03bbut(xt) (22.45)\nTo make the connection to message passing, de\ufb01ne Mts(xs)=e x p ( \u03bbts(xs)). With this\nnotation, we can rewrite the above equations (after taking exponents of both sides) as follows:\n\u03c4s(xs)\u221dexp(\u03b8s(xs))/productdisplay\nt\u2208nbr(s)Mts(xs) (22.46)\n\u03c4st(xs,xt)\u221dexp(\u03b8st(xs,xt)+\u03b8s(xs)+\u03b8t(xt))\n\u00d7/productdisplay\nu\u2208nbr(s)\\tMus(xs)/productdisplay\nu\u2208nbr(t)\\sMut(xt) (22.47)", "813": "22.4. Extensionsofbeliefpropagation* 783\nwhere the \u03bbterms are absorbed into the constant of proportionality. We see that this is\nequivalent to the usual expression for the node and edge marginals in LBP.\nTo derive an equation for the messages in terms of other messages (rather than in terms of\n\u03bbts), we enforce the marginalization condition/summationtext\nxt\u03c4st(xs,xt)=\u03c4s(xs). Then one can show\nthat\nMts(xs)\u221d/summationdisplay\nxt\u23a1\n\u23a3exp{\u03b8st(xs,xt)+\u03b8t(xt)}/productdisplay\nu\u2208nbr(t)\\sMut(xt)\u23a4\u23a6 (22.48)\nWe see that this is equivalent to the usual expression for the messages in LBP.\n22.3.6 Loopy BP vs mean \ufb01eld\nIt is interesting to compare the naive mean \ufb01eld (MF) and LBP approximations. There are several\nobvious differences. First, LBP is exact for trees whereas MF is not, suggesting LBP will in generalbe more accurate (see (Wainwright et al. 2003) for an analysis). Second, LBP optimizes over nodeand edge marginals, whereas MF only optimizes over node marginals, again suggesting LBP willbe more accurate. Third, in the case that the true edge marginals factorize, so \u03bc\nst=\u03bcs\u03bct, the\nfree energy approximations will be the same in both cases.\nWhat is less obvious, but which nevertheless seems to be true, is that the MF objective has\nmany more local optima than the LBP objective, so optimizing the MF objective seems to beharder. In particular, (Weiss 2001), shows empirically that optimizing MF starting from uniformor random initial conditions often leads to poor results, whereas optimizing BP from uniforminitial messages often leads to good results. Furthermore, initializing MF with the BP marginalsalso leads to good results (although MF tends to be more overcon\ufb01dent than BP), indicating thatthe problem is caused not by the inaccuracy of the MF approximation, but rather by the severenon-convexity of the MF objective, and by the weakness of the standard coordinate descentoptimization method used by MF.\n2However, the advantage of MF is that it gives a lower bound\non the partition function, unlike BP, which is useful when using it as a subroutine inside alearning algorithm. Also, MF is easier to extend to other distributions besides discrete andGaussian, as we saw in Chapter 21. Intuitively, this is because MF only works with marginaldistributions, which have a single type, rather than needing to de\ufb01ne pairwise distributions,which may need to have two different types.\n22.4 Extensions of belief propagation *\nIn this section, we discuss various extensions of LBP.\n22.4.1 Generalized belief propagation\nWe can improve the accuracy of loopy BP by clustering together nodes that form a tight loop.This is known as the cluster variational method. The result is a hyper-graph, which is a graph\n2. (Honkela et al. 2003) discusses the use of the pattern search algorithm to speedup mean \ufb01eld inference in the case\nof continuous random variables. It is possible that similar ideas could be adapted to the discrete case, although there\nmay be no reason to do this, given that LBP already works well in the discrete case.", "814": "784 Chapter22. Morevariationalinference\n1 2\n8 743\n95 61245\n5 22356\n5\n547 858\n568945 56\nFigure 22.9 (a) Kikuchi clusters superimposed on a 3\u00d73lattice graph. (b) Corresponding hyper-graph.\nSource: Figure 4.5 of (Wainwright and Jordan 2008b). Used with kind permission of Martin Wainwright.\nwhere there are hyper-edges between sets of vertices instead of between single vertices. Note\nthat a junction tree (Section 20.4.1) is a kind of hyper-graph. We can represent hyper-graph usinga poset (partially ordered set) diagram, where each node represents a hyper-edge, and there isan arrow e\n1\u2192e2ife2\u2282e1. See Figure 22.9 for an example.\nLettbe the size of the largest hyper-edge in the hyper-graph. If we allow tto be as large as\nthe treewidth of the graph, then we can represent the hyper-graph as a tree, and the methodwill be exact, just as LBP is exact on regular trees (with treewidth 1). In this way, we can de\ufb01nea continuum of approximations, from LBP all the way to exact inference.\nDe\ufb01ne L\nt(G)to be the set of all pseudo-marginals such that normalization and marginaliza-\ntion constraints hold on a hyper-graph whose largest hyper-edge is of size t+1. For example,\nin Figure 22.9, we impose constraints of the form\n/summationdisplay\nx1,x2\u03c41245(x1,x2,x4,x5)=\u03c445(x4,x5),/summationdisplay\nx6\u03c456(x5,x6)=\u03c45(x5),... (22.49)\nFurthermore, we approximate the entropy as follows:\nHKikuchi(\u03c4)/defines/summationdisplay\ng\u2208Ec(g)Hg(\u03c4g) (22.50)\nwhereHg(\u03c4g)is the entropy of the joint (pseudo) distribution on the vertices in set g, andc(g)\nis called the overcounting number of setg. These are related to Mobious numbers in set\ntheory. Rather than giving a precise de\ufb01nition, we just give a simple example. For the graph inFigure 22.9, we have\nH\nKikuchi(\u03c4)=[H1245+H2356+H4578+H5689]\n\u2212[H25+H45+H56+H58]+H5 (22.51)\nPutting these two approximations together, we can de\ufb01ne the Kikuchi free energy3as follows:\nFKikuchi(\u03c4)/defines\u2212/bracketleftBig\n\u03b8T\u03c4+HKikuchi(\u03c4)/bracketrightBig\n(22.52)\n3. Ryoichi Kikuchi is a Japanese physicist.", "815": "22.4. Extensionsofbeliefpropagation* 785\nOur variational problem becomes\nmin\n\u03c4\u2208Lt(G)FKikuchi(\u03c4)= m a x\n\u03c4\u2208Lt(G)\u03b8T\u03c4+HKikuchi(\u03c4) (22.53)\nJust as with the Bethe free energy, this is not a concave objective. There are several possible\nalgorithms for \ufb01nding a local optimum of this objective, including a message passing algorithm\nknown as generalized belief propagation. However, the details are beyond the scope of this\nchapter. See e.g., (Wainwright and Jordan 2008b, Sec 4.2) or (Koller and Friedman 2009, Sec11.3.2) for more information. Suffice it to say that the method gives more accurate results thanLBP, but at increased computational cost (because of the need to handle clusters of nodes). Thiscost, plus the complexity of the approach, have precluded it from widespread use.\n22.4.2 Convex belief propagation\nThe mean \ufb01eld energy functional is concave, but it is maximized over a non-convex innerapproximation to the marginal polytope. The Bethe and Kikuchi energy functionals are notconcave, but they are maximized over a convex outer approximation to the marginal polytope.Consequently, for both MF and LBP, the optimization problem has multiple optima, so themethods are sensitive to the initial conditions. Given that the exact formulation (Equation 22.24)a concave objective maximized over a convex set, it is natural to try to come up with anappproximation which involves a concave objective being maximized over a convex set.\nWe now describe one method, known as convex belief propagation. This involves working\nwith a set of tractable submodels, F, such as trees or planar graphs. For each model F\u2282G,\nthe entropy is higher, H(\u03bc(F))\u2265H(\u03bc(G)), since Fhas fewer constraints. Consequently, any\nconvex combination of such subgraphs will have higher entropy, too:\nH(\u03bc(G))\u2264/summationdisplay\nF\u2208F\u03c1(F)H(\u03bc(F))/definesH(\u03bc,\u03c1) (22.54)\nwhere\u03c1(F)\u22650and/summationtext\nF\u03c1(F)=1. Furthermore, H(\u03bc,\u03c1)is a concave function of \u03bc.W en o w\nde\ufb01ne the convex free energy as\nFConvex(\u03bc,\u03c1)/defines\u2212/bracketleftbig\n\u03bcT\u03b8+H(\u03bc,\u03c1)/bracketrightbig\n(22.55)\nWe de\ufb01ne the concave energy functional as the negative of the convex free energy. We discusshow to optimize \u03c1below.\nHaving de\ufb01ned an upper bound on the entropy, we now consider a convex outerbound on\nthe marginal polytope of mean parameters. We want to ensure we can evaluate the entropy ofany vector \u03c4in this set, so we restrict it so that the projection of \u03c4onto the subgraph Glives\nin the projection of MontoF:\nL(G;F)/defines{\u03c4\u2208R\nd:\u03c4(F)\u2208M(F)\u2200F\u2208F} (22.56)\nThis is a convex set since each M(F)is a projection of a convex set. Hence we de\ufb01ne our\nproblem as\nmin\n\u03c4\u2208L(G;F)FConvex(\u03c4,\u03c1)= m a x\n\u03c4\u2208L(G;F)\u03c4T\u03b8+H(\u03c4,\u03c1) (22.57)\nThis is a concave objective being maximized over a convex set, and hence has a unique maxi-mum. We give a speci\ufb01c example below.", "816": "786 Chapter22. Morevariationalinference\nb\nef\nb\nef\nb\nef\nb\nef\nFigure 22.10 (a) A graph. (b-d) Some of its spanning trees. Source: Figure 7.1 of (Wainwright and Jordan\n2008b). Used with kind permission of Martin Wainwright.\n22.4.2.1 Tree-reweighted belief propagation\nConsider the speci\ufb01c case where Fis all spanning trees of a graph. For any given tree, the\nentropy is given by Equation 22.35. To compute the upper bound, obtained by averaging over\nall trees, note that the terms/summationtext\nF\u03c1(F)H(\u03bc(F)s)for single nodes will just be Hs, since node s\nappears in every tree, and/summationtext\nF\u03c1(F)=1. But the mutual information term Istreceives weight\n\u03c1st=E\u03c1[I((s,t)\u2208E(T))], known as the edge appearance probability. Hence we have the\nfollowing upper bound on the entropy:\nH(\u03bc)\u2264/summationdisplay\ns\u2208VHs(\u03bcs)\u2212/summationdisplay\n(s,t)\u2208E\u03c1stIst(\u03bcst) (22.58)\nThe edge appearance probabilities live in a space called the spanning tree polytope . This\nis because they are constrained to arise from a distribution over trees. Figure 22.10 gives anexample of a graph and three of its spanning trees. Suppose each tree has equal weight under\u03c1. The edge foccurs in 1 of the 3 trees, so \u03c1\nf=1/3. The edge eoccurs in 2 of the 3 trees,\nso\u03c1e=2/3. The edge bappears in all of the trees, so \u03c1b=1. And so on. Ideally we can\n\ufb01nd a distribution \u03c1, or equivalently edge probabilities in the spanning tree polytope, that make\nthe above bound as tight as possible. An algorithm to do this is described in (Wainwright et al.2005). (A simpler approach is to generate spanning trees of Gat random until all edges are\ncovered, or use all single edges with weight \u03c1\ne=1/E.)\nWhat about the set we are optimizing over? We require \u03bc(T)\u2208M(T)for each tree T, which\nmeans enforcing normalization and local consistency. Since we have to do this for every tree,we are enforcing normalization and local consistency on every edge. Hence L(G;F)=L(G).\nSo our \ufb01nal optimization problem is as follows:\nmax\n\u03c4\u2208L(G)\u23a7\n\u23a8\n\u23a9\u03c4T\u03b8+/summationdisplay\ns\u2208VHs(\u03c4s)\u2212/summationdisplay\n(s,t)\u2208E(G)\u03c1stIst(\u03c4st)\u23ab\n\u23ac\n\u23ad(22.59)\nwhich is the same as the LBP objective except for the crucial \u03c1stweights. So long as \u03c1st>0\nfor all edges (s,t), this problem is strictly concave with a unique maximum.\nHow can we \ufb01nd this global optimum? As for LBP, there are several algorithms, but perhaps the\nsimplest is a modi\ufb01cation of belief propagation known as tree reweighted belief propagation ,", "817": "22.5. Expectationpropagation 787\nalso called TRWorTRBPfor short. The message from ttosis now a function of all messages\nsent from other neighbors vtot, as before, but now it is also a function of the message sent\nfromstot. Speci\ufb01cally\nMts(xs)\u221d/summationdisplay\nxtexp/parenleftbigg1\n\u03c1st\u03b8st(xs,xt)+\u03b8t(xt)/parenrightbigg/producttext\nv\u2208nbr(t)\\s[Mvt(xt)]\u03c1vt\n[Mst(xt)]1\u2212\u03c1ts(22.60)\nAt convergence, the node and edge pseudo marginals are given by\n\u03c4s(xs)\u221dexp(\u03b8s(xs))/productdisplay\nv\u2208nbr(s)[Mvs(xs)]\u03c1vs(22.61)\n\u03c4st(xs,xt)\u221d\u03d5st(xs,xt)/producttext\nv\u2208nbr(s)\\t[Mvs(xs)]\u03c1vs\n[Mts(xs)]1\u2212\u03c1st/producttext\nv\u2208nbr(t)\\s[Mvt(xt)]\u03c1vt\n[Mst(xt)]1\u2212\u03c1ts(22.62)\n\u03d5st(xs,xt)/definesexp/parenleftbigg1\n\u03c1st\u03b8st(xs,xt)+\u03b8s(xs)+\u03b8t(xt)/parenrightbigg\n(22.63)\nThis algorithm can be derived using a method similar to that described in Section 22.3.5.4.\nIf\u03c1st=1for all edges (s,t)\u2208E, the algorithm reduces to the standard LBP algorithm.\nHowever, the condition \u03c1st=1implies every edge is present in every spanning tree with\nprobability 1, which is only possible if the original graph is a tree. Hence the method is only\nequivalent to standard LBP on trees, when the method is of course exact.\nIn general, this message passing scheme is not guaranteed to converge to the unique global\noptimum. One can devise double-loop methods that are guaranteed to converge (Hazan andShashua 2008), but in practice, using damped updates as in Equation 22.7 is often sufficient toensure convergence.\nIt is also possible to produce a convex version of the Kikuchi free energy, which one can\noptimize with a modi\ufb01ed version of generalized belief propagation. See (Wainwright and Jordan2008b, Sec 7.2.2) for details.\nFrom Equation 22.59, and using the fact that the TRBP entropy approximation is an upper\nbound on the true entropy, wee see that the TRBP objective is an upper bound on logZ. Using\nthe fact that I\nst=Hs+Ht\u2212Hst, we can rewrite the upper bound as follows:\nlog\u02c6Z(\u03b8)/defines\u03c4T\u03b8+/summationdisplay\nst\u03c1stHst(\u03c4st)+/summationdisplay\nscsHs(\u03c4s)\u2264logZ(\u03b8) (22.64)\nwherecs/defines1\u2212/summationtext\nt\u03c1st.\n22.5 Expectation propagation\nExpectation propagation (EP) (Minka 2001c) is a form of belief propagation where the mes-\nsages are approximated. It is a generalization of the assumed density \ufb01ltering (ADF) algorithm,discussed in Section 18.5.3. In that method, we approximated the posterior at each step usingan assumed functional form, such as a Gaussian. This posterior can be computed using mo-ment matching, which locally optimizes KL(p||q)for a single term. From this, we derived the\nmessage to send to the next time step.", "818": "788 Chapter22. Morevariationalinference\nADF works well for sequential Bayesian updating, but the answer it gives depends on the\norder in which the data is seen. EP essentially corrects this \ufb02aw by making multiple passes over\nthe data (thus EP is an offline or batch inference algorithm).\n22.5.1 EP as a variational inference problem\nWe now explain how to view EP in terms of variational inference. We follow the presentation of(Wainwright and Jordan 2008b, Sec 4.3), which should be consulted for further details.\nSuppose the joint distribution can be written in exponential family form as follows:\np(x|\u03b8,\u02dc\u03b8)\u221df\n0(x)exp(\u03b8T\u03c6(x))dI/productdisplay\ni=1exp(\u02dc\u03b8T\ni\u03a6i(x)) (22.65)\nwhere we have partitioned the parameters and the sufficient statistics into a tractable term \u03b8of\nsizedTanddIintractable terms \u02dc\u03b8i, each of size b.\nFor example, consider the problem of inferring an unknown vector x, when the observation\nmodel is a mixture of two Gaussians, one centered at xand one centered at 0. (This can be\nused to represent outliers, for example.) Minka (who invented EP) calls this the clutter problem.\nMore formally, we assume an observation model of the form\np(y|x)=( 1\u2212w)N(y|x,I)+wN(y|0,aI) (22.66)\nwhere0<w<1 is the known mixing weight (fraction of outliers), and a>0is the variance\nof the background distribution. Assuming a \ufb01xed prior of the form p(x)=N(x|0,\u03a3), we can\nwrite our model in the required form as follows:\np(x|y1:N)\u221dN(x|0,\u03a3)N/productdisplay\ni=1p(yi|x) (22.67)\n=e x p/parenleftbigg\n\u22121\n2xT\u03a3\u22121x/parenrightbigg\nexp/parenleftBiggN/summationdisplay\ni=1logp(yi|x)/parenrightBigg\n(22.68)\nThis matches our canonical form where f0(x)exp(\u03b8T\u03c6(x))corresponds to exp/parenleftbig\n\u22121\n2xT\u03a3\u22121x/parenrightbig\n,\nusing\u03c6(x)=(x,xxT), and we set \u03a6i(x)=l o gp(yi|x),\u02dc\u03b8i=1, anddI=N.\nThe exact inference problem corresponds to\nmax\n(\u03c4,\u02dc\u03c4)\u2208M(\u03c6,\u03a6)\u03c4T\u03b8+\u02dc\u03c4T\u02dc\u03b8+H((\u03c4,\u02dc\u03c4)) (22.69)\nwhereM(\u03c6,\u03a6)is the set of mean parameters realizable by any probability distribution as seen\nthrough the eyes of the sufficient statistics:\nM(\u03c6,\u03a6)={(\u03bc,\u02dc\u03bc)\u2208RdT\u00d7RdIb:(\u03bc,\u02dc\u03bc)=E[(\u03c6(X),\u03a61(X),...,\u03a6dI(X))]}(22.70)\nAs it stands, it is intractable to perform inference in this distribution. For example, in our\nclutter example, the posterior contains 2Nmodes. But suppose we incorporate just one of the\nintractable terms, say the i\u2019th one; we will call this the \u03a6i-augmented distribution:\np(x|\u03b8,\u02dc\u03b8i)\u221df0(x)exp(\u03b8T\u03c6(x))exp(\u02dc\u03b8Ti\u03a6i(x)) (22.71)", "819": "22.5. Expectationpropagation 789\nIn our clutter example, this becomes\np(x|\u03b8,\u02dc\u03b8i)=e x p/parenleftbigg\n\u22121\n2xT\u03a3\u22121x/parenrightbigg\n[wN(yi|0,aI)+(1\u2212 w)N(yi|x,I)] (22.72)\nThisistractable to compute, since it is just a mixture of 2 Gaussians.\nThe key idea behind EP is to work with these the \u03a6i-augmented distributions in an iterative\nfashion. First, we approximate the convex set M(\u03c6,\u03a6)with another, larger convex set:\nL(\u03c6,\u03a6)/defines{(\u03c4,\u02dc\u03c4):\u03c4\u2208M(\u03c6),(\u03c4,\u02dc\u03c4i)\u2208M(\u03c6,\u03a6i)} (22.73)\nwhereM(\u03c6)={\u03bc\u2208RdT:\u03bc=E[\u03c6(X)]}andM(\u03c6,\u03a6i)={(\u03bc,\u02dc\u03bci)\u2208RdT\u00d7Rb:\n(\u03bc,\u02dc\u03bci)=E[(\u03c6(X),\u03a6i(X))]. Next we approximate the entropy by the following term-by-term\napproximation:\nHep(\u03c4,\u02dc\u03c4)/definesH(\u03c4)+dI/summationdisplay\ni=1[H(\u03c4,\u02dc\u03c4i)\u2212H(\u03c4)] (22.74)\nThen the EP problem becomes\nmax\n(\u03c4,\u02dc\u03c4)\u2208L(\u03c6,\u03a6)\u03c4T\u03b8+\u02dc\u03c4T\u02dc\u03b8+Hep(\u03c4,\u02dc\u03c4) (22.75)\n22.5.2 Optimizing the EP objective using moment matching\nWe now discuss how to maximize the EP objective in Equation 22.75. Let us duplicate \u03c4dI\ntimes to yield \u03b7i=\u03c4. The augmented set of parameters we need to optimize is now\n(\u03c4,(\u03b7i,\u02dc\u03c4i)dI\ni=1)\u2208RdT\u00d7(RdT\u00d7Rb)dI(22.76)\nsubject to the constraints that \u03b7i=\u03c4and(\u03b7i,\u02dc\u03c4i)\u2208M(\u03c6;\u03a6i). Let us associate a vector of\nLagrange multipliers \u03bbi\u2208RdTwith the \ufb01rst set of constraints. Then the partial Lagrangian\nbecomes\nL(\u03c4;\u03bb)=\u03c4T\u03b8+H(\u03c4)+di/summationdisplay\ni=1/bracketleftBig\n\u02dc\u03c4T\ni\u02dc\u03b8i+H((\u03b7i,\u02dc\u03c4i))\u2212H(\u03b7i)+\u03bbT\ni(\u03c4\u2212\u03b7i)/bracketrightBig\n(22.77)\nBy solving\u2207\u03c4L(\u03c4;\u03bb)=0, we can show that the corresponding distribution in M(\u03c6)has\nthe form\nq(x|\u03b8,\u03bb)\u221df0(x)exp{(\u03b8+dI/summationdisplay\ni=1\u03bbi)T\u03c6(x)} (22.78)\nThe\u03bbTi\u03c6(x)terms represents an approximation to the i\u2019th intractable term using the sufficient\nstatisticsfromthebasedistribution, aswewillseebelow. Similarly, bysolving \u2207(\u03b7i,\u02dc\u03c4i)L(\u03c4;\u03bb)=\n0, we \ufb01nd that the corresponding distribution in M(\u03c6,\u03a6i)has the form\nqi(x|\u03b8,\u02dc\u03b8i,\u03bb)\u221df0(x)exp{(\u03b8+/summationdisplay\nj/negationslash=i\u03bbj)T\u03c6(x)+\u02dc\u03b8T\ni\u03a6i(x)} (22.79)", "820": "790 Chapter22. Morevariationalinference\nThis corresponds to removing the approximation to the i\u2019th term, \u03bbi, from the base distribution,\nand adding in the correct i\u2019th term, \u03a6i. Finally,\u2207\u03bbL(\u03c4;\u03bb)=0just enforces the constraints\nthat\u03c4=Eq[\u03c6(X)]and\u03b7i=Eqi[\u03c6(X)]are equal. In other words, we get the following\nmoment matching constraints:\n/integraldisplay\nq(x|\u03b8,\u03bb)\u03c6(x)dx=/integraldisplay\nqi(x|\u03b8,\u02dc\u03b8i,\u03bb)\u03c6(x)dx (22.80)\nThus the overall algorithm is as follows. First we initialize the \u03bbi. Then we iterate the following\nto convergence: pick a term i; compute qi(corresponding to removing the old approximation\nto\u03a6iand adding in the new one); then update the \u03bbiterm inqby solving the moment\nmatching equation Eqi[\u03c6(X)] = Eq[\u03c6(X)]. (Note that this particular optimization scheme is\nnot guaranteed to converge to a \ufb01xed point.)\nAn equivalent way of stating the algorithm is as follows. Let us assume the true distribution\nis given by\np(x|D)=1\nZ/productdisplay\nifi(x) (22.81)\nWe approximate each fiby\u02dcfiand set\nq(x)=1\nZ/productdisplay\ni\u02dcfi(x) (22.82)\nNow we repeat the following until convergence:\n1. Choose a factor \u02dcfito re\ufb01ne.\n2. Remove \u02dcfifrom the posterior by dividing it out:\nq\u2212i(x)=q(x)\n\u02dcfi(x)(22.83)\nThis can be implemented by substracting off the natural parameters of \u02dcfifromq.\n3. Compute the new posterior qnew(x)by solving\nmin\nqnew(x)KL/parenleftbigg1\nZifi(x)q\u2212i(x)||qnew(x)/parenrightbigg\n(22.84)\nThis can be done by equating the moments of qnew(x)with those of qi(x)\u221dq\u2212i(x)fi(x).\nThe corresponding normalization constant has the form\nZi=/integraldisplay\nq\u2212i(x)fi(x)dx (22.85)\n4. Compute the new factor (message) that was implicitly used (so it can be later removed):\n\u02dcfi(x)=Ziqnew(x)\nq\u2212i(x)(22.86)", "821": "22.5. Expectationpropagation 791\nAfter convergence, we can approximate the marginal likelihood using\np(D)\u2248/integraldisplay/productdisplay\ni\u02dcfi(x)dx (22.87)\nWe will give some examples of this below which will make things clearer.\n22.5.3 EP for the clutter problem\nLet us return to considering the clutter problem. Our presentation is based on (Bishop 2006b).4\nFor simplicity, we will assume that the prior is a spherical Gaussian, p(x)=N(0,bI). Also, we\nchoose to approximate the posterior by a spherical Gaussian, q(x)=N(m,vI).W es e tf0(x)\nto be the prior; this can be held \ufb01xed. The factor approximations will be \u201cGaussian like\u201d terms\nof the form\n\u02dcfi(x)=siN(x|mi,viI) (22.88)\nNote, however, that in the EP updates, the variances may be negative! Thus these terms shouldbe interpreted as functions, but not necessarily probability distributions. (If the variance isnegative, it means the that \u02dcf\nicurves upwards instead of downwards.)\nFirst we remove \u02dcfi(x)fromq(x)by division, which yields q\u2212i(x)=N(m\u2212i,v\u2212iI),w h e r e\nv\u22121\n\u2212i=v\u22121\u2212v\u22121\ni (22.89)\nm\u2212i=m+v\u2212iv\u22121\ni(m\u2212mi) (22.90)\nThe normalization constant is given by\nZi=( 1\u2212w)N(yi|m\u2212i,(v\u2212i+1)I)+wN(yi|0,aI) (22.91)\nNext we compute qnew(x)by computing the mean and variance of q\u2212i(x)fi(x)as follows:\nm=m\u2212i+\u03c1iv\u2212i\nv\u2212i+1(yi\u2212m\u2212i) (22.92)\nv=v\u2212i\u2212\u03c1iv2\n\u2212i\nv\u2212i+1+\u03c1i(1\u2212\u03c1i)v2\n\u2212i||yi\u2212mi||2\nD(v\u2212i+1)2(22.93)\n\u03c1i=1\u2212w\nZiN(yi|0,aI) (22.94)\nwhereDis the dimensionality of xand\u03c1ican be interpreted as the probability that yiis not\nclutter.\nFinally, we compute the new factor \u02dcfiwhose parameters are given by\nv\u22121\ni=v\u22121\u2212v\u22121\n\u2212i (22.95)\nmi=m\u2212i+(vi+v\u2212i)v\u22121\n\u2212i(m\u2212m\u2212i) (22.96)\nsi=Zi\n(2\u03c0vi)D/2N(mi|m\u2212i,(vi+v\u2212i)I)(22.97)\n4. For a handy \u201ccrib sheet\u201d, containing many of the standard equations needed for deriving Gaussian EP algorithms, see\nhttp://research .microsoft .com/en-us/um/people/minka/papers/ep/minka-ep-quickref .pdf.", "822": "792 Chapter22. Morevariationalinference\nAt convergence, we can approximate the marginal likelihood as follows:\np(D)\u2248(2\u03c0v)D/2exp(c/2)N/productdisplay\ni=1si(2\u03c0vi)\u2212D/2(22.98)\nc/definesmTm\nv\u2212N/summationdisplay\ni=1mT\nimi\nvi(22.99)\nIn (Minka 2001d), it is shown that, at least on this example, EP gives better accuracy per unit\nof CPU time than VB and MCMC.\n22.5.4 LBP is a special case of EP\nWe now show that loopy belief propagation is a special case of EP, where the base distribution\ncontains the node marginals and the \u201cintractable\u201d terms correspond to the edge potentials. Weassume the model has the pairwise form shown in Equation 22.12. If there are mnodes, the\nbase distribution takes the form\np(x|\u03b8\n1,...,\u03b8m,0)\u221d/productdisplay\ns\u2208Vexp(\u03b8s(xs)) (22.100)\nThe entropy of this distribution is simply\nH(\u03c41:m)=/summationdisplay\nsH(\u03c4s) (22.101)\nIf we add in the u\u2212vedge, the \u03a6uvaugmented distribution has the form\np(x|\u03b81:m,\u03b8uv)\u221d/bracketleftBigg/productdisplay\ns\u2208Vexp(\u03b8s(xs))/bracketrightBigg\nexp(\u03b8uv(xu,xv)) (22.102)\nSince this graph is a tree, the exact entropy of this distribution is given by\nH(\u03c41:m,\u02dc\u03c4uv)=/summationdisplay\nsH(\u03c4s)\u2212I(\u02dc\u03c4uv) (22.103)\nwhereI(\u03c4uv)=H(\u03c4u)+H(\u03c4v)\u2212H(\u03c4uv)is the mutual information. Thus the EP approxi-\nmation to the entropy of the full distribution is given by\nHep(\u03c4,\u02dc\u03c4)=H (\u03c4)+/summationdisplay\n(u,v)\u2208E[H(\u03c41:m,\u02dc\u03c4uv)\u2212H(\u03c4)] (22.104)\n=/summationdisplay\nsH(\u03c4s)+/summationdisplay\n(u,v)\u2208E/bracketleftBigg/summationdisplay\nsH(\u03c4s)\u2212I(\u02dc\u03c4uv)\u2212/summationdisplay\nsH(\u03c4s)/bracketrightBigg\n(22.105)\n=/summationdisplay\nsH(\u03c4s)\u2212/summationdisplay\n(u,v)\u2208EI(\u02dc\u03c4uv) (22.106)\nwhich is precisely the Bethe approximation to the entropy.", "823": "22.5. Expectationpropagation 793\nWe now show that the convex set that EP is optimizing over, L(\u03c6,\u03a6)given by Equation 22.73,\nis the same as the one that LBP is optimizing over, L(G)given in Equation 22.33. First, let us\nconsider the set M(\u03c6). This consists of all marginal distributions (\u03c4s,s\u2208V), realizable by\na factored distribution. This is therefore equivalent to the set of all distributions which satisfy\nnon-negativity \u03c4s(xs)\u22650and the local normalization constraint/summationtext\nxs\u03c4(xs)=1. Now consider\nthe setM(\u03c6,\u03a6uv)for a single u\u2212vedge. This is equivalent to the marginal polytope M(Guv),\nwhereGuvis the graph with the single u\u2212vedge added. Since this graph corresponds to a\ntree, this set also satis\ufb01es the marginalization conditions\n/summationdisplay\nxv\u03c4uv(xu,xv)=\u03c4u(xu),/summationdisplay\nxu\u03c4uv(xu,xv)=\u03c4v(xv) (22.107)\nSinceL(\u03c6,\u03a6)is the union of such sets, as we sweep over all edges in the graph, we recover\nthe same set as L(G).\nWe have shown that the Bethe approximation is equivalent to the EP approximation. We now\nshow how the EP algorithm reduces to LBP. Associated with each intractable term i=(u,v)\nwill be a pair of Lagrange multipliers, (\u03bbuv(xv),\u03bbvu(xu)). Recalling that \u03b8T\u03c6(x)=[\u03b8s(xs)]s,\nthe base distribution in Equation 22.78 has the form\nq(x|\u03b8,\u03bb)\u221d/productdisplay\nsexp(\u03b8s(xs))/productdisplay\n(u,v)\u2208Eexp(\u03bbuv(xv)+\u03bbvu(xu)) (22.108)\n=/productdisplay\nsexp\u239b\n\u239d\u03b8s(xs)+/summationdisplay\nt\u2208N(s)\u03bbts(xs)\u239e\u23a0 (22.109)\nSimilarly, the augmented distribution in Equation 22.79 has the form\nq\nuv(x|\u03b8,\u03bb)\u221dq(x|\u03b8,\u03bb)exp(\u03b8uv(xu,xv)\u2212\u03bbuv(xv)\u2212\u03bbvu(xu)) (22.110)\nWe now need to update \u03c4u(xu)and\u03c4v(xv)to enforce the moment matching constraints:\n(Eq[xs],Eq[xt]) = (E quv[xs],Equv[xt]) (22.111)\nIt can be shown that this can be done by performing the usual sum-product message passing\nstep along the u\u2212vedge (in both directions), where the messages are given by Muv(xv)=\nexp(\u03bbuv(xv)), andMvu(xu)=e x p ( \u03bbvu(xu)). Once we have updated q, we can derive the\ncorresponding messages \u03bbuvand\u03bbvu.\nThe above analysis suggests a natural extension, where we make the base distribution be a\ntree structure instead of a fully factored distribution. We then add in one edge at a time, absorbits effect, and approximate the resulting distribution by a new tree. This is known as tree EP\n(Minka and Qi 2003), and is more accurate than LBP, and sometimes faster. By considering otherkinds of structured base distributions, we can derive algorothms that outperform generalizationbelief propagation (Welling et al. 2005).\n22.5.5 Ranking players using TrueSkill\nWe now present an interesting application of EP to the problem of ranking players who competein games. Microsoft uses this method \u2014 known as TrueSkill (Herbrich et al. 2007) \u2014 to rank", "824": "794 Chapter22. Morevariationalinference\n6\u0014\n6\u0015\n6\u0016\n6\u0017\n3\u0014 3\u0015 3\u0016 3\u0017\nW\u0016 W\u0015 W\u0014\nG\u0014 G\u0015\n\\\u0014 \\\u0015\n(a)\n)\u0014 )\u0015 )\u0016\nK\u0014 K\u0015\nN\u0014 N\u00156\u0014 6\u0015 6\u0016\nG\u0014 G\u0015\n\\\u0014 \\\u0015\u0014\n\u0019\n\u0015\n\u0016\n\u0017\u0018\u0015\u0019\u0014\n(b)\nFigure 22.11 (a) A DGM representing the TrueSkill model for 4 players and 3 teams, where team 1 is player\n1, team 2 is players 2 and 3, and team 3 is player 4. We assume there are two games, team 1 vs team 2,\nand team 2 vs team 3. Nodes with double circles are deterministic. (b) A factor graph representation of themodel where we assume there are 3 players (and no teams). There are 2 games, player 1 vs player 2, andplayer 2 vs player 3. The numbers inside circles represent steps in the message passing algorithm.", "825": "22.5. Expectationpropagation 795\nplayers who use the Xbox 360 Live online gaming system; this system process over 105games\nper day, making this one of the largest application of Bayesian statistics to date.5The same\nmethod can also be applied to other games, such as tennis or chess.6\nThe basic idea is shown in Figure 22.11(a). We assume each player ihas a latent or true\nunderlying skill level si\u2208R. These skill levels can evolve over time according to a simple\ndynamical model, p(st\ni|st\u22121\ni)=N(sti|st\u22121\ni,\u03b32). In any given game, we de\ufb01ne the performance\nof player ito bepi, which has the conditional distribution p(pi|si)=N(pi|si,\u03b22). We then\nde\ufb01ne the performance of a team to be the sum of the performance of its constituent players.\nFor example, in Figure 22.11(a), we assume team 2 is composed of players 2 and 3, so we de\ufb01net\n2=p2+p3. Finally, we assume that the outcome of a game depends on the difference in\nperformance levels of the two teams. For example, in Figure 22.11(a), we assume y1=s i g n (d1),\nwhered1=t1\u2212t2, and where y1=+ 1means team 1 won, and y1=\u22121means team 2 won.\nThus the prior probability that team 1 wins is\np(y1=+ 1|s)=/integraldisplay\np(d1>0|t1,t2)p(t1|s1)p(t2|s2)dt1dt2 (22.112)\nwheret1\u223cN(s1,\u03b22)andt2\u223cN(s2+s3,\u03b22).7\nTo simplify the presentation of the algorithm, we will ignore the dynamical model and assume\na common static factored Gaussian prior, N(\u03bc0,\u03c32\n0), on the skills. Also, we will assume that\neach team consists of 1 player, so ti=pi, and that there can be no ties. Finally, we will integrate\nout the performance variables pi, and assume \u03b22=1, leading to a \ufb01nal model of the form\np(s)=/productdisplay\niN(si|\u03bc0,\u03c32) (22.113)\np(dg|s)=N (dg|sig\u2212sjg,1) (22.114)\np(yg|dg)=I(yg=s i g n (dg)) (22.115)\nwhereigis the \ufb01rst player of game g, andjgis the second player. This is represented in\nfactor graph form in in Figure 22.11(b). We have 3 kinds of factors: the prior factor, fi(si)=\nN(si|\u03bc0,\u03c32\n0), the game factor, hg(sig,sjg,dg)=N(dg|sig\u2212sjg,1), and the outcome factor,\nkg(dg,yg)=I(yg=s i g n (dg)).\nSince the likelihood term (yg|dg)is not conjugate to the Gaussian priors, we will have to\nperform approximate inference. Thus even when the graph is a tree, we will need to iterate.(If there were an additional game, say between player 1 and player 3, then the graph would nolonger be a tree.) We will represent all messages and marginal beliefs by 1d Gaussians. We willuse the notation \u03bcandvfor the mean and variance (the moment parameters), and \u03bb=1/v\nand\u03b7=\u03bb\u03bcfor the precision and precision-adjusted mean (the natural parameters).\n5. Naive Bayes classi\ufb01ers, which are widely used in spam \ufb01lters, are often described as the most common application\nof Bayesian methods. However, the parameters of such models are usually \ufb01t using non-Bayesian methods, such as\npenalized maximum likelihood.\n6. Our presentation of this algorithm is based in part on lecture notes by Carl Rasmussen Joaquin Quinonero-Candela,\navailable at http://mlg .eng.cam .ac.uk/teaching/4f13/1112/lect13 .pdf.\n7. Note that this is very similar to probit regression, discussed in Section 9.4, except the inputs are (the differences of)latent 1 dimensional factors. If we assume a logistic noise model instead of a Gaussian noise model, we recover the\nBradley Terry model of ranking.", "826": "796 Chapter22. Morevariationalinference\nWe initialize by assuming that at iteration 0, the initial upward messages from factors hgto\nvariables siare uniform, i.e.,\nm0\nhg\u2192sig(sig)=1,\u03bb0h\ng\u2192sig=0,\u03b70\nhg\u2192sig=0 (22.116)\nand similarly m0\nhg\u2192sjg(sjg)=1. The messages passing algorithm consists of 6 steps per game,\nas illustrated in Figure 22.11(b). We give the details of these steps below.\n1. Compute the posterior over the skills variables:\nqt(si)=f (si)/productdisplay\ngmt\u22121\nhg\u2192si(si)=Nc(si|\u03b7t\ni,\u03bbt\ni) (22.117)\n\u03bbti=\u03bb0+/summationdisplay\ng\u03bbt\u22121\nhg\u2192si,\u03b7t\ni=\u03b70+/summationdisplay\ng\u03b7t\u22121\nhg\u2192si(22.118)\n2. Compute the message from the skills variables down to the game factor hg:\nmts\nig\u2192hg(sig)=qt(sig)\nmt\nhg\u2192sig(sig),mt\nsjg\u2192hg(sjg)=qt(sjg)\nmt\nhg\u2192sjg(sjg)(22.119)\nwhere the division is implemented by subtracting the natural parameters as follows:\n\u03bbt\nsig\u2192hg=\u03bbts\nig\u2212\u03bbth\ng\u2192sig,\u03b7t\nsig\u2192hg=\u03b7t\nsig\u2212\u03b7t\nhg\u2192sig(22.120)\nand similarly for sjg.\n3. Compute the message from the game factor hgdown to the difference variable dg:\nmth\ng\u2192dg(dg)=/integraldisplay/integraldisplay\nhg(dg,sig,sjg)mts\nig\u2192hg(sig)mts\njg\u2192hg(sjg)dsigdsjg(22.121)\n=/integraldisplay/integraldisplay\nN(dg|sig\u2212sjg,1)N(sig|\u03bcts\nig\u2192hg,vt\nsig\u2192hg) (22.122)\nN(sjg|\u03bcts\njg\u2192hg,vt\nsjg\u2192hg)dsigdsjg (22.123)\n=N(dg|\u03bcth\ng\u2192dg,vt\nhg\u2192dg) (22.124)\nvt\nhg\u2192dg=1 +vt\nsig\u2192hg+vt\nsjg\u2192hg(22.125)\n\u03bcth\ng\u2192dg=\u03bcts\nig\u2192hg\u2212\u03bcts\njg\u2192hg(22.126)\n4. Compute the posterior over the difference variables:\nqt(dg)\u221dmth\ng\u2192dg(dg)mkg\u2192dg(dg) (22.127)\n=N(dg|\u03bcth\ng\u2192dg,vt\nhg\u2192dg)I(yg=s i g n (dg)) (22.128)\n\u2248N(dg|\u03bctg,vt\ng) (22.129)", "827": "22.5. Expectationpropagation 797\n\u22126 \u22124 \u22122 0 2 4 601234567\u03a8 function\n(a)\u22126 \u22124 \u22122 0 2 4 600.10.20.30.40.50.60.70.80.91\u039b function\n(b)\nFigure 22.12 (a)\u03a8function. (b) \u039bfunction. Based on Figure 2 of (Herbrich et al. 2007). Figure generated\nbytrueskillPlot .\n(Note that the upward message from the kgfactor is constant.) We can \ufb01nd these parameters\nby moment matching as follows:\n\u03bct\ng=yg\u03bcth\ng\u2192dg+\u03c3t\nhg\u2192dg\u03a8/parenleftBigg\nyg\u03bct\nhg\u2192dg\n\u03c3t\nhg\u2192dg/parenrightBigg\n(22.130)\nvt\ng=vt\nhg\u2192dg/bracketleftBigg\n1\u2212\u039b/parenleftBigg\nyg\u03bcth\ng\u2192dg\n\u03c3t\nhg\u2192dg/parenrightBigg/bracketrightBigg\n(22.131)\n\u03a8(x) /definesN(x|0,1)\n\u03a6(x)(22.132)\n\u039b(x) /defines\u03a8(x)(\u03a8(x)+x) (22.133)\n(The derivation of these equations is left as a modi\ufb01cation to Exercise 11.15.) These functions\nare plotted in Figure 22.12. Let us try to understand these equations. Suppose \u03bct\nhg\u2192dgis a\nlarge positive number. That means we expect, based on the current estimate of the skills,\nthatdgwill be large and positive. Consequently, if we observe yg=+ 1, we will not be\nsurprised that igis the winner, which is re\ufb02ected in the fact that the update factor for the\nmean is small, \u03a8(yg\u03bct\nhg\u2192dg)\u22480. Similarly, the update factor for the variance is small,\n\u039b(yg\u03bcth\ng\u2192dg)\u22480. However, if we observe yg=\u22121, then the update factor for the mean\nand variance becomes quite large.\n5. Compute the upward message from the difference variable to the game factor hg:\nmt\ndg\u2192hg(dg)=qt(dg)\nmt\ndg\u2192hg(dg)(22.134)\n\u03bbt\ndg\u2192hh=\u03bbtg\u2212\u03bbth\ng\u2192dg,\u03b7t\ndg\u2192hh=\u03b7t\ng\u2212\u03b7t\nhg\u2192dg(22.135)\n6. Compute the upward messages from the game factor to the skill variables. Let us assume", "828": "798 Chapter22. Morevariationalinference\n2\n53\n4\n61\n(a)0 1 2 3 4 5 6 7\u22122\u22121.5\u22121\u22120.500.511.52\n(b)\nFigure 22.13 (a) A DAG representing a partial ordering of players. (b) Posterior mean plus/minus 1 standard\ndeviation for the latent skills of each player based on 26 games. Figure generated by trueskillDemo .\nthatigis the winner, and jgis the loser. Then we have\nmt\nhg\u2192sig(sig)=/integraldisplay/integraldisplay\nhg(dg,sig,sjg)mtd\ng\u2192hg(dg)mts\njg\u2192hg(sjg)ddgdsjg(22.136)\n=N(sig|\u03bcth\ng\u2192sig,vt\nhg\u2192sig) (22.137)\nvt\nhg\u2192sig=1 +vt\ndg\u2192hg+vt\nsjg\u2192hg(22.138)\n\u03bcth\ng\u2192sig=\u03bctd\ng\u2192hg+\u03bcts\njg\u2192hg(22.139)\nAnd similarly\nmth\ng\u2192sjg(sjg)=/integraldisplay/integraldisplay\nhg(dg,sig,sjg)mtd\ng\u2192hg(dg)mts\nig\u2192hg(sig)ddgdsig(22.140)\n=N(sjg|\u03bcth\ng\u2192sjg,vt\nhg\u2192sjg) (22.141)\nvt\nhg\u2192sjg=1 +vt\ndg\u2192hg+vt\nsig\u2192hg(22.142)\n\u03bcth\ng\u2192sjg=\u03bctd\ng\u2192hg\u2212\u03bcts\nig\u2192hg(22.143)\nWhen we compute qt+1(sig)at the next iteration, by combining mt\nhg\u2192sig(sig)with the\nprior factor, we will see that the posterior mean of siggoes up. Similarly, the posterior mean\nofsjggoes down.\nIt is straightforward to combine EP with ADF to perform online inference, which is necessary\nfor most practical applications.\nLet us consider a simple example of this method. We create a partial ordering of 5 players\nas shown in Figure 22.13(a). We then sample some game outcomes from this graph, where a", "829": "22.6. MAPstateestimation 799\nparent always beats a child. We pass this data into (5 iterations of) the EP algorithm and infer the\nposterior mean and variance for each player\u2019s skill level. The results are shown in Figure 22.13(b).We see that the method has correctly inferred the rank ordering of the players.\n22.5.6 Other applications of EP\nThe TrueSkill model was developed by researchers at Microsoft. They and others have extendedthe model to a variety of other interesting applications, including personalized ad recommenda-tion (Stern et al. 2009), predicting click-through-rate on ads in the Bing search engine (Graepelet al. 2010), etc. They have also developed a general purpose Bayesian inference toolbox basedon EP called infer.net (Minka et al. 2010).\nEP has also been used for a variety of other models, such as Gaussian process classi\ufb01cation\n(Nickisch and Rasmussen 2008). See http://research .microsoft .com/en-us/um/people/\nminka/papers/ep/roadmap .htmlfor a list of other EP applications.\n22.6 MAP state estimation\nIn this section, we consider the problem of \ufb01nding the most probable con\ufb01guration of variablesin a discrete-state graphical model, i.e., our goal is to \ufb01nd a MAP assignment of the followingform:\nx\n\u2217=a r gm a x\nx\u2208Xmp(x|\u03b8) = arg max\nx\u2208Xm/summationdisplay\ni\u2208V\u03b8i(xi)+/summationdisplay\nf\u2208F\u03b8f(xf) = arg max\nx\u2208Xm\u03b8T\u03c6(x)(22.144)\nwhere\u03b8iare the singleton node potentials, and \u03b8fare the factor potentials. (In this section, we\nfollow the notation of (Sontag et al. 2011), which considers the case of general potentials, not justpairwise ones.) Note that the partition function Z(\u03b8)plays no role in MAP estimation.\nIf the treewidth is low, we can solve this problem with the junction tree algorithm (Sec-\ntion 20.4), but in general this problem is intractable. In this section, we discuss various approxi-mations, building on the material from Section 22.3.\n22.6.1 Linear programming relaxation\nWe can rewrite the objective in terms of the variational parameters as follows:\narg max\nx\u2208Xm\u03b8T\u03c6(x) = arg max\n\u03bc\u2208M(G)\u03b8T\u03bc (22.145)\nwhere\u03c6(x)=[{I(xs=j)},{I(xf=k)})and\u03bcis a probability vector in the marginal\npolytope. To see why this equation is true, note that we can just set \u03bcto be a degenerate\ndistribution with \u03bc(xs)=I(xs=x\u2217\ns),w h e r ex\u2217sis the optimal assigment of node s. So instead\nof optimizing over discrete assignments, we now optimize over probability distributions \u03bc.\nIt seems like we have an easy problem to solve, since the objective in Equation 22.145 is linear\nin\u03bc, and the constraint set M(G)is convex. The trouble is, M(G)in general has a number of\nfacets that is exponential in the number of nodes.\nA standard strategy in combinatorial optimization is to relax the constraints. In this case,\ninstead of requiring probability vector \u03bcto live in the marginal polytope M(G), we allow it to", "830": "800 Chapter22. Morevariationalinference\nlive inside a convex outer bound L(G). Having de\ufb01ned this relaxed constraint set, we have\nmax\nx\u2208Xm\u03b8T\u03c6(x)= m a x\n\u03bc\u2208M(G)\u03b8T\u03bc\u2264max\n\u03c4\u2208L(G)\u03b8T\u03c4 (22.146)\nIf the solution is integral, it is exact; if it is fractional, it is an approximation. This is called\na (\ufb01rst order) linear programming relaxtion. The reason it is called \ufb01rst-order is that the\nconstraints that are enforced are those that correspond to consistency on a tree, which is agraph of treewidth 1. It is possible to enforce higher-order consistency, using graphs with largertreewidth (see (Wainwright and Jordan 2008b, sec 8.5) for details).\nHow should we actually perform the optimization? We can use a generic linear programming\npackage, but this is often very slow. Fortunately, in the case of graphical models, it is possible todevise specialised distributed message passing algorithms for solving this optimization problem,as we explain below.\n22.6.2 Max-product belief propagation\nThe MAP objective in Equation 22.145, max\u03bc\u2208M(G)\u03b8T\u03bc, is almost identical to the inference\nobjective in Equation 22.23, max\u03bc\u2208M(G)\u03b8T\u03bc+H(\u03bc), apart from the entropy term. One\nheuristic way to proceed would be to consider the zero temperature limit of the probability\ndistribution \u03bc, where the probability distribution has all its mass centered on its mode (see\nSection 4.2.2). In such a setting, the entropy term becomes zero. We can then modify themessage passing methods used to solve the inference problem so that they solve the MAPestimation problem instead. In particular, in the zero temperature limit, the sum operatorbecomes the max operator, which results in a method called max-product belief propagation.\nIn more detail, let\nA(\u03b8)/definesmax\n\u03bc\u2208M(G)\u03b8T\u03bc+H(\u03bc) (22.147)\nNow consider an inverse temperature \u03b2going to in\ufb01nity. We have\nlim\n\u03b2\u2192+\u221eA(\u03b2\u03b8)\n\u03b2= lim\n\u03b2\u2192+\u221e1\n\u03b2max\n\u03bc\u2208M(G)/braceleftbig\n(\u03b2\u03b8)T\u03bc+H(\u03bc)/bracerightbig\n(22.148)\n=m a x\n\u03bc\u2208M(G)/braceleftbigg\n\u03b8T\u03bc+ lim\n\u03b2\u2192+\u221e1\n\u03b2H(\u03bc)/bracerightbigg\n(22.149)\n=m a x\n\u03bc\u2208M(G)\u03b8T\u03bc (22.150)\nIt is the concavity of the objective function that allows us to interchange the limandmax\noperators (see (Wainwright and Jordan 2008b, p274) for details).\nNow consider the Bethe approximation, which has the form max\u03c4\u2208L(G)\u03b8T\u03c4+HBethe(\u03c4).\nWe showed that loopy BP \ufb01nds a local optimum of this objective. In the zero temperature limit,this objective is equivalent to the LP relaxation of the MAP problem. Unfortunately, max-productloopy BP does not solve this LP relaxation unless the graph is a tree (Wainwright and Jordan2008b, p211). The reason is that Bethe energy functional is not concave (except on trees), so weare not licensed to swap the limit and max operators in the above zero-temperature derivation.However, if we use tree-reweighted BP, or TRBP/ TRW, we have a concave objective. In this case,", "831": "22.6. MAPstateestimation 801\none can show (Kolmogorov and Wainwright 2005) that the max-product version of TRBP does\nsolve the above LP relaxation.\nA certain scheduling of this algorithm, known as sequential TRBP, TRBP-S,o rTRW-S, can\nbe shown to always converge (Kolmogorov 2006), and furthermore, it typically does so fasterthan the standard parallel updates. The idea is to pick an arbitrary node ordering X\n1,...,X N.\nWe then consider a set of trees which is a subsequence of this ordering. At each iteration, weperform max-product BP from X\n1towardsXNand back along one of these trees. It can be\nshown that this monotonically minimizes a lower bound on the energy, and thus is guaranteedto converge to the global optimum of the LP relaxation.\n22.6.3 Graphcuts\nIn this section, we show how to \ufb01nd MAP state estimates, or equivalently, minimum energycon\ufb01gurations, by using the max \ufb02ow/min cut algorithm for graphs.\n8This class of methods is\nknown as graphcuts and is very widely used, especially in computer vision applications.\nWe will start by considering the case of MRFs with binary nodes and a restricted class of\npotentials; in this case, graphcuts will \ufb01nd the exact global optimum. We then consider thecase of multiple states per node, which are assumed to have some underlying ordering; we canapproximately solve this case by solving a series of binary subproblems, as we will see.\n22.6.3.1 Graphcuts for the generalized Ising model\nLet us start by considering a binary MRF where the edge energies have the following form:\nE\nuv(xu,xv)=/braceleftbigg0ifxu=xv\n\u03bbstifxu/negationslash=xv(22.151)\nwhere\u03bbst\u22650is the edge cost. This encourages neighboring nodes to have the same value\n(since we are trying to minimize energy). Since we are free to add any constant we like to theoverall energy without affecting the MAP state estimate, let us rescale the local energy termssuch that either E\nu(1) = 0orEu(0) = 0.\nNow let us construct a graph which has the same set of nodes as the MRF, plus two distin-\nguished nodes: the source sand the sinkt.I f Eu(1) = 0, we add the edge xu\u2192twith cost\nEu(0). (This ensures that if uis not in partition Xt, meaning uis assigned to state 0, we will\npay a cost of Eu(0)in the cut.) Similarly, If Eu(0) = 0, we add the edge xu\u2192swith cost\nEu(1). Finally, for every pair of variables that are connected in the MRF, we add edges xu\u2192xv\nandxv\u2192xu, both with cost \u03bbu,v\u22650. Figure 22.14 illustrates this construction for an MRF\nwith 4 nodes, and with the following non-zero energy values:\nE1(0) = 7,E 2(1) = 2,E 3(1) = 1,E4(1) = 6 (22.152)\n\u03bb1,2=6,\u03bb2,3=6,\u03bb3,4=2,\u03bb1,4=1 (22.153)\nHaving constructed the graph, we compute a minimal s\u2212tcut. This is a partition of the nodes\ninto two sets,Xs, which are nodes connected to s, andXt, which are nodes connected to t.W e\n8. There are a variety of ways to implement this algorithm, see e.g., (Sedgewick and Wayne 2011). The best take\nO(EVlogV)orO(V3)time, where Eis the number of edges and Vis the number of nodes.", "832": "802 Chapter22. Morevariationalinference\nz1\nz4z2\nz3\nst\n1\n12\n26\n6\n67\nFigure 22.14 Illustration of graphcuts applied to an MRF with 4 nodes. Dashed lines are ones which\ncontribute to the cost of the cut (for bidirected edges, we only count one of the costs). Here the min cut\nhas cost 6. Source: Figure 13.5 from (Koller and Friedman 2009). Used with kind permission of DaphneKoller.\npick the partition which minimizes the sum of the cost of the edges between nodes on different\nsides of the partition:\ncost(Xs,Xt)=/summationdisplay\nxu\u2208Xs,xv\u2208Xtcost(xu,sv) (22.154)\nIn Figure 22.14, we see that the min-cut has cost 6.\nMinimizing the cost in this graph is equivalent to minimizing the energy in the MRF. Hence\nnodes that are assigned to shave an optimal state of 0, and the nodes that are assigned to t\nhave an optimal state of 1. In Figure 22.14, we see that the optimal MAP estimate is (1,1,1,0).\n22.6.3.2 Graphcuts for binary MRFs with submodular potentials\nWe now discuss how to extend the graphcuts construction to binary MRFs with more generalkinds of potential functions. In particular, suppose each pairwise energy satis\ufb01es the followingcondition:\nE\nuv(1,1)+Euv(0,0)\u2264Euv(1,0)+Euv(0,1) (22.155)\nIn other words, the sum of the diagonal energies is less than the sum of the off-diagonal energies.In this case, we say the energies are submodular (Kolmogorov and Zabin 2004).\n9An example\nof a submodular energy is an Ising model where \u03bbuv>0. This is also known as an attractive\nMRForassociative MRF, since the model \u201cwants\u201d neighboring states to be the same.\n9. Submodularity is the discrete analog of convexity. Intuitively, it corresponds to the \u201claw of diminishing returns\u201d, that\nis, the extra value of adding one more element to a set is reduced if the set is already large. More formally, we say that\nf:2S\u2192Ris submodular if for any A\u2282B\u2282Sandx\u2208S,w eh a v ef (A\u222a{x})\u2212f(A)\u2265f(B\u222a{x})\u2212f(B).\nIf\u2212fis submodular, then fissupermodular.", "833": "22.6. MAPstateestimation 803\nTo apply graphcuts to a binary MRF with submodular potentials, we construct the pairwise\nedge weights as follows:\nE/prime\nu,v(0,1) =Eu,v(1,0)+Eu,v(0,1)\u2212Eu,v(0,0)\u2212Eu,v(1,1) (22.156)\nThis is guaranteed to be non-negative by virtue of the submodularity assumption. In addition,\nwe construct new local edge weights as follows: \ufb01rst we initialize E/prime(u)=E(u), and then for\neach edge pair (u,v), we update these values as follows:\nE/prime\nu(1) = E/prime\nu(1)+(Eu,v(1,0)\u2212Eu,v(0,0)) (22.157)\nE/prime\nv(1) = E/prime\nu(1)+(Eu,v(1,1)\u2212Eu,v(1,0)) (22.158)\nWe now construct a graph in a similar way to before. Speci\ufb01cally, if E/prime\nu(1)>E/prime\nu(0),w e\nadd the edge u\u2192swith cost E/prime\nu(1)\u2212E/prime\nu(0), otherwise we add the edge u\u2192twith cost\nE/prime\nu(0)\u2212E/prime\nu(1). Finally for every MRF edge for which E/prime\nu,v(0,1)>0, we add a graphcuts edge\nxu\u2212xvwith cost E/prime\nu,v(0,1). (We don\u2019t need to add the edge in both directions.)\nOne can show (Exercise 22.1) that the min cut in this graph is the same as the minimum\nenergy con\ufb01guration. Thus we can use max \ufb02ow/min cut to \ufb01nd the globally optimal MAPestimate (Greig et al. 1989).\n22.6.3.3 Graphcuts for nonbinary metric MRFs\nWe now discuss how to use graphcuts for approximate MAP estimation in MRFs where eachnode can have multiple states (Boykov et al. 2001). However, we require that the pairwise energiesform a metric. We call such a model a metric MRF. For example, suppose the states have a\nnatural ordering, as commonly arises if they are a discretization of an underlying continuousspace. In this case, we can de\ufb01ne a metric of the form E(x\ns,xt)=m i n ( \u03b4,||xs\u2212xt||)or a\nsemi-metric of the form E(xs,xt)=m i n ( \u03b4,(xs\u2212xt)2), for some constant \u03b4>0. This energy\nencourages neighbors to have similar labels, but never \u201cpunishes\u201d them by more than \u03b4. This\u03b4\nterm prevents over-smoothing, which we illustrate in Figure 19.20.\nOne version of graphcuts is the alpha expansion. At each step, it picks one of the available\nlabels or states and calls it \u03b1; then it solves a binary subproblem where each variable can choose\nto remain in its current state, or to become state \u03b1(see Figure 22.15(d) for an illustration). More\nprecisely, we de\ufb01ne a new MRF on binary nodes, and we de\ufb01ne the energies of this new model,relative to the current assignment x, as follows:\nE\n/prime\nu(0) =Eu(xu),E/prime\nu(1) =Eu(\u03b1),E/prime\nu,v(0,0) =Eu,v(xu,xv)(22.159)\nE/prime\nu,v(0,1) =Eu,v(xu,\u03b1),E/prime\nu,v(1,0) =Eu,v(\u03b1,xv),E/prime\nu,v(1,1) =Eu,v(\u03b1,\u03b1) (22.160)\nTo optimize E/primeusing graph cuts (and thus \ufb01gure out the optimal alpha expansion move),\nwe require that the energies be submodular. Plugging in the de\ufb01nition we get the followingconstraint:\nE\nu,v(xu,xv)+Eu,v(\u03b1,\u03b1)\u2264Eu,v(xu,\u03b1)+Eu,v(\u03b1,xv) (22.161)\nFor any distance function, Eu,v(\u03b1,\u03b1)=0 , and the remaining inequality follows from the\ntriangle inequality. Thus we can apply the alpha expansion move to any metric MRF.", "834": "804 Chapter22. Morevariationalinference\n(a) initial labeling (b) standard move (c) \u03b1-\u03b2-swap (d) \u03b1-expansion\nFigure 22.15 (a) An image with 3 labels. (b) A standard local move (e.g., by iterative conditional modes)\njust \ufb02ips the label of one pixel. (c) An \u03b1\u2212\u03b2swap allows all nodes that are currently labeled as \u03b1to\nbe relabeled as \u03b2if this decreases the energy. (d) An \u03b1expansion allows all nodes that are not currently\nlabeled as \u03b1to be relabeled as \u03b1if this decreases the energy. Source: Figure 2 of (Boykov et al. 2001).\nUsed with kind permission of Ramin Zabih.\nAt each step of alpha expansion, we \ufb01nd the optimal move from amongst an exponentially\nlarge set; thus we reach a strong local optimum , of much lower energy than the local optima\nfound by standard greedy label \ufb02ipping methods such as iterative conditional modes. In fact,\none can show that, once the algorithm has converged, the energy of the resulting solution is at\nmost2ctimes the optimal energy, where\nc=m a x\n(u,v)\u2208Emax\u03b1/negationslash=\u03b2Euv(\u03b1,\u03b2)\nmin\u03b1/negationslash=\u03b2Euv(\u03b1,\u03b2)(22.162)\nSee Exercise 22.3 for the proof. In the case of the Potts model, c=1,s ow eh a v ea2 -\napproximation.\nAnother version of graphcuts is the alpha-beta swap . At each step, two labels are chosen,\ncall them \u03b1and\u03b2. All the nodes currently labeled \u03b1can change to \u03b2(and vice versa) if this\nreduces the energy (see Figure 22.15(c) for an illustration). The resulting binary subproblem can\nbe solved exactly, even if the energies are only semi-metric (that is, the triangle inequality need\nnot hold; see Exercise 22.2). Although the \u03b1\u2212\u03b2swap version can be applied to a broader class\nof models than the \u03b1-expansion version, it is theoretically not as powerful. Indeed, in various\nlow-level vision problems, (Szeliski et al. 2008) show empirically that the expansion version is\nusually better than the swap version (see Section 22.6.4).\n22.6.4 Experimental comparison of graphcuts and BP\nIn Section 19.6.2.7, we described lattice-structured CRFs for various low-level vision problems.\n(Szeliski et al. 2008) performed an extensive comparison of different approximate optimization\ntechniques for this class of problems. Some of the results, for the problem of stereo depth\nestimation, are shown in Figure 22.16. We see that the graphcut and tree-reweighted max-\nproduct BP (TRW) give the best results, with regular max-product BP being much worse. In terms\nof speed, graphcuts is the fastest, with TRW a close second. Other algorithms, such as ICM,\nsimulated annealing or a standard domain-speci\ufb01c heuristic known as normalize correlation, are", "835": "22.6. MAPstateestimation 805\nMax-Product BP\na-Expansion\na-b Swap\nTRW\nMax-Product BP\na-Expansion\na-b Swap\nTRW2\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3Energy\nRunning Time (s)100\u00d7106\n1011021034.2\n4.1\n4\n3.9\n3.8\n3.7\n3.6Energy\nRunning Time (s)100\u00d7105\n101102\nFigure 22.16 Energy minimization on a CRF for stereo depth estimation. Top row: two input images along\nwith the ground truth depth values. Bottom row: energy vs time for 4 different optimization algorithms.\nBottom left: results are for the Teddy image (shown in top row). Bottom right: results are for the Tsukuba\nimage (shown in Figure 22.17(a)). Source: Figure 13.B.1 of (Koller and Friedman 2009). Used with kind\npermission of Daphne Koller.\neven worse, as shown qualitatively in Figure 22.17.\nSince TRW is optimizing the dual of the relaxed LP problem, we can use its value at conver-\ngence to evaluate the optimal energy. It turns out that for many of the images in the stereo\nbenchmark dataset, the ground truth has higher energy (lower probability) than the globally op-\ntimal estimate (Meltzer et al. 2005). This indicates that we are optimizing the wrong model. This\nis not surprising, since the pairwise CRF ignores known long-range constraints. Unfortunately,\nif we add these constraints to the model, the graph either becomes too dense (making BP slow),\nand/or the potentials become non-submodular (making graphcuts inapplicable).\nOne way around this is to generate a diverse set of local modes, using repeated applications\nof graph cuts, as described in (Yadollahpour et al. 2011). We can then apply a more sophisticated\nmodel, which uses global features, to rerank the solutions.", "836": "806 Chapter22. Morevariationalinference\n(a) Left image: 384x288, 15 labels (b) Ground truth\n(c) Swap algorithm (d) Expansion algorithm\n(e) Normalized correlation (f) Simulated annealing\nFigure 22.17 An example of stereo depth estimation using an MRF. (a) Left image, of size 384\u00d7288\npixels, from the University of Tsukuba. (The corresponding right image is similar, but not shown.) (b)\nGround truth depth map, quantized to 15 levels. (c-f): MAP estimates using different methods: (c) \u03b1\u2212\u03b2\nswap, (d) \u03b1expansion, (e) normalized cross correlation, (f) simulated annealing. Source: Figure 10 of\n(Boykov et al. 2001). Used with kind permission of Ramin Zabih.\n22.6.5 Dual decomposition\nWe are interested in computing\np\u2217=m a x\nx\u2208Xm/summationdisplay\ni\u2208V\u03b8i(xi)+/summationdisplay\nf\u2208F\u03b8f(xf) (22.163)\nwhereFrepresents a set of factors. We will assume that we can tractably optimize each local\nfactor, but the combination of all of these factors makes the problem intractable. One way to\nproceed is to optimize each term independently, but then to introduce constraints that force all\nthe local estimates of the variables\u2019 values to agree with each other. We explain this in more\ndetail below, following the presentation of (Sontag et al. 2011).", "837": "22.6. MAPstateestimation 807\nx1 x2\nx3 x4\u03b8f(x1,x2)\n\u03b8h(x2,x4)\n\u03b8k(x3,x4)\u03b8g(x1,x3)\n(a)x1 x2\nx3 x4xf\n1 xf2\nxg1\nxg3xh\n2\nxh4\nxk3 xk4\n\u03b8k(xk3,xk4)\u03b8h(xh2,xh4)\u03b8f(xf\n1,xf2)\n== =\n=\n=\n===\n\u03b8g(xg\n1,xg3)\n(b)\nFigure 22.18 (a) A pairwise MRF with 4 different edge factors. (b) We have 4 separate variables, plus a\ncopy of each variable for each factor it participates in. Source: Figure 1.2-1.3 of (Sontag et al. 2011). Used\nwith kind permission of David Sontag.\n22.6.5.1 Basic idea\nLet us duplicate the variables xi, once for each factor, and then force them to be equal.\nSpeci\ufb01cally, let xf\nf={xf\ni}i\u2208fbe the set of variables used by factor f. This construction is\nillustrated in Figure 22.18. We can reformulate the objective as follows:\np\u2217=m a x\nx,xf/summationdisplay\ni\u2208V\u03b8i(xi)+/summationdisplay\nf\u2208F\u03b8f(xf\nf)s.t.xf\ni=xi\u2200f,i\u2208f (22.164)\nLet us now introduce Lagrange multipliers, or dual variables, \u03b4fi(k), to enforce these constraints.\nThe Lagrangian becomes\nL(\u03b4,x,xf)=/summationdisplay\ni\u2208V\u03b8i(xi)+/summationdisplay\nf\u2208F\u03b8f(xf\nf) (22.165)\n+/summationdisplay\nf\u2208F/summationdisplay\ni\u2208f/summationdisplay\n\u02c6xi\u03b4fi(\u02c6xi)/parenleftBig\nI(xi=\u02c6xi)\u2212I(xf\ni=\u02c6xi)/parenrightBig\n(22.166)\nThis is equivalent to our original problem in the following sense: for any value of \u03b4,w eh a v e\np\u2217=m a x\nx,xfL(\u03b4,x,xf)s.t.xfi=xi\u2200f,i\u2208f (22.167)\nsince if the constraints hold, the last term is zero. We can get an upper bound by dropping the\nconsistency constraints, and just optimizing the following upper bound:\nL(\u03b4)/definesmax\nx,xfL(\u03b4,x,xf) (22.168)\n=/summationdisplay\nimax\nxi\u239b\n\u239d\u03b8i(xi)+/summationdisplay\nf:i\u2208f\u03b4fi(xi)\u239e\u23a0+/summationdisplay\nfmax\nxf\u239b\u239d\u03b8\nf(xf)\u2212/summationdisplay\ni\u2208f\u03b4fi(xi)\u239e\u23a0(22.169)\nSee Figure 22.19 for an illustration.", "838": "808 Chapter22. Morevariationalinference\nx1\u03b4f2(x2)\n\u03b4f1(x1)\n\u03b4k4(x4) \u03b4k3(x3)\u03b4g1(x1)+\n\u2212\u2212\u2212\u2212\u03b4f1(x1)\n\u03b4g3(x3)\n\u03b4g1(x1)\u2212\n\u2212\u03b4h2(x2)\n\u03b4h4(x4)\u2212\n\u2212\n+x3\u03b4g3(x3)\n\u03b4k3(x3)x4+\u03b4k4(x4)\n\u03b4h4(x4)+x2\u03b4f2(x2)\n\u03b4h2(x2)\u03b8f(x1,x2)\n\u03b8h(x2,x4)\n\u03b8k(x3,x4)\u03b8g(x1,x3)\nx3 x4x4x2x2 x1\nx1\nx3\nFigure 22.19 Illustration of dual decomposition. Source: Figure 1.2 of (Sontag et al. 2011). Used with\nkind permission of David Sontag.\nThis objective is tractable to optimize, since each xfterm is decoupled. Furthermore, we see\nthatL(\u03b4)\u2265p\u2217, since by relaxing the consistency constraints, we are optimizing over a larger\nspace. Furthermore, we have the property that\nmin\n\u03b4L(\u03b4)=p\u2217(22.170)\nso the upper bound is tight at the optimal value of \u03b4, which enforces the original constraints.\nMinimizing this upper bound is known as dual decomposition orLagrangian relaxation\n(Komodakis et al. 2011; Sontag et al. 2011; Rush and Collins 2012). Furthemore, it can be shown\nthatL(\u03b4)is the dual to the same LP relaxation we saw before. We will discuss several possible\noptimization algorithms below.\nThe main advantage of dual decomposition from a practical point of view is that it allows\none to mix and match different kinds of optimization algorithms in a convenient way. Forexample, we can combine a grid structured graph with local submodular factors to performimage segmentation, together with a tree structured model to perform pose estimation (seeExercise 22.4). Analogous methods can be used in natural language processing, where we oftenhave a mix of local and global constraints (see e.g., (Koo et al. 2010; Rush and Collins 2012)).\n22.6.5.2 Theoretical guarantees\nWhat can we say about the quality of the solutions obtained in this way? To understand this, letus \ufb01rst introduce some more notation:\n\u03b8\u03b4\ni(xi)/defines\u03b8i(xi)+/summationdisplay\nf:i\u2208f\u03b4fi(xi) (22.171)\n\u03b8\u03b4f(xf)/defines\u03b8f(xf)\u2212/summationdisplay\ni\u2208f\u03b4fi(xi) (22.172)", "839": "22.6. MAPstateestimation 809\nThis represents a reparameterization of the original problem, in the sense that\n/summationdisplay\ni\u03b8i(xi)+/summationdisplay\nf\u03b8f(xf)=/summationdisplay\ni\u03b8\u03b4\ni(xi)+/summationdisplay\nf\u03b8\u03b4f(xf) (22.173)\nand hence\nL(\u03b4)=/summationdisplay\nimax\nxi\u03b8\u03b4i(xi)+/summationdisplay\nfmax\nxf\u03b8\u03b4f(xf) (22.174)\nNow suppose there is a set of dual variables \u03b4\u2217and an assignment x\u2217such that the maxi-\nmizing assignments to the singleton terms agrees with the assignments to the factor terms, i.e.,\nso thatx\u2217\ni\u2208argmaxxi\u03b8\u03b4\u2217\ni(xi)andx\u2217\nf\u2208argmaxxf\u03b8\u03b4\u2217\nf(xf). In this case, we have\nL(\u03b4\u2217)=/summationdisplay\ni\u03b8\u03b4\u2217\ni(x\u2217\ni)+/summationdisplay\nf\u03b8\u03b4\u2217\nf(x\u2217f)=/summationdisplay\ni\u03b8i(x\u2217i)+/summationdisplay\nf\u03b8f(x\u2217f) (22.175)\nNow since\n/summationdisplay\ni\u03b8i(x\u2217i)+/summationdisplay\nf\u03b8f(x\u2217f)\u2264p\u2217\u2264L(\u03b4\u2217) (22.176)\nwe conclude that L(\u03b4\u2217)=p\u2217,s ox\u2217is the MAP assignment.\nSo if we can \ufb01nd a solution where all the subproblems agree, we can be assured that it is the\nglobal optimum. This happens surprisingly often in practical problems.\n22.6.5.3 Subgradient descent\nL(\u03b4)is a convex and continuous objective, but it is non-differentiable at points \u03b4where\u03b8\u03b4\ni(xi)\nor\u03b8\u03b4f(xf)have multiple optima. One approach is to use subgradient descent. This updates all\nthe elements of \u03b4at the same time, as follows:\n\u03b4t+1\nfi(xi)=\u03b4t\nfi(xi)\u2212\u03b1tgt\nfi(xi) (22.177)\nwheregtthe subgradient of L(\u03b4)at\u03b4t. If the step sizes \u03b1tare set appropriately (see Sec-\ntion 8.5.2.1), this method is guaranteed to converge to a global optimum of the dual. (See\n(Komodakis et al. 2011) for details.)\nOne can show that the gradient is given by the following sparse vector. First let xs\ni\u2208\nargmaxxi\u03b8\u03b4t\ni(xi)andxf\nf\u2208argmaxxf\u03b8\u03b4t\nf(xf).N e x tl e t gfi(xi)=0for all elements. Finally,\nifxf\ni/negationslash=xs\ni(so factor fdisagrees with the local term on how to set variable i), we setgfi(xsi)=\n+1andgfi(xf\ni)=\u22121. This has the effect of decreasing \u03b8\u03b4t\ni(xs\ni)and increasing \u03b8\u03b4t\ni(xf\ni),\nbringing them closer to agreement. Similarly, the subgradient update will decrease the value of\n\u03b8\u03b4t\nf(xfi,xf\\i)and increasing the value of \u03b8\u03b4t\nf(xs\ni,xf\\i).\nTo compute the gradient, we need to be able to solve subproblems of the following form:\nargmax\nxf\u03b8\u03b4t\nf(xf) = argmax\nxf\u23a1\n\u23a3\u03b8f(xf)\u2212/summationdisplay\ni\u2208f\u03b4t\nfi(xi)\u23a4\u23a6 (22.178)", "840": "810 Chapter22. Morevariationalinference\n(In (Komodakis et al. 2011), these subproblems are called slaves, whereas L(\u03b4)is called the\nmaster.) Obviously if the scope of factor fis small, this is simple. For example, if each factor is\npairwise, and each variable has Kstates, the cost is just K2. However, there are some kinds of\nglobal factors that also support exact and efficient maximization, including the following:\n\u2022 Graphical models with low tree width.\n\u2022 Factors that correspond to bipartite graph matchings (see e.g., (Duchi et al. 2007)). This\nis useful for data association problems, where we must match up a sensor reading with\nan unknown source. We can \ufb01nd the maximal matching using the so-called Hungarianalgorithm in O(|f|\n3)time (see e.g., (Padadimitriou and Steiglitz 1982)).\n\u2022 Supermodular functions. We discuss this case in more detail in Section 22.6.3.2.\n\u2022 Cardinality constraints. For example, we might have a factor over a large set of binary\nvariables that enforces that a certain number of bits are turned on; this can be useful in\nproblems such as image segmentation. In particular, suppose \u03b8f(xf)=0if/summationtext\ni\u2208fxi=L\nand\u03b8f(xf)=\u2212\u221eotherwise. We can \ufb01nd the maximizing assignment in O(|f|log|f|)\ntime as follows: \ufb01rst de\ufb01ne ei=\u03b4fi(1)\u2212\u03b4fi(0); now sort the ei; \ufb01nally set xi=1for the\n\ufb01rstLvalues, and xi=0for the rest (Tarlow et al. 2010).\n\u2022 Factors which are constant for all but a small set Sof distinguished values of xf. Then we\ncan optimize over the factor in O(|S|)time (Rother et al. 2009).\n22.6.5.4 Coordinate descent\nAn alternative to updating the entire \u03b4vector at once (albeit sparsely) is to update it using block\ncoordinate descent. By choosing the size of the blocks, we can trade off convergence speed withease of the local optimization problem.\nOne approach, which optimizes \u03b4\nfi(xi)for alli\u2208fand allxiat the same time (for a\n\ufb01xed factor f), is known as max product linear programming (Globerson and Jaakkola 2008).\nAlgorithmically, this is similar to belief propagation on a factor graph. In particular, we de\ufb01ne\u03b4\nf\u2192ias messages sent from factor fto variable i, and we de\ufb01ne \u03b4i\u2192fas messages sent from\nvariableito factorf. These messages can be computed as follows (see (Globerson and Jaakkola\n2008) for the derivation):10\n\u03b4i\u2192f(xi)=\u03b8 i(xi)+/summationdisplay\ng/negationslash=f\u03b4g\u2192i(xi) (22.179)\n\u03b4f\u2192i(xi)=\u2212\u03b4 i\u2192f(xi)+1\n|f|max\nxf\\i\u23a1\n\u23a3\u03b8f(xf)+/summationdisplay\nj\u2208f\u03b4j\u2192f(xj)\u23a4\u23a6 (22.180)\nWe then set the dual variables \u03b4\nfi(xi)to be the messages \u03b4f\u2192i(xi).\nFor example, consider a 2\u00d72grid MRF, with the following pairwise factors: \u03b8f(x1,x2),\n\u03b8g(x1,x3),\u03b8h(x2,x4), and\u03b8k(x3,x4). The outgoing message from factor fto variable 2 is a\n10. Note that we denote their \u03b4\u2212f\ni(xi)by\u03b4i\u2192f(xi).", "841": "22.6. MAPstateestimation 811\nfunction of all messages coming into f, andf\u2019s local factor:\n\u03b4f\u21922(x2)=\u2212\u03b42\u2192f(x2)+1\n2max\nx1[\u03b8f(x1,x2)+\u03b41\u2192f(x1)+\u03b42\u2192f(x2)] (22.181)\nSimilarly, the outgoing message from variable 2 to factor fis a function of all the messages\nsent into variable 2 from other connected factors (in this example, just factor h) and the local\npotential:\n\u03b42\u2192f(x2)=\u03b82(2)+\u03b4h2(x2) (22.182)\nThe key computational bottleneck is computing the max marginals of each factor, where we\nmax out all the variables from xfexcept for xi, i.e., we need to be able to compute the following\nmax marginals efficiently:\nmax\nxf\\ih(xf\\i,xi),h(xf\\i,xi)/defines\u03b8f(xf)+/summationdisplay\nj\u2208f\u03b4jf(xj) (22.183)\nThe difference from Equation 22.178 is that we are maxing over all but one of the variables. We\ncan solve this efficiently for low treewidth graphical models using message passing; we can alsosolve this efficiently for factors corresponding to bipartite matchings (Duchi et al. 2007) or tocardinality constraints (Tarlow et al. 2010). However, there are cases where maximizing over allthe variables in a factor\u2019s scope is computationally easier than maximizing over all-but-one (see(Sontag et al. 2011, Sec 1.5.4) for an example); in such cases, we may prefer to use a subgradientmethod.\nCoordinate descent is a simple algorithm that is often much faster at minimizing the dual than\ngradient descent, especially in the early iterations. It also reduces the objective monotonically,and does not need any step size parameters. Unfortunately, it is not guaranteed to converge tothe global optimum, since L(\u03b4)is convex but not strictly convex (which implies there may be\nmore than one globally optimizing value). One way to ensure convergence is to replace the maxfunction in the de\ufb01nition of L(\u03b4)with the soft-max function, which makes the objective strictly\nconvex (see e.g., (Hazan and Shashua 2010) for details).\n22.6.5.5 Recovering the MAP assignment\nSo far, we have been focussing on \ufb01nding the optimal value of \u03b4\n\u2217. But what we really want is\nthe optimal value of x\u2217. In general, computing x\u2217from\u03b4\u2217is NP-hard, even if the LP relaxation\nis tight and the MAP assignment is unique (Sontag et al. 2011, Theorem 1.4). (The troublesomecases arise when there are fractional assignments with the same optimal value as the MAPestimate.)\nHowever, suppose that each\n\u03b8\u03b4\u2217\nihas a unique maximum, x\u2217\ni; in this case, we say that \u03b4\u2217is\nlocally decodable tox\u2217. One can show than in this case, the LP relaxation is unique and its\nsolution is indeed x\u2217. If many, but not all, of the nodes are uniquely decodable, we can \u201cclamp\u201d\nthe uniquely decodable ones to their MAP value, and then use exact inference algorithms to\n\ufb01gure out the optimal assignment to the remaining variables. Using this method, (Meltzer et al.2005) was able to optimally solve various stereo vision CRF estimation problems, and (Yanoveret al. 2007) was able to optimally solve various protein side-chain structure predicition problems.\nAnother approach is to use the upper bound provided by the dual in a branch and bound\nsearch procedure (Geoffrion 1974).", "842": "812 Chapter22. Morevariationalinference\nExercises\nExercise 22.1 Graphcuts for MAP estimation in binary submodular MRFs\n(Source: Ex. 13.14 of (Koller and Friedman 2009).). Show that using the graph construction described in\nSection 22.6.3.2, the cost of the cut is equal to the energy of the corresponding assignment, up to anirrelevant constant. (Warning: this exercise involves a lot of algebraic book-keeping.)\nExercise 22.2 Graphcuts for alpha-beta swap\n(Source: Ex. 13.15 of (Koller and Friedman 2009).). Show how the optimal alpha-beta swap can be found by\nrunning min-cut on an appropriately constructed graph. More precisely,\na. De\ufb01ne a set of binary variables t\n1,...,t nsuch that ti=0meansx/prime\ni=\u03b1,ti=1ifx/primei=\u03b2, and\nx/primei=xiis unchanged f xi/negationslash=\u03b1andxi/negationslash=\u03b2.\nb. De\ufb01ne an energy function over the new variables such that E/prime(t)=E(x)+const .\nc. Show that E/primeis submodular if Eis a semimetric.\nExercise 22.3 Constant factor optimality for alpha-expansion\n(Source: Daphne Koller.). Let Xbe a pairwise metric Markov random \ufb01eld over a graph G=(V,E).\nSuppose that the variables are nonbinary and that the node potentials are nonnegative. Let Adenote the\nset of labels for each X\u2208X. Though it is not possible to (tractably) \ufb01nd the globally optimal assignment\nx\u22c6in general, the \u03b1-expansion algorithm provides a method for \ufb01nding assignments \u02c6xthat are locally\noptimal with respect to a large set of transformations, i.e., the possible \u03b1-expansion moves.\nDespite the fact that \u03b1-expansion only produces a locally optimal MAP assignment, it is possible to prove\nthat the energy of this assignment is within a known factor of the energy of the globally optimal solution\nx\u22c6. In fact, this is a special case of a more general principle that applies to a wide variety of algorithms,\nincluding max-product belief propagation and more general move-making algorithms: If one can provethat the solutions obtained by the algorithm are \u2018strong local minima\u2019, i.e., local minima with respect to\na large set of potential moves, then it is possible to derive bounds on the (global) suboptimality of thesesolutions, and the quality of the bounds will depend on the nature of the moves considered. (There is aprecise de\ufb01nition of \u2018large set of moves\u2019.)\nConsider the following approach to proving the suboptimality bound for \u03b1-expansion.\na. Let\u02c6xbe a local minimum with respect to expansion moves. For each \u03b1\u2208A,l e tV\n\u03b1={s\u2208V|x\u22c6\ns=\n\u03b1},i.e., the set of nodes labelled \u03b1in the global minimum. Let x/primebe an assignment that is equal to\nx\u22c6onV\u03b1and equal to \u02c6xelsewhere; this is an \u03b1-expansion of \u02c6x. Verify that E(x\u22c6)\u2264E(\u02c6x)\u2264E(x/prime).\nb. Building on the previous part, show that E(\u02c6x)\u22642cE(x\u22c6),wherec=m a x (s,t)\u2208E/parenleftBigmax\u03b1/negationslash=\u03b2\u03b5st(\u03b1,\u03b2)\nmin\u03b1/negationslash=\u03b2\u03b5st(\u03b1,\u03b2)/parenrightBig\nandEdenotes the energy of an assignment.\nHint. Think about where x/primeagrees with \u02c6xand where it agrees with x\u22c6.\nExercise 22.4 Dual decomposition for pose segmentation\n(Source: Daphne Koller.). Two important problems in computer vision are that of parsing articulated objects\n(e.g., the human body), called pose estimation, and segmenting the foreground and the background, called\nsegmentation. Intuitively, these two problems are linked, in that solving either one would be easier if thesolution to the other were available. We consider solving these problems simultaneously using a jointmodel over human poses and foreground/background labels and then using dual decomposition for MAPinference in this model.\nWe construct a two-level model, where the high level handles pose estimation and the low level handles\npixel-level background segmentation. Let G=(V,E)be an undirected grid over the pixels. Each node\ni\u2208Vrepresents a pixel. Suppose we have one binary variable x\nifor each pixel, where xi=1means that\npixeliis in the foreground. Denote the full set of these variables by x=(xi).", "843": "22.6. MAPstateestimation 813\nIn addition, suppose we have an undirected tree structure T=(V/prime,E/prime)on the parts. For each body\npart, we have a discrete set of candidate poses that the part can be in, where each pose is characterized\nby parameters specifying its position and orientation. (These candidates are generated by a procedureexternal to the algorithm described here.) De\ufb01ne y\njkto be a binary variable indicating whether body part\nj\u2208V/primeis in con\ufb01guration k. Then the full set of part variables is given by y=(yjk), withj\u2208V/prime\nandk=1,...,K,w h e r eJis the total number of body parts and Kis the number of candidate poses\nfor each part. Note that in order to describe a valid con\ufb01guration, ymust satisfy the constraint that/summationtextK\nk=1yjk=1for eachj.\nSuppose we have the following energy function on pixels:\nE1(x)=/summationdisplay\ni\u2208V1[xi=1 ]\u00b7\u03b8i+/summationdisplay\n(i,j)\u2208E1[xi/negationslash=xj]\u00b7\u03b8ij.\nAssume that the \u03b8ijarises from a metric (e.g., based on differences in pixel intensities), so this can be\nviewed as the energy for a pairwise metric MRF with respect to G.\nWe then have the following energy function for parts:\nE2(y)=/summationdisplay\np\u2208V/prime\u03b8p(yp)+/summationdisplay\n(p,q)\u2208E/prime\u03b8pq(yp,yq).\nSince each part candidate yjkis assumed to come with a position and orientation, we can compute a\nbinary mask in the image plane. The mask assigns a value to each pixel, denoted by {wi\njk}i\u2208V,w h e r e\nwi\njk=1if pixelilies on the skeleton and decreases as we move away. We can use this to de\ufb01ne an\nenergy function relating the parts and the pixels:\nE3(x,y)=/summationdisplay\ni\u2208V/summationdisplay\nj\u2208V/primeK/summationdisplay\nk=11[xi=0,yjk=1 ]\u00b7wi\njk.\nIn other words, this energy term only penalizes the case where a part candidate is active but the pixel\nunderneath is labeled as background.\nFormulate the minimization of E1+E2+E3as an integer program and show how you can use dual\ndecomposition to solve the dual of this integer program. Your solution should describe the decomposition\ninto slaves, the method for solving each one, and the update rules for the overall algorithm. Brie\ufb02y justifyyour design choices, particularly your choice of inference algorithms for the slaves.", "844": "", "845": "23 Monte Carlo inference\n23.1 Introduction\nSo far, we discussed various deterministic algorithms for posterior inference. These meth-\nods enjoy many of the bene\ufb01ts of the Bayesian approach, while still being about as fast asoptimization-based point-estimation methods. The trouble with these methods is that they canbe rather complicated to derive, and they are somewhat limited in their domain of applicabil-ity (e.g., they usually assume conjugate priors and exponential family likelihoods, although see(Wand et al. 2011) for some recent extensions of mean \ufb01eld to more complex distributions). Fur-thermore, although they are fast, their accuracy is often limited by the form of the approximationwhich we choose.\nIn this chapter, we discuss an alternative class of algorithms based on the idea of Monte Carlo\napproximation, which we \ufb01rst introduced in Section 2.7. The idea is very simple: generate some(unweighted) samples from the posterior, x\ns\u223cp(x|D), and then use these to compute any\nquantity of interest, such as a posterior marginal, p(x1|D), or the posterior of the difference of\ntwo quantities, p(x1\u2212x2|D), or the posterior predictive, p(y|D), etc. All of these quantities\ncan be approximated by E[f|D]\u22481\nS/summationtextS\ns=1f(xs)for some suitable function f.\nBy generating enough samples, we can achieve any desired level of accuracy we like. The main\nissue is: how do we efficiently generate samples from a probability distribution, particularly in\nhigh dimensions? In this chapter, we discuss non-iterative methods for generating independentsamples. In the next chapter, we discuss an iterative method known as Markov Chain MonteCarlo, or MCMC for short, which produces dependent samples but which works well in highdimensions. Note that sampling is a large topic. The reader should consult other books, such as(Liu 2001; Robert and Casella 2004), for more information.\n23.2 Sampling from standard distributions\nWe brie\ufb02y discuss some ways to sample from 1 or 2 dimensional distributions of standard form.These methods are often used as subroutines by more complex methods.\n23.2.1 Using the cdf\nThe simplest method for sampling from a univariate distribution is based on the inverse prob-\nability transform.L e t Fbe a cdf of some distribution we want to sample from, and let F\u22121", "846": "816 Chapter23. MonteCarloinference\n0 x0u1 F\nFigure 23.1 Sampling using an inverse CDF. Figure generated by sampleCdf .\nbe its inverse. Then we have the following result.\nTheorem 23.2.1. IfU\u223cU(0,1)isauniformrv,then F\u22121(U)\u223cF.\nProof.\nPr(F\u22121(U)\u2264x) = Pr( U\u2264F(x))(applying Fto both sides) (23.1)\n=F(x)(because Pr(U\u2264y)=y (23.2)\nwhere the \ufb01rst line follows since Fis a monotonic function, and the second line follows since\nUis uniform on the unit interval.\nHence we can sample from any univariate distribution, for which we can evaluate its inverse\ncdf, as follows: generate a random number u\u223cU(0,1)using apseudo random number\ngenerator (see e.g., (Press et al. 1988) for details). Let urepresent the height up the yaxis. Then\n\u201cslide along\u201d the xaxis until you intersect the Fcurve, and then \u201cdrop down\u201d and return the\ncorresponding xvalue. This corresponds to computing x=F\u22121(u). See Figure 23.1 for an\nillustration.\nFor example, consider the exponential distribution\nExpon(x| \u03bb)/defines\u03bbe\u2212\u03bbxI(x\u22650) (23.3)\nThe cdf is\nF(x)=1\u2212e\u2212\u03bbxI(x\u22650) (23.4)\nwhose inverse is the quantile function\nF\u22121(p)=\u2212ln(1\u2212p)\n\u03bb(23.5)\nBy the above theorem, if U\u223cUnif(0,1), we know that F\u22121(U)\u223cExpon(\u03bb) . Furthermore,\nsince1\u2212U\u223cUnif(0,1)as well, we can sample from the exponential distribution by \ufb01rst\nsampling from the uniform and then transforming the results using \u2212ln(u)/\u03bb.", "847": "23.3. Rejectionsampling 817\n23.2.2 Sampling from a Gaussian (Box-Muller method)\nWe now describe a method to sample from a Gaussian. The idea is we sample uniformly from\na unit radius circle, and then use the change of variables formula to derive samples from aspherical 2d Gaussian. This can be thought of as two samples from a 1d Gaussian.\nIn more detail, sample z\n1,z2\u2208(\u22121,1)uniformly, and then discard pairs that do not satisfy\nz2\n1+z2\n2\u22641. The result will be points uniformly distributed inside the unit circle, so p(z)=\n1\n\u03c0I(zinside circle) . Now de\ufb01ne\nxi=zi/parenleftbigg\u22122lnr2\nr2/parenrightbigg1\n2\n(23.6)\nfori=1:2,w h e r er2=z2\n1+z2\n2. Using the multivariate change of variables formula, we have\np(x1,x2)=p(z1,z2)|\u2202(z1,z2)\n\u2202(x1,x2)|=/bracketleftbigg1\u221a\n2\u03c0exp(\u22121\n2x2\n1)/bracketrightbigg/bracketleftbigg1\u221a\n2\u03c0exp(\u22121\n2x22)/bracketrightbigg\n(23.7)\nHencex1andx2are two independent samples from a univariate Gaussian. This is known as\ntheBox-Muller method.\nTo sample from a multivariate Gaussian, we \ufb01rst compute the Cholesky decomposition of its\ncovariance matrix, \u03a3=LLT,w h e r eLis lower triangular. Next we sample x\u223cN(0,I)using\nthe Box-Muller method. Finally we set y=Lx+\u03bc. This is valid since\ncov[y]=Lcov[x]LT=LILT=\u03a3 (23.8)\n23.3 Rejection sampling\nWhen the inverse cdf method cannot be used, one simple alternative is to use rejection sam-\npling, which we now explain.\n23.3.1 Basic idea\nIn rejection sampling, we create a proposal distribution q(x)which satisifes Mq(x)\u2265\u02dcp(x),\nfor some constant M,w h e r e\u02dcp(x)is an unnormalized version of p(x)(i.e.,p(x)=\u02dcp(x)/Zp\nfor some possibly unknown constant Zp). The function Mq(x)provides an upper envelope for\n\u02dcp. We then sample x\u223cq(x), which corresponds to picking a random xlocation, and then\nwe sample u\u223cU(0,1), which corresponds to picking a random height (y location) under the\nenvelope. If u>\u02dcp(x)\nMq(x), we reject the sample, otherwise we accept it. See Figure 23.2(a). where\nthe acceptance region is shown shaded, and the rejection region is the white region between\nthe shaded zone and the upper envelope.\nWe now prove that this procedure is correct. Let\nS={(x,u):u\u2264\u02dcp(x)/Mq(x)},S0={(x,u):x\u2264x0,u\u2264\u02dcp(x)/Mq(x)} (23.9)", "848": "818 Chapter23. MonteCarloinference\n/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\nxuMq(x  )\nAccept region Reject region\nx  ~ q(x)(i)(i)(i)(i)\np(x  )Mq(x  )\n(a)0 2 4 6 8 1000.20.40.60.811.21.4\n  \ntarget p(x)\ncomparison function Mq(x)\n(b)\nFigure 23.2 (a) Schematic illustration of rejection sampling. Source: Figure 2 of (Andrieu et al. 2003).\nUsed with kind permission of Nando de Freitas. (b) Rejection sampling from a Ga(\u03b1=5.7,\u03bb=2 )\ndistribution (solid blue) using a proposal of the form MGa(k,\u03bb\u22121)(dotted red), where k=\u230a5.7\u230b=5.\nThe curves touch at \u03b1\u2212k=0.7. Figure generated by rejectionSamplingDemo .\nThen the cdf of the accepted points is given by\nP(x\u2264x0|xaccepted)=P(x\u2264x0,xaccepted)\nP(xaccepted)(23.10)\n=/integraltext/integraltext\nI((x,u)\u2208S0)q(x)dudx/integraltext/integraltext\nI((x,u)\u2208S)q(x)dudx=/integraltextx0\n\u2212\u221e\u02dcp(x)dx/integraltext\u221e\n\u2212\u221e\u02dcp(x)dx(23.11)\nwhich is the cdf of p(x), as desired.\nHow efficient is this method? Since we generate with probability q(x)and accept with\nprobability\u02dcp(x)\nMq(x), the probability of acceptance is\np(accept)=/integraldisplay\u02dcp(x)\nMq(x)q(x)dx=1\nM/integraldisplay\n\u02dcp(x)dx (23.12)\nHence we want to choose Mas small as possible while still satisfying Mq(x)\u2265\u02dcp(x).\n23.3.2 Example\nFor example, suppose we want to sample from a Gamma distribution:1\nGa(x|\u03b1,\u03bb)=1\n\u0393(\u03b1)x\u03b1\u22121\u03bb\u03b1exp(\u2212\u03bbx) (23.13)\nOne can show that if Xiiid\u223cExpon(\u03bb), andY=X1+\u00b7\u00b7\u00b7+Xk, thenY\u223cGa(k,\u03bb).F o r\nnon-integer shape parameters, we cannot use this trick. However, we can use rejection sampling\n1. This section is based on notes by Ioana A. Cosma, available at http://users .aims.ac.za/~ioana/cp2 .pdf.", "849": "23.3. Rejectionsampling 819\n(a)\u22128 \u22126 \u22124 \u22122 0 2 4 6 800.10.20.30.40.50.60.70.80.91f(x) half\u2212gaussian\n(b)\u22128 \u22126 \u22124 \u22122 0 2 4 6 801002003004005006007008009001000samples from f(x) (by ARS)\n(c)\nFigure 23.3 (a) Idea behind adaptive rejection sampling. We place piecewise linear upper (and lower)\nbounds on the log-concave density. Based on Figure 1 of (Gilks and Wild 1992). Figure generated by\narsEnvelope . (b-c) Using ARS to sample from a half-Gaussian. Figure generated by arsDemo, written by\nDaniel Eaton.\nusing aGa(k,\u03bb\u22121)distribution as a proposal, where k=\u230a\u03b1\u230b. The ratio has the form\np(x)\nq(x)=Ga(x|\u03b1,\u03bb)\nGa(x|k,\u03bb\u22121)=x\u03b1\u22121\u03bb\u03b1exp(\u2212\u03bbx)/\u0393(\u03b1)\nxk\u22121(\u03bb\u22121)kexp(\u2212(\u03bb\u22121)x)/\u0393(k)(23.14)\n=\u0393(k)\u03bb\u03b1\n\u0393(\u03b1)(\u03bb\u22121)kx\u03b1\u2212kexp(\u2212x) (23.15)\nThis ratio attains its maximum when x=\u03b1\u2212k. Hence\nM=Ga(\u03b1\u2212k|\u03b1,\u03bb)\nGa(\u03b1\u2212k|k,\u03bb\u22121)(23.16)\nSee Figure 23.2(b) for a plot. (Exercise 23.2 asks you to devise a better proposal distribution\nbased on the Cauchy distribution.)\n23.3.3 Application to Bayesian statistics\nSupposewewanttodraw(unweighted)samplesfromtheposterior, p(\u03b8|D)=p(D|\u03b8)p(\u03b8)/p(D).\nWe can use rejection sampling with \u02dcp(\u03b8)=p(D|\u03b8)p(\u03b8)as the target distribution, q(\u03b8)=p(\u03b8)\nas our proposal, and M=p(D|\u02c6\u03b8),w h e r e\u02c6\u03b8= argmax p(D|\u03b8)is the MLE; this was \ufb01rst\nsuggested in (Smith and Gelfand 1992). We accept points with probability\n\u02dcp(\u03b8)\nMq(\u03b8)=p(D|\u03b8)\np(D|\u02c6\u03b8)(23.17)\nThus samples from the prior that have high likelihood are more likely to be retained in theposterior. Of course, if there is a big mismatch between prior and posterior (which will be thecase if the prior is vague and the likelihood is informative), this procedure is very inefficient. Wediscuss better algorithms later.\n23.3.4 Adaptive rejection sampling\nWe now describe a method that can automatically come up with a tight upper envelope q(x)\nto any log concave density p(x). The idea is to upper bound the log density with a piecewise", "850": "820 Chapter23. MonteCarloinference\nlinear function, as illustrated in Figure 23.3(a). We choose the initial locations for the pieces\nbased on a \ufb01xed grid over the support of the distribution. We then evaluate the gradient of thelog density at these locations, and make the lines be tangent at these points.\nSince the log of the envelope is piecewise linear, the envelope itself is piecewise exponential:\nq(x)=M\ni\u03bbiexp(\u2212\u03bbi(x\u2212xi\u22121)),xi\u22121<x\u2264xi (23.18)\nwherexiare the grid points. It is relatively straightforward to sample from this distribution. If\nthe sample xis rejected, we create a new grid point at x, and thereby re\ufb01ne the envelope. As\nthe number of grid points is increased, the tightness of the envelope improves, and the rejectionrate goes down. This is known as adaptive rejection sampling (ARS) (Gilks and Wild 1992).\nFigure 23.3(b-c) gives an example of the method in action. As with standard rejection sampling,it can be applied to unnormalized distributions.\n23.3.5 Rejection sampling in high dimensions\nIt is clear that we want to make our proposal q(x)as close as possible to the target distribution\np(x), while still being an upper bound. But this is quite hard to achieve, especially in high\ndimensions. To see this, consider sampling from p(x)=N(0,\u03c32\npI)using as a proposal\nq(x)=N(0,\u03c32\nqI). Obviously we must have \u03c32\nq\u2265\u03c32\npin order to be an upper bound. In D\ndimensions, the optimum value is given by M=(\u03c3q/\u03c3p)D. The acceptance rate is 1/M(since\nbothpandqare normalized), which decreases exponentially fast with dimension. For example,\nif\u03c3qexceeds\u03c3pby just 1%, then in 1000 dimensions the acceptance ratio will be about 1/20,000.\nThis is a fundamental weakness of rejection sampling.\nIn Chapter 24, we will describe MCMC sampling, which is a more efficient way to sample\nfrom high dimensional distributions. Sometimes this uses (adaptive) rejection sampling as asubroutine, which is known as adaptive rejection Metropolis sampling (Gilks et al. 1995).\n23.4 Importance sampling\nWe now describe a Monte Carlo method known as importance sampling for approximating\nintegrals of the form\nI=E[f]=/integraldisplay\nf(x)p(x)dx (23.19)\n23.4.1 Basic idea\nThe idea is to draw samples xin regions which have high probability, p(x), but also where\n|f(x)|is large. The result can be super efficient, meaning it needs less samples than if we\nwere to sample from the exact distribution p(x). The reason is that the samples are focussed\non the important parts of space. For example, suppose we want to estimate the probability ofarare event. De\ufb01ne f(x)=I(x\u2208E), for some set E. Then it is better to sample from a\nproposal of the form q(x)\u221df(x)p(x)than to sample from p(x)itself.\nImportance sampling samples from any proposal, q(x). It then uses these samples to estimate", "851": "23.4. Importancesampling 821\nthe integral as follows:\nE[f]=/integraldisplay\nf(x)p(x)\nq(x)q(x)dx\u22481\nSS/summationdisplay\ns=1wsf(xs)=\u02c6I (23.20)\nwherews/definesp(xs)\nq(xs)are theimportance weights. Note that, unlike rejection sampling, we use all\nthe samples.\nHow should we choose the proposal? A natural criterion is to minimize the variance of the\nestimate\u02c6I=/summationtext\nswsf(xs).N o w\nvarq(x)[f(x)w(x)] =Eq(x)/bracketleftbig\nf2(x)w2(x)/bracketrightbig\n\u2212I2(23.21)\nSince the last term is independent of q, we can ignore it. By Jensen\u2019s inequality, we have the\nfollowing lower bound:\nEq(x)/bracketleftbig\nf2(x)w2(x)/bracketrightbig\n\u2265(Eq(x)[|f(x)w(x)|])2=/parenleftbigg/integraldisplay\n|f(x)|p(x)dx/parenrightbigg2\n(23.22)\nThe lower bound is obtained when we use the optimal importance distribution:\nq\u2217(x)=|f(x)|p(x)/integraltext\n|f(x/prime)|p(x/prime)dx/prime(23.23)\nWhen we don\u2019t have a particular target function f(x)in mind, we often just try to make\nq(x)as close as possible to p(x). In general, this is difficult, especially in high dimensions, but\nit is possible to adapt the proposal distribution to improve the approximation. This is known as\nadaptive importance sampling (Oh and Berger 1992).\n23.4.2 Handling unnormalized distributions\nIt is frequently the case that we can evaluate the unnormalized target distribution, \u02dcp(x), but not\nits normalization constant, Zp. We may also want to use an unnormalized proposal, \u02dcq(x), with\npossibly unknown normlization constant Zq. We can do this as follows. First we evaluate\nE[f]=Zq\nZp/integraldisplay\nf(x)\u02dcp(x)\n\u02dcq(x)q(x)dx\u2248Zq\nZp1\nSS/summationdisplay\ns=1\u02dcwsf(xs) (23.24)\nwhere\u02dcws/defines\u02dcp(xs)\n\u02dcq(xs)is the unnormalized importance weight. We can use the same set of samples\nto evaluate the ratio Zp/Zqas follows:\nZp\nZq=1\nZq/integraldisplay\n\u02dcp(x)dx=/integraldisplay\u02dcp(x)\n\u02dcq(x)q(x)dx\u22481\nSS/summationdisplay\ns=1\u02dcws (23.25)\nHence\n\u02c6I=1\nS/summationtext\ns\u02dcwsf(xs)\n1\nS/summationtext\ns\u02dcws=S/summationdisplay\ns=1wsf(xs) (23.26)", "852": "822 Chapter23. MonteCarloinference\nwhere\nws/defines\u02dcws/summationtext\ns/prime\u02dcws/prime(23.27)\nare the normalized importance weights. The resulting estimate is a ratio of two estimates, and\nhence is biased. However, as S\u2192\u221e, we have that \u02c6I\u2192I, under weak assumptions (see e.g.,\n(Robert and Casella 2004) for details).\n23.4.3 Importance sampling for a DGM: likelihood weighting\nWe now describe a way to use importance sampling to generate samples from a distributionwhich can be represented as a directed graphical model (Chapter 10).\nIf we have no evidence, we can sample from the unconditional joint distribution of a DGM\np(x)as follows: \ufb01rst sample the root nodes, then sample their children, then sample their\nchildren, etc. This is known as ancestral sampling. It works because, in a DAG, we can always\ntopologically order the nodes so that parents preceed children. (Note that there is no equivalenteasy method for sampling from an unconditional undirected graphical model.)\nNow suppose we have some evidence, so some nodes are \u201cclamped\u201d to observed values, and\nwe want to sample from the posterior p(x|D). If all the variables are discrete, we can use the\nfollowing simple procedure: perform ancestral sampling, but as soon as we sample a value thatis inconsistent with an observed value, reject the whole sample and start again. This is knownaslogic sampling (Henrion 1988).\nNeedless to say, logic sampling is very inefficient, and it cannot be applied when we have\nreal-valued evidence. However, it can be modi\ufb01ed as follows. Sample unobserved variables asbefore, conditional on their parents. But don\u2019t sample observed variables; instead we just usetheir observed values. This is equivalent to using a proposal of the form\nq(x)=/productdisplay\nt/negationslash\u2208Ep(xt|xpa(t))/productdisplay\nt\u2208E\u03b4x\u2217\nt(xt) (23.28)\nwhereEis the set of observed nodes, and x\u2217\ntis the observed value for node t. We should\ntherefore give the overall sample an importance weight as follows:\nw(x)=p(x)\nq(x)=/productdisplay\nt/negationslash\u2208Ep(xt|xpa(t))\np(xt|xpa(t))/productdisplay\nt\u2208Ep(xt|xpa(t))\n1=/productdisplay\nt\u2208Ep(xt|xpa(t)) (23.29)\nThis technique is known as likelihood weighting (Fung and Chang 1989; Shachter and Peot\n1989).\n23.4.4 Sampling importance resampling (SIR)\nWe can draw unweighted samples from p(x)by \ufb01rst using importance sampling (with proposal\nq) to generate a distribution of the form\np(x)\u2248/summationdisplay\nsws\u03b4xs(x) (23.30)", "853": "23.5. Particle\ufb01ltering 823\nwherewsare the normalized importance weights. We then sample with replacement from\nEquation 23.30, where the probability that we pick xsisws. Let this procedure induce a\ndistribution denoted by \u02c6p. To see that this is valid, note that\n\u02c6p(x\u2264x0)=/summationdisplay\nsI(xs\u2264x0)ws=/summationtext\nsI(xs\u2264x0)\u02dcp(xs)/q(xs)/summationtext\ns\u02dcp(xs)/q(xs)(23.31)\n\u2192/integraltext\nI(x\u2264x0)\u02dcp(x)\nq(x)q(x)dx\n/integraltext\u02dcp(x)\nq(x)q(x)dx(23.32)\n=/integraltext\nI(x\u2264x0)\u02dcp(x)dx/integraltext\n\u02dcp(x)dx=/integraldisplay\nI(x\u2264x0)p(x)dx=p(x\u2264x0) (23.33)\nThis is known as sampling importance resampling (SIR) (Rubin 1998). The result is an un-\nweighted approximation of the form\np(x)\u22481\nS/primeS/prime/summationdisplay\ns=1\u03b4xs(x) (23.34)\nNote that we typically take S/prime/lessmuchS.\nThis algorithm can be used to perform Bayesian inference in low-dimensional settings (Smith\nand Gelfand 1992). That is, suppose we want to draw (unweighted) samples from the posterior,\np(\u03b8|D)=p(D|\u03b8)p(\u03b8)/p(D). We can use importance sampling with \u02dcp(\u03b8)=p(D|\u03b8)p(\u03b8)as\nthe unnormalized posterior, and q(\u03b8)=p(\u03b8)as our proposal. The normalized weights have the\nform\nws=\u02dcp(\u03b8s)/q(\u03b8s)/summationtext\ns/prime\u02dcp(\u03b8s/prime)/q(\u03b8s/prime)=p(D|\u03b8s)/summationtext\ns/primep(D|\u03b8s/prime)(23.35)\nWe can then use SIR to sample from p(\u03b8|D).\nOf course, if there is a big discrepancy between our proposal (the prior) and the target (the\nposterior), we will need a huge number of importance samples for this technique to work reliably,since otherwise the variance of the importance weights will be very large, implying that mostsamples carry no useful information. (This issue will come up again in Section 23.5, when wediscuss particle \ufb01ltering.)\n23.5 Particle \ufb01ltering\nParticle \ufb01ltering (PF) is a Monte Carlo, or simulation based, algorithm for recursive Bayesian\ninference. That is, it approximates the predict-update cycle described in Section 18.3.1. It isvery widely used in many areas, including tracking, time-series forecasting, online parameterlearning, etc. We explain the basic algorithm below. For a book-length treatment, see (Doucetet al. 2001); for a good tutorial, see (Arulampalam et al. 2002), or just read on.", "854": "824 Chapter23. MonteCarloinference\n23.5.1 Sequential importance sampling\nThe basic idea is to appproximate the belief state (of the entire state trajectory) using a weighted\nset of particles:\np(z1:t|y1:t)\u2248S/summationdisplay\ns=1\u02c6ws\nt\u03b4zs\n1:t(z1:t) (23.36)\nwhere\u02c6ws\ntis the normalized weight of sample sat timet. From this representation, we can\neasily compute the marginal distribution over the most recent state, p(zt|y1:t), by simply\nignoring the previous parts of the trajectory, z1:t\u22121. (The fact that PF samples in the space of\nentire trajectories has various implications which we will discuss later.)\nWe update this belief state using importance sampling. If the proposal has the form\nq(zs\n1:t|y1:t), then the importance weights are given by\nws\nt\u221dp(zs1:t|y1:t)\nq(zs1:t|y1:t)(23.37)\nwhich can be normalized as follows:\n\u02c6ws\nt=ws\nt/summationtext\ns/primews/prime\nt(23.38)\nWe can rewrite the numerator recursively as follows:\np(z1:t|y1:t)=p(yt|z1:t,y1:t\u22121)p(z1:t|y1:t\u22121)\np(yt|y1:t\u22121)(23.39)\n=p(yt|zt)p(zt|z1:t\u22121,y1:t\u22121)p(z1:t\u22121|y1:t\u22121)\np(yt|y1:t\u22121)(23.40)\n\u221dp(yt|zt)p(zt|zt\u22121)p(z1:t\u22121|y1:t\u22121) (23.41)\nwhere we have made the usual Markov assumptions. We will restrict attention to proposal\ndensities of the following form:\nq(z1:t|y1:t)=q(zt|z1:t\u22121,y1:t)q(z1:t\u22121|y1:t\u22121) (23.42)\nso that we can \u201cgrow\u201d the trajectory by adding the new state ztto the end. In this case, the\nimportance weights simplify to\nws\nt\u221dp(yt|zs\nt)p(zst|zs\nt\u22121)p(zs1:t\u22121|y1:t\u22121)\nq(zs\nt|zs\n1:t\u22121,y1:t)q(zs1:t\u22121|y1:t\u22121)(23.43)\n=ws\nt\u22121p(yt|zs\nt)p(zst|zs\nt\u22121)\nq(zs\nt|zs\n1:t\u22121,y1:t)(23.44)\nIf we further assume that q(zt|z1:t\u22121,y1:t)=q(zt|zt\u22121,yt), then we only need to keep the\nmost recent part of the trajectory and observation sequence, rather than the whole history, in\norder to compute the new sample. In this case, the weight becomes\nws\nt\u221dws\nt\u22121p(yt|zs\nt)p(zst|zs\nt\u22121)\nq(zs\nt|zs\nt\u22121,yt)(23.45)", "855": "23.5. Particle\ufb01ltering 825\nHence we can approximate the posterior \ufb01ltered density using\np(zt|y1:t)\u2248S/summationdisplay\ns=1\u02c6ws\nt\u03b4zs\nt(zt) (23.46)\nAsS\u2192\u221e, one can show that this approaches the true posterior (Crisan et al. 1999).\nThe basic algorithm is now very simple: for each old sample s, propose an extension using\nzs\nt\u223cq(zt|zs\nt\u22121,yt), and give this new particle weight ws\ntusing Equation 23.45. Unfortunately,\nthis basic algorithm does not work very well, as we discuss below.\n23.5.2 The degeneracy problem\nThe basic sequential importance sampling algorithm fails after a few steps because most of\nthe particles will have negligible weight. This is called the degeneracy problem, and occurs\nbecause we are sampling in a high-dimensional space (in fact, the space is growing in size overtime), using a myopic proposal distribution.\nWe can quantify the degree of degeneracy using the effective sample size, de\ufb01ned by\nSeff/definesS\n1+v ar[w\u2217s\nt](23.47)\nwherew\u2217s\nt=p(zs\nt|y1:t)/q(zst|zs\nt\u22121,yt)is the \u201ctrue weight\u201d of particle s. This quantity cannot\nbe computed exactly, since we don\u2019t know the true posterior, but we can approximate it using\n\u02c6Seff=1/summationtextS\ns=1(ws\nt)2(23.48)\nIf the variance of the weights is large, then we are wasting our resources updating particles with\nlow weight, which do not contribute much to our posterior estimate.\nThere are two main solutions to the degeneracy problem: adding a resampling step, and using\na good proposal distribution. We discuss both of these in turn.\n23.5.3 The resampling step\nThe main improvement to the basic SIS algorithm is to monitor the effective sampling size,and whenever it drops below a threshold, to eliminate particles with low weight, and thento create replicates of the surviving particles. (Hence PF is sometimes called survival of the\n\ufb01ttest(Kanazawa et al. 1995).) In particular, we generate a new set {z\ns\u2217\nt}S\ns=1by sampling with\nreplacement Stimes from the weighted distribution\np(zt|y1:t)\u2248S/summationdisplay\ns=1\u02c6ws\nt\u03b4zs\nt(zt) (23.49)\nwhere the probability of choosing particle jfor replication is wj\nt. (This is sometimes called\nrejuvenation.) The result is an iid unweighted sample from the discrete density Equation 23.49,\nso we set the new weights to ws\nt=1/S. This scheme is illustrated in Figure 23.4.", "856": "826 Chapter23. MonteCarloinference\nSURSRVDO\nZHLJKWLQJ\nUHVDPSOH3\u000b]\u000bW\u0010\u0014\f\u0003_\u0003\\\u000b\u0014\u001dW\u0010\u0014\f\f\n3\u000b]\u000bW\f\u0003_\u0003\\\u000b\u0014\u001dW\u0010\u0014\f\f\n3\u000b\\\u000bW\f\u0003_\u0003]\u000bW\f\f\n3\u000b]\u000bW\f\u0003_\u0003\\\u000b\u0014\u001dW\f\f\nFigure 23.4 Illustration of particle \ufb01ltering.\nThere are a variety of algorithms for peforming the resampling step. The simplest is multi-\nnomial resampling, which computes\n(K1,...,K S)\u223cMu(S,(w1\nt,...,wS\nt)) (23.50)\nWe then make Kscopies of zs\nt. Various improvements exist, such as systematic resampling\nresidual resampling, and strati\ufb01ed sampling, which can reduce the variance of the weights.\nAll these methods take O(S)time. See (Doucet et al. 2001) for details.\nThe overall particle \ufb01ltering algorithm is summarized in Algorithm 6. (Note that if an estimate\nof the state is required, it should be computed before the resampling step, since this will result\nin lower variance.)\nAlgorithm 23.1: One step of a generic particle \ufb01lter\n1fors=1:Sdo\n2Drawzs\nt\u223cq(zt|zs\nt\u22121,yt);\n3Compute weight ws\nt\u221dws\nt\u22121p(yt|zs\nt)p(zst|zst\u22121)\nq(zs\nt|zs\nt\u22121,yt);\n4Normalize weights: ws\nt=ws\nt/summationtext\ns/primews/prime\nt;\n5Compute \u02c6Seff=1/summationtextS\ns=1(ws\nt)2;\n6if\u02c6Seff<Sminthen\n7Resample Sindices\u03c0\u223cwt;\n8z:\nt=z\u03c0t;\n9ws\nt=1/S;\nAlthough the resampling step helps with the degeneracy problem, it introduces problems of\nits own. In particular, since the particles with high weight will be selected many times, there is\na loss of diversity amongst the population. This is known as sample impoverishment. In the", "857": "23.5. Particle\ufb01ltering 827\nextreme case of no process noise (e.g., if we have static but unknown parameters as part of the\nstate space), then all the particles will collapse to a single point within a few iterations.\nTo mitigate this problem, several solutions have been proposed. (1) Only resample when\nnecessary, not at every time step. (The original bootstrap \ufb01lter (Gordon 1993) resampled at\nevery step, but this is suboptimal.) (2) After replicating old particles, sample new values usingan MCMC step which leaves the posterior distribution invariant (see e.g., the resample-move\nalgorithm in (Gilks and Berzuini 2001)). (3) Create a kernel density estimate on top of theparticles,\np(z\nt|y1:t)\u2248S/summationdisplay\ns=1ws\nt\u03ba(zt\u2212zs\nt) (23.51)\nwhere\u03bais some smoothing kernel. We then sample from this smoothed distribution. This is\nknown as a regularized particle \ufb01lter (Musso et al. 2001). (4) When performing inference on\nstatic parameters, add some arti\ufb01cial process noise. (If this is undesirable, other algorithms must\nbe used for online parameter estimation, e.g., (Andrieu et al. 2005)).\n23.5.4 The proposal distribution\nThe simplest and most widely used proposal distribution is to sample from the prior:\nq(zt|zs\nt\u22121,yt)=p(zt|zst\u22121) (23.52)\nIn this case, the weight update simpli\ufb01es to\nws\nt\u221dws\nt\u22121p(yt|zst) (23.53)\nThis can be thought of a \u201cgenerate and test\u201d approach: we sample values from the dynamic\nmodel, and then evaluate how good they are after we see the data (see Figure 23.4). Thisis the approach used in the condensation algorithm (which stands for \u201cconditional density\npropagation\u201d) used for visual tracking (Isard and Blake 1998). However, if the likelihood isnarrower than the dynamical prior (meaning the sensor is more informative than the motionmodel, which is often the case), this is a very inefficient approach, since most particles will beassigned very low weight.\nIt is much better to actually look at the data y\ntwhen generating a proposal. In fact, the\noptimal proposal distribution has the following form:\nq(zt|zs\nt\u22121,yt)=p(zt|zst\u22121,yt)=p(yt|zt)p(zt|zs\nt\u22121)\np(yt|zst\u22121)(23.54)\nIf we use this proposal, the new weight is given by\nws\nt\u221dws\nt\u22121p(yt|zs\nt\u22121)=ws\nt\u22121/integraldisplay\np(yt|z/primet)p(z/primet|zst\u22121)dz/primet(23.55)\nThis proposal is optimal since, for any given zs\nt\u22121, the new weight ws\nttakes the same value\nregardless of the value drawn for zs\nt. Hence, conditional on the old values z.\nt\u22121, the variance of\ntrue weights var[w\u2217s\nt],i sz e r o .", "858": "828 Chapter23. MonteCarloinference\nIn general, it is intractable to sample from p(zt|zs\nt\u22121,yt)and to evaluate the integral needed\nto compute the predictive density p(yt|zst\u22121). However, there are two cases when the optimal\nproposal distribution can be used. The \ufb01rst setting is when ztis discrete, so the integral becomes\na sum. Of course, if the entire state space is discrete, we can use an HMM \ufb01lter instead, but\nin some cases, some parts of the state are discrete, and some continuous. The second settingis whenp(z\nt|zs\nt\u22121,yt)is Gaussian. This occurs when the dynamics are nonlinear but the\nobservations are linear. See Exercise 23.3 for the details.\nIn cases where the model is not linear-Gaussian, we may still compute a Gaussian approxima-\ntion top(zt|zst\u22121,yt)using the unscented transform (Section 18.5.2) and use this as a proposal.\nThis is known as the unscented particle \ufb01lter (van der Merwe et al. 2000). In more general\nsettings, we can use other kinds of data-driven proposals, perhaps based on discriminative\nmodels. Unlike MCMC, we do not need to worry about the proposals being reversible.\n23.5.5 Application: robot localization\nConsider a mobile robot wandering around an office environment. We will assume that it already\nhas a map of the world, represented in the form of an occupancy grid, which just speci\ufb01es\nwhether each grid cell is empty space or occupied by an something solid like a wall. The goalis for the robot to estimate its location. This can be solved optimally using an HMM \ufb01lter, sincewe are assuming the state space is discrete. However, since the number of states, K,i so f t e n\nvery large, the O(K\n2)time complexity per update is prohibitive. We can use a particle \ufb01lter as\na sparse approximation to the belief state. This is known as Monte Carlo localization, and is\ndescribed in detail in (Thrun et al. 2006).\nFigure 23.5 gives an example of the method in action. The robot uses a sonar range \ufb01nder,\nso it can only sense distance to obstacles. It starts out with a uniform prior, re\ufb02ecting the factthat the owner of the robot may have turned it on in an arbitrary location. (Figuring out whereyou are, starting from a uniform prior, is called global localization.) After the \ufb01rst scan, which\nindicates two walls on either side, the belief state is shown in (b). The posterior is still fairlybroad, since the robot could be in any location where the walls are fairly close by, such as acorridor or any of the narrow rooms. After moving to location 2, the robot is pretty sure it mustbe in the corridor, as shown in (c). After moving to location 3, the sensor is able to detect theend of the corridor. However, due to symmetry, it is not sure if it is in location I (the truelocation) or location II. (This is an example of perceptual aliasing, which refers to the fact that\ndifferent things may look the same.) After moving to locations 4 and 5, it is \ufb01nally able to \ufb01gureout precisely where it is. The whole process is analogous to someone getting lost in an officebuilding, and wandering the corridors until they see a sign they recognize.\nIn Section 23.6.3, we discuss how to estimate location and the map at the same time.\n23.5.6 Application: visual object tracking\nOur next example is concerned with tracking an object (in this case, a remote-controlled heli-copter) in a video sequence. The method uses a simple linear motion model for the centroidof the object, and a color histogram for the likelihood model, using Bhattacharya distance to\ncompare histograms. The proposal distribution is obtained by sampling from the likelihood. See(Nummiaro et al. 2003) for further details.", "859": "23.5. Particle\ufb01ltering 829\n3 4Room A\nRoom BStart 2 15\nRoom C\nI\nII\nIII(a) Path and reference poses\n(c) Belief at reference pose 2\n(e) Belief at reference pose 4(b) Belief at reference pose 1\n(d) Belief at reference pose 3\n(f) Belief at reference pose 5\nFigure 23.5 Illustration of Monte Carlo localization. Source: Figure 8.7 of (Thrun et al. 2006). Used\nwith kind permission of Sebastian Thrun.\nFigure 23.6 shows some example frames. The system uses S= 250particles, with an effective\nsample size of \u02c6Seff= 134. (a) shows the belief state at frame 1. The system has had to resample\n5 times to keep the effective sample size above the threshold of 150; (b) shows the belief state\nat frame 251; the red lines show the estimated location of the center of the object over the last250 frames. (c) shows that the system can handle visual clutter, as long as it does not have thesame color as the target object. (d) shows that the system is confused between the grey of thehelicopter and the grey of the building. The posterior is bimodal. The green ellipse, representingthe posterior mean and covariance, is in between the two modes. (e) shows that the probabilitymass has shifted to the wrong mode: the system has lost track. (f) shows the particles spreadout over the gray building; recovery of the object is very unlikely from this state using this", "860": "830 Chapter23. MonteCarloinference\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 23.6 Example of particle \ufb01ltering applied to visual object tracking, based on color histograms.\n(a-c) succesful tracking: green ellipse is on top of the helicopter. (d-f): tracker gets distracted by gray clutter\nin the background. See text for details. Figure generated by pfColorTrackerDemo , written by Sebastien\nParis.\nproposal.\nWe see that the method is able to keep track for a fairly long time, despite the presence\nof clutter. However, eventually it loses track of the object. Note that since the algorithm is\nstochastic, simply re-running the demo may \ufb01x the problem. But in the real world, this is not\nan option. The simplest way to improve performance is to use more particles. An alternative\nis to perform tracking by detection , by running an object detector over the image every few\nframes. See (Forsyth and Ponce 2002; Szeliski 2010; Prince 2012) for details.", "861": "23.6. Rao-Blackwellisedparticle\ufb01ltering(RBPF) 831\n23.5.7 Application: time series forecasting\nIn Section 18.2.4, we discussed how to use the Kalman \ufb01lter to perform time series forecasting.\nThis assumes that the model is a linear-Gaussian state-space model. There are many modelswhich are either non-linear and/or non-Gaussian. For example, stochastic volatility models,\nwhich are widely used in \ufb01nance, assume that the variance of the system and/or observationnoise changes over time. Particle \ufb01ltering is widely used in such settings. See e.g., (Doucet et al.2001) and references therein for details.\n23.6 Rao-Blackwellised particle \ufb01ltering (RBPF)\nIn some models, we can partition the hidden variables into two kinds, qtandzt, such that\nwe can analytically integrate out ztprovided we know the values of q1:t. This means we only\nhave sample q1:t, and can represent p(zt|q1:t)parametrically. Thus each particle srepresents\na value for qs\n1:tand a distribution of the form p(zt|y1:t,qs1:t). These hybrid particles are are\nsometimes called distributional particles orcollapsed particles (Koller and Friedman 2009,\nSec 12.4).\nThe advantage of this approach is that we reduce the dimensionality of the space in which\nwe are sampling, which reduces the variance of our estimate. Hence this technique is known\nasRao-Blackwellised particle \ufb01ltering orRBPFfor short, named after Theorem 24.20. The\nmethod is best explained using a speci\ufb01c example.\n23.6.1 RBPF for switching LG-SSMs\nA canonical example for which RBPF can be applied is the switching linear dynamical system(SLDS) model discussed in Section 18.6 (Chen and Liu 2000; Doucet et al. 2001). We can representp(z\nt|y1:t,qs\n1:t)using a mean and covariance matrix for each particle s,w h e r eqt\u2208{1,...,K}.\nIf we propose from the prior, q(qt=k|qs\nt\u22121), the weight update becomes\nws\nt\u221dws\nt\u22121p(yt|qt=k,qs\n1:t\u22121,y1:t\u22121)=ws\nt\u22121Lst,k(23.56)\nwhere\nLstk=/integraldisplay\np(yt|qt=k,zt,\u0018\u0018\u0018y1:t\u22121,\b\b\bqs1:t\u22121)p(zt|qt=k,y1:t\u22121qs1:t\u22121,)dzt (23.57)\nThe quantity Ls\ntkis the predictive density for the new observation ytconditioned on qt=kand\nthe history qs\n1:t\u22121. In the case of SLDS models, this can be computed using the normalization\nconstant of the Kalman \ufb01lter, Equation 18.41.\nWe give some pseudo-code in Algorithm 8. (The step marked \u201cKFupdate\u201d refers to the Kalman\n\ufb01lter update equations in Section 18.3.1.) This is known as a mixture of Kalman \ufb01lters.\nIfKis small, we can compute the optimal proposal distribution, which is\np(qt=k|y1:t,qs\n1:t\u22121)=\u02c6pst\u22121(qt=k|yt) (23.58)\n=\u02c6ps\nt\u22121(yt|qt=k)\u02c6pst\u22121(qt=k)\n\u02c6pst\u22121(yt)(23.59)\n=Ls\ntkp(qt=k|qs\nt\u22121)/summationtext\nk/primeLstk/primep(qt=k/prime|qs\nt\u22121)(23.60)", "862": "832 Chapter23. MonteCarloinference\nAlgorithm 23.2: One step of RBPF for SLDS using prior as proposal\n1fors=1:Sdo\n2k\u223cp(qt|qs\nt\u22121);\n3qs\nt:=k;\n4(\u03bcs\nt,\u03a3s\nt,Ls\ntk)=KFupdate( \u03bcs\nt\u22121,\u03a3s\nt\u22121,yt,\u03b8k);\n5ws\nt=ws\nt\u22121Lk\nts;\n6Normalize weights: ws\nt=ws\nt/summationtext\ns/primews/prime\nt;\n7Compute \u02c6Seff=1/summationtextS\ns=1(ws\nt)2;\n8if\u02c6Seff<Sminthen\n9Resample Sindices\u03c0\u223cwt;\n10q:\nt=q\u03c0t,\u03bc:t=\u03bc\u03c0t,\u03a3:\nt=\u03a3\u03c0t,;\n11ws\nt=1/S;\nwhere we use the following shorthand:\n\u02c6ps\nt\u22121(\u00b7)=p(\u00b7|y1:t\u22121,qs1:t\u22121) (23.61)\nWe then sample from p(qt|qs\n1:t\u22121,y1:t)and give the resulting particle weight\nws\nt\u221dws\nt\u22121p(yt|qs\n1:t\u22121,y1:t\u22121)=ws\nt\u22121/summationdisplay\nk/bracketleftbig\nLstkp(qt=k|qs\nt\u22121)/bracketrightbig\n(23.62)\nSince the weights of the particles in Equation 23.62 are independent of the new value that is\nactually sampled for qt, we can compute these weights \ufb01rst, and use them to decide which\nparticles to propagate. That is, we choose the \ufb01ttest particles at time t\u22121using information\nfrom time t. This is called look-ahead RBPF (de Freitas et al. 2004).\nIn more detail, the idea is this. We pass each sample in the prior through all Kmodels\nto getKposteriors, one per sample. The normalization constants of this process allow us to\ncompute the optimal weights in Equation 23.62. We then resample Sindices. Finally, for each\nold particle sthat is chosen, we sample one new state qs\nt=k, and use the corresponding\nposterior from the Kpossible alternative that we have already computed. The pseudo-code is\nshown in Algorithm 7. This method needs O(KS)storage, but has the advantage that each\nparticle is chosen using the latest information, yt.\nA further improvement can be obtained by exploiting the fact that the state space is discrete.\nHence we can use the resampling method of (Fearnhead 2004) which avoids duplicating particles.\n23.6.2 Application: tracking a maneuvering target\nOne application of SLDS is to track moving objects that have piecewise linear dynamics. Forexample, suppose we want to track an airplane or missile; q\ntcan specify if the object is \ufb02ying\nnormally or is taking evasive action. This is called maneuvering target tracking.\nFigure 23.7 gives an example of an object moving in 2d. The setup is essentially the same\nas in Section 18.2.1, except that we add a three-state discrete Markov chain which controls the", "863": "23.6. Rao-Blackwellisedparticle\ufb01ltering(RBPF) 833\nAlgorithm 23.3: One step of look-ahead RBPF for SLDS using optimal proposal\n1fors=1:Sdo\n2fork=1:Kdo\n3 (\u03bcs\ntk,\u03a3s\ntk,Lk\nts)=KFupdate( \u03bcs\nt\u22121,\u03a3s\nt\u22121,yt,\u03b8k);\n4ws\nt=ws\nt\u22121[/summationtext\nkLk\ntsp(qt=k|qs\nt\u22121)];\n5Normalize weights: ws\nt=ws\nt/summationtext\ns/primews/prime\nt;\n6Resample Sindices\u03c0\u223cwt;\n7fors\u2208\u03c0do\n8Compute optimal proposal p(k|qs\n1:t\u22121,y1:t)=Ls\ntkp(qt=k|qs\nt\u22121)/summationtext\nk/primeLs\ntkp(qt=k|qs\nt\u22121);\n9Samplek\u223cp(k|qs\n1:t\u22121,y1:t);\n10qs\nt=k,\u03bcs\nt=\u03bcs\ntk,\u03a3s\nt=\u03a3stk;\n11ws\nt=1/S;\nMethod misclassi\ufb01cation rate MSE Time (seconds)\nPF 0.440 21.051 6.086\nRBPF 0.340 18.168 10.986\nTable 23.1 Comparison of PF an RBPF on the maneuvering target problem in Figure 23.7.\ninput to the system. We de\ufb01ne ut=1and set\nB1=( 0,0,0,0)T,B2=(\u22121.225,\u22120.35,1.225,0.35)T,B3=( 1.225,0.35,\u22121.225,\u22120.35)T\nso the system will turn in different directions depending on the discrete state.\nFigure 23.7(a) shows the true state of the system from a sample run, starting at (0,0): the\ncolored symbols denote the discrete state, and the location of the symbol denotes the (x,y)\nlocation. The small dots represent noisy observations. Figure 23.7(b) shows the estimate of\nthe state computed using particle \ufb01ltering with 500 particles, where the proposal is to samplefrom the prior. The colored symbols denote the MAP estimate of the state, and the location ofthe symbol denotes the MMSE (minimum mean square error) estimate of the location, which isgiven by the posterior mean. Figure 23.7(c) shows the estimate computing using RBPF with 500particles, using the optimal proposal distribution. A more quantitative comparison is shown inTable 23.1. We see that RBPF has slightly better performance, although it is also slightly slower.\nFigure 23.8 visualizes the belief state of the system. In (a) we show the distribution over the\ndiscrete states. We see that the particle \ufb01lter estimate of the belief state (second column) is notas accurate as the RBPF estimate (third column) in the beginning, although after the \ufb01rst fewobservations performance is similar for both methods. In (b), we plot the posterior over the x\nlocations. For simplicity, we use the PF estimate, which is a set of weighted samples, but wecould also have used the RBPF estimate, which is a set of weighted Gaussians.", "864": "834 Chapter23. MonteCarloinference\n\u221290 \u221280 \u221270 \u221260 \u221250 \u221240 \u221230 \u221220 \u221210 0\u2212250\u2212200\u2212150\u2212100\u221250050data\n(a)\u221280 \u221270 \u221260 \u221250 \u221240 \u221230 \u221220 \u221210 0\u2212250\u2212200\u2212150\u2212100\u221250050pf, mse 21.051\n(b)\n\u221280 \u221270 \u221260 \u221250 \u221240 \u221230 \u221220 \u221210 0\u2212250\u2212200\u2212150\u2212100\u221250050rbpf, mse 18.168\n(c)\nFigure 23.7 (a) A maneuvering target. The colored symbols represent the hidden discrete state. (b)\nParticle \ufb01lter estimate. (c) RBPF estimate. Figure generated by rbpfManeuverDemo , based on code by\nNando de Freitas.\n23.6.3 Application: Fast SLAM\nIn Section 18.2.2, we introduced the problem of simultaneous localization and mapping or SLAM\nfor mobile robotics. The main problem with the Kalman \ufb01lter implementation is that it is cubicin the number of landmarks. However, by looking at the DGM in Figure 18.2, we see that,conditional on knowing the robot\u2019s path, q\n1:t,w h e r eqt\u2208R2, the landmark locations z\u2208R2L\nare independent. (We assume the landmarks don\u2019t move, so we drop the tsubscript). That is,\np(z|q1:t,y1:t)=/producttextL\nl=1p(zl|q1:t,y1:t). Consequently we can use RBPF, where we sample the\nrobot\u2019s trajectory, q1:t, and we run Lindependent 2d Kalman \ufb01lters inside each particle. This\ntakesO(L)time per particle. Fortunately, the number of particles needed for good performance\nis quite small (this partly depends on the control / exploration policy), so the algorithm is\nessentially linear in the number of particles. This technique has the additional advantage that", "865": "23.6. Rao-Blackwellisedparticle\ufb01ltering(RBPF) 835\ntruth\n1 2 310\n20\n30\n40\n50\n60\n70\n80\n90\n100\npf, error rate 0.440\n1 2 310\n20\n30\n40\n50\n60\n70\n80\n90\n100\nrbpf, error rate 0.340\n1 2 310\n20\n30\n40\n50\n60\n70\n80\n90\n100\n(a)\u221280\u221260\u221240\u2212200\n02040608010000.51\nx1,tPF\ntp(x1,t|y1:t)\n(b)\nFigure 23.8 Belief states corresponding to Figure 23.7. (a) Discrete state. The system starts in state\n2 (red x in Figure 23.7), then moves to state 3 (black * in Figure 23.7), returns brie\ufb02y to state 2, then\nswitches to state 1 (blue circle in Figure 23.7), etc. (b) Horizontal location (PF estimate). Figure generated\nbyrbpfManeuverDemo , based on code by Nando de Freitas.\nit is easy to use sampling to handle the data association ambiguity, and that it allows for other\nrepresentations of the map, such as occupancy grids. This idea was \ufb01rst suggested in (Murphy\n2000), and was subsequently extended and made practical in (Thrun et al. 2004), who christened\nthe technique FastSLAM . See rbpfSlamDemo for a simple demo in a discrete grid world.\nExercises\nExercise 23.1 Sampling from a Cauchy\nShow how to use inverse probability transform to sample from a standard Cauchy, T(x|0,1,1).\nExercise 23.2 Rejection sampling from a Gamma using a Cauchy proposal\nShow how to use a Cauchy proposal to perform rejection sampling from a Gamma distribution. Derive the\noptimal constant M, and plot the density and its upper envelope.\nExercise 23.3 Optimal proposal for particle \ufb01ltering with linear-Gaussian measurement model\nConsider a state-space model of the following form:\nzt=ft(zt\u22121)+N(0,Qt\u22121) (23.63)\nyt=Htzt+N(0,Rt) (23.64)\nDerive expressions for p(zt|zt\u22121,yt)andp(yt|zt\u22121), which are needed to compute the optimal (minimum\nvariance) proposal distribution. Hint: use Bayes rule for Gaussians.", "866": "", "867": "24Markov chain Monte Carlo (MCMC)\ninference\n24.1 Introduction\nIn Chapter 23, we introduced some simple Monte Carlo methods, including rejection sampling\nand importance sampling. The trouble with these methods is that they do not work well in highdimensional spaces. The most popular method for sampling from high-dimensional distributionsisMarkov chain Monte Carlo orMCMC.I nas u r v e yb ySIAMNews\n1, MCMC was placed in the\ntop 10 most important algorithms of the 20th century.\nThe basic idea behind MCMC is to construct a Markov chain (Section 17.2) on the state space\nXwhose stationary distribution is the target density p\u2217(x)of interest (this may be a prior or a\nposterior). That is, we perform a random walk on the state space, in such a way that the fractionof time we spend in each state xis proportional to p\n\u2217(x). By drawing (correlated!) samples\nx0,x1,x2,...,from the chain, we can perform Monte Carlo integration wrt p\u2217. We give the\ndetails below.\nThe MCMC algorithm has an interesting history. It was discovered by physicists working\non the atomic bomb at Los Alamos during World War II, and was \ufb01rst published in the openliterature in (Metropolis et al. 1953) in a chemistry journal. An extension was published inthe statistics literature in (Hastings 1970), but was largely unnoticed. A special case (Gibbssampling, Section 24.2) was independently invented in 1984 in the context of Ising models andwas published in (Geman and Geman 1984). But it was not until (Gelfand and Smith 1990) thatthe algorithm became well-known to the wider statistical community. Since then it has becomewildly popular in Bayesian statistics, and is becoming increasingly popular in machine learning.\nIt is worth brie\ufb02y comparing MCMC to variational inference (Chapter 21). The advantages\nof variational inference are (1) for small to medium problems, it is usually faster; (2) it isdeterministic; (3) is it easy to determine when to stop; (4) it often provides a lower bound onthe log likelihood. The advantages of sampling are: (1) it is often easier to implement; (2) itis applicable to a broader range of models, such as models whose size or structure changesdepending on the values of certain variables (e.g., as happens in matching problems), or modelswithout nice conjugate priors; (3) sampling can be faster than variational methods when appliedto really huge models or datasets.\n2\n1. Source: http://www .siam.org/pdf/news/637 .pdf.\n2. The reason is that sampling passes speci\ufb01c values of variables (or sets of variables), whereas in variational inference,\nwe pass around distributions. Thus sampling passes sparse messages, whereas variational inference passes dense\nmessages For comparisons of the two approaches, see e.g., (Yoshida and West 2010) and articles in (Bekkerman et al.", "868": "838 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n24.2 Gibbs sampling\nIn this section, we present one of the most popular MCMC algorithms, known as Gibbs sam-\npling.3(In physics, this method is known as Glauber dynamics or theheat bath method.) This\nis the MCMC analog of coordinate descent.\n24.2.1 Basic idea\nThe idea behind Gibbs sampling is that we sample each variable in turn, conditioned on the\nvalues of all the other variables in the distribution. That is, given a joint sample xsof all the\nvariables, we generate a new sample xs+1by sampling each component in turn, based on the\nmost recent values of the other variables. For example, if we have D=3variables, we use\n\u2022xs+1\n1\u223cp(x1|xs\n2,xs3)\n\u2022xs+1\n2\u223cp(x2|xs+11,xs\n3)\n\u2022xs+1\n3\u223cp(x3|xs+11,xs+12)\nThis readily generalizes to Dvariables. If xiis a visible variable, we do not sample it, since its\nvalue is already known.\nThe expression p(xi|x\u2212i)is called the full conditional for variable i. In general, ximay only\ndepend on some of the other variables. If we represent p(x)as a graphical model, we can infer\nthe dependencies by looking at i\u2019s Markov blanket, which are its neighbors in the graph. Thus\nto sample xi, we only need to know the values of i\u2019s neighbors. In this sense, Gibbs sampling\nis a distributed algorithm. However, it is not a parallel algorithm, since the samples must be\ngenerated sequentially.\nFor reasons that we will explain in Section 24.4.1, it is necessary to discard some of the\ninitial samples until the Markov chain has burned in, or entered its stationary distribution. We\ndiscuss how to estimate when burnin has occured in Section 24.4.1. In the examples below, wejust discard the initial 25% of the samples, for simplicity.\n24.2.2 Example: Gibbs sampling for the Ising model\nIn Section 21.3.2, we applied mean \ufb01eld to an Ising model. Here we apply Gibbs sampling.\nGibbs sampling in pairwise MRF/CRF takes the form\np(xt|x\u2212t,\u03b8)\u221d/productdisplay\ns\u2208nbr(t)\u03c8st(xs,xt) (24.1)\nIn the case of an Ising model with edge potentials \u03c8(xs,xt)=e x p ( Jxsxt),w h e r ext\u2208\n2011)\n3. Josiah Willard Gibbs, 1839\u20131903, was an American physicist.", "869": "24.2. Gibbssampling 839\nsamp le 1, Gibb s\n  \n\u22121\u22120.500.51\n(a)\nsample 5, Gibbs\n  \n\u22121\u22120.500.51\n(b)\nmean after 15 sweeps of Gibbs\n  \n\u22121\u22120.500.51\n(c)\nFigure 24.1 Example of image denoising. We use an Ising prior with Wij=J=1and a Gaussian\nnoise model with \u03c3=2. We use Gibbs sampling (Section 24.2) to perform approximate inference. (a)\nSample from the posterior after one sweep over the image. (b) Sample after 5 sweeps. (c) Posterior mean,\ncomputed by averaging over 15 sweeps. Compare to Figure 21.3 which shows the results of using mean\n\ufb01eld inference. Figure generated by isingImageDenoiseDemo .\n{\u22121,+1}, the full conditional becomes\np(xt=+ 1|x\u2212t,\u03b8)=/producttext\ns\u2208nbr(t)\u03c8st(xt=+ 1,xs)\n/producttext\ns\u2208nbr(t)\u03c8(st=+ 1,xs)+/producttext\ns\u2208nbr(t)\u03c8(xt=\u22121,xs)(24.2)\n=exp[J/summationtext\ns\u2208nbr(t)xs]\nexp[J/summationtext\ns\u2208nbr(t)xs]+exp[\u2212J/summationtext\ns\u2208nbr(t)xs](24.3)\n=exp[J\u03b7t]\nexp[J\u03b7t]+exp[\u2212J\u03b7t]=s i g m ( 2 J\u03b7t) (24.4)\nwhereJis the coupling strength, \u03b7t/defines/summationtext\ns\u2208nbr(t)xtandsigm(u)=1/(1+e\u2212u)is the sigmoid\nfunction. It is easy to see that \u03b7t=xt(at\u2212dt),w h e r eatis the number of neighbors that agree\nwith (have the same sign as) t, anddtis the number of neighbors who disagree. If this number\nis equal, the \u201cforces\u201d on xtcancel out, so the full conditional is uniform.\nWe can combine an Ising prior with a local evidence term \u03c8t. For example, with a Gaussian\nobservation model, we have \u03c8t(xt)=N(yt|xt,\u03c32). The full conditional becomes\np(xt=+ 1|x\u2212t,y,\u03b8)=exp[J\u03b7t]\u03c8t(+1)\nexp[J\u03b7t]\u03c8t(+1)+exp[\u2212J\u03b7t]\u03c8t(\u22121)(24.5)\n=s i g m/parenleftbigg\n2J\u03b7t\u2212log\u03c8t(+1)\n\u03c8t(\u22121)/parenrightbigg\n(24.6)\nNow the probability of xtentering each state is determined both by compatibility with its\nneighbors (the Ising prior) and compatibility with the data (the local likelihood term).\nSee Figure 24.1 for an example of this algorithm applied to a simple image denoising problem.\nThe results are similar to mean \ufb01eld (Figure 21.3) except that the \ufb01nal estimate (based on\naveraging the samples) is somewhat \u201cblurrier\u201d, due to the fact that mean \ufb01eld tends to be\nover-con\ufb01dent.", "870": "840 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n24.2.3 Example: Gibbs sampling for inferring the parameters of a GMM\nIt is straightforward to derive a Gibbs sampling algorithm to \u201c\ufb01t\u201d a mixture model, especially\nif we use conjugate priors. We will focus on the case of mixture of Gaussians, although theresults are easily extended to other kinds of mixture models. (The derivation, which follows fromthe results of Section 4.6, is much easier than the corresponding variational Bayes algorithm inSection 21.6.1.)\nSuppose we use a semi-conjugate prior. Then the full joint distribution is given by\np(x,z,\u03bc,\u03a3,\u03c0)=p( x|z,\u03bc,\u03a3)p(z|\u03c0)p(\u03c0)\nK/productdisplay\nk=1p(\u03bck)p(\u03a3k) (24.7)\n=/parenleftBiggN/productdisplay\ni=1K/productdisplay\nk=1(\u03c0kN(xi|\u03bck,\u03a3k))I(zi=k)/parenrightBigg\n\u00d7 (24.8)\nDir(\u03c0|\u03b1)K/productdisplay\nk=1N(\u03bck|m0,V0)IW(\u03a3 k|S0,\u03bd0) (24.9)\nWe use the same prior for each mixture component. The full conditionals are as follows. Forthe discrete indicators, we have\np(z\ni=k|xi,\u03bc,\u03a3,\u03c0)\u221d\u03c0kN(xi|\u03bck,\u03a3k) (24.10)\nFor the mixing weights, we have (using results from Section 3.4)\np(\u03c0|z)=D i r ({\u03b1k+N/summationdisplay\ni=1I(zi=k)}K\nk=1) (24.11)\nFor the means, we have (using results from Section 4.6.1)\np(\u03bck|\u03a3k,z,x)=N (\u03bck|mk,Vk) (24.12)\nV\u22121\nk=V\u22121\n0+Nk\u03a3\u22121\nk (24.13)\nmk=Vk(\u03a3\u22121\nkNkxk+V\u22121\n0m0) (24.14)\nNk/definesN/summationdisplay\ni=1I(zi=k) (24.15)\nxk/defines/summationtextN\ni=1I(zi=k)xi\nNk(24.16)\nFor the covariances, we have (using results from Section 4.6.2)\np(\u03a3k|\u03bck,z,x)=I W ( \u03a3k|Sk,\u03bdk) (24.17)\nSk=S0+N/summationdisplay\ni=1I(zi=k)(xi\u2212\u03bck)(xi\u2212\u03bck)T(24.18)\n\u03bdk=\u03bd0+Nk (24.19)\nSeegaussMissingFitGibbs for some Matlab code. (This code can also sample missing\nvalues for x, if necessary.)", "871": "24.2. Gibbssampling 841\n24.2.3.1 Label switching\nAlthough it is simple to implement, Gibbs sampling for mixture models has a fundamental\nweakness. The problem is that the parameters of the model \u03b8, and the indicator functions z,a r e\nunidenti\ufb01able, since we can arbitrarily permute the hidden labels without affecting the likelihood(see Section 11.3.1). Consequently, we cannot just take a Monte Carlo average of the samples tocompute posterior means, since what one sample considers the parameters for cluster 1 may bewhat another sample considers the parameters for cluster 2. Indeed, if we could average overall modes, we would \ufb01nd E[\u03bc\nk|D]is the same for all k(assuming a symmetric prior). This is\ncalled the label switching problem.\nThis problem does not arise in EM or VBEM, which just \u201clock on\u201d to a single mode. However,\nit arises in any method that visits multiple modes. In 1d problems, one can try to prevent thisproblem by introducing constraints on the parameters to ensure identi\ufb01ability, e.g., \u03bc\n1<\u03bc2<\n\u03bc3(Richardson and Green 1997). However, this does not always work, since the likelihood might\noverwhelm the prior and cause label switching anyway. Furthermore, this technique does notscale to higher dimensions. Another approach is to post-process the samples by searching for aglobal label permutation to apply to each sample that minimizes some loss function (Stephens2000); however, this can be slow.\nPerhaps the best solution is simply to \u201cnot ask\u201d questions that cannot be uniquely identi\ufb01ed.\nFor example, instead of asking for the probability that data point ibelongs to cluster k, ask\nfor the probability that data points iandjbelong to the same cluster. The latter question is\ninvariant to the labeling. Furthermore, it only refers to observable quantities (are iandjgrouped\ntogether or not), rather than referring to unobservable quantities, such as latent clusters. Thisapproach has the further advantage that it extends to in\ufb01nite mixture models, discussed inSection 25.2, where Kis unbounded; in such models, the notion of a hidden cluster is not well\nde\ufb01ned, but the notion of a partitioning of the data iswell de\ufb01ned\n24.2.4 Collapsed Gibbs sampling *\nIn some cases, we can analytically integrate out some of the unknown quantities, and justsample the rest. This is called a collapsed Gibbs sampler, and it tends to be much more\nefficient, since it is sampling in a lower dimensional space.\nMore precisely, suppose we sample zand integrate out \u03b8. Thus the \u03b8parameters do not\nparticipate in the Markov chain; consequently we can draw conditionally independent samples\u03b8\ns\u223cp(\u03b8|zs,D), which will have much lower variance than samples drawn from the joint state\nspace (Liu et al. 1994). This process is called Rao-Blackwellisation, named after the following\ntheorem:\nTheorem 24.2.1 (Rao-Blackwell). Letzand\u03b8be dependent random variables, and f(z,\u03b8)be\nsomescalarfunction. Then\nvarz,\u03b8[f(z,\u03b8)]\u2265varz[E\u03b8[f(z,\u03b8)|z]] (24.20)\nThis theorem guarantees that the variance of the estimate created by analytically integrating\nout\u03b8will always be lower (or rather, will never be higher) than the variance of a direct MC\nestimate. In collapsed Gibbs, we sample zwith\u03b8integrated out; the above Rao-Blackwell\ntheorem still applies in this case (Liu et al. 1994).", "872": "842 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n\u03b1\n\u03c0\nz1zizN\nx1xixN\n\u03b8K\n\u03b2\n(a)\u03b1\nz1zizN\nx1xixN\n\u03b2\n(b)\nFigure 24.2 (a) A mixture model. (b) After integrating out the parameters.\nWe will encounter Rao-Blackwellisation again in Section 23.6. Although it can reduce statistical\nvariance, it is only worth doing if the integrating out can be done quickly, otherwise we will not\nbe able to produce as many samples per second as the naive method. We give an example ofthis below.\n24.2.4.1 Example: collapsed Gibbs for \ufb01tting a GMM\nConsider a GMM with a fully conjugate prior. In this case we can analytically integrate out themodel parameters \u03bc\nk,\u03a3kand\u03c0, and just sample the indicators z. Once we integrate out \u03c0,\nall thezinodes become inter-dependent. Similarly, once we integrate out \u03b8k, all thexinodes\nbecome inter-dependent, as shown in Figure 24.2(b). Nevertheless, we can easily compute thefull conditionals as follows:\np(z\ni=k|z\u2212i,x,\u03b1,\u03b2)\u221dp(zi=k|z\u2212i,\u03b1,\u0013\u0013\u03b2)p(x|zi=k,z\u2212i,\u001a\u03b1,\u03b2) (24.21)\n\u221dp(zi=k|z\u2212i,\u03b1)p(xi|x\u2212i,zi=k,z\u2212i,\u03b2)\np(x\u2212i|\u0018\u0018\u0018zi=k,z\u2212i,\u03b2) (24.22)\n\u221dp(zi=k|z\u2212i,\u03b1)p(xi|x\u2212i,zi=k,z\u2212i,\u03b2) (24.23)\nwhere\u03b2=(m0,V0,S0,\u03bd0)are the hyper-parameters for the class-conditional densities. The\n\ufb01rst term can be obtained by integrating out \u03c0. Suppose we use a symmetric prior of the form\n\u03c0\u223cDir(\u03b1),w h e r e\u03b1 k=\u03b1/K. From Equation 5.26 we have\np(z1,...,z N|\u03b1)=\u0393(\u03b1)\n\u0393(N+\u03b1)K/productdisplay\nk=1\u0393(Nk+\u03b1/K)\n\u0393(\u03b1/K)(24.24)", "873": "24.2. Gibbssampling 843\nHence\np(zi=k|z\u2212i,\u03b1)=p(z1:N|\u03b1)\np(z\u2212i|\u03b1)=1\n\u0393(N+\u03b1)\n1\n\u0393(N+\u03b1\u22121)\u00d7\u0393(Nk+\u03b1/K)\n\u0393(Nk,\u2212i+\u03b1/K)(24.25)\n=\u0393(N+\u03b1\u22121)\n\u0393(N+\u03b1)\u0393(Nk,\u2212i+1+\u03b1/K)\n\u0393(Nk,\u2212i+\u03b1/K)=Nk,\u2212i+\u03b1/K\nN+\u03b1\u22121(24.26)\nwhereNk,\u2212i/defines/summationtext\nn/negationslash=iI(zn=k)=Nk\u22121, and where we exploited the fact that \u0393(x+1)=\nx\u0393(x).\nTo obtain the second term in Equation 24.23, which is the posterior predictive distribution for\nxigiven all the other data and all the assignments, we use the fact that\np(xi|x\u2212i,z\u2212i,zi=k,\u03b2)=p(xi|D\u2212i,k) (24.27)\nwhereD\u2212i,k={xj:zj=k,j/negationslash=i}is all the data assigned to cluster kexcept for xi.I fw e\nuse a conjugate prior for \u03b8k, we can compute p(xi|D\u2212i,k)in closed form. Furthermore, we can\nefficiently update these predictive likelihoods by caching the sufficient statistics for each cluster.\nTo compute the above expression, we remove xi\u2019s statistics from its current cluster (namely zi),\nand then evaluate xiunder each cluster\u2019s posterior predictive. Once we have picked a new\ncluster, we add xi\u2019s statistics to this new cluster.\nSome pseudo-code for one step of the algorithm is shown in Algorithm 1, based on (Sud-\nderth 2006, p94). (We update the nodes in random order to improve the mixing time, assuggested in (Roberts and Sahu 1997).) We can initialize the sample by sequentially samplingfromp(z\ni|z1:i\u22121,x1:i). (See fmGibbs for some Matlab code, by Yee-Whye Teh.) In the case of\nGMMs, both the naive sampler and collapsed sampler take O(NKD)time per step.\nAlgorithm 24.1: Collapsed Gibbs sampler for a mixture model\n1foreachi=1:Ninrandomorder do\n2Removexi\u2019s sufficient statistics from old cluster zi;\n3foreachk=1:Kdo\n4 Compute pk(xi)/definesp(xi|{xj:zj=k,j/negationslash=i});\n5Compute p(zi=k|z\u2212i,D)\u221d(Nk,\u2212i+\u03b1/K)pk(xi);\n6Samplezi\u223cp(zi|\u00b7);\n7Addxi\u2019s sufficient statistics to new cluster zi\nA comparison of this method with the standard Gibbs sampler is shown in Figure 24.3. The\nvertical axis is the data log probability at each iteration, computed using\nlogp(D|z,\u03b8)=N/summationdisplay\ni=1log[\u03c0zip(xi|\u03b8zi)] (24.28)\nTo compute this quantity using the collapsed sampler, we have to sample \u03b8=(\u03c0,\u03b81:K)given\nthe data and the current assignment z.\nIn Figure 24.3 we see that the collapsed sampler does indeed generally work better than the\nvanilla sampler. Occasionally, however, both methods can get stuck in poor local modes. (Note", "874": "844 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n100101102103\u2212600\u2212550\u2212500\u2212450\u2212400\u2212350\nIterationlog p(x | \u03c0, \u03b8)\n  \nStandard Gibbs Sampler\nRao\u2212Blackwellized Sampler\n(a)100101102103\u2212600\u2212550\u2212500\u2212450\u2212400\u2212350\nIterationlog p(x | \u03c0, \u03b8)\n  \nStandard Gibbs Sampler\nRao\u2212Blackwellized Sampler\n(b)\nFigure 24.3 Comparison of collapsed (red) and vanilla (blue) Gibbs sampling for a mixture of K=4two-\ndimensional Gaussians applied to N= 300data points (shown in Figure 25.7). We plot log probability of\nthe data vs iteration. (a) 20 different random initializations. (b) logprob averaged over 100 different random\ninitializations. Solid line is the median, thick dashed in the 0.25 and 0.75 quantiles, and thin dashed arethe 0.05 and 0.95 quintiles. Source: Figure 2.20 of (Sudderth 2006). Used with kind permission of ErikSudderth.\n\u22122 \u22121 0 1 220304050607080\nSESMath Score\n(a)5 10 15 20 25 30\u221250510\nSample SizeSlope\n(b)\u22122 \u22121 0 1 220304050607080\nSESMath Score\n(c)\nFigure 24.4 (a) Least squares regression lines for math scores vs socio-economic status for 100 schools.\nPopulation mean (pooled estimate) is in bold. (b) Plot of \u02c6w2j(the slope) vs Nj(sample size) for the 100\nschools. The extreme slopes tend to correspond to schools with smaller sample sizes. (c) Predictions fromthe hierarchical model. Population mean is in bold. Based on Figure 11.1 of (Hoff 2009). Figure generatedbymultilevelLinregDemo , written by Emtiyaz Khan.\nthat the error bars in Figure 24.3(b) are averaged over starting values, whereas the theorem refers\nto MC samples in a single run.)\n24.2.5 Gibbs sampling for hierarchical GLMs\nOften we have data from multiple related sources. If some sources are more reliable and/ordata-rich than others, it makes sense to model all the data simultaneously, so as to enable theborrowing of statistical strength. One of the most natural way to solve such problems is to usehierarchical Bayesian modeling, also called multi-level modeling. In Section 9.6, we discusseda way to perform approximate inference in such models using variational methods. Here wediscuss how to use Gibbs sampling.\nTo explain the method, consider the following example. Suppose we have data on students", "875": "24.2. Gibbssampling 845\nxijyijwj\n\u03c32\u03bcw\u03a3w\nNj\nJ\nFigure 24.5 Multi-level model for linear regression.\nin different schools. Such data is naturally modeled in a two-level hierarchy: we let yijbe the\nresponse variable we want to predict for student iin school j. This prediction can be based on\nschool and student speci\ufb01c covariates, xij. Since the quality of schools varies, we want to use\na separate parameter for each school. So our model becomes\nyij=xT\nijwj+/epsilon1ij (24.29)\nWe will illustrate this model below, using a dataset from (Hoff 2009, p197), where xijis the\nsocio-economic status (SES) of student iin school y, andyijis their math score.\nWe could \ufb01t each wjseparately, but this can give poor results if the sample size of a given\nschool is small. This is illustrated in Figure 24.4(a), which plots the least squares regression\nline estimated separately for each of the J= 100schools. We see that most of the slopes are\npositive, but there are a few \u201cerrant\u201d cases where the slope is negative. It turns out that the lineswith extreme slopes tend to be in schools with small sample size, as shown in Figure 24.4(b).Thus we may not necessarily trust these \ufb01ts.\nWe can get better results if we construct a hierarchical Bayesian model, in which the w\njare\nassumed to come from a common prior: wj\u223cN(\u03bcw,\u03a3w). This is illustrated in Figure 24.5. In\nthis model, the schools with small sample size borrow statistical strength from the schools withlarger sample size, because the w\nj\u2019s are correlated via the latent common parents (\u03bcw,\u03a3w). (It\nis crucial that these hyper-parameters be inferrred from data; if they were \ufb01xed constants, thew\njwould be conditionally independent, and there would be no information sharing between\nthem.)\nTo complete the model speci\ufb01cation, we must specify priors for the shared parameters. Fol-\nlowing (Hoff 2009, p198), we will use the following semi-conjugate forms, for convenience:\n\u03bcw\u223cN(\u03bc0,V0) (24.30)\n\u03a3w\u223cIW(\u03b70,S\u22121\n0) (24.31)\n\u03c32\u223cIG(\u03bd0/2,\u03bd0\u03c32\n0/2) (24.32)\nGiven this, it is simple to show that the full conditionals needed for Gibbs sampling have the", "876": "846 Chapter24. MarkovchainMonteCarlo(MCMC)inference\nfollowing forms. For the group-speci\ufb01c weights:\np(wj|Dj,\u03b8)=N (wj|\u03bcj,\u03a3j) (24.33)\n\u03a3\u22121\nj=\u03a3\u22121+XT\njXj/\u03c32(24.34)\n\u03bcj=\u03a3j(\u03a3\u22121\u03bc+XTjyj/\u03c32) (24.35)\nFor the overall mean:\np(\u03bcw|w1:J,\u03a3w)=N (\u03bc|\u03bcN,\u03a3N) (24.36)\n\u03a3\u22121\nN=V\u22121\n0+J\u03a3\u22121(24.37)\n\u03bcN=\u03a3N(V\u22121\n0\u03bc0+J\u03a3\u22121w) (24.38)\nwherew=1\nJ/summationtext\njwj. For the overall covariance:\np(\u03a3w|\u03bcw,w1:J)=I W ( ( S0+S\u03bc)\u22121,\u03b70+J) (24.39)\nS\u03bc=/summationdisplay\nj(wj\u2212\u03bcw)(wj\u2212\u03bcw)T(24.40)\nFor the noise variance:\np(\u03c32|D,w1:J)=I G ( [ \u03bd0+N]/2,[\u03bd0\u03c32\n0+SSR(w1:J)]/2) (24.41)\nSSR(w1:J)=J/summationdisplay\nj=1Nj/summationdisplay\ni=1(yij\u2212wT\njxij)2(24.42)\nApplying Gibbs sampling to our hierarchical model, we get the results shown in Figure 24.4(c).\nThe light gray lines plot the mean of the posterior predictive distribution for each school:\nE[yj|xij]=xTij\u02c6wj (24.43)\nwhere\n\u02c6wj=E[wj|D]\u22481\nSS/summationdisplay\ns=1w(s)\nj (24.44)\nThe dark gray line in the middle plots the prediction using the overall mean parameters, xT\nij\u02c6\u03bcw.\nWe see that the method has regularized the \ufb01ts quite nicely, without enforcing too much\nuniformity. (The amount of shrinkage is controlled by \u03a3w, which in turns depends on the\nhyper-parameters; in this example, we used vague values.)\n24.2.6 BUGS and JAGS\nOne reason Gibbs sampling is so popular is that it is possible to design general purpose softwarethat will work for almost any model. This software just needs a model speci\ufb01cation, usuallyin the form a directed graphical model (speci\ufb01ed in a \ufb01le, or created with a graphical userinterface), and a library of methods for sampling from different kinds of full conditionals. (Thiscan often be done using adaptive rejection sampling, described in Section 23.3.4.) An example", "877": "24.2. Gibbssampling 847\nof such a package is BUGS(Lunn et al. 2000), which stands for \u201cBayesian updating using Gibbs\nSampling\u201d. BUGS is very widely used in biostatistics and social science. Another more recent,\nbut very similar, package is JAGS(Plummer 2003), which stands for \u201cJust Another Gibbs Sampler\u201d.\nThis uses a similar model speci\ufb01cation language to BUGS.\nFor example, we can describe the model in Figure 24.5 as follows:\nmodel {\nfor (i in 1:N) {\nfor (j in 1:J) {\ny[i,j] ~ dnorm(y.hat[i,j], tau.y)y.hat[i,j] <- inprod(W[j, ], X[i, j, ])\n}\n}tau.y <- pow(sigma.y, -2)sigma.y ~ dunif(0,100)\nfor (j in 1:J) {\nW[j,] ~ dmnorm(mu, SigmaInv)\n}\nSigmaInv ~ dwish(S0[,], eta0)mu ~ dmnorm(mu0, V0inv)}\nWe can then just pass this model to BUGS or JAGS, which will generate samples for us. See\nthe webpages for details.\nAlthough this approach is appealing, unfortunately it can be much slower than using hand-\nwritten code, especially for complex models. There has been some work on automaticallyderiving model-speci\ufb01c optimized inference code (Fischer and Schumann 2003), but fast codestill typically requires human expertise.\n24.2.7 The Imputation Posterior (IP) algorithm\nTheImputation Posterior or IP algorithm (Tanner and Wong 1987) is a special case of Gibbs\nsampling in which we group the variables into two classes: hidden variables zand parameters\n\u03b8. This should sound familiar: it is basically an MCMC version of EM, where the E step gets\nreplaced by the I step, and the M step gets replaced the P step. This is an example of a moregeneral strategy called data augmentation, whereby we introduce auxiliary variables in order\nto simplify the posterior computations (here the computation of p(\u03b8|D)). See (Tanner 1996; van\nDyk and Meng 2001) for more information.\n24.2.8 Blocking Gibbs sampling\nGibbs sampling can be quite slow, since it only updates one variable at a time (so-called single\nsite updating). If the variables are highly correlated, it will take a long time to move awayfrom the current state. This is illustrated in Figure 24.6, where we illustrate sampling from a 2dGaussian (see Exercise 24.1 for the details). If the variables are highly correlated, the algorithm", "878": "848 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n/g1/g2/g1/g3/g1/g4/g5/g4/g3/g2\n/g1/g2 /g1/g3 /g1/g4 /g5 /g4 /g3 /g2/g6\n/g7\nFigure 24.6 Illustration of potentially slow sampling when using Gibbs sampling for a skewed 2D Gaus-\nsian. Based on Figure 11.11 of (Bishop 2006b). Figure generated by gibbsGaussDemo .\nwill move very slowly through the state space. In particular, the size of the moves is controlled\nby the variance of the conditional distributions. If this is /lscriptin thex1direction, and the support\nof the distribution is Lalong this dimension, then we need O((L//lscript)2)steps to obtain an\nindependent sample.\nIn some cases we can efficiently sample groups of variables at a time. This is called blocking\nGibbs sampling orblocked Gibbs sampling (Jensen et al. 1995; Wilkinson and Yeung 2002),\nand can make much bigger moves through the state space.\n24.3 Metropolis Hastings algorithm\nAlthough Gibbs sampling is simple, it is somewhat restricted in the set of models to which itcan be applied. For example, it is not much help in computing p(w|D)for a logistic regression\nmodel, since the corresponding graphical model has no useful Markov structure. In addition,Gibbs sampling can be quite slow, as we mentioned above.\nFortunately, there is a more general algorithm that can be used, known as the Metropolis\nHastings orMHalgorithm, which we describe below.\n24.3.1 Basic idea\nThe basic idea in MH is that at each step, we propose to move from the current state xto a\nnew state x/primewith probability q(x/prime|x),w h e r eqis called the proposal distribution (also called\nthekernel). The user is free to use any kind of proposal they want, subject to some conditions\nwhich we explain below. This makes MH quite a \ufb02exible method. A commonly used proposal isa symmetric Gaussian distribution centered on the current state, q(x\n/prime|x)=N(x/prime|x,\u03a3); this is\ncalled arandom walk Metropolis algorithm. We discuss how to choose \u03a3in Section 24.3.3. If\nwe use a proposal of the form q(x/prime|x)=q(x/prime), where the new state is independent of the old\nstate, we get a method known as the independence sampler, which is similar to importance\nsampling (Section 23.4).\nHaving proposed a move to x/prime, we then decide whether to acceptthis proposal or not\naccording to some formula, which ensures that the fraction of time spent in each state isproportional to p\n\u2217(x). If the proposal is accepted, the new state is x/prime, otherwise the new state", "879": "24.3. MetropolisHastingsalgorithm 849\nis the same as the current state, x(i.e., we repeat the sample).\nIf the proposal is symmetric, so q(x/prime|x)=q(x|x/prime), the acceptance probability is given by the\nfollowing formula:\nr=m i n ( 1,p\u2217(x/prime)\np\u2217(x)) (24.45)\nWe see that if x/primeis more probable than x, we de\ufb01nitely move there (sincep\u2217(x/prime)\np\u2217(x)>1), but if\nx/primeis less probable, we may still move there anyway, depending on the relative probabilities. So\ninstead of greedily moving to only more probable states, we occasionally allow \u201cdownhill\u201d moves\nto less probable states. In Section 24.3.6, we prove that this procedure ensures that the fractionof time we spend in each state xis proportional to p\n\u2217(x).\nIf the proposal is asymmetric, so q(x/prime|x)/negationslash=q(x|x/prime), we need the Hastings correction, given\nby the following:\nr=m i n ( 1 ,\u03b1) (24.46)\n\u03b1=p\u2217(x/prime)q(x|x/prime)\np\u2217(x)q(x/prime|x)=p\u2217(x/prime)/q(x/prime|x)\np\u2217(x)/q(x|x/prime)(24.47)\nThis correction is needed to compensate for the fact that the proposal distribution itself (ratherthan just the target distribution) might favor certain states.\nAn important reason why MH is a useful algorithm is that, when evaluating \u03b1, we only need to\nknow the target density up to a normalization constant. In particular, suppose p\n\u2217(x)=1\nZ\u02dcp(x),\nwhere\u02dcp(x)is an unnormalized distribution and Zis the normalization constant. Then\n\u03b1=(\u02dcp(x/prime)/Z)q(x|x/prime)\n(\u02dcp(x)/Z)q(x/prime|x)(24.48)\nso theZ\u2019s cancel. Hence we can sample from p\u2217even ifZis unknown. In particular, all we\nhave to do is evaluate \u02dcppointwise, where \u02dcp(x)=p\u2217(x)Z.\nThe overall algorithm is summarized in Algorithm 2.\n24.3.2 Gibbs sampling is a special case of MH\nIt turns out that Gibbs sampling, which we discussed in Section 24.2, is a special case of MH. Inparticular, it is equivalent to using MH with a sequence of proposals of the form\nq(x\n/prime|x)=p(x/prime\ni|x\u2212i)I(x/prime\u2212i=x\u2212i) (24.49)\nThat is, we move to a new state where xiis sampled from its full conditional, but x\u2212iis left\nunchanged.\nWe now prove that the acceptance rate of each such proposal is 1, so the overall algorithm\nalso has an acceptance rate of 100%. We have\n\u03b1=p(x/prime)q(x|x/prime)\np(x)q(x/prime|x)=p(x/prime\ni|x/prime\u2212i)p(x/prime\u2212i)p(xi|x/prime\u2212i)\np(xi|x\u2212i)p(x\u2212i)p(x/primei|x\u2212i)(24.50)\n=p(x/primei|x\u2212i)p(x\u2212i)p(xi|x\u2212i)\np(xi|x\u2212i)p(x\u2212i)p(x/primei|x\u2212i)=1 (24.51)", "880": "850 Chapter24. MarkovchainMonteCarlo(MCMC)inference\nAlgorithm 24.2: Metropolis Hastings algorithm\n1Initializex0;\n2fors=0,1,2,...do\n3De\ufb01nex=xs;\n4Samplex/prime\u223cq(x/prime|x);\n5Compute acceptance probability\n\u03b1=\u02dcp(x/prime)q(x|x/prime)\n\u02dcp(x)q(x/prime|x)\nCompute r=m i n ( 1,\u03b1);\n6Sampleu\u223cU(0,1);\n7Set new sample to\nxs+1=/braceleftbiggx/primeifu<r\nxsifu\u2265r\nwhere we exploited the fact that x/prime\n\u2212i=x\u2212i, and that q(x/prime|x)=p(x/primei|x\u2212i).\nThe fact that the acceptance rate is 100% does not necessarily mean that Gibbs will converge\nrapidly, since it only updates one coordinate at a time (see Section 24.2.8). Fortunately, there are\nmany other kinds of proposals we can use, as we discuss below.\n24.3.3 Proposal distributions\nFor a given target distribution p\u2217, a proposal distribution qis valid or admissible if it gives\na non-zero probability of moving to the states that have non-zero probability in the target.Formally, we can write this as\nsupp(p\n\u2217)\u2286\u222axsupp(q(\u00b7|x)) (24.52)\nFor example, a Gaussian random walk proposal has non-zero probability density on the entirestate space, and hence is a valid proposal for any continuous state space.\nOf course, in practice, it is important that the proposal spread its probability mass in just the\nright way. Figure 24.7 shows an example where we use MH to sample from a mixture of two1D Gaussians using a random walk proposal, q(x\n/prime|x)=N(x/prime|x,v). This is a somewhat tricky\ntarget distribution, since it consists of two well separated modes. It is very important to set thevariance of the proposal vcorrectly: If the variance is too low, the chain will only explore one\nof the modes, as shown in Figure 24.7(a), but if the variance is too large, most of the moveswill be rejected, and the chain will be very sticky, i.e., it will stay in the same state for a long\ntime. This is evident from the long stretches of repeated values in Figure 24.7(b). If we setthe proposal\u2019s variance just right, we get the trace in Figure 24.7(c), where the samples clearlyexplore the support of the target distribution. We discuss how to tune the proposal below.\nOne big advantage of Gibbs sampling is that one does not need to choose the proposal", "881": "24.3. MetropolisHastingsalgorithm 851\n0\n200\n400\n600\n800\n1000\u2212100\u22125005010000.10.2\nSamplesMH with N(0,1.0002) proposal\nIterations\n(a)0\n200\n400\n600\n800\n1000\u2212100\u22125005010000.020.040.06\nSamplesMH with N(0,500.0002) proposal\nIterations\n(b)\n0\n200\n400\n600\n800\n1000\u2212100\u22125005010000.010.020.03\nSamplesMH with N(0,8.0002) proposal\nIterations\n(c)\nFigure 24.7 An example of the Metropolis Hastings algorithm for sampling from a mixture of two 1D\nGaussians (\u03bc =(\u221220,20),\u03c0=( 0.3,0.7),\u03c3= (100,100)), using a Gaussian proposal with variances of\nv\u2208{1,500,8}. (a) When v=1, the chain gets trapped near the starting state and fails to sample from\nthe mode at \u03bc=\u221220. (b) When v= 500, the chain is very \u201csticky\u201d, so its effective sample size is low (as\nre\ufb02ected by the rough histogram approximation at the end). (c) Using a variance of v=8is just right and\nleads to a good approximation of the true distribution (shown in red). Figure generated by mcmcGmmDemo .\nBased on code by Christophe Andrieu and Nando de Freitas.\ndistribution, and furthermore, the acceptance rate is 100%. Of course, a 100% acceptance can\ntrivially be achieved by using a proposal with variance 0 (assuming we start at a mode), but thisis obviously not exploring the posterior. So having a high acceptance is not the ultimate goal.We can increase the amount of exploration by increasing the variance of the Gaussian kernel.Often one experiments with different parameters until the acceptance rate is between 25% and40%, which theory suggests is optimal, at least for Gaussian target distributions. These shortinitial runs, used to tune the proposal, are called pilot runs.", "882": "852 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n\u2212120 \u2212100 \u221280 \u221260 \u221240 \u221220 000.020.040.060.080.10.120.140.160.180.2\nw0w1\n(a)\u2212120 \u2212100 \u221280 \u221260 \u221240 \u221220 0050010001500w0 intercept\n(b)0 0.05 0.1 0.15 0.2050010001500w1 slope\n(c)\nFigure 24.8 (a) Joint posterior of the parameters for 1d logistic regression when applied to some SAT data.\n(b) Marginal for the offset w0. (c) Marginal for the slope w1. We see that the marginals do not capture the\nfact that the parameters are highly correlated. Figure generated by logregSatMhDemo .\n24.3.3.1 Gaussian proposals\nIf we have a continuous state space, the Hessian Hat a local mode \u02c6wcan be used to de\ufb01ne\nthe covariance of a Gaussian proposal distribution. This approach has the advantage that the\nHessian models the local curvature and length scales of each dimension; this approach thereforeavoids some of the slow mixing behavior of Gibbs sampling shown in Figure 24.6.\nThere are two obvious approaches: (1) an independence proposal, q(w\n/prime|w)=N(w/prime|\u02c6w,H\u22121)\nor (2), a random walk proposal, q(w/prime|w)=N(w/prime|w,s2H\u22121),w h e r es2is a scale factor chosen\nto facilitate rapid mixing. (Roberts and Rosenthal 2001) prove that, if the posterior is Gaussian,the asymptotically optimal value is to use s\n2=2.382/D,w h e r eDis the dimensionality of w;\nthis results in an acceptance rate of 0.234.\nFor example, consider MH for binary logistic regression. From Equation 8.7, we have that\nthe Hessian of the log-likelihood is Hl=XTDX,w h e r eD= diag(\u03bc i(1\u2212\u03bci))and\u03bci=\nsigm(\u02c6wTxi). If we assume a Gaussian prior, p(w)=N(0,V0),w eh a v e H=V\u22121\n0+Hl,s o\nthe asymptotically optimal Gaussian proposal has the form\nq(w/prime|w)=N/parenleftbigg\nw,2.382\nD/parenleftbig\nV\u22121\n0+XTDX/parenrightbig\u22121/parenrightbigg\n(24.53)\nSee (Gamerman 1997; Rossi et al. 2006; Fruhwirth-Schnatter and Fruhwirth 2010) for furtherdetails. The approach is illustrated in Figure 24.8, where we sample parameters from a 1dlogistic regression model \ufb01t to some SAT data. We initialize the chain at the mode, computedusing IRLS, and then use the above random walk Metropolis sampler.\nIf you cannot afford to compute the mode or its Hessian XDX, an alternative approach,\nsuggested in (Scott 2009), is to approximate the above proposal as follows:\nq(w\n/prime|w)=N/parenleftBigg\nw,/parenleftbigg\nV\u22121\n0+6\n\u03c02XTX/parenrightbigg\u22121/parenrightBigg\n(24.54)", "883": "24.3. MetropolisHastingsalgorithm 853\n24.3.3.2 Mixture proposals\nIf one doesn\u2019t know what kind of proposal to use, one can try a mixture proposal, which is a\nconvex combination of base proposals:\nq(x/prime|x)=K/summationdisplay\nk=1wkqk(x/prime|x) (24.55)\nwherewkare the mixing weights. As long as each qkis individually valid, the overall proposal\nwill also be valid.\n24.3.3.3 Data-driven MCMC\nThe most efficient proposals depend not just on the previous hidden state, but also the visible\ndata, i.e., they have the form q(x/prime|x,D). This is called data-driven MCMC (see e.g., (Tu and\nZhu 2002)). To create such proposals, one can sample (x,D)pairs from the forwards model\nand then train a discriminative classi\ufb01er to predict p(x|f(D)),w h e r e f(D)are some features\nextracted from the visible data.\nTypically xis a high-dimensional vector (e.g., position and orientation of all the limbs of a\nperson in a visual object detector), so it is hard to predict the entire state vector, p(x|f(D)).\nInstead we might train a discriminative detector to predict parts of the state-space, p(xk|fk(D)),\nsuch as the location of just the face of a person. We can then use a proposal of the form\nq(x/prime|x,D)=\u03c00q0(x/prime|x)+/summationdisplay\nk\u03c0kqk(x/prime\nk|fk(D)) (24.56)\nwhereq0is a standard data-independent proposal (e.g., random walk), and qkupdates the k\u2019th\ncomponent of the state space. For added efficiency, the discriminative proposals should suggest\njoint changes to multiple variables, but this is often hard to do.\nThe overall procedure is a form of generate and test: the discriminative proposals q(x/prime|x)\ngenerate new hypotheses, which are then \u201ctested\u201d by computing the posterior ratiop(x/prime|D)\np(x|D),t o\nsee if the new hypothesis is better or worse. By adding an annealing step, one can modifythe algorithm to \ufb01nd posterior modes; this is called simulated annealing, and is described in\nSection 24.6.1. One advantage of using the mode-seeking version of the algorithm is that we donot need to ensure the proposal distribution is reversible.\n24.3.4 Adaptive MCMC\nOne can change the parameters of the proposal as the algorithm is running to increase efficiency.This is called adaptive MCMC. This allows one to start with a broad covariance (say), allowing\nlarge moves through the space until a mode is found, followed by a narrowing of the covarianceto ensure careful exploration of the region around the mode.\nHowever, one must be careful not to violate the Markov property; thus the parameters of the\nproposal should not depend on the entire history of the chain. It turns out that a sufficientcondition to ensure this is that the adaption is \u201cfaded out\u201d gradually over time. See e.g., (Andrieuand Thoms 2008) for details.", "884": "854 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n24.3.5 Initialization and mode hopping\nIt is necessary to start MCMC in an initial state that has non-zero probability. If the model has\ndeterministic constraints, \ufb01nding such a legal con\ufb01guration may be a hard problem in itself. Itis therefore common to initialize MCMC methods at a local mode, found using an optimizer.\nIn some domains (especially with discrete state spaces), it is a more effective use of computa-\ntion time to perform multiple restarts of an optimizer, and to average over these modes, ratherthan exploring similar points around a local mode. However, in continuous state spaces, themode contains negligible volume (Section 5.2.1.3), so it is necessary to locally explore aroundeach mode, in order to visit enough posterior probability mass.\n24.3.6 Why MH works *\nTo prove that the MH procedure generates samples from p\u2217, we have to use a bit of Markov\nchain theory, so be sure to read Section 17.2.3 \ufb01rst.\nThe MH algorithm de\ufb01nes a Markov chain with the following transition matrix:\np(x/prime|x)=/braceleftbiggq(x/prime|x)r(x/prime|x) ifx/prime/negationslash=x\nq(x|x)+/summationtext\nx/prime/negationslash=xq(x/prime|x)(1\u2212r(x/prime|x))otherwise(24.57)\nThis follows from a case analysis: if you move to x/primefromx, you must have proposed it (with\nprobability q(x/prime|x)) and it must have been accepted (with probability r(x/prime|x)); otherwise you\nstay in state x, either because that is what you proposed (with probability q(x|x)), or because\nyou proposed something else (with probability q(x/prime|x)) but it was rejected (with probability\n1\u2212r(x/prime|x)).\nLet us analyse this Markov chain. Recall from Section 17.2.3.4 that a chain satis\ufb01es detailed\nbalanceif\np(x/prime|x)p\u2217(x)=p(x|x/prime)p\u2217(x/prime) (24.58)\nWe also showed that if a chain satis\ufb01es detailed balance, then p\u2217is its stationary distribution.\nOur goal is to show that the MH algorithm de\ufb01nes a transition function that satis\ufb01es detailedbalance and hence that p\n\u2217is its stationary distribution. (If Equation 24.58 holds, we say that p\u2217\nis aninvariant distribution wrt the Markov transition kernel q.)\nTheorem 24.3.1. Ifthetransitionmatrixde\ufb01nedbytheMHalgorithm(givenbyEquation24.57)is\nergodicandirreducible,then p\u2217isitsuniquelimitingdistribution.\nProof.Consider two states xandx/prime. Either\np\u2217(x)q(x/prime|x)<p\u2217(x/prime)q(x|x/prime) (24.59)\nor\np\u2217(x)q(x/prime|x)>p\u2217(x/prime)q(x|x/prime) (24.60)\nWe will ignore ties (which occur with probability zero for continuous distributions). Without lossof generality, assume that p\n\u2217(x)q(x/prime|x)>p\u2217(x/prime)q(x|x/prime). Hence\n\u03b1(x/prime|x)=p\u2217(x/prime)q(x|x/prime)\np\u2217(x)q(x/prime|x)<1 (24.61)", "885": "24.3. MetropolisHastingsalgorithm 855\nHence we have r(x/prime|x)=\u03b1(x/prime|x)andr(x|x/prime)=1.\nN o wt om o v ef r o mx tox/primewe must \ufb01rst propose x/primeand then accept it. Hence\np(x/prime|x)=q(x/prime|x)r(x/prime|x)=q(x/prime|x)p\u2217(x/prime)q(x|x/prime)\np\u2217(x)q(x/prime|x)=p\u2217(x/prime)\np\u2217(x)q(x|x/prime) (24.62)\nHence\np\u2217(x)p(x/prime|x)=p\u2217(x/prime)q(x|x/prime) (24.63)\nThe backwards probability is\np(x|x/prime)=q(x|x/prime)r(x|x/prime)=q(x|x/prime) (24.64)\nsincer(x|x/prime)=1. Inserting this into Equation 24.63 we get\np\u2217(x)p(x/prime|x)=p\u2217(x/prime)p(x|x/prime) (24.65)\nso detailed balance holds wrt p\u2217. Hence, from Theorem 17.2.3, p\u2217is a stationary distribution.\nFurthermore, from Theorem 17.2.2, this distribution is unique, since the chain is ergodic and\nirreducible.\n24.3.7 Reversible jump (trans-dimensional) MCMC *\nSuppose we have a set of models with different numbers of parameters, e.g., mixture models inwhich the number of mixture components is unknown. Let the model be denoted by m, and\nlet its unknowns (e.g., parameters) be denoted by x\nm\u2208Xm(e.g.,Xm=Rnm,w h e r enmis\nthe dimensionality of model m). Sampling in spaces of differing dimensionality is called trans-\ndimensional MCMC (Green 2003). We could sample the model indicator m\u2208{1,...,M}and\nsample all the parameters from the product space/producttextM\nm=1Xm, but this is very inefficient. It is\nmore parsimonious to sample in the union space X=\u222aM\nm=1{m}\u00d7X m, where we only worry\nabout parameters for the currently active model.\nThe difficulty with this approach arises when we move between models of different dimen-\nsionality. The trouble is that when we compute the MH acceptance ratio, we are comparing\ndensities de\ufb01ned in different dimensionality spaces, which is meaningless. It is like trying tocompare a sphere with a circle. The solution, proposed by (Green 1998) and known as reversible\njump MCMC orRJMCMC, is to augment the low dimensional space with extra random variables\nso that the two spaces have a common measure.\nUnfortunately, we do not have space to go into details here. Suffice it to say that the method\ncan be made to work in theory, although it is a bit tricky in practice. If, however, the continuousparameters can be integrated out (resulting in a method called collapsed RJMCMC), much of thedifficulty goes away, since we are just left with a discrete state space, where there is no needto worry about change of measure. For example, (Denison et al. 2002) includes many examplesof applications of collapsed RJMCMC applied to Bayesian inference fro adaptive basis-functionmodels. They sample basis functions from a \ufb01xed set of candidates (e.g., centered on the datapoints), and integrate out the other parameters analytically. This provides a Bayesian alternativeto using RVMs or SVMs.", "886": "856 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n0 5 10 15 20p(0)(x)\n0 5 10 15 20p(1)(x)\n0 5 10 15 20p(2)(x)\n0 5 10 15 20p(3)(x)\n0 5 10 15 20p(10)(x)\n0 5 10 15 20p(100)(x)\n0 5 10 15 20p(200)(x)Initial Condition X0 = 10\n0 5 10 15 20p(400)(x)\n(a)0 5 10 15 20p(0)(x)\n0 5 10 15 20p(1)(x)\n0 5 10 15 20p(2)(x)\n0 5 10 15 20p(3)(x)\n0 5 10 15 20p(10)(x)\n0 5 10 15 20p(100)(x)\n0 5 10 15 20p(200)(x)Initial Condition X0 = 17\n0 5 10 15 20p(400)(x)\n(b)\nFigure 24.9 Illustration of convergence to the uniform distribution over {0,1,...,20} using a symmetric\nrandom walk starting from (left) state 10, and (right) state 17. Based on Figures 29.14 and 29.15 of (MacKay\n2003). Figure generated by randomWalk0to20Demo .\n24.4 Speed and accuracy of MCMC\nIn this section, we discuss a number of important theoretical and practical issues to do with\nMCMC.\n24.4.1 The burn-in phase\nWe start MCMC from an arbitrary initial state. As we explained in Section 17.2.3, only when thechainhas \u201cforgotten\u201dwhere itstarted fromwill thesamples becoming fromthe chain\u2019sstationarydistribution. Samples collected before the chain has reached its stationary distribution do notcome from p\n\u2217, and are usually thrown away. The initial period, whose samples will be ignored,\nis called the burn-in phase.\nFor example, consider a uniform distribution on the integers {0,1,...,20}. Suppose we\nsample from this using a symmetric random walk. In Figure 24.9, we show two runs of thealgorithm. On the left, we start in state 10; on the right, we start in state 17. Even in this smallproblem it takes over 100 steps until the chain has \u201cforgotten\u201d where it started from.\nIt is difficult to diagnose when the chain has burned in, an issue we discuss in more detail\nbelow. (This is one of the fundamental weaknesses of MCMC.) As an interesting example of whatcan happen if you start collecting samples too early, consider the Potts model. Figure 24.10(a),shows a sample after 500 iterations of Gibbs sampling. This suggests that the model likes", "887": "24.4. SpeedandaccuracyofMCMC 857\n(a)\n (b)\nFigure 24.10 Illustration of problems caused by poor mixing. (a) One sample from a 5-state Potts model\non a128\u00d7128grid with 8 nearest neighbor connectivity and J=2/3(as in (Geman and Geman 1984)),\nafter 200 iterations. (b) One sample from the same model after 10,000 iterations. Used with kind permission\nof Erik Sudderth.\nmedium-sized regions where the label is the same, implying the model would make a good\nprior for image segmentation. Indeed, this was suggested in the original Gibbs sampling paper\n(Geman and Geman 1984).\nHowever, it turns out that if you run the chain long enough, you get isolated speckles, as\nin Figure 24.10(b). The results depend on the coupling strength, but in general, it is very hard\nto \ufb01nd a setting which produces nice medium-sized blobs: most parameters result in a few\nsuper-clusters, or lots of small fragments. In fact, there is a rapid phase transition between these\ntwo regimes. This led to a paper called \u201cThe Ising/Potts model is not well suited to segmentation\ntasks\u201d (Morris et al. 1996). It is possible to create priors more suited to image segmentation\n(e.g., (Sudderth and Jordan 2008)), but the main point here is that sampling before reaching\nconvergence can lead to erroneous conclusions.\n24.4.2 Mixing rates of Markov chains *\nThe amount of time it takes for a Markov chain to converge to the stationary distribution, and\nforget its initial state, is called the mixing time . More formally, we say that the mixing time\nfrom state x0is the minimal time such that, for any constant /epsilon1>0,w eh a v et h a t\n\u03c4/epsilon1(x0)/definesmin{t:||\u03b4x0(x)Tt\u2212p\u2217||1\u2264/epsilon1} (24.66)\nwhere\u03b4x0(x)is a distribution with all its mass in state x0,Tis the transition matrix of the\nchain (which depends on the target p\u2217and the proposal q), and\u03b4x0(x)Ttis the distribution\naftertsteps. The mixing time of the chain is de\ufb01ned as\n\u03c4/epsilon1/definesmax\nx0\u03c4/epsilon1(x0) (24.67)\nThe mixing time is determined by the eigengap \u03b3=\u03bb1\u2212\u03bb2, which is the difference of the", "888": "858 Chapter24. MarkovchainMonteCarlo(MCMC)inference\nx2\nx4\nx7x5\nx6 x1\nx3\nFigure 24.11 A Markov chain with low conductance. The dotted arcs represent transitions with very low\nprobability. Source: Figure 12.6 of (Koller and Friedman 2009). Used with kind permission of Daphne\nKoller.\n\ufb01rst and second eigenvalues of the transition matrix. In particular, one can show that\n\u03c4/epsilon1\u2264O(1\n\u03b3logn\n/epsilon1) (24.68)\nwherenis the number of states. Since computing the transition matrix can be hard to do,\nespecially for high dimensional and/or continuous state spaces, it is useful to \ufb01nd other ways to\nestimate the mixing time.\nAn alternative approach is to examine the geometry of the state space. For example, consider\nthe chain in Figure 24.11. We see that the state space consists of two \u201cislands\u201d, each of whichis connected via a narrow \u201cbottleneck\u201d. (If they were completely disconnected, the chain wouldnot be ergodic, and there would no longer be a unique stationary distribution.) We de\ufb01ne theconductance \u03c6of a chain as the minimum probability, over all subsets of states, of transitioning\nfrom that set to its complement:\n\u03c6/definesmin\nS:0\u2264p\u2217(S)\u22640.5/summationtext\nx\u2208S,x/prime\u2208ScT(x\u2192x/prime)\np\u2217(S), (24.69)\nOne can show that\n\u03c4/epsilon1\u2264O(1\n\u03c62logn\n/epsilon1) (24.70)\nHence chains with low conductance have high mixing time. For example, distributions withwell-separated modes usually have high mixing time. Simple MCMC methods often do not workwell in such cases, and more advanced algorithms, such as parallel tempering, are necessary(see e.g., (Liu 2001)).\n24.4.3 Practical convergence diagnostics\nComputing the mixing time of a chain is in general quite difficult, since the transition matrixis usually very hard to compute. In practice various heuristics have been proposed to diagnose", "889": "24.4. SpeedandaccuracyofMCMC 859\nconvergence \u2014 see (Geyer 1992; Cowles and Carlin 1996; Brooks and Roberts 1998) for a review.\nStrictly speaking, these methods do not diagnose convergence, but rather non-convergence. Thatis, the method may claim the chain has converged when in fact it has not. This is a \ufb02aw commonto all convergence diagnostics, since diagnosing convergence is computationally intractable ingeneral (Bhatnagar et al. 2010).\nOne of the simplest approaches to assessing when the method has converged is to run\nmultiple chains from very different overdispersed starting points, and to plot the samples of\nsome variables of interest. This is called a trace plot. If the chain has mixed, it should have\n\u201cforgotten\u201d where it started from, so the trace plots should converge to the same distribution,and thus overlap with each other.\nFigure 24.12 gives an example. We show the traceplot for xwhich was sampled from a\nmixture of two 1D Gaussians using four different methods: MH with a symmetric Gaussianproposal of variance \u03c3\n2\u2208{1,8,500}, and Gibbs sampling. We see that \u03c32=1has not mixed,\nwhich is also evident from Figure 24.7(a), which shows that a single chain never leaves the areawhere it started. The results for the other methods indicate that the chains rapidly converge tothe stationary distribution, no matter where they started. (The sticky nature of the \u03c3\n2= 500\nproposal is very evident. This reduces the computational efficiency, as we discuss below, butnot the statistical validity.)\n24.4.3.1 Estimated potential scale reduction (EPSR)\nWe can assess convergence more quantitatively as follows. The basic idea is to compare thevariance of a quantity within each chain to its variance across chains. More precisely, supposewe collect Ssamples (after burn-in) from each of Cchains of Dvariables, x\nisc,i=1:D,\ns=1:S,c=1:C.L e tyscbe a scalar quantity of interest derived from x1:D,s,c(e.g.,\nysc=xiscfor some chosen i). De\ufb01ne the within-sequence mean and overall mean as\ny\u00b7c/defines1\nSS/summationdisplay\ns=1ysc,y\u00b7\u00b7/defines1\nCC/summationdisplay\nc=1y\u00b7c (24.71)\nDe\ufb01ne the between-sequence and within-sequence variance as\nB/definesS\nC\u22121C/summationdisplay\nc=1(y\u00b7c\u2212y\u00b7\u00b7)2,W/defines1\nCC/summationdisplay\nc=1/bracketleftBigg\n1\nS\u22121S/summationdisplay\ns=1(ysc\u2212y\u00b7c)2/bracketrightBigg\n(24.72)\nWe can now construct two estimates of the variance of y. The \ufb01rst estimate is W: this should\nunderestimate var[y]if the chains have not ranged over the full posterior. The second estimate\nis\n\u02c6V=S\u22121\nSW+1\nSB (24.73)\nThis is an estimate of var[y]that is unbiased under stationarity, but is an overestimate if the\nstarting points were overdispersed (Gelman and Rubin 1992). From this, we can de\ufb01ne thefollowing convergence diagnostic statistic, known as the estimated potential scale reduction\norEPSR:\n\u02c6R/defines/radicalBigg\n\u02c6V\nW(24.74)", "890": "860 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n0 200 400 600 800 1000\u22121001020304050MH N(0,1.0002), Rhat = 1.493\n(a)0 200 400 600 800 1000\u221260\u221240\u2212200204060MH N(0,8.0002), Rhat = 1.039\n(b)\n0 200 400 600 800 1000\u221250\u221240\u221230\u221220\u22121001020304050MH N(0,500.0002), Rhat = 1.005\n(c)0 200 400 600 800 1000\u221260\u221240\u2212200204060gibbs, Rhat = 1.007\n(d)\nFigure 24.12 Traceplots for MCMC samplers. Each color represents the samples from a different starting\npoint. (a-c) MH with proposal N(x/prime|x,\u03c32)for\u03c32\u2208{1,8,500}, corresponding to Figure 24.7. (d) Gibbs\nsampling. Figure generated by mcmcGmmDemo .\nThis quantity, which was \ufb01rst proposed in (Gelman and Rubin 1992), measures the degree to\nwhich the posterior variance would decrease if we were to continue sampling in the S\u2192\n\u221elimit. If \u02c6R\u22481for any given quantity, then that estimate is reliable (or at least is not\nunreliable). The \u02c6Rvalues for the four samplers in Figure 24.12 are 1.493, 1.039, 1.005 and 1.007.\nSo this diagnostic has correctly identi\ufb01ed that the sampler using the \ufb01rst (\u03c32=1) proposal is\nuntrustworthy.\n24.4.4 Accuracy of MCMC\nThe samples produced by MCMC are auto-correlated, and this reduces their information contentrelative to independent or \u201cperfect\u201d samples. We can quantify this as follows.\n4Suppose we want\n4. This Section is based on (Hoff 2009, Sec 6.6).", "891": "24.4. SpeedandaccuracyofMCMC 861\n0 5 10 15 20 25 30 35 40 4500.10.20.30.40.50.60.70.80.91MH N(0,1.0002)\n(a)0 5 10 15 20 25 30 35 40 4500.10.20.30.40.50.60.70.80.91MH N(0,8.0002)\n(b)\n0 5 10 15 20 25 30 35 40 45\u22120.200.20.40.60.811.2MH N(0,500.0002)\n(c)0 5 10 15 20 25 30 35 40 45\u22120.200.20.40.60.811.2gibbs\n(d)\nFigure 24.13 Autocorrelation functions corresponding to Figure 24.12. Figure generated by mcmcGmmDemo .\nto estimate the mean of f(X), for some function f,w h e r eX\u223cp(). Denote the true mean by\nf\u2217/definesE[f(X)] (24.75)\nA Monte Carlo estimate is given by\nf=1\nSS/summationdisplay\ns=1fs (24.76)", "892": "862 Chapter24. MarkovchainMonteCarlo(MCMC)inference\nwherefs/definesf(xs)andxs\u223cp(x). An MCMC estimate of the variance of this estimate is given\nby\nVarMCMC[f]=E/bracketleftbig\n(f\u2212f\u2217)2/bracketrightbig\n(24.77)\n=E\u23a1\n\u23a3/braceleftBigg\n1\nSS/summationdisplay\ns=1(fs\u2212f\u2217)/bracerightBigg2\u23a4\u23a6 (24.78)\n=1\nS2E/bracketleftBiggS/summationdisplay\ns=1(fs\u2212f\u2217)2/bracketrightBigg\n+1\nS2/summationdisplay\ns/negationslash=tE[(fs\u2212f\u2217)(ft\u2212f\u2217)] (24.79)\n=V a r MC(f)+1\nS2/summationdisplay\ns/negationslash=tE[(fs\u2212f\u2217)(ft\u2212f\u2217)] (24.80)\nwhere the \ufb01rst term is the Monte Carlo estimate of the variance if the samples weren\u2019t correlated,\nand the second term depends on the correlation of the samples. We can measure this as follows.De\ufb01ne the sample-based auto-correlation at lag tof a set of samples f\n1,...,f Sas follows:\n\u03c1t/defines1\nS\u2212t/summationtextS\u2212t\ns=1(fs\u2212f)(fs+t\u2212f)\n1\nS\u22121/summationtextS\ns=1(fs\u2212f)2(24.81)\nThis is called the autocorrelation function (ACF). This is plotted in Figure 24.13 for our four\nsamplers for the Gaussian mixture model. We see that the ACF of the Gibbs sampler (bottom\nright) dies off to 0 much more rapidly than the MH samplers, indicating that each Gibbs sampleis \u201cworth\u201d more than each MH sample.\nA simple method to reduce the autocorrelation is to use thinning, in which we keep every\nn\u2019th sample. This does not increase the efficiency of the underlying sampler, but it does savespace, since it avoids storing highly correlated samples.\nWe can estimate the information content of a set of samples by computing the effective\nsample size (ESS)Seff, de\ufb01ned by\nSeff/definesVar\nMC(f)\nVarMCMC(f)(24.82)\nFrom Figure 24.12, it is clear that the effective sample size of the Gibbs sampler is higher thanthat of the other samplers (in this example).\n24.4.5 How many chains?\nA natural question to ask is: how many chains should we run? We could either run one longchain to ensure convergence, and then collect samples spaced far apart, or we could run manyshort chains, but that wastes the burnin time. In practice it is common to run a mediumnumber of chains (say 3) of medium length (say 100,000 steps), and to take samples from eachafter discarding the \ufb01rst half of the samples. If we initialize at a local mode, we may be able touse all the samples, and not wait for burn-in.", "893": "24.5. AuxiliaryvariableMCMC* 863\nModel Goal Method Reference\nProbit MAP Gradient Section 9.4.1\nProbit MAP EM Section 11.4.6\nProbit Post EP (Nickisch and Rasmussen 2008)Probit Post Gibbs+ Exercise 24.6Probit Post Gibbs with ARS (Dellaportas and Smith 1993)Probit Post MH using IRLS proposal (Gamerman 1997)\nLogit MAP Gradient Section 8.3.4\nLogit Post Gibbs+ with Student (Fruhwirth-Schnatter and Fruhwirth 2010)Logit Post Gibbs+ with KS (Holmes and Held 2006)\nTable 24.1 Summary of some possible algorithms for estimation and inference for binary classi\ufb01cation\nproblems using Gaussian priors. Abbreviations: Aux. = auxiliary variable sampling, ARS = adaptive rejection\nsampling, EP = expectation propagation, Gibbs+ = Gibbs sampling with auxiliary variables, IRLS = iterativereweighted least squares, KS = Kolmogorov Smirnov, MAP = maximum a posteriori, MH = MetropolisHastings, Post = posterior.\n24.5 Auxiliary variable MCMC *\nSometimes we can dramatically improve the efficiency of sampling by introducing dummy\nauxiliary variables, in order to reduce correlation between the original variables. If the originalvariables are denoted by x, and the auxiliary variables by z, we require that/summationtext\nzp(x,z)=p(x),\nand that p(x,z)is easier to sample from than just p(x). If we meet these two conditions,\nwe can sample in the enlarged model, and then throw away the sampled zvalues, thereby\nrecovering samples from p(x). We give some examples below.\n24.5.1 Auxiliary variable sampling for logistic regression\nIn Section 9.4.2, we discussed the latent variable interpretation of probit regression. Recall thatthis had the form\nz\ni/defineswTxi+/epsilon1i (24.83)\n/epsilon1i\u223cN(0,1) (24.84)\nyi=1 = I(zi\u22650) (24.85)\nWe exploited this representation in Section 11.4.6, where we used EM to \ufb01nd an ML estimate. Itis straightforward to convert this into an auxiliary variable Gibbs sampler (Exercise 24.6), sincep(w|D)is Gaussian and p(z\ni|xi,yi,w)is truncated Gaussian, both of which are easy to sample\nfrom.\nNow let us discuss how to derive an auxiliary variable Gibbs sampler for logistic regression.\nLet/epsilon1ifollow alogistic distribution, with pdf\npLogistic(/epsilon1)=e\u2212/epsilon1\n(1+e\u2212/epsilon1)2(24.86)\nwith mean E[/epsilon1]=0and variance var[/epsilon1]=\u03c02/3. The cdf has the form F(/epsilon1) = sigm(/epsilon1 ), which", "894": "864 Chapter24. MarkovchainMonteCarlo(MCMC)inference\nis the logistic function. Since yi=1iffwTxi+/epsilon1>0, we have, by symmetry, that\np(yi=1|xi,w)=/integraldisplay\u221e\n\u2212wTxif(/epsilon1)d/epsilon1=/integraldisplaywTxi\n\u2212\u221ef(/epsilon1)d/epsilon1=F(wTxi) = sigm(wTxi)(24.87)\nas required.\nWecanderiveanauxiliaryvariableGibbssamplerbysamplingfrom p(z|w,D)andp(w|z,D).\nUnfortunately, sampling directly from p(w|z,D)is not possible. One approach is to de\ufb01ne\n/epsilon1i\u223cN(0,\u03bbi),w h e r e\u03bbi=( 2\u03c8i)2and\u03c8i\u223cKS, the Kolmogorov Smirnov distribution, and then\nto sample w,z,\u03bband\u03c8(Holmes and Held 2006).\nA simpler approach is to approximate the logistic distribution by the Student distribution\n(Albert and Chib 1993). Speci\ufb01cally, we will make the approximation /epsilon1i\u223cT(0,1,\u03bd),w h e r e\n\u03bd\u22488. We can now use the scale mixture of Gaussians representation of the Student to simplify\ninference. In particular, we write\n\u03bbi\u223cGa(\u03bd/2,\u03bd/2) (24.88)\n/epsilon1i\u223cN(0,\u03bb\u22121\ni) (24.89)\nzi/defineswTxi+/epsilon1i (24.90)\nyi=1|zi=I(zi\u22650) (24.91)\nAll of the full conditionals now have a simple form; see Exercise 24.7 for the details.\nN o t et h a ti fw es e t \u03bd=1, thenzi\u223cN(wTxi,1), which is equivalent to probit regression (see\nSection 9.4). Rather than choosing between probit or logit regression, we can simply estimate\nthe\u03bdparameter. There is no convenient conjugate prior, but we can consider a \ufb01nite range of\npossible values and evaluate the posterior as follows:\np(\u03bd|\u03bb)\u221dp(\u03bd)N/productdisplay\ni=11\n\u0393(\u03bd/2)(\u03bd/2)\u03bd/2\u03bb\u03bd/2\u22121\nie\u2212\u03bd\u03bbi/2(24.92)\nFurthermore, if we de\ufb01ne V0=v0I, we can sample v0as well. For example, suppose we use\naIG(\u03b41,\u03b42)prior forv0. The posterior is given by p(v0|w)=I G (\u03b41+1\n2D,\u03b42+1\n2/summationtextD\nj=1w2\nj).\nThis can be interleaved with the other Gibbs sampling steps, and provides an appealing Bayesian\nalternative to cross validation for setting the strength of the regularizer.\nSee Table 24.1 for a summary of various algorithms for \ufb01tting probit and logit models. Many\nof these methods can also be extended to the multinomial logistic regression case. For details,see (Scott 2009; Fruhwirth-Schnatter and Fruhwirth 2010).\n24.5.2 Slice sampling\nConsider sampling from a univariate, but multimodal, distribution \u02dcp(x). We can sometimes\nimprove the ability to make large moves by adding an auxiliary variable u. We de\ufb01ne the joint\ndistribution as follows:\n\u02c6p(x,u)=/braceleftbigg1/Zpif0\u2264u\u2264\u02dcp(x)\n0otherwise(24.93)", "895": "24.5. AuxiliaryvariableMCMC* 865\nx xx u(i+1)\n(i)(i+1)f(x  )(i)\n(a)\u22125 \u22124 \u22123 \u22122 \u22121 0 1 2 3 4020406080100120140160180\n(b)\nFigure 24.14 (a) Illustration of the principle behind slice sampling. Given a previous sample xi,w e\nsampleui+1uniformly on [0,f(xi)],w h e r efis the target density. We then sample xi+1along the slice\nwheref(x)\u2265ui+1. Source: Figure 15 of (Andrieu et al. 2003) . Used with kind permission of Nando de\nFreitas. (b) Slice sampling in action. Figure generated by sliceSamplingDemo1d .\n3456\n\u22122.5\n\u22122\n\u22121.5\n\u2212101234x 10\u221211\nInterceptSlopePosterior density\n(a)\n (b)\nFigure 24.15 Binomial regression for 1d data. (a) Grid approximation to posterior. (b) Slice sampling\napproximation. Figure generated by sliceSamplingDemo2d .\nwhereZp=/integraltext\n\u02dcp(x)dx. The marginal distribution over xis given by\n/integraldisplay\n\u02c6p(x,u)du=/integraldisplay\u02dcp(x)\n01\nZpdu=\u02dcp(x)\nZp=p(x) (24.94)\nso we can sample from p(x)by sampling from \u02c6p(x,u)and then ignoring u. The full conditionals\nhave the form\np(u|x)=U[0,\u02dcp(x)](u) (24.95)\np(x|u)=UA(x) (24.96)\nwhereA={x:\u02dcp(x)\u2265u}is the set of points on or above the chosen height u. This\ncorresponds to a slice through the distribution, hence the term slice sampling (Neal 2003a).\nSee Figure 24.14(a).\nIn practice, it can be difficult to identify the set A. So we can use the following approach:\nconstruct an interval xmin\u2264x\u2264xmaxaround the current point xsof some width. We then", "896": "866 Chapter24. MarkovchainMonteCarlo(MCMC)inference\ntest to see if each end point lies within the slice. If it does, we keep extending in that direction\nuntil it lies outside the slice. This is called stepping out. A candidate value x/primeis then chosen\nuniformly from this region. If it lies within the slice, it is kept, so xs+1=x/prime. Otherwise we\nshrink the region such that x/primeforms one end and such that the region still contains xs. Then\nanother sample is drawn. We continue in this way until a sample is accepted.\nTo apply the method to multivariate distributions, we can sample one extra auxiliary variable\nfor each dimension. The advantage of slice sampling over Gibbs is that it does not needa speci\ufb01cation of the full-conditionals, just the unnormalized joint. The advantage of slicesampling over MH is that it does not need a user-speci\ufb01ed proposal distribution (although itdoes require a speci\ufb01cation of the width of the stepping out interval).\nFigure 24.14(b) illustrates the algorithm in action on a synthetic 1d problem. Figure 24.15\nillustrates its behavior on a slightly harder problem, namely binomial logistic regression. Themodel has the form\ny\ni\u223cBin(ni,logit(\u03b21+\u03b22xi)) (24.97)\nWe use a vague Gaussian prior for the \u03b2j\u2019s. Figure 24.15(a) shows a grid-based approximation\nto the posterior, and Figure 24.15(b) shows a sample-based approximation. In this example, thegrid is faster to compute, but for any problem with more than 2 dimensions, the grid approachis infeasible.\n24.5.3 Swendsen Wang\nConsider an Ising model of the following form:\np(x)=1\nZ/productdisplay\nefe(xe) (24.98)\nwherexe=(xi,xj)for edgee=(i,j),xi\u2208{+1,\u22121}, and the edge factor feis de\ufb01ned by/parenleftbiggeJe\u2212J\ne\u2212JeJ/parenrightbigg\n,w h e r eJis the edge strength. Gibbs sampling in such models can be slow when\nJis large in absolute value, because neighboring states can be highly correlated. The Swendsen\nWangalgorithm (Swendsen and Wang 1987) is a auxiliary variable MCMC sampler which mixes\nmuch faster, at least for the case of attractive or ferromagnetic models, with J>0.\nSuppose we introduce auxiliary binary variables, one per edge.5These are called bond\nvariables, and will be denoted by z. We then de\ufb01ne an extended model p(x,z)of the form\np(x,z)=1\nZ/prime/productdisplay\nege(xe,ze) (24.99)\nwhereze\u2208{0,1}, and we de\ufb01ne the new factor as follows: ge(xe,ze=0 )=/parenleftbigge\u2212Je\u2212J\ne\u2212Je\u2212J/parenrightbigg\n,\nandge(xe,ze=1 )=/parenleftbiggeJ\u2212e\u2212J0\n0eJ\u2212e\u2212J/parenrightbigg\n. It is clear that/summationtext1\nze=0ge(xe,ze)=fe(xe),\n5. Our presentation of the method is based on some notes by David Mackay, available from http://www .inference\n.phy.cam.ac.uk/mackay/itila/swendsen .pdf.", "897": "24.5. AuxiliaryvariableMCMC* 867\nFigure 24.16 Illustration of the Swendsen Wang algorithm on a 2d grid. Used with kind permission of\nKevin Tang.\nand hence that/summationtext\nzp(x,z)=p(x). So if we can sample from this extended model, we can just\nthrow away the zsamples and get valid xsamples from the original distribution.\nFortunately, it is easy to apply Gibbs sampling to this extended model. The full conditional\np(z|x)factorizes over the edges, since the bond variables are conditionally independent given\nthe node variables. Furthermore, the full conditional p(ze|xe)is simple to compute: if the\nnodes on either end of the edge are in the same state ( xi=xj), we set the bond zeto 1 with\nprobability p=1\u2212e\u22122J, otherwise we set it to 0. In Figure 24.16 (top right), the bonds that\ncould be turned on (because their corresponding nodes are in the same state) are represented\nby dotted edges. In Figure 24.16 (bottom right), the bonds that are randomly turned on are\nrepresented by solid edges.\nTo sample p(x|z), we proceed as follows. Find the connected components de\ufb01ned by the\ngraph induced by the bonds that are turned on. (Note that a connected component may consist\nof a singleton node.) Pick one of these components uniformly at random. All the nodes in each\nsuch component must have the same state, since the off-diagonal terms in the ge(xe,ze=1 )\nfactor are 0. Pick a state \u00b11uniformly at random, and force all the variables in this component\nto adopt this new state. This is illustrated in Figure 24.16 (bottom left), where the green square", "898": "868 Chapter24. MarkovchainMonteCarlo(MCMC)inference\ndenotes the selected connected component, and we choose to force all nodes within in to enter\nthe white state.\nThe validity of this algorithm is left as an exercise, as is the extension to handle local evidence\nand non-stationary potentials.\nIt should be intuitively clear that Swendsen Wang makes much larger moves through the state\nspace than Gibbs sampling. In fact, SW mixes much faster than Gibbs sampling on 2d latticeIsing models for a variety of values of the coupling parameter, provided J>0. More precisely,\nlet the edge strength be parameterized by J/T,w h e r eT>0is a computational temperature.\nFor large T, the nodes are roughly independent, so both methods work equally well. However,\nasTapproaches a critical temperature T\nc, the typical states of the system have very long\ncorrelation lengths, and Gibbs sampling takes a very long time to generate independent samples.As the temperature continues to drop, the typical states are either all on or all off. The frequencywith which Gibbs sampling moves between these two modes is exponentiall small. By contrast,SW mixes rapidly at all temperatures.\nUnfortunately, if any of the edge weights are negative, J<0, the system is frustrated, and\nthere are exponentially many modes, even at low temperature. SW does not work very well inthis setting, since it tries to force many neighboring variables to have the same state. In fact,computation in this regime is provably hard for any algorithm (Jerrum and Sinclair 1993, 1996).\n24.5.4 Hybrid/Hamiltonian MCMC *\nIn this section, we brie\ufb02y mention a way to perform MCMC sampling for continuous statespaces, for which we can compute the gradient of the (unnormalized) log-posterior. This is thecase in neural network models, for example.\nThe basic idea is to think of the parameters as a particle in space, and to create auxiliary\nvariables which represent the \u201cmomentum\u201d of this particle. We then update this parameter/momentum pair according to certain rules (see e.g., (Duane et al. 1987; Neal 1993; MacKay 2003;Neal 2010) for details). The resulting method is called hybrid MCMC orHamiltonian MCMC.\nThe two main parameters that the user must specify are how many leapfrog steps to take\nwhen updating the position/ momentum, and how big to make these steps. Performance canbe quite sensitive to these parameters (although see (Hoffman and Gelman 2011) for a recentway to set them automatically). This method can be combined with stochastic gradient descent(Section 8.5.2) in order to handle large datasets, as explained in (Ahn et al. 2012).\nRecently, a more powerful extension of this method has been developed, that exploits second-\norder gradient information. See (Girolami et al. 2010) for details.\n24.6 Annealing methods\nMany distributions are multimodal and hence hard to sample from. However, by analogy to theway metals are heated up and then cooled down in order to make the molecules align, we canimagine using a computational temperature parameter to smooth out a distribution, graduallycooling it to recover the original \u201cbumpy\u201d distribution. We \ufb01rst explain this idea in more detailin the context of an algorithm for MAP estimation. We then discuss extensions to the samplingcase.", "899": "24.6. Annealingmethods 869\n01020304050\n02040600246810\nxtemp 1.000\ny\n(a)01020304050\n02040600246810x 105\nxtemp 0.200\ny\n(b)\nFigure 24.17 An energy surface at different temperatures. Note the different vertical scales. (a) T=1.\n(b)T=0.5. Figure generated by saDemoPeaks .\n24.6.1 Simulated annealing\nSimulated annealing (Kirkpatrick et al. 1983) is a stochastic algorithm that attempts to \ufb01nd\nthe global optimum of a black-box function f(x). It is closely related to the Metropolis-\nHastings algorithm for generating samples from a probability distribution, which we discussed\nin Section 24.3. SA can be used for both discrete and continuous optimization.\nThe method is inspired by statistical physics. The key quantity is the Boltzmann distribution,\nwhich speci\ufb01es that the probability of being in any particular state xis given by\np(x)\u221dexp(\u2212f(x)/T) (24.100)\nwheref(x)is the \u201cenergy\u201d of the system and Tis the computational temperature. As the\ntemperature approaches 0 (so the system is cooled), the system spends more and more time inits minimum energy (most probable) state.\nFigure 24.17 gives an example of a 2d function at different temperatures. At high temperatures,\nT/greatermuch1, the surface is approximately \ufb02at, and hence it is easy to move around (i.e., to avoid\nlocal optima). As the temperature cools, the largest peaks become larger, and the smallest peaksdisappear. By cooling slowly enough, it is possible to \u201ctrack\u201d the largest peak, and thus \ufb01nd theglobal optimum. This is an example of a continuation method.\nWe can generate an algorithm from this as follows. At each step, sample a new state according\nto some proposal distribution x\n/prime\u223cq(\u00b7|xk). For real-valued parameters, this is often simply a\nrandom walk proposal, x/prime=xk+/epsilon1k,w h e r e/epsilon1k\u223cN(0,\u03a3). For discrete optimization, other\nkinds of local moves must be de\ufb01ned.\nHaving proposed a new state, we compute\n\u03b1= exp((f (x)\u2212f(x/prime))/T) (24.101)\nWe then accept the new state (i.e., set xk+1=x/prime) with probability min(1,\u03b1), otherwise we stay\nin the current state (i.e., set xk+1=xk). This means that if the new state has lower energy (is\nmore probable), we will de\ufb01nitely accept it, but it it has higher energy (is less probable), we mightstill accept, depending on the current temperature. Thus the algorithm allows \u201cdown-hill\u201d movesin probability space (up-hill in energy space), but less frequently as the temperature drops.", "900": "870 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n0 200 400 600 800 1000 120000.10.20.30.40.50.60.70.80.91temperature vs iteration\n(a)0 200 400 600 800 1000\u22128.5\u22128\u22127.5\u22127\u22126.5\u22126\u22125.5\u22125\u22124.5energy vs iteration\n(b)\nFigure 24.18 A run of simulated annealing on the energy surface in Figure 24.17. (a) Temperature vs\niteration. (b) Energy vs iteration. Figure generated by saDemoPeaks .\n353637383940\n22242628300102030405060\nx\niter 550, temp 0.064\ny\n(a)353637383940\n2224262830050100150\nx\niter 1000, temp 0.007\ny\n(b)\nFigure 24.19 Histogram of samples from the annealed \u201cposterior\u201d at 2 different time points produced by\nsimulated annealing on the energy surface shown in Figure 24.17. Note that at cold temperatures, most of\nthe samples are concentrated near the peak at (38,25). Figure generated by saDemoPeaks .\nThe rate at which the temperature changes over time is called the cooling schedule .I t\nhas been shown (Kirkpatrick et al. 1983) that if one cools sufficiently slowly, the algorithm will\nprovably \ufb01nd the global optimum. However, it is not clear what \u201csufficient slowly\u201d means.\nIn practice it is common to use an exponential cooling schedule of the following form:\nTk=T0Ck,w h e r eT0is the initial temperature (often T0\u223c1) andCis the cooling rate (often\nC\u223c0.8). See Figure 24.18(a) for a plot of this cooling schedule. Cooling too quickly means one\ncan get stuck in a local maximum, but cooling too slowly just wastes time. The best cooling\nschedule is difficult to determine; this is one of the main drawbacks of simulated annealing.\nFigure 24.18(b) shows an example of simulated annealing applied to the function in Figure 24.17\nusing a random walk proposal. We see that the method stochastically reduces the energy\nover time. Figures 24.19 illustrate (a histogram of) samples drawn from the cooled probability\ndistribution over time. We see that most of the samples are concentrated near the global\nmaximum. When the algorithm has converged, we just return the largest value found.", "901": "24.6. Annealingmethods 871\n24.6.2 Annealed importance sampling\nWe now describe a method known as annealed importance sampling (Neal 2001) that com-\nbines ideas from simulated annealing and importance sampling in order to draw independent\nsamples from difficult (e.g., multimodal) distributions.\nSuppose we want to sample from p0(x)\u221df0(x), but we cannot do so easily; for example,\nthis might represent a multimodal posterior. Suppose however that there is an easier distributionwhich we can sample from, call it p\nn(x)\u221dfn(x); for example, this might be the prior. We\ncan now construct a sequence of intermediate distributions than move slowly from pntop0as\nfollows:\nfj(x)=f0(x)\u03b2jfn(x)1\u2212\u03b2j(24.102)\nwhere1=\u03b20>\u03b21>\u00b7\u00b7\u00b7>\u03b2n=0,w h e r e\u03b2jis an inverse temperature. (Contrast this to the\nscheme used by simulated annealing which has the form fj(x)=f0(x)\u03b2j; this makes it hard\nto sample from pn.) Furthermore, suppose we have a series of Markov chains Tj(x,x/prime)(fromx\ntox/prime) which leave each pjinvariant. Given this, we can sample xfromp0by \ufb01rst sampling a\nsequence z=(zn\u22121,...,z0)as follows: sample zn\u22121\u223cpn; samplezn\u22122\u223cTn\u22121(zn\u22121,\u00b7); ...;\nsamplez0\u223cT1(z1,\u00b7). Finally we set x=z0and give it weight\nw=fn\u22121(zn\u22121)\nfn(zn\u22121)fn\u22122(zn\u22122)\nfn\u22121(zn\u22122)\u00b7\u00b7\u00b7f1(z1)\nf2(z1)f0(z0)\nf1(z0)(24.103)\nThis can be shown to be correct by viewing the algorithm as a form of importance sampling\nin an extended state space z=(z0,...,zn\u22121). Consider the following distribution on this state\nspace:\np(z)\u221df(z)=f0(z0)\u02dcT1(z0,z1)\u02dcT2(z1,z2)\u00b7\u00b7\u00b7\u02dcTn\u22121(zn\u22122,zn\u22121) (24.104)\nwhere\u02dcTjis the reversal of Tj:\n\u02dcTj(z,z/prime)=Tj(z/prime,z)pj(z/prime)/pj(z)=Tj(z/prime,z)fj(z/prime)/fj(z) (24.105)\nIt is clear that/summationtext\nz1,...,zn\u22121f(z)=f0(z0), so we can safely just use the z0part of these\nsequences to recover the original ditribution.\nNow consider the proposal distribution de\ufb01ned by the algorithm:\nq(z)\u221dg(z)=fn(zn\u22121)Tn\u22121(zn\u22121,zn\u22122)\u00b7\u00b7\u00b7T2(z2,z1)T1(z1,z0) (24.106)\nOne can show that the importance weights w=f(z0,...,zn\u22121)\ng(z0,...,zn\u22121)are given by Equation 24.103.\n24.6.3 Parallel tempering\nAnother way to combine MCMC and annealing is to run multiple chains in parallel at differenttemperatures, and allow one chain to sample from another chain at a neighboring temperature.In this way, the high temperature chain can make long distance moves through the state space,and have this in\ufb02uence lower temperature chains. This is known as parallel tempering. See\ne.g., (Earl and Deem 2005) for details.", "902": "872 Chapter24. MarkovchainMonteCarlo(MCMC)inference\n24.7 Approximating the marginal likelihood\nThe marginal likelihood p(D|M)is a key quantity for Bayesian model selection, and is given by\np(D|M)=/integraldisplay\np(D|\u03b8,M)p(\u03b8|M)d\u03b8 (24.107)\nUnfortunately, this integral is often intractable to compute, for example if we have non conjugate\npriors, and/or we have hidden variables. In this section, we brie\ufb02y discuss some ways toapproximate this expression using Monte Carlo. See (Gelman and Meng 1998) for a moreextensive review.\n24.7.1 The candidate method\nThere is a simple method for approximating the marginal likelihood known as the Candidate\nmethod(Chib 1995). This exploits the following identity:\np(D|M)=p(D|\u03b8,M)p(\u03b8|M)\np(\u03b8|D,M)(24.108)\nThis holds for any value of \u03b8. Once we have picked some value, we can evaluate p(D|\u03b8,M)\nandp(\u03b8|M)quite easily. If we have some estimate of the posterior near \u03b8, we can then evaluate\nthe denominator as well. This posterior is often approximated using MCMC.\nThe \ufb02aw with this method is that it relies on the assumption that p(\u03b8|D,M)has marginalized\nover all the modes of the posterior, which in practice is rarely possible. Consequently the methodcan give very inaccurate results in practice (Neal 1998).\n24.7.2 Harmonic mean estimate\nNewton and Raftery (1994) proposed a simple method for approximating p(D)using the output\nof MCMC, as follows:\n1/p(D)\u22481\nSS/summationdisplay\ns=11\np(D|\u03b8s)(24.109)\nwhere\u03b8s\u223cp(\u03b8|D). This expression is the harmonic mean of the likelihood of the data under\neach sample. The theoretical correctness of this expression follows from the following identity:\n/integraldisplay1\np(D|\u03b8)p(\u03b8|D)d\u03b8=/integraldisplay1\np(D|\u03b8)p(D|\u03b8)p(\u03b8)\np(D)d\u03b8=1\np(D)/integraldisplay\np(\u03b8|D)d\u03b8=1\np(D)(24.110)\nUnfortunately, in practice this method works very poorly. Indeed, Radford Neal called this \u201ctheworst Monte Carlo method ever\u201d.\n6. The reason it is so bad is that it depends only on samples\ndrawn from the posterior. But the posterior is often very insensitive to the prior, whereas themarginal likelihood is not. We only mention this method in order to warn against its use. Wepresent a better method below.\n6. Source: radfordneal .wordpress .com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-mon\nte-carlo-method-ever .", "903": "24.7. Approximatingthemarginallikelihood 873\n24.7.3 Annealed importance sampling\nWe can use annealed importance sampling (Section 24.6.2) to evaluate a ratio of partition\nfunctions. Notice that Z0=/integraltext\nf0(x)dx=/integraltext\nf(z)dz, andZn=/integraltext\nfn(x)dx=/integraltext\ng(z)dz. Hence\nZ0\nZn=/integraltext\nf(z)dz/integraltext\ng(z)dz=/integraltextf(z)\ng(z)g(z)dz/integraltext\ng(z)dz=Eq/bracketleftbiggf(z)\ng(z)/bracketrightbigg\n\u22481\nSS/summationdisplay\ns=1ws (24.111)\nIffnis a prior and f0is the posterior, we can estimate Zn=p(D)using the above equation,\nprovided the prior has a known normalization constant Z0. This is generally considered the\nmethod of choice for evaluating difficult partition functions.\nExercises\nExercise 24.1 Gibbs sampling from a 2D Gaussian\nSuppose x\u223cN(\u03bc,\u03a3),w h e r e\u03bc=( 1,1)and\u03a3=( 1,\u22120.5;\u22120.5,1). Derive the full condition-\nalsp(x1|x2)andp(x2|x1). Implement the algorithm and plot the 1d marginals p(x1)andp(x2)as\nhistograms. Superimpose a plot of the exact marginals.\nExercise 24.2 Gibbs sampling for a 1D Gaussian mixture model\nConsider applying Gibbs sampling to a univariate mixture of Gaussians, as in Section 24.2.3. Derive the\nexpressions for the full conditionals. Hint: if we know zn=j(say), then \u03bcjgets \u201cconnected\u201d to xn, but\nall other values of \u03bci, for alli/negationslash=j, are irrelevant. (This is an example of context-speci\ufb01c independence,\nwhere the structure of the graph simpli\ufb01es once we have assigned values to some of the nodes.) Hence,given all the z\nnvalues, the posteriors of the \u03bc\u2019s should be independent, so the conditional of \u03bcjshould\nbe independent of \u03bc\u2212j. (Similarly for \u03c3j.)\nExercise 24.3 Gibbs sampling from the Potts model\nModify the code in gibbsDemoIsing to draw samples from a Potts prior at different temperatures, as in\nFigure 19.8.\nExercise 24.4 Full conditionals for hierarchical model of Gaussian means\nLet us reconsider the Gaussian-Gaussian model in Section 5.6.2 for modelling multiple related mean\nparameters \u03b8j. In this exercise we derive a Gibbs sampler instead of using EB. Suppose, following (Hoff\n2009, p134)), that we use the following conjugate priors on the hyper-parameters:\n\u03bc\u223cN(\u03bc0,\u03b32\n0) (24.112)\n\u03c42\u223cIG(\u03b70/2,\u03b70\u03c42\n0/2) (24.113)\n\u03c32\u223cIG(\u03bd0/2,\u03bd0\u03c32\n0/2) (24.114)", "904": "874 Chapter24. MarkovchainMonteCarlo(MCMC)inference\nWe can set \u03b7=(\u03bc0,\u03b30,\u03b70,\u03c40,\u03bd0,\u03c30)to uninformative values. Given this model speci\ufb01cation, show that\nthe full conditionals for \u03bc,\u03c4,\u03c3and the\u03b8jare as follows:\np(\u03bc|\u03b81:D,\u03c42)=N (\u03bc|D\u03b8/\u03c42+\u03bc0/\u03b32\n0\nD/\u03c42+1/\u03b32\n0,[D/\u03c42+1/\u03b32\n0]\u22121) (24.115)\np(\u03b8j|\u03bc,\u03c42,Dj,\u03c32)=N (\u03b8j|Njxj/\u03c32+1/\u03c42\nNj/\u03c32+1/\u03c42,[Nj/\u03c32+1/\u03c42]\u22121) (24.116)\np(\u03c42|\u03b81:D,\u03bc)=I G ( \u03c42|\u03b70+D\n2,\u03b70\u03c42\n0+/summationtext\nj(\u03b8j\u2212\u03bc)2\n2) (24.117)\np(\u03c32|\u03b81:D,D)=I G ( \u03c32|1\n2[\u03bd0+D/summationdisplay\nj=1Nj],1\n2[\u03bd0\u03c32\n0+D/summationdisplay\nj=1Nj/summationdisplay\ni=1(xij\u2212\u03b8j)2]) (24.118)\nExercise 24.5 Gibbs sampling for robust linear regression with a Student t likelihood\nModify the EM algorithm in Exercise 11.12 to perform Gibbs sampling for p(w,\u03c32,z|D,\u03bd).\nExercise 24.6 Gibbs sampling for probit regression\nModify the EM algorithm in Section 11.4.6 to perform Gibbs sampling for p(w,z|D). Hint: we can\nsample from a truncated Gaussian, N(z|\u03bc,\u03c3)I(a\u2264z\u2264b)in two steps: \ufb01rst sample u\u223cU(\u03a6((a\u2212\n\u03bc)/\u03c3),\u03a6((b\u2212\u03bc)/\u03c3)), then set z=\u03bc+\u03c3\u03a6\u22121(u)(Robert 1995).\nExercise 24.7 Gibbs sampling for logistic regression with the Student approximation\nDerive the full conditionals for the joint model de\ufb01ned by Equations 24.88 to 24.91.", "905": "25 Clustering\n25.1 Introduction\nClustering is the process of grouping similar objects together. There are two kinds of inputs we\nmight use. In similarity-based clustering, the input to the algorithm is an N\u00d7Ndissimilarity\nmatrixordistance matrix D.I nfeature-based clustering, the input to the algorithm is an\nN\u00d7Dfeature matrix or design matrix X. Similarity-based clustering has the advantage that it\nallows for easy inclusion of domain-speci\ufb01c similarity or kernel functions (Section 14.2). Feature-\nbased clustering has the advantage that it is applicable to \u201craw\u201d, potentially noisy data. We willsee examples of both below.\nIn addition to the two types of input, there are two possible types of output: \ufb02at cluster-\ning, also called partitional clustering, where we partition the objects into disjoint sets; and\nhierarchical clustering , where we create a nested tree of partitions. We will discuss both of\nthese below. Not surprisingly, \ufb02at clusterings are usually faster to create (O (ND)for \ufb02at vs\nO(N\n2logN)for hierarchical), but hierarchical clusterings are often more useful. Furthermore,\nmost hierarchical clustering algorithms are deterministic and do not require the speci\ufb01cation ofK, the number of clusters, whereas most \ufb02at clustering algorithms are sensitive to the initial\nconditions and require some model selection method for K. (We will discuss how to choose K\nin more detail below.)\nThe \ufb01nal distinction we will make in this chapter is whether the method is based on a\nprobabilistic model or not. One might wonder why we even bother discussing non-probabilisticmethods for clustering. The reason is two-fold: \ufb01rst, they are widely used, so readers shouldknow about them; second, they often contain good ideas, which can be used to speed upinference in a probabilistic models.\n25.1.1 Measuring (dis)similarity\nA dissimilarity matrix Dis a matrix where di,i=0anddi,j\u22650is a measure of \u201cdistance\u201d\nbetween objects iandj. Subjectively judged dissimilarities are seldom distances in the strict\nsense, since the triangle inequality, di,j\u2264di,k+dj,k, often does not hold. Some algorithms\nrequireDto be a true distance matrix, but many do not. If we have a similarity matrix S,w e\ncan convert it to a dissimilarity matrix by applying any monotonically decreasing function, e.g.,D= max(S )\u2212S.\nThe most common way to de\ufb01ne dissimilarity between objects is in terms of the dissimilarity", "906": "876 Chapter25. Clustering\nof their attributes:\n\u0394(xi,xi/prime)=D/summationdisplay\nj=1\u0394j(xij,xi/primej) (25.1)\nSome common attribute dissimilarity functions are as follows:\n\u2022 Squared (Euclidean) distance:\n\u0394j(xij,xi/primej)=(xij\u2212xi/primej)2(25.2)\nOf course, this only makes sense if attribute jis real-valued.\n\u2022 Squared distance strongly emphasizes large differences (because differences are squared). A\nmore robust alternative is to use an /lscript1distance:\n\u0394j(xij,xi/primej)=|xij\u2212xi/primej| (25.3)\nThis is also called city block distance, since, in 2D, the distance can be computed by\ncounting how many rows and columns we have to move horizontally and vertically to get\nfromxitoxi/prime.\n\u2022I fxiis a vector (e.g., a time-series of real-valued data), it is common to use the correlation\ncoefficient (see Section 2.5.1). If the data is standardized, then corr[xi,xi/prime]=/summationtext\njxijxi/primej,\nand hence/summationtext\nj(xij\u2212xi/primej)2=2 ( 1\u2212corr[xi,xi/prime]). So clustering based on correlation\n(similarity) is equivalent to clustering based on squared distance (dissimilarity).\n\u2022 For ordinal variables, such as {low, medium, high}, it is standard to encode the values as\nreal-valued numbers, say 1/3,2/3,3/3if there are 3 possible values. One can then apply\nany dissimilarity function for quantitative variables, such as squared distance.\n\u2022 For categorical variables, such as {red, green, blue}, we usually assign a distance of 1 if the\nfeatures are different, and a distance of 0 otherwise. Summing up over all the categoricalfeatures gives\n\u0394(x\ni,xi)=D/summationdisplay\nj=1I(xij/negationslash=xi/primej) (25.4)\nThis is called the hamming distance.\n25.1.2 Evaluating the output of clustering methods *\nThe validation of clustering structures is the most difficult and frustrating part of clusteranalysis. Without a strong effort in this direction, cluster analysis will remain a black artaccessible only to those true believers who have experience and great courage. \u2014 Jainand Dubes (Jain and Dubes 1988)", "907": "25.1. Introduction 877\n$\u0003$\u0003$\u0003\n$\u0003$\u0003%$\u0003%\u0003%%\u0003%\u0003&$\u0003$\u0003&\u0003&\u0003&\nFigure 25.1 Three clusters with labeled objects inside. Based on Figure 16.4 of (Manning et al. 2008).\nClustering is an unupervised learning technique, so it is hard to evaluate the quality of the output\nof any given method. If we use probabilistic models, we can always evaluate the likelihood ofa test set, but this has two drawbacks: \ufb01rst, it does not directly assess any clustering that isdiscovered by the model; and second, it does not apply to non-probabilistic methods. So nowwe discuss some performance measures not based on likelihood.\nIntuitively, the goal of clustering is to assign points that are similar to the same cluster,\nand to ensure that points that are dissimilar are in different clusters. There are several waysof measuring these quantities e.g., see (Jain and Dubes 1988; Kaufman and Rousseeuw 1990).However, these internal criteria may be of limited use. An alternative is to rely on some externalform of data with which to validate the method. For example, suppose we have labels for eachobject, as in Figure 25.1. (Equivalently, we can have a reference clustering; given a clustering, wecan induce a set of labels and vice versa.) Then we can compare the clustering with the labelsusing various metrics which we describe below. We will use some of these metrics later, whenwe compare clustering methods.\n25.1.2.1 Purity\nLetN\nijbe the number of objects in cluster ithat belong to class j, and letNi=/summationtextC\nj=1Nijbe\nthe total number of objects in cluster i. De\ufb01ne pij=Nij/Ni; this is the empirical distribution\nover class labels for cluster i. We de\ufb01ne the purityof a cluster as pi/definesmaxjpij, and the\noverall purity of a clustering as\npurity/defines/summationdisplay\niNi\nNpi (25.5)\nFor example, in Figure 25.1, we have that the purity is\n6\n175\n6+6\n174\n6+5\n173\n5=5+4+3\n17=0.71 (25.6)\nThe purity ranges between 0 (bad) and 1 (good). However, we can trivially achieve a purity of\n1 by putting each object into its own cluster, so this measure does not penalize for the number\nof clusters.\n25.1.2.2 Rand index\nLetU={u1,...,u R}andV={v1,...,V C}be two different partitions of the Ndata points,\ni.e., two different (\ufb02at) clusterings. For example, Umight be the estimated clustering and V\nis reference clustering derived from the class labels. Now de\ufb01ne a 2\u00d72contingency table,", "908": "878 Chapter25. Clustering\ncontaining the following numbers: TPis the number of pairs that are in the same cluster in\nbothUandV(true positives); TNis the number of pairs that are in the different clusters in\nbothUandV(true negatives); FNis the number of pairs that are in the different clusters in\nUbut the same cluster in V(false negatives); and FPis the number of pairs that are in the\nsame cluster in Ubut different clusters in V(false positives). A common summary statistic is\ntheRand index:\nR/definesTP+TN\nTP+FP+FN+TN(25.7)\nThis can be interpreted as the fraction of clustering decisions that are correct. Clearly 0\u2264R\u2264\n1.\nFor example, consider Figure 25.1, The three clusters contain 6, 6 and 5 points, so the number\nof \u201cpositives\u201d (i.e., pairs of objects put in the same cluster, regardless of label) is\nTP+FP=/parenleftbigg6\n2/parenrightbigg\n+/parenleftbigg62/parenrightbigg\n+/parenleftbigg52/parenrightbigg\n=4 0 (25.8)\nOf these, the number of true positives is given by\nTP=/parenleftbigg52/parenrightbigg\n+/parenleftbigg52/parenrightbigg\n+/parenleftbigg32/parenrightbigg\n+/parenleftbigg22/parenrightbigg\n=2 0 (25.9)\nwhere the last two terms come from cluster 3: there are/parenleftbigg32/parenrightbigg\npairs labeled Cand/parenleftbigg22/parenrightbigg\npairs\nlabeledA.S oFP=4 0\u221220 = 20. Similarly, one can show FN=2 4andTN=7 2. So the\nRand index is (20+72) /(20+20+24+72) = 0. 68.\nThe Rand index only achieves its lower bound of 0 if TP=TN=0, which is a rare event.\nOne can de\ufb01ne an adjust\n ed Rand index (Hubert and Arabie 1985) as follows:\nAR/definesindex\u2212expected index\nmax index\u2212expected index(25.10)\nHere the model of randomness is based on using the generalized hyper-geometric distribution,i.e., the two partitions are picked at random subject to having the original number of classesand objects in each, and then the expected value of TP+TNis computed. This model can\nbe used to compute the statistical signi\ufb01cance of the Rand index.\nThe Rand index weights false positives and false negatives equally. Various other summary\nstatistics for binary decision problems, such as the F-score (Section 5.7.2.2), can also be used.One can compute their frequentist sampling distribution, and hence their statistical signi\ufb01cance,using methods such as bootstrap.\n25.1.2.3 Mutual information\nAnother way to measure cluster quality is to compute the mutual information between Uand\nV(Vaithyanathan and Dom 1999). To do this, let p\nUV(i,j)=|ui\u2229vj|\nNbe the probability that\na randomly chosen object belongs to cluster uiinUandvjinV. Also, let pU(i)=|ui|/N\nbe the be the probability that a randomly chosen object belongs to cluster uiinU; de\ufb01ne", "909": "25.2. Dirichletprocessmixturemodels 879\npV(j)=|vj|/Nsimilarly. Then we have\nI(U,V)=R/summationdisplay\ni=1C/summationdisplay\nj=1pUV(i,j)logpUV(i,j)\npU(i)pV(j)(25.11)\nThis lies between 0 and min{H(U),H(V)}. Unfortunately, the maximum value can be\nachieved by using lots of small clusters, which have low entropy. To compensate for this,\nwe can use the normalized mutual information,\nNMI(U,V)/definesI(U,V)\n(H(U)+H(V))/2(25.12)\nThis lies between 0 and 1. A version of this that is adjusted for chance (under a particularrandom data model) is described in (Vinh et al. 2009). Another variant, called variation of\ninformation, is described in (Meila 2005).\n25.2 Dirichlet process mixture models\nThe simplest approach to (\ufb02at) clustering is to use a \ufb01nite mixture model, as we discussed inSection 11.2.3. This is sometimes called model-based clustering, since we de\ufb01ne a probabilistic\nmodel of the data, and optimize a well-de\ufb01ned objective (the likelihood or posterior), as opposedto just using some heuristic algorithm.\nThe principle problem with \ufb01nite mixture models is how to choose the number of components\nK. We discussed several techniques in Section 11.5. However, in many cases, there is no well-\nde\ufb01ned number of clusters. Even in the simple 2d height-weight data (Figure 1.8), it is not clearif the \u201ccorrect\u201d value of Kshould be 2, 3, or 4. It would be much better if we did not have to\nchooseKat all.\nIn this section, we discuss in\ufb01nite mixture models , in which we do not impose any a priori\nbound on K. To do this, we will use a non-parametric prior based on the Dirichlet process\n(DP). This allows the number of clusters to grow as the amount of data increases. It will alsoprove useful later when we discuss hiearchical clustering.\nThe topic of non-parametric Bayes is currently very active, and we do not have space to\ngo into details (see (Hjort et al. 2010) for a recent book on the topic). Instead we just give abrief review of the DP and its application to mixture modeling, based on the presentation in(Sudderth 2006, sec 2.2).\n25.2.1 From \ufb01nite to in\ufb01nite mixture models\nConsider a \ufb01nite mixture model, as shown in Figure 25.2(a). The usual representation is asfollows:\np(x\ni|zi=k,\u03b8)=p( xi|\u03b8k) (25.13)\np(zi=k|\u03c0)=\u03c0 k (25.14)\np(\u03c0|\u03b1)=D i r ( \u03c0|(\u03b1/K)1K) (25.15)\nThe form of p(\u03b8k|\u03bb)is chosen to be conjugate to p(xi|\u03b8k). We can write p(xi|\u03b8k)asxi\u223c\nF(\u03b8zi),w h e r eFis the observation distribution. Similarly, we can write \u03b8k\u223cH(\u03bb),w h e r eH\nis the prior.", "910": "880 Chapter25. Clustering\n(a) (b)\nFigure 25.2 Two different representations of a \ufb01nite mixture model. Left: traditional representation.\nRight: representation where parameters are samples from G, a discrete measure. The picture on the right\nillustrates the case where K=4, and we sample 4 Gaussian means \u03b8kfrom a Gaussian prior H(.|\u03bb). The\nheight of the spikes re\ufb02ects the mixing weights \u03c0k. This weighted sum of delta functions is G. We then\ngenerate two parameters, \u03b81and\u03b82,f r o mG, one per data point. Finally, we generate two data points,\nx1andx2,f r o mN(\u03b81,\u03c32)andN(\u03b82,\u03c32). Source: Figure 2.9 of (Sudderth 2006) . Used with kind\npermission of Erik Sudderth.\nAn equivalent representation for this model is shown in Figure 25.2(b). Here \u03b8iis the\nparameter used to generate observation xi; these parameters are sampled from distribution G,\nwhich has the form\nG(\u03b8)=K/summationdisplay\nk=1\u03c0k\u03b4\u03b8k(\u03b8) (25.16)\nwhere\u03c0\u223cDir(\u03b1\nK1), and\u03b8k\u223cH. Thus we see that Gis a \ufb01nite mixture of delta functions,\ncentered on the cluster parameters \u03b8k. The probability that \u03b8iis equal to \u03b8kis exactly \u03c0k, the\nprior probability for that cluster.\nIf we sample from this model, we will always (with probability one) get exactly Kclusters,\nwith data points scattered around the cluster centers. We would like a more \ufb02exible model,\nthat can generate a variable number of clusters. Furthermore, the more data we generate, themore likely we should be to see a new cluster. The way to do this is to replace the discretedistribution Gwith arandom probability measure. Below we will show that the Dirichlet\nprocess, denoted G\u223cDP(\u03b1,H ), is one way to do this.\nBefore we go into the details, we show some samples from this non-parametric model in\nFigure 25.3. We see that it has the desired properties of generating a variable number of clusters,with more clusters as the amount of data increases. The resulting samples look much more likereal data than samples from a \ufb01nite mixture model.\nOf course, working with an \u201cin\ufb01nite\u201d model sounds scary. Fortunately, as we show below,\neven though this model is potentially in\ufb01nite, we can perform inference using an amount ofcomputation that is not only tractable, but is often much less than that required to \ufb01t a set", "911": "25.2. Dirichletprocessmixturemodels 881\n(a) (b)\n(c) (d)\n(e) (f)\nFigure 25.3 Some samples from a Dirichlet process mixture model of 2D Gaussians, with concentration\nparameter \u03b1=1. From left to right, we show N=5 0,N= 500andN= 1000samples. Each row is a\ndifferent run. We also show the model parameters as ellipses, which are sampled from a vague NIW base\ndistribution. Based on Figure 2.25 of (Sudderth 2006). Figure generated by dpmSampleDemo , written by\nYee-Whye Teh.\nof \ufb01nite mixture models for different K. The intuitive reason is that we can get evidence that\ncertain values of Kare appropriate (have high posterior support) long before we have been able\nto estimate the parameters, so we can focus our computational efforts on models of appropriate\ncomplexity. Thus going to the in\ufb01nite limit can sometimes be faster. This is especially truewhen we have multiple model selection problems to solve.", "912": "882 Chapter25. Clustering\n(a) (b) (c)\nFigure 25.4 (a) A base measure Hon a 2d space \u0398. (b) One possible partition into K=3regions,\nwhere the shading of cell Tkis proportional to E[G(Tk)] =H(Tk). (c) A re\ufb01ned partition into K=5\nregions. Source: Figure 2.21 of (Sudderth 2006). Used with kind permission of Erik Sudderth.\n25.2.2 The Dirichlet process\nRecall from Chapter 15 that a Gaussian process is a distribution over functions of the form\nf:X\u2192R. It is de\ufb01ned implicitly by the requirement that p(f(x1),...,f(xN))be jointly\nGaussian, for any set of points xi\u2208X. The parameters of this Gaussian can be computed using\na mean function \u03bc()and covariance (kernel) function K(). We write f\u223cGP(\u03bc(),K()). Fur-\nthermore, the GP is consistently de\ufb01ned, so that p(f(x1))can be derived from p(f(x1),f(x2)),\netc.\nADirichlet process is a distribution over probability measures G:\u0398\u2192R+,w h e r ew e\nrequireG(\u03b8)\u22650and/integraltext\n\u0398G(\u03b8)d\u03b8=1. The DP is de\ufb01ned implicitly by the requirement that\n(G(T1),...,G(TK))has a joint Dirichlet distribution\nDir(\u03b1H(T1),...,\u03b1H (TK)) (25.17)\nfor any \ufb01nite partition (T1,...,T K)of\u0398. If this is the case, we write G\u223cDP(\u03b1,H),w h e r e\n\u03b1is called the concentration parameter andHis called the base measure .1\nAn example of a DP is shown in Figure 25.4, where the base measure is a 2d Gaussian. The\ndistribution over all the cells, p(G(T1),...,G(TK)), is Dirichlet, so the marginals in each cell\nare beta distributed:\nBeta(\u03b1H(Ti),\u03b1/summationdisplay\nj/negationslash=iH(Tj)) (25.18)\nThe DP is consistently de\ufb01ned in the sense that if T1andT2form a partition of \u02dcT1, then\nG(T1)+G(T2)andG(\u02dcT1)both follow the same beta distribution.\nRecall that if \u03c0\u223cDir(\u03b1), andz|\u03c0\u223cCat(\u03c0), then we can integrate out \u03c0to get the\npredictive distribution for the Dirichlet-multinoulli model:\nz\u223cCat(\u03b11/\u03b10,...,\u03b1 K/\u03b10) (25.19)\n1. Unlike a GP, knowing something about G(Tk)does not tell us anything about G(Tk/prime), beyond the sum-to-one\nconstraint; we say that the DP is a neutral process . Other stochastic processes can be de\ufb01ned that do not have this\nproperty, but they are not so computationally convenient.", "913": "25.2. Dirichletprocessmixturemodels 883\n\u03b21\n\u03c01\n\u03c02\n\u03c03\n\u03c04\n\u03c05\u03b22\n\u03b23\n\u03b24\n\u03b251\u2212\u03b2 1\n1\u2212\u03b2 2\n1\u2212\u03b2 3\n1\u2212\u03b2 4\n(a)0 10 20 3000.10.20.30.40.5\u03b1 = 2\n0 10 20 3000.10.20.30.4\u03b1 = 2\n0 10 20 3000.10.20.30.4\u03b1 = 5\n0 10 20 3000.050.10.150.2\u03b1 = 5\n(b)\nFigure 25.5 Illustration of the stick breaking construction. (a) We have a unit length stick, which we\nbreak at a random point \u03b21; the length of the piece we keep is called \u03c01; we then recursively break off\npieces of the remaining stick, to generate \u03c02,\u03c03,.... Source: Figure 2.22 of (Sudderth 2006). Used with\nkind permission of Erik Sudderth. (b) Samples of \u03c0kfrom this process for \u03b1=2(top row) and \u03b1=5\n(bottom row). Figure generated by stickBreakingDemo , written by Yee-Whye Teh.\nwhere\u03b10=/summationtext\nk\u03b1k. In other words, p(z=k|\u03b1)=\u03b1k/\u03b10. Also, the updated posterior for \u03c0\ngiven one observation is given by\n\u03c0|z\u223cDir(\u03b11+I(z=1 ),...,\u03b1 K+I(z=K)) (25.20)\nThe DP generalizes this to arbitrary partitions. If G\u223cDP(\u03b1,H ), thenp(\u03b8\u2208Ti)=H(Ti)and\nthe posterior is\np(G(T1),...,G(TK)|\u03b8,\u03b1,H) = Dir(\u03b1H(T1)+I(\u03b8\u2208T1),...,\u03b1H (TK)+I(\u03b8\u2208TK))(25.21)\nThis holds for any set of partitions. Hence if we observe multiple samples \u03b8i\u223cG, the new\nposterior is given by\nG|\u03b81,...,\u03b8N,\u03b1,H\u223cDP/parenleftBigg\n\u03b1+N,1\n\u03b1+N/parenleftBigg\n\u03b1H+N/summationdisplay\ni=1\u03b4\u03b8i/parenrightBigg/parenrightBigg\n(25.22)\nThus we see that the DP effectively de\ufb01nes a conjugate prior for arbitrary measurable spaces.\nThe concentration parameter \u03b1is like the effective sample size of the base measure H.\n25.2.2.1 Stick breaking construction of the DP\nOur discussion so far has been very abstract. We now give a constructive de\ufb01nition for the DP,known as the stick-breaking construction.\nLet\u03c0={\u03c0\nk}\u221e\nk=1be an in\ufb01nite sequence of mixture weights derived from the following\nprocess:\n\u03b2k\u223cBeta(1,\u03b1) (25.23)\n\u03c0k=\u03b2kk\u22121/productdisplay\nl=1(1\u2212\u03b2l)=\u03b2k(1\u2212k\u22121/summationdisplay\nl=1\u03c0l) (25.24)", "914": "884 Chapter25. Clustering\nThis is often denoted by\n\u03c0\u223cGEM(\u03b1) (25.25)\nwhere GEM stands for Griffiths, Engen and McCloskey (this term is due to (Ewens 1990)). Some\nsamples from this process are shown in Figure 25.5. One can show that this process processwill terminate with probability 1, although the number of elements it generates increases with\u03b1. Furthermore, the size of the \u03c0\nkcomponents decreases on average.\nNow de\ufb01ne\nG(\u03b8)=\u221e/summationdisplay\nk=1\u03c0k\u03b4\u03b8k(\u03b8) (25.26)\nwhere\u03c0\u223cGEM(\u03b1)and\u03b8k\u223cH. Then one can show that G\u223cDP(\u03b1,H ).\nAs a consequence of this construction, we see that samples from a DP are discrete with\nprobability one. In other words, if you keep sampling it, you will get more and more repetitionsof previously generated values. So if we sample\n\u03b8i\u223cG, we will see repeated values; let us\nnumber the unique values \u03b81,\u03b82, etc. Data sampled from \u03b8iwill therefore cluster around the\n\u03b8k. This is evident in Figure 25.3, where most data comes from the Gaussians with large \u03c0k\nvalues, represented by ellipses with thick borders. This is our \ufb01rst indication that the DP mightbe useful for clustering.\n25.2.2.2 The Chinese restaurant process (CRP)\nWorking with in\ufb01nite dimensional sticks is problematic. However, we can exploit the clusteringproperty to draw samples form a GP, as we now show.\nThe key result is this: If\n\u03b8i\u223cGareNobservations from G\u223cDP(\u03b1,H ), taking on K\ndistinct values \u03b8k, then the predictive distribution of the next observation is given by\np(\u03b8N+1=\u03b8|\u03b81:N,\u03b1,H)=1\n\u03b1+N/parenleftBigg\n\u03b1H(\u03b8)+K/summationdisplay\nk=1Nk\u03b4\u03b8k(\u03b8)/parenrightBigg\n(25.27)\nwhereNkis the number of previous observations equal to \u03b8k. This is called the Polya urn or\nBlackwell-MacQueen sampling scheme. This provides a constructive way to sample from a DP.\nIt is much more convenient to work with discrete variables ziwhich specify which value of\n\u03b8kto use. That is, we de\ufb01ne \u03b8i=\u03b8zi. Based on the above expression, we have\np(zN+1=z|z1:N,\u03b1)=1\n\u03b1+N/parenleftBigg\n\u03b1I(z=k\u2217)+K/summationdisplay\nk=1NkI(z=k)/parenrightBigg\n(25.28)\nwherek\u2217represents a new cluster index that has not yet been used. This is called the Chinese\nrestaurant process orCRP, based on the seemingly in\ufb01nite supply of tables at certain Chinese\nrestaurants. The analogy is as follows: The tables are like clusters, and the customers are likeobservations. When a person enters the restaurant, he may choose to join an existing table withprobability proportional to the number of people already sitting at this table (the N\nk); otherwise,\nwith a probability that diminishes as more people enter the room (due to the 1/(\u03b1+N)term),", "915": "25.2. Dirichletprocessmixturemodels 885\n(a) (b)\nFigure 25.6 Two views of a DP mixture model. Left: in\ufb01nite number of clusters parameters, \u03b8k, and\n\u03c0\u223cGEM(\u03b1). Right:Gis drawn from a DP. Compare to Figure 25.2. Source: Figure 2.24 of (Sudderth\n2006). Used with kind permission of Erik Sudderth.\nhe may choose to sit at a new table k\u2217. The result is a distribution over partitions of the\nintegers, which is like a distribution of customers to tables.\nThe fact that currently occupied tables are more likely to get new customers is sometimes\ncalled the rich get richer phenomenon. Indeed, one can derive an expression for the distri-\nbution of cluster sizes induced by this prior process; it is basically a power law. The number\nof occupied tables Kalmost surely approaches \u03b1log(N)asN\u2192\u221e, showing that the model\ncomplexity will indeed grow logarithmically with dataset size. More \ufb02exible priors over clustersizes can also be de\ufb01ned, such as the two-parameter Pitman-Yor process.\n25.2.3 Applying Dirichlet processes to mixture modeling\nThe DP is not particularly useful as a model for data directly, since data vectors rarely repeatexactly. However, it is useful as a prior for the parameters of a stochastic data generatingmechanism, such as a mixture model. To create such a model, we follow exactly the same setupas Section 11.2, but we de\ufb01ne G\u223cDP(\u03b1,H ). Equivalently, we can write the model as follows:\n\u03c0\u223cGEM(\u03b1) (25.29)\nz\ni\u223c\u03c0 (25.30)\n\u03b8k\u223cH(\u03bb) (25.31)\nxi\u223cF(\u03b8zi) (25.32)\nThis is illustrated in Figure 25.6. We see that Gis now a random draw of an unbounded number\nof parameters \u03b8kfrom the base distribution H, each with weight \u03c0k. Each data point xiis\ngenerated by sampling its own \u201cprivate\u201d parameter \u03b8ifromG. As we get more and more data,\nit becomes increasingly likely that \u03b8iwill be equal to one of the \u03b8k\u2019s we have seen before, and\nthusxiwill be generated close to an existing datapoint.", "916": "886 Chapter25. Clustering\n25.2.4 Fitting a DP mixture model\nThe simplest way to \ufb01t a DPMM is to modify the collapsed Gibbs sampler of Section 24.2.4.\nFrom Equation 24.23 we have\np(zi=k|z\u2212i,x,\u03b1,\u03bb)\u221dp(zi=k|z\u2212i,\u03b1)p(xi|x\u2212i,zi=k,z\u2212i,\u03bb) (25.33)\nBy exchangeability, we can assume that ziis the last customer to enter the restaurant. Hence\nthe \ufb01rst term is given by\np(zi|z\u2212i,\u03b1)=1\n\u03b1+N\u22121/parenleftBigg\n\u03b1I(zi=k\u2217)+K/summationdisplay\nk=1Nk,\u2212iI(zi=k)/parenrightBigg\n(25.34)\nwhereKis the number of clusters used by z\u2212i, andk\u2217is a new cluster. Another way to write\nthis is as follows:\np(zi=k|z\u2212i,\u03b1)=/braceleftBigg\nNk,\u2212i\n\u03b1+N\u22121ifkhas been seen before\n\u03b1\n\u03b1+N\u22121ifkis a new cluster(25.35)\nInterestingly, this is equivalent to Equation 24.26, which has the form p(zi=k|z\u2212i,\u03b1)=\nNk,\u2212i+\u03b1/K\n\u03b1+N\u22121,i nt h eK\u2192\u221elimit (Rasmussen 2000; Neal 2000).\nTo compute the second term, p(xi|x\u2212i,zi=k,z\u2212i,\u03bb), let us partition the data x\u2212iinto\nclusters based on z\u2212i.L e tx\u2212i,c={xj:zj=c,j/negationslash=i}be the data assigned to cluster c.I f\nzi=k, thenxiis conditionally independent of all the data points except those assigned to\nclusterk. Hence we have\np(xi|x\u2212i,z\u2212i,zi=k,\u03bb)=p( xi|x\u2212i,k,\u03bb)=p(xi,x\u2212i,k|\u03bb)\np(x\u2212i,k|\u03bb)(25.36)\nwhere\np(xi,x\u2212i,k|\u03bb)=/integraldisplay\np(xi|\u03b8k)\u23a1\n\u23a3/productdisplay\nj/negationslash=i:zj=kp(xj|\u03b8k)\u23a4\u23a6H(\u03b8\nk|\u03bb)d\u03b8k (25.37)\nis the marginal likelihood of all the data assigned to cluster k, including i, andp(x\u2212i,k|\u03bb)is an\nanalogous expression excluding i. Thus we see that the term p(xi|x\u2212i,z\u2212i,zi=k,\u03bb)is the\nposterior preditive distribution for cluster kevaluated at xi.\nIfzi=k\u2217, corresponding to a new cluster, we have\np(xi|x\u2212i,z\u2212i,zi=k\u2217,\u03bb)=p(xi|\u03bb)=/integraldisplay\np(xi|\u03b8)H(\u03b8|\u03bb)d\u03b8 (25.38)\nwhich is just the prior predictive distribution for a new cluster evaluated at xi.\nSee Algorithm 1 for the pseudocode. (This is called \u201cAlgorithm 3\u201d in (Neal 2000).) This is very\nsimilar to collapsed Gibbs for \ufb01nite mixtures except that we have to consider the case zi=k\u2217.\nAn example of this procedure in action is shown in Figure 25.7. The sample clusterings, and\nthe induced posterior over K, seems reasonable. The method tends to rapidly discover a good\nclustering. By contrast, Gibbs sampling (and EM) for a \ufb01nite mixture model often gets stuck in", "917": "25.3. Affinitypropagation 887\nAlgorithm 25.1: Collapsed Gibbs sampler for DP mixtures\n1foreachi=1:Ninrandomorder do\n2Removexi\u2019s sufficient statistics from old cluster zi;\n3foreachk=1:Kdo\n4 Compute pk(xi)=p(xi|x\u2212i(k));\n5 SetNk,\u2212i=d i m (x\u2212i(k));\n6 Compute p(zi=k|z\u2212i,D)=Nk,\u2212i\n\u03b1+N\u22121;\n7Compute p\u2217(xi)=p(xi|\u03bb);\n8Compute p(zi=\u2217|z\u2212i,D)=\u03b1\n\u03b1+N\u22121;\n9Normalize p(zi|\u00b7);\n10Samplezi\u223cp(zi|\u00b7);\n11Addxi\u2019s sufficient statistics to new cluster zi;\n12If any cluster is empty, remove it and decrease K;\npoor local optima (not shown). This is because the DPMM is able to create extra redundant\nclusters early on, and to use them to escape local optima. Figure 25.8 shows that most of thetime, the DPMM converges more rapidly than a \ufb01nite mixture model.\nA variety of other \ufb01tting methods have been proposed. (Daume 2007a) shows how one can use\nA star search and beam search to quickly \ufb01nd an approximate MAP estimate. (Mansinghka et al.2007) discusses how to \ufb01t a DPMM online using particle \ufb01ltering, which is a like a stochasticversion of beam search. This can be more efficient than Gibbs sampling, particularly for largedatasets. (Kurihara et al. 2006) develops a variational approximation that is even faster (see also(Zobay 2009)). Extensions to the case of non-conjugate priors are discussed in (Neal 2000).\nAnother important issue is how to set the hyper-parameters. For the DP, the value of \u03b1\ndoes not have much impact on predictive accuracy, but it does affect the number of clusters.One approach is to put a Ga(a,b) prior for\u03b1, and then to from its posterior, p(\u03b1|K,N,a,b ),\nusing auxiliary variable methods (Escobar and West 1995). Alternatively, one can use empiricalBayes (McAuliffe et al. 2006). Similarly, for the base distribution, we can either sample thehyper-parameters \u03bb(Rasmussen 2000) or use empirical Bayes (McAuliffe et al. 2006).\n25.3 Affinity propagation\nMixture models, whether \ufb01nite or in\ufb01nite, require access to the raw N\u00d7Ddata matrix, and\nneed to specify a generative model of the data. An alternative approach takes as input an N\u00d7N\nsimilarity matrix, and then tries to identify examplars, which will act as cluster centers. TheK-medoids or K-centers algorithm (Section 14.4.2) is one approach, but it can suffer from local\nminima. Here we describe an alternative approach called affinity propagation (Frey and Dueck\n2007) that works substantially better in practice.\nThe idea is that each data point must choose another data point as its exemplar or centroid;\nsome data points will choose themselves as centroids, and this will automatically determine thenumber of clusters. More precisely, let c\ni\u2208{1,...,N}represent the centroid for datapoint i.", "918": "888 Chapter25. Clustering\n\u22126 \u22124 \u22122 0 2 4\u22126\u22124\u22122024iter# 50\n(a)\u22126 \u22124 \u22122 0 2 4\u22126\u22124\u22122024iter# 100\n(b)\n\u22126 \u22124 \u22122 0 2 4\u22126\u22124\u22122024iter# 200\n(c)1 2 3 4 5 600.10.20.30.40.50.60.7\n(d)\nFigure 25.7 100 data points in 2d are clustered using a DP mixture \ufb01t with collapsed Gibbs sampling.\nWe show samples from the posterior after 50,100, 200 samples. We also show the posterior over K, based\non 200 samples, discarding the \ufb01rst 50 as burnin. Figure generated by dpmGauss2dDemo , written by Yee\nWhye Teh.\nThe goal is to maximize the following function\nS(c)=N/summationdisplay\ni=1s(i,ci)+N/summationdisplay\nk=1\u03b4k(c) (25.39)\nThe \ufb01rst term measures the similarity of each point to its centroid. The second term is a penalty\nterm that is\u2212\u221eif some data point ihas chosen kas its exemplar (i.e., ci=k), butkhas not\nchosen itself as an exemplar (i.e., we do not have ck=k). More formally,\n\u03b4k(c)=/braceleftbigg\u2212\u221eifck/negationslash=kbut\u2203i:ci=k\n0otherwise(25.40)\nThe objective function can be represented as a factor graph. We can either use Nnodes,", "919": "25.3. Affinitypropagation 889\n100101102103\u2212600\u2212550\u2212500\u2212450\u2212400\u2212350\nIterationlog p(x | \u03c0, \u03b8)\n  \nDirichlet Process Mixture\nFinite Mixture\n(a)100101102103\u2212600\u2212550\u2212500\u2212450\u2212400\u2212350\nIterationlog p(x | \u03c0, \u03b8)\n  \nDirichlet Process Mixture\nFinite Mixture\n(b)\nFigure 25.8 Comparison of collapsed Gibbs samplers for a DP mixture (dark blue) and a \ufb01nite mixture\n(light red) with K=4applied to N= 300data points (shown in Figure 25.7). Left: logprob vs iteration\nfor 20 different starting values. Right: median (thick line) and quantiles (dashed lines) over 100 different\nstarting values. Source: Figure 2.27 of (Sudderth 2006). Used with kind permission of Erik Sudderth.\nc1 c2\n1\n 2\ns(1,\n) s(2,\n)A\ncN c32\n N\n 3\ns(3,\n) s(N,\n)\u2026\nk\n\u2026\ns(i,\n)\u2026 \u2026\nci\nFigure 25.9 Factor graphs for affinity propagation. Circles are variables, squares are factors. Each cinode\nhasNpossible states. From Figure S2 of (Frey and Dueck 2007). Used with kind permission of Brendan\nFrey.\neach with Npossible values, as shown in Figure 25.9, or we can use N2binary nodes (see\n(Givoni and Frey 2009) for the details). We will assume the former representation.\nWe can \ufb01nd a strong local maximum of the objective by using max-product loopy belief\npropagation (Section 22.2). Referring to the model in Figure 25.9, each variable nodes cisends\na message to each factor node \u03b4k. It turns out that this vector of Nnumbers can be reduced\nto a scalar message, denote ri\u2192k, known as the responsibility. This is a measure of how much\nithinkskwould make a good exemplar, compared to all the other exemplars ihas looked at.\nIn addition, each factor node \u03b4ksends a message to each variable node ci. Again this can be\nreduced to a scalar message, ai\u2190k, known as the availability. This is a measure of how strongly\nkbelieves it should an exemplar for i, based on all the other data points khas looked at.\nAs usual with loopy BP, the method might oscillate, and convergence is not guaranteed.", "920": "890 Chapter25. Clustering\n/g8/g15 /g10 /g6/g15/g8/g15/g10/g6/g15\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n/g8/g15 /g10 /g6/g15/g8/g15/g10/g6/g15\u0001\u0001\u0001\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n/g8/g15 /g10 /g6/g15/g8/g15/g10/g6/g15\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n/g8/g15 /g10 /g6/g15/g8/g15/g10/g6/g15\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001 \u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\n\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n/g8/g15 /g10 /g6/g15/g8/g15/g10/g6/g15\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\n\u0001\u0001\u0001 \u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\n\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\n\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n/g8/g15 /g10 /g6/g15/g8/g15/g10/g6/g15\u0001\u0001\u0001\u0001\u0001\n\u0001\n\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\n\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\n/g8/g15 /g10 /g6/g15/g8/g15/g10/g6/g15\u0001\u0001\u0001\u0001\u0001\n\u0001\n\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\n\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\n/g8/g15 /g10 /g6/g15/g8/g15/g10/g6/g15\u0001\u0001\n\u0001\n\u0001\u0001\n\u0001\n\u0001\n\u0001\u0001\u0001\u0001\n\u0001\n\u0001\n\u0001\u0001\u0001\u0001\n\u0001\n\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001/g23/g25/g23/g29/g23/g17/g24/g23/g31/g17/g29/g23/g26/g25\n/g23/g29/g20/g27/g17/g29/g23/g26/g25/g1/g2/g16 /g23/g29/g20/g27/g17/g29/g23/g26/g25/g1/g2/g15 /g23/g29/g20/g27/g17/g29/g23/g26/g25/g1/g2/g14/g23/g29/g20/g27/g17/g29/g23/g26/g25/g1/g2/g13 /g23/g29/g20/g27/g17/g29/g23/g26/g25/g1/g2/g12 /g23/g29/g20/g27/g17/g29/g23/g26/g25/g1/g2/g11\n/g18/g26/g25/g30/g20/g27/g22/g20/g25/g18/g20\n/g44/g45/g44/g8/g36/g52/g36/g43/g46/g42/g32/g47/g1 /g36/g52/g36/g43/g46/g42/g32/g47\nFigure 25.10 Example of affinity propagation. Each point is colored coded by how much it wants to be\nan exemplar (red is the most, green is the least). This can be computed by summing up all the incoming\navailability messages and the self-similarity term. The darkness of the i\u2192karrow re\ufb02ects how much\npointiwants to belong to exemplar k. From Figure 1 of (Frey and Dueck 2007). Used with kind permission\nof Brendan Frey.\nHowever, by using damping, the method is very reliable in practice. If the graph is densely\nconnected, message passing takes O(N2)time, but with sparse similarity matrices, it only takes\nO(E)time, where Eis the number of edges or non-zero entries in S.\nThe number of clusters can be controlled by scaling the diagonal terms S(i,i), which re\ufb02ect\nhow much each data point wants to be an exemplar. Figure 25.10 gives a simple example of some\n2d data, where the negative Euclidean distance was used to measured similarity. The S(i,i)\nvalues were set to be the median of all the pairwise similarities. The result is 3 clusters. Many\nother results are reported in (Frey and Dueck 2007), who show that the method signi\ufb01cantly\noutperforms K-medoids.\n25.4 Spectral clustering\nAn alternative view of clustering is in terms of graph cuts . The idea is we create a weighted\nundirected graph Wfrom the similarity matrix S, typically by using the nearest neighbors of\neach point; this ensures the graph is sparse, which speeds computation. If we want to \ufb01nd a\npartition into Kclusters, say A1,...,A K, one natural criterion is to minimize\ncut(A1,...,A K)/defines1\n2K/summationdisplay\nk=1W(Ak,Ak) (25.41)", "921": "25.4. Spectralclustering 891\nwhereAk=V\\Akis the complement of Ak, andW(A,B)/defines/summationtext\ni\u2208A,j\u2208Bwij.F o rK=2this\nproblem is easy to solve. Unfortunately the optimal solution often just partitions off a single\ndata point from the rest. To ensure the sets are reasonably large, we can de\ufb01ne the normalized\ncutto be\nNcut(A1,...,A K)/defines1\n2K/summationdisplay\nk=1cut(Ak,Ak)\nvol(Ak)(25.42)\nwhere vol (A)/defines/summationtext\ni\u2208Adi, anddi=/summationtextN\nj=1wijis the weighted degree of node i. This splits\nthe graph into Kclusters such that nodes within each cluster are similar to each other, but are\ndifferent to nodes in other clusters.\nWe can formulate the Ncut problem in terms of searching for binary vectors ci\u2208{0,1}N,\nwherecik=1if pointibelongs to cluster k, that minimize the objective. Unfortunately this\nis NP-hard (Wagner and Wagner 1993). Affinity propagation is one way to solve the problem.\nAnother is to relax the constraints that cibe binary, and allow them to be real-valued. The\nresult turns into an eigenvector problem known as spectral clustering (see e.g., (Shi and Malik\n2000)). In general, the technique of performing eigenalysis of graphs is called spectral graph\ntheory(Chung 1997).\nGoing into the details would take us too far a\ufb01eld, but below we give a very brief summary,\nbased on (von Luxburg 2007), since we will encounter some of these ideas later on.\n25.4.1 Graph Laplacian\nLetWbe a symmetric weight matrix for a graph, where wij=wji\u22650.L e tD=d i a g (di)be a\ndiaogonal matrix containing the weighted degree of each node. We de\ufb01ne the graph Laplacian\nas follows:\nL/definesD\u2212W (25.43)\nThis matrix has various important properties. Because each row sums to zero, we have\nthat1is an eigenvector with eigenvalue 0. Furthermore, the matrix is symmetric and positive\nsemi-de\ufb01nite. To see this, note that\nfTLf=fTDf\u2212fTWf=/summationdisplay\nidif2\ni\u2212/summationdisplay\ni,jfifjwij (25.44)\n=1\n2\u239b\n\u239d/summationdisplay\nidif2\ni\u22122/summationdisplay\ni,jfifjwij+/summationdisplay\njdjf2\nj\u239e\u23a0=1\n2/summationdisplay\ni,jwij(fi\u2212fj)2(25.45)\nHencefTLf\u22650for allf\u2208RN. Consequently we see that LhasNnon-negative, real-valued\neigenvalues, 0\u2264\u03bb1\u2264\u03bb2\u2264...\u2264\u03bbN.\nTo get some intuition as to why Lmight be useful for graph-based clustering, we note the\nfollowing result.\nTheorem 25.4.1. Thesetofeigenvectorsof Lwitheigenvalue0isspannedbytheindicatorvectors\n1A1,...,1AK,whereAkaretheKconnectedcomponentsofthegraph.", "922": "892 Chapter25. Clustering\nProof.Let us start with the case K=1.I ffis an eigenvector with eigenvalue 0, then\n0=/summationtext\nijwij(fi\u2212fj)2. If two nodes are connected, so wij>0, we must have that fi=fj.\nHencefis constant for all vertices which are connected by a path in the graph. Now suppose\nK>1. In this case, Lwill be block diagonal. A similar argument to the above shows that we\nwill have Kindicator functions, which \u201cselect out\u201d the connected components.\nThis suggests the following algorithm. Compute the \ufb01rst Keigenvectors ukofL.L e t\nU=[u1,...,uK]be anN\u00d7Kmatrix with the eigenvectors in its columns. Let yi\u2208RKbe\nthei\u2019th row of U. Since these yiwill be piecewise constant, we can apply K-means clustering\nto them to recover the connected components. Now assign point ito cluster kiff rowiofY\nwas assigned to cluster k.\nIn reality, we do not expect a graph derived from a real similarity matrix to have isolated\nconnected components \u2014 that would be too easy. But it is reasonable to suppose the graph is\na small \u201cperturbation\u201d from such an ideal. In this case, one can use results from perturbationtheory to show that the eigenvectors of the perturbed Laplacian will be close to these idealindicator functions (Ng et al. 2001).\nNote that this approach is related to kernel PCA (Section 14.4.4). In particular, KPCA uses the\nlargest eigenvectors of W; these are equivalent to the smallest eigenvectors of I\u2212W. This is\nsimilar to the above method, which computes the smallest eigenvectors of L=D\u2212W. See\n(Bengio et al. 2004) for details. In practice, spectral clustering gives much better results thanKPCA.\n25.4.2 Normalized graph Laplacian\nIn practice, it is important to normalize the graph Laplacian, to account for the fact that somenodes are more highly connected than others. There are two comon ways to do this. Onemethod, used in e.g., (Shi and Malik 2000; Meila 2001), creates a stochastic matrix where eachrow sums to one:\nL\nrw/definesD\u22121L=I\u2212D\u22121W (25.46)\nThe eigenvalues and eigenvectors of LandLrware closely related to each other (see (von\nLuxburg 2007) for details). Furthemore, one can show that for Lrw, the eigenspace of 0 is\nagain spanned by the indicator vectors 1Ak. This suggests the following algorithm: \ufb01nd the\nsmallestKeigenvectors of Lrw,c r e a t eU, cluster the rows of Uusing K-means, then infer the\npartitioning of the original points (Shi and Malik 2000). (Note that the eigenvectors/ values ofL\nrware equivalent to the generalized eigenvectors/ values of L, which solve Lu=\u03bbDU.)\nAnother method, used in e.g., (Ng et al. 2001), creates a symmetric matrix\nLsym/definesD\u22121\n2LD\u22121\n2=I\u2212D\u22121\n2WD\u22121\n2 (25.47)\nThis time the eigenspace of 0 is spanned by D1\n21Ak. This suggest the following algorithm: \ufb01nd\nthe smallest Keigenvectors of Lsym,c r e a t eU, normalize each row to unit norm by creating\ntij=uij//radicalbig\n(/summationtext\nku2\nik), cluster the rows of Tusing K-means, then infer the partitioning of the\noriginal points (Ng et al. 2001).\nThere is an interesting connection between Ncuts and random walks on a graph (Meila\n2001). First note that P=D\u22121W=I\u2212Lrwis a stochastic matrix, where pij=wij/di", "923": "25.5. Hierarchicalclustering 893\n\u22126 \u22124 \u22122 0 2 4 6\u22125\u22124\u22123\u22122\u22121012345k\u2212means clustering\nxy\n(a)\u22126 \u22124 \u22122 0 2 4 6\u22125\u22124\u22123\u22122\u22121012345spectral clustering\nxy\n(b)\nFigure 25.11 Clustering data consisting of 2 spirals. (a) K-means. (b) Spectral clustering. Figure generated\nbyspectralClusteringDemo , written by Wei-Lwun Lu.\ncan be interpreted as the probability of going from itoj. If the graph is connected and\nnon-bipartite, it possesses a unique stationary distribution \u03c0=(\u03c01,...,\u03c0N),w h e r e\u03c0i=\ndi/vol(V). Furthermore, one can show that\nNcut(A,A)=p(A|A)+p(A|A) (25.48)\nThis means that we are looking for a cut such that a random walk rarely makes transitions from\nAtoAor vice versa.\n25.4.3 Example\nFigure 25.11 illustrates the method in action. In Figure 25.11(a), we see that K-means does a poorjob of clustering, since it implicitly assumes each cluster corresponds to a spherical Gaussian.Next we try spectral clustering. We de\ufb01ne a similarity matrix using the Gaussian kernel. Wecompute the \ufb01rst two eigenvectors of the Laplacian. From this we can infer the clustering inFigure 25.11(b).\nSince the method is based on \ufb01nding the smallest Keigenvectors of a sparse matrix, it takes\nO(N\n3)time. However, a variety of methods can be used to scale it up for large datasets (see\ne.g., (Yan et al. 2009)).\n25.5 Hierarchical clustering\nMixture models, whether \ufb01nite or in\ufb01nite, produce a \u201c\ufb02at\u201d clustering. Often we want to learn ahierarchical clustering, where clusters can be nested inside each other.\nThere are two main approaches to hierarchical clustering: bottom-up or agglomerative clus-\ntering, and top-down or divisive clustering. Both methods take as input a dissimilarity matrix\nbetween the objects. In the bottom-up approach, the most similar groups are merged at each", "924": "894 Chapter25. Clustering\n0 1 2 3 4 5 6 7 800.511.522.533.544.55\n12\n3\n45\n(a)4 5 1 3 211.522.5\n(b)\nFigure 25.12 (a) An example of single link clustering using city block distance. Pairs (1,3) and (4,5) are\nboth distance 1 apart, so get merged \ufb01rst. (b) The resulting dendrogram. Based on Figure 7.5 of (Alpaydin\n2004). Figure generated by agglomDemo .\n(a)0 10 20\u22123\u22122\u221210\n0 10 20\u22122\u221210\n0 10 20\u22122.5\u22122\u22121.5\u22121\u22120.5\n0 10 20\u22123\u22122\u2212101\n0 10 20\u2212202\n0 10 20\u2212202\n0 10 20\u22124\u221220\n0 10 20\u22122024\n0 10 20\u22124\u2212202\n0 10 20\u22122024\n0 10 20\u22121012\n0 10 200.511.522.5\n0 10 200.511.52\n0 10 20012\n0 10 20\u22122\u221210Hierarchical Clustering of Profiles\n0 10 20\u22121012\n(b)\nFigure 25.13 Hierarchical clustering applied to the yeast gene expression data. (a) The rows are permuted\naccording to a hierarchical clustering scheme (average link agglomerative clustering), in order to bring\nsimilar rows close together. (b) 16 clusters induced by cutting the average linkage tree at a certain height.\nFigure generated by hclustYeastDemo .\nstep. In the top-down approach, groups are split using various different criteria. We give the\ndetails below.\nNote that agglomerative and divisive clustering are both just heuristics, which do not optimize\nany well-de\ufb01ned objective function. Thus it is hard to assess the quality of the clustering they\nproduce in any formal sense. Furthermore, they will always produce a clustering of the input\ndata, even if the data has no structure at all (e.g., it is random noise). Later in this section we\nwill discuss a probabilistic version of hierarchical clustering that solves both these problems.", "925": "25.5. Hierarchicalclustering 895\nAlgorithm 25.2: Agglomerative clustering\n1initialize clusters as singletons: fori\u21901tondoCi\u2190{i};\n2initialize set of clusters available for merging: S\u2190{1,...,n};\n3repeat\n4Pick 2 most similar clusters to merge: (j,k)\u2190argmin j,k\u2208Sdj,k;\n5Create new cluster C/lscript\u2190Cj\u222aCk;\n6Markjandkas unavailable: S\u2190S\\{j,k};\n7ifC/lscript/negationslash={1,...,n}then\n8 Mark/lscriptas available, S\u2190S\u222a{/lscript};\n9foreachi\u2208Sdo\n10 Update dissimilarity matrix d(i,/lscript);\n11untilnomoreclustersareavailableformerging ;\n(a)\n (b)\n (c)\nFigure 25.14 Illustration of (a) Single linkage. (b) Complete linkage. (c) Average linkage.\n25.5.1 Agglomerative clustering\nAgglomerative clustering starts with Ngroups, each initially containing one object, and then at\neach step it merges the two most similar groups until there is a single group, containing all the\ndata. See Algorithm 11 for the pseudocode. Since picking the two most similar clusters to merge\ntakesO(N2)time, and there are O(N)steps in the algorithm, the total running time is O(N3).\nHowever, by using a priority queue, this can be reduced to O(N2logN) (see e.g., (Manning\net al. 2008, ch. 17) for details). For large N, a common heuristic is to \ufb01rst run K-means, which\ntakesO(KND)time, and then apply hierarchical clustering to the estimated cluster centers.\nThe merging process can be represented by a binary tree , called a dendrogram , as shown\nin Figure 25.12(b). The initial groups (objects) are at the leaves (at the bottom of the \ufb01gure),\nand every time two groups are merged, we join them in the tree. The height of the branches\nrepresents the dissimilarity between the groups that are being joined. The root of the tree (which\nis at the top) represents a group containing all the data. If we cut the tree at any given height,\nwe induce a clustering of a given size. For example, if we cut the tree in Figure 25.12(b) at\nheight 2, we get the clustering {{{4,5},{1,3}},{2}}. We discuss the issue of how to choose\nthe height/ number of clusters below.\nA more complex example is shown in Figure 25.13(a), where we show some gene expression\ndata. If we cut the tree in Figure 25.13(a) at a certain height, we get the 16 clusters shown in\nFigure 25.13(b).\nThere are actually three variants of agglomerative clustering, depending on how we de\ufb01ne\nthe dissimilarity between groups of objects. These can give quite different results, as shown in", "926": "896 Chapter25. Clustering\n0.050.10.150.20.250.3single link\n(a)\n0.20.40.60.811.21.41.61.82complete link\n(b)\n0.20.40.60.811.21.41.61.8average link\n(c)\nFigure 25.15 Hierarchical clustering of yeast gene expression data. (a) Single linkage. (b) Complete linkage.\n(c) Average linkage. Figure generated by hclustYeastDemo .", "927": "25.5. Hierarchicalclustering 897\nFigure 25.15. We give the details below.\n25.5.1.1 Single link\nInsingle link clustering, also called nearest neighbor clustering, the distance between two\ngroupsGandHis de\ufb01ned as the distance between the two closest members of each group:\ndSL(G,H)= m i n\ni\u2208G,i/prime\u2208Hdi,i/prime (25.49)\nSee Figure 25.14(a).\nThe tree built using single link clustering is a minimum spanning tree of the data, which\nis a tree that connects all the objects in a way that minimizes the sum of the edge weights\n(distances). To see this, note that when we merge two clusters, we connect together the twoclosest members of the clusters; this adds an edge between the corresponding nodes, and thisis guaranteed to be the \u201clightest weight\u201d edge joining these two clusters. And once two clustershave been merged, they will never be considered again, so we cannot create cycles. As aconsequence of this, we can actually implement single link clustering in O(N\n2)time, whereas\nthe other variants take O(N3)time.\n25.5.1.2 Complete link\nIncomplete link clustering, also called furthest neighbor clustering, the distance between\ntwo groups is de\ufb01ned as the distance between the two most distant pairs:\ndCL(G,H)= m a x\ni\u2208G,i/prime\u2208Hdi,i/prime (25.50)\nSee Figure 25.14(b).\nSingle linkage only requires that a single pair of objects be close for the two groups to\nbe considered close together, regardless of the similarity of the other members of the group.Thus clusters can be formed that violate the compactness property, which says that all the\nobservations within a group should be similar to each other. In particular if we de\ufb01ne thediameter of a group as the largest dissimilarity of its members, d\nG=m a x i\u2208G,i/prime\u2208Gdi,i/prime, then\nwe can see that single linkage can produce clusters with large diameters. Complete linkagerepresents the opposite extreme: two groups are considered close only if all of the observationsin their union are relatively similar. This will tend to produce clusterings with small diameter,i.e., compact clusters.\n25.5.1.3 Average link\nIn practice, the preferred method is average link clustering, which measures the average\ndistance between all pairs:\nd\navg(G,H)=1\nnGnH/summationdisplay\ni\u2208G/summationdisplay\ni/prime\u2208Hdi,i/prime (25.51)\nwherenGandnHare the number of elements in groups GandH. See Figure 25.14(c).\nAverage link clustering represents a compromise between single and complete link clustering.\nIt tends to produce relatively compact clusters that are relatively far apart. However, since it", "928": "898 Chapter25. Clustering\ninvolves averaging of the di,i/prime\u2019s, any change to the measurement scale can change the result. In\ncontrast, single linkage and complete linkage are invariant to monotonic transformations of di,i/prime,\nsince they leave the relative ordering the same.\n25.5.2 Divisive clustering\nDivisive clustering starts with all the data in a single cluster, and then recursively divides each\ncluster into two daughter clusters, in a top-down fashion. Since there are 2N\u22121\u22121ways to split\na group of Nitems into 2 groups, it is hard to compute the optimal split, so various heuristics\nare used. One approach is pick the cluster with the largest diameter, and split it in two using theK-means or K-medoids algorithm with K=2. This is called the bisecting K-means algorithm\n(Steinbach et al. 2000). We can repeat this until we have any desired number of clusters. Thiscan be used as an alternative to regular K-means, but it also induces a hierarchical clustering.\nAnother method is to build a minimum spanning tree from the dissimilarity graph, and then\nto make new clusters by breaking the link corresponding to the largest dissimilarity. (Thisactually gives the same results as single link agglomerative clustering.)\nYet another method, called dissimilarity analysis (Macnaughton-Smith et al. 1964), is as\nfollows. We start with a single cluster containing all the data, G={1,...,N}. We then\nmeasure the average dissimilarity of i\u2208Gto all the other i\n/prime\u2208G:\ndG\ni=1\nnG/summationdisplay\ni/prime\u2208Gdi,i/prime (25.52)\nWe remove the most dissimilar object and put it in its own cluster H:\ni\u2217=a r gm a x\ni\u2208GdGi,G=G\\{i\u2217},H={i\u2217} (25.53)\nWe now continue to move objects from GtoHuntil some stopping criterion is met. Speci\ufb01cally,\nwe pick a point i\u2217to move that maximizes the average dissimilarity to each i/prime\u2208Gbut minimizes\nthe average dissimilarity to each i/prime\u2208H:\ndHi=1\nnH/summationdisplay\ni/prime\u2208Hdi,i/prime,i\u2217=a r gm a x\ni\u2208GdGi\u2212dHi(25.54)\nWe continue to do this until dG\ni\u2212dHiis negative. The \ufb01nal result is that we have split Ginto\ntwo daughter clusters, GandH. We can then recursively call the algorithm on Gand/orH,o r\non any other node in the tree. For example, we might choose to split the node Gwhose average\ndissimilarity is highest, or whose maximum dissimilarity (i.e., diameter) is highest. We continue\nthe process until the average dissimilarity within each cluster is below some threshold, and/orall clusters are singletons.\nDivisive clustering is less popular than agglomerative clustering, but it has two advantages.\nFirst, it can be faster, since if we only split for a constant number of levels, it takes just O(N)\ntime. Second, the splitting decisions are made in the context of seeing all the data, whereasbottom-up methods make myopic merge decisions.", "929": "25.5. Hierarchicalclustering 899\n25.5.3 Choosing the number of clusters\nIt is difficult to choose the \u201cright\u201d number of clusters, since a hierarchical clustering algorithm\nwill always create a hierarchy, even if the data is completely random. But, as with choosing K\nfor K-means, there is the hope that there will be a visible \u201cgap\u201d in the lengths of the links in thedendrogram (represent the dissimilarity between merged groups) between natural clusters andunnatural clusters. Of course, on real data, this gap might be hard to detect. In Section 25.5.4,we will present a Bayesian approach to hierarchical clustering that nicely solves this problem.\n25.5.4 Bayesian hierarchical clustering\nThere are several ways to make probabilistic models which produce results similar to hierarchicalclustering, e.g., (Williams 2000; Neal 2003b; Castro et al. 2004; Lau and Green 2006). Here wepresent one particular approach called Bayesian hierarchical clustering (Heller and Ghahra-\nmani 2005). Algorithmically it is very similar to standard bottom-up agglomerative clustering,and takes comparable time, whereas several of the other techniques referenced above are muchslower. However, it uses Bayesian hypothesis tests to decide which clusters to merge (if any),rather than computing the similarity between groups of points in some ad-hoc way. Thesehypothesis tests are closely related to the calculations required to do inference in a Dirichletprocess mixture model, as we will see. Furthermore, the input to the model is a data matrix,not a dissimilarity matrix.\n25.5.4.1 The algorithm\nLetD={x\n1,...,xN}represent all the data, and let Dibe the set of datapoints at the leaves\nof the substree Ti. At each step, we compare two trees TiandTjto see if they should be\nmerged into a new tree. De\ufb01ne Dijas their merged data, and let Mij=1if they should be\nmerged, and Mij=0otherwise.\nThe probability of a merge is given by\nrij/definesp(Dij|Mij=1 )p(Mij=1 )\np(Dij|Tij)(25.55)\np(Dij|Tij)=p(Dij|Mij=1 )p(Mij=1 )+p(Dij|Mij=0 )p(Mij=0 ) (25.56)\nHerep(Mij=1 )is the prior probability of a merge, which can be computed using a bottom-up\nalgorithm described below. We now turn to the likelihood terms. If Mij=1, the data inDijis\nassumed to come from the same model, and hence\np(Dij|Mij=1 )=/integraldisplay\u23a1\n\u23a3/productdisplay\nxn\u2208Dijp(xn|\u03b8)\u23a4\u23a6p(\u03b8|\u03bb)d\u03b8 (25.57)\nIfM\nij=0, the data inDijis assumed to have been generated by each tree independently, so\np(Dij|Mij=0 )=p(Di|Ti)p(Dj|Tj) (25.58)\nThese two terms will have already been computed by the bottom-up process. Consequently\nwe have all the quantities we need to decide which trees to merge. See Algorithm 9 for thepseudocode, assuming p(M\nij)is uniform. When \ufb01nished, we can cut the tree at points where\nrij<0.5.", "930": "900 Chapter25. Clustering\nAlgorithm 25.3: Bayesian hierarchical clustering\n1InitializeDi={xi},i=1:N;\n2Compute p(Di|Ti),i=1:N;\n3repeat\n4foreachpairofclusters i,jdo\n5 Compute p(Dij|Tij)\n6Find the pairDiandDjwith highest merge probability rij;\n7MergeDk:=Di\u222aDj;\n8DeleteDi,Dj;\n9untilallclustersmerged ;\n25.5.4.2 The connection with Dirichlet process mixture models\nIn this section, we will establish the connection between BHC and DPMMs. This will in turn\ngive us an algorithm to compute the prior probabilities p(Mij=1 ).\nNote that the marginal likelihood of a DPMM, summing over all 2N\u22121partitions, is given by\np(Dk)=/summationdisplay\nv\u2208Vp(v)p(Dv) (25.59)\np(v)=\u03b1mv/producttextmv\nl=1\u0393(nv\nl)\n\u0393(nk+\u03b1)\n\u0393(\u03b1)(25.60)\np(Dv)=mv/productdisplay\nl=1p(Dv\nl) (25.61)\nwhereVis the set of all possible partitions of Dk,p(v)is the probability of partition v,mvis\nthe number of clusters in partition v,nvlis the number of points in cluster lof partition v,Dv\nl\nare the points in cluster lof partition v, andnkare the number of points in Dk.\nOne can show (Heller and Ghahramani 2005) that p(Dk|Tk)computed by the BHC algorithm\nis similar to p(Dk)given above, except for the fact that it only sums over partitions which are\nconsistent with tree Tk. (The number of tree-consistent partitions is exponential in the number\nof data points for balanced binary trees, but this is obviously a subset of all possible partitions.)\nIn this way, we can use the BHC algorithm to compute a lower bound on the marginal likelihoodof the data from a DPMM. Furthermore, we can interpret the algorithm as greedily searchingthrough the exponentially large space of tree-consistent partitions to \ufb01nd the best ones of agiven size at each step.\nWe are now in a position to compute \u03c0\nk=p(Mk=1 ), for each node kwith children iand\nj. This is equal to the probability of cluster Dkcoming from the DPMM, relative to all other\npartitions ofDkconsistent with the current tree. This can be computed as follows: initialize\ndi=\u03b1and\u03c0i=1for each leaf i; then as we build the tree, for each internal node k, compute\ndk=\u03b1\u0393(nk)+didj, and\u03c0k=\u03b1\u0393(nk)\ndk,w h e r ei andjarek\u2019s left and right children.", "931": "25.6. Clusteringdatapointsandfeatures 901\nData Set Single Linkage Complete Linkage Average Linkage BHC\nSynthetic 0.599\u00b10.033 0.634\u00b10.024 0. 668\u00b10.0400.828\u00b10.025\nNewsgroups 0.275\u00b10.001 0.315\u00b10.008 0. 282\u00b10.0020.465\u00b10.016\nSpambase 0.598\u00b10.017 0.699\u00b10.017 0. 668\u00b10.0190.728\u00b10.029\nDigits 0.224\u00b10.004 0.299\u00b10.006 0. 342\u00b10.0050.393\u00b10.015\nFglass 0.478\u00b10.009 0.476\u00b10.009 0.491\u00b10.009 0.467\u00b10.011\nTable 25.1 Purity scores for various hierarchical clustering schemes applied to various data sets. The\nsynthetic data has N= 200,D =2,C=4and real features. Newsgroups is extracted from the 20\nnewsgroups dataset (D = 500,N = 800,C =4, binary features). Spambase has N= 100,C =2,D=\n57, binary features. Digits is the CEDAR Buffalo digits (N = 200,C =1 0,D=6 4, binarized features).\nFglass is forensic glass dataset (N = 214,C =6,D=9, real features). Source: Table 1 of (Heller and\nGhahramani 2005). Used with kind permission of Katherine Heller.\n25.5.4.3 Learning the hyper-parameters\nThe model has two free-parameters: \u03b1and\u03bb,w h e r e \u03bbare the hyper-parameters for the prior\non the parameters \u03b8. In (Heller and Ghahramani 2005), they show how one can back-propagate\ngradients of the form\u2202p(Dk|Tk)\n\u2202\u03bbthrough the tree, and thus perform an empirical Bayes estimate\nof the hyper-parameters.\n25.5.4.4 Experimental results\n(Heller and Ghahramani 2005) compared BHC with traditional agglomerative clustering algo-\nrithms on various data sets in terms of purity scores. The results are shown in Table 25.1. Wesee that BHC did much better than the other methods on all datasets except the forensic glassone.\nFigure 25.16 visualizes the tree structure estimated by BHC and agglomerative hierarchical\nclustering (AHC) on the newsgroup data (using a beta-Bernoulli model). The BHC tree is clearlysuperior (look at the colors at the leaves, which represent class labels). Figure 25.17 is a zoom-inon the top few nodes of these two trees. BHC splits off clusters concerning sports from clustersconcerning cars and space. AHC keeps sports and cars merged together. Although sports andcars both fall under the same \u201crec\u201d newsgroup heading (as opposed to space, that comes underthe \u201csci\u201d newsgroup heading), the BHC clustering still seems more reasonable, and this is borneout by the quantitative purity scores.\nBHC has also been applied to gene expression data, with good results (Savage et al. 2009).\n25.6 Clustering datapoints and features\nSo far, we have been concentrating on clustering datapoints. But each datapoint is oftendescribed by multiple features, and we might be interested in clustering them as well. Below wedescribe some methods for doing this.", "932": "902 Chapter25. Clustering\n4 Newsgroups Average Linkage Clustering\n(a)\n4 Newsgroups Bayesian Hierarchical Clustering\n(b)\nFigure 25.16 Hierarchical clustering applied to 800 documents from 4 newsgroups (red is rec.autos, blue\nis rec.sport.baseball, green is rec.sport.hockey, and magenta is sci.space). Top: average linkage hierarchical\nclustering. Bottom: Bayesian hierarchical clustering. Each of the leaves is labeled with a color, accordingto which newsgroup that document came from. We see that the Bayesian method results in a clusteringthat is more consistent with these labels (which were not used during model \ufb01tting). Source: Figure 7 of(Heller and Ghahramani 2005). Used with kind permission of Katherine Heller.", "933": "25.6. Clusteringdatapointsandfeatures 903\nAll Data\nGame\nTeam PlayCarSpace NASA\nBaseball  Pitch     HitNHLHockey Round CarDealer DriveSpaceNASA Orbit354 446\n205 149 284 162\n(a)All Data\nCar\nBaseball Engine\nPitcherBoston BallCarPlayer SpaceQuebecJet Boston\nVehicleDealer DriverTeamGame Hockey1 799\n2 797\n1 796\n(b)\nFigure 25.17 Zoom-in on the top nodes in the trees of Figure 25.16. (a) Bayesian method. (b) Average\nlinkage. We show the 3 most probable words per cluster. The number of documents at each cluster is also\ngiven. Source: Figure 5 of (Heller and Ghahramani 2005). Used with kind permission of Katherine Heller.\n25.6.1 Biclustering\nClustering the rows and columns is known as biclustering orcoclustering. This is widely used\nin bioinformatics, where the rows often represent genes and the columns represent conditions.\nIt can also be used for collaborative \ufb01ltering, where the rows represent users and the columnsrepresent movies.\nA variety of ad hoc methods for biclustering have been proposed; see (Madeira and Oliveira\n2004) for a review. Here we present a simple probabilistic generative model, based on (Kempet al. 2006) (see also (Sheng et al. 2003) for a related approach). The idea is to associate eachrow and each column with a latent indicator, r\ni\u2208{1,...,Kr},cj\u2208{1,...,Kc}. We then\nassume the data are iid across samples and across features within each block:\np(x|r,c,\u03b8)=/productdisplay\ni/productdisplay\njp(xij|ri,cj,\u03b8)=p(xij|\u03b8ri,cj) (25.62)\nwhere\u03b8a,bare the parameters for row cluster aand column cluster b. Rather than using a \ufb01nite\nnumber of clusters for the rows and columns, we can use a Dirchlet process, as in the in\ufb01niterelational model which we discuss in Section 27.6.1. We can \ufb01t this model using e.g., (collapsed)Gibbs sampling.\nThe behavior of this model is illustrated in Figure 25.18. The data has the form X(i,j)=1\niff animal ihas feature j,w h e r ei=1:5 0andj=1:8 5. The animals represent whales, bears,\nhorses, etc. The features represent properties of the habitat (jungle, tree, coastal), or anatomicalproperties (has teeth, quadrapedal), or behavioral properties (swims, eats meat), etc. The model,using a Bernoulli likelihood, was \ufb01t to the data. It discovered 12 animal clusters and 33 featureclusters. For example, it discovered a bicluster that represents the fact that mammals tend tohave aquatic features.\n25.6.2 Multi-view clustering\nThe problem with biclustering is that each object (row) can only belong to one cluster. Intuitively,an object can have multiple roles, and can be assigned to different clusters depending on which", "934": "904 Chapter25. Clustering\nO1 killer whale, blue whale, humpback, seal, walrus, dolphin\nO2 antelope, horse, giraffe, zebra, deer\nO3 monkey, gorilla, chimp\nO4 hippo, elephant, rhino\nO5 grizzly bear, polar bear\nF1 \ufb02ippers, strain teeth, swims, arctic, coastal, ocean, water\nF2 hooves, long neck, horns\nF3 hands, bipedal, jungle, tree\nF4 bulbous body shape, slow, inactive\nF5 meat teeth, eats meat, hunter, \ufb01erce\nF6 walks, quadrapedal, groundF1 2 4 35\nO1\nO2\nO3\nO4\nO56\nFigure 25.18 Illustration of biclustering . We show 5 of the 12 animal clusters, and 6 of the 33 feature\nclusters. The original data matrix is shown, partitioned according to the discovered clusters. From Figure\n3 of (Kemp et al. 2006). Used with kind permission of Charles Kemp.\n(a)riv\nv=1:\u221exij\ni=1:N\u03b2cj \u03b8jk\nk=1:\u221e\nj=1:D\u03b1 \u03b3\n(b)\nFigure 25.19 (a) Illustration of multi-view clustering. Here we have 3 views (column partitions). In the\n\ufb01rst view, we have 2 clusters (row partitions). In the second view, we have 3 clusters. In the third view,\nwe have 2 clusters. The number of views and partitions are inferred from data. Rows within each colored\nblock are assumed to generated iid; however, each column can have a different distributional form, which\nis useful for modeling discrete and continuous data. From Figure 1 of (Guan et al. 2010). Used with kind\npermission of Jennifer Dy. (b) Corresponding DGM.\nsubset of features you use. For example, in the animal dataset, we may want to group the\nanimals on the basis of anatomical features (e.g., mammals are warm blooded, reptiles are not),\nor on the basis of behavioral features (e.g., predators vs prey).\nWe now present a model that can capture this phenomenon. This model was indepen-\ndently proposed in (Shafto et al. 2006; Mansinghka et al. 2011), who call it crosscat (for cross-\ncategorization), and in (Guan et al. 2010; Cui et al. 2010), who call it (non-parametric) multi-clust .\n(See also (Rodriguez and Ghosh 2011) for a very similar model.) The idea is that we partition the\ncolumns (features) into Vgroups or views,s ocj\u2208{1,...,V},w h e r ej\u2208{1,...,D}indexes", "935": "25.6. Clusteringdatapointsandfeatures 905\nfeatures. We will use a Dirichlet process prior for p(c), which allows Vto grow automatically.\nThen for each partition of the columns (i.e., each view), call it v, we partition the rows, again\nusing a DP, as illustrated in Figure 25.19(a). Let riv\u2208{1,...,K(v)}be the cluster to which\nthei\u2019th row belongs in view v. Finally, having partitioned the rows and columns, we generate\nthe data: we assume all the rows and columns within a block are iid. We can de\ufb01ne the model\nmore precisely as follows:\np(c,r,D)=p( c)p(r|c)p(D|r,c) (25.63)\np(c)=DP( c|\u03b1) (25.64)\np(r|c)=V(c)/productdisplay\nv=1DP(rv|\u03b2) (25.65)\np(D|r,c,\u03b8)=V(c)/productdisplay\nv=1/productdisplay\nj:cj=v\u23a1\n\u23a3K(rv)/productdisplay\nk=1/integraldisplay/productdisplay\ni:riv=kp(xij|\u03b8jk)p(\u03b8jk)d\u03b8jk\u23a4\u23a6 (25.66)\nSee Figure 25.19(b) for the DGM.\n2\nIf the data is binary, and we use a Beta(\u03b3,\u03b3)prior for\u03b8jk, the likelihood reduces to\np(D|r,c,\u03b3)=V(c)/productdisplay\nv=1/productdisplay\nj:cj=vK(rv)/productdisplay\nk=1Beta(n j,k,v+\u03b3,nj,k,v+\u03b3)\nBeta(\u03b3,\u03b3)(25.67)\nwherenj,k,v=/summationtext\ni:ri,v=kI(xij=1 )counts the number of features which are on in the j\u2019th\ncolumn for view vand for row cluster k. Similarly, nj,k,vcounts how many features are off.\nThe model is easily extended to other kinds of data, by replacing the beta-Bernoulli with, say,\nthe Gaussian-Gamma-Gaussian model, as discussed in (Guan et al. 2010; Mansinghka et al. 2011).\nApproximate MAP estimation can be done using stochastic search (Shafto et al. 2006), and\napproximate inference can be done using variational Bayes (Guan et al. 2010) or Gibbs sampling(Mansinghka et al. 2011). The hyper-parameter \u03b3for the likelihood can usually be set in a non-\ninformative way, but results are more sensitive to the other two parameters, since \u03b1controls\nthe number of column partitions, and \u03b2controls the number of row partitions. Hence a more\nrobust technique is to infer the hyper-parameters using MH. This also speeds up convergence(Mansinghka et al. 2011).\nFigure 25.20 illustrates the model applied to some binary data containing 22 animals and 106\nfeatures. The \ufb01gures shows the (approximate) MAP partition. The \ufb01rst partition of the columnscontains taxonomic features, such as \u201chas bones\u201d, \u201cis warm-blooded\u201d, \u201clays eggs\u201d, etc. Thisdivides the animals into birds, reptiles/ amphibians, mammals, and invertebrates. The secondpartition of the columns contains features that are treated as noise, with no apparent structure(except for the single row labeled \u201cfrog\u201d). The third partition of the columns contains ecologicalfeatures like \u201cdangerous\u201d, \u201ccarnivorous\u201d, \u201clives in water\u201d, etc. This divides the animals into prey,land predators, sea predators and air predators. Thus each animal (row) can belong to a different\n2. The dependence between randcis not shown, since it is not a dependence between the values of rivandcj, but\nbetween the cardinality of vandcj. In other words, the number of row partitions we need to specify (the number of\nviews, indexed by v) depends on the number of column partitions (clusters) that we have.", "936": "906 Chapter25. Clustering\nlives in lakes\nis an amphibian\nis a rodent\nis tall\nis a fish\nis slimy\nhas horns\nhas hooves\nis a feline\nroars\nhas fins\nhas webbed feet\neats nuts\nis smooth\nlives in trees\nis large\nlives in cold climates \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nis ferocious\nis dangerous\nis a carnivore\nis a predator\nlives in water\nflies\nis long\neats leaves\neats animals\nlives in grass\neats fish\nlives in hot climatesLeopard\nAlligator\nPython\nSeal\nDolphin\nFrog\nJellyfish\nOctopus\nPenguin\nFinch\nSeagull\nOwl\nEagle\nDragonfly\nBat\nGrasshopper\nAnt\nBee\nSheep\nMonkey\nIguana\nOstrich\nhas bones\nlays eggs\nis warm\u2212blooded\nis a mammal\nsquawks\nhas a beak\nhas a tongue\nis green\nhas a spinal cord\nis a lizard\nhas antennae\nhas flippers\nhas paws\nhas a large brain\nhas a tail\nis furry\neats mice\neats rodents\nhas a snout\nis brown\nmakes loud noises\nhas teeth\nhas feet\nis smart\ntravels in groupsLeopard\nSheep\nSeal\nDolphin\nMonkey\nBat\nAlligator\nIguana\nFrog\nPython\nFinch\nOstrich\nSeagull\nOwl\nPenguin\nEagle\nGrasshopper\nAnt\nBee\nJellyfish\nOctopus\nDragonflyA BC\nFrog\nFigure 25.20 MAP estimate produced by the crosscat system when applied to a binary data matrix of\nanimals (rows) by features (columns). See text for details. Source: Figure 7 of (Shafto et al. 2006) . Used\nwith kind permission of Vikash Mansingkha.\ncluster depending on what set of features are considered. Uncertainty about the partitions can\nbe handled by sampling.\nIt is interesting to compare this model to a standard in\ufb01nite mixture model. While the\nstandard model can represent any density on \ufb01xed-sized vectors as N\u2192\u221e, it cannot cope\nwithD\u2192\u221e, since it has no way to handle irrelevant, noisy or redundant features. By contrast,\nthe crosscat/multi-clust system is robust to irrelevant features: it can just partition them off,\nand cluster the rows only using the relevant features. Note, however, that it does not need a\nseparate \u201cbackground\u201d model, since everything is modelled using the same mechanism. This is\nuseful, since one\u2019s person\u2019s noise is another person\u2019s signal. (Indeed, this symmetry may explain\nwhy multi-clust outperformed the sparse mixture model approach of (Law et al. 2004) in the\nexperiments reported in (Guan et al. 2010).)", "937": "26 Graphical model structure learning\n26.1 Introduction\nWe have seen how graphical models can be used to express conditional independence assump-\ntions between variables. In this chapter, we discuss how to learn the structure of the graphicalmodel itself. That is, we want to compute p(G|D),w h e r eG is the graph structure, represented\nas anV\u00d7Vadjacency matrix.\nAs we discussed in Section 1.3.3, there are two main applications of structure learning: knowl-\nedge discovery and density estimation. The former just requires a graph topology, whereas thelatter requires a fully speci\ufb01ed model.\nThe main obstacle in structure learning is that the number of possible graphs is exponential in\nthe number of nodes: a simple upper bound is O(2\nV(V\u22121)/2). Thus the full posterior p(G|D)\nis prohibitively large: even if we could afford to compute it, we could not even store it. So wewill seek appropriate summaries of the posterior. These summary statistics depend on our task.\nIf our goal is knowledge discovery, we may want to compute posterior edge marginals,\np(G\nst=1|D); we can then plot the corresponding graph, where the thickness of each edge\nrepresents our con\ufb01dence in its presence. By setting a threshold, we can generate a sparsegraph, which can be useful for visualization purposes (see Figure 1.11).\nIfourgoalisdensityestimation, wemaywanttocomputetheMAPgraph, \u02c6G\u2208argmax\nGp(G|D).\nIn most cases, \ufb01nding the globally optimal graph will take exponential time, so we will use dis-crete optimization methods such as heuristic search. However, in the case of trees, we can\ufb01nd the globally optimal graph structure quite efficiently using exact methods, as we discuss inSection 26.3.\nIf density estimation is our only goal, it is worth considering whether it would be more\nappropriate to learn a latent variable model, which can capture correlation between the visiblevariables via a set of latent common causes (see Chapters 12 and 27). Such models are ofteneasier to learn and, perhaps more importantly, they can be applied (for prediction purposes)much more efficiently, since they do not require performing inference in a learned graph withpotentially high treewidth. The downside with such models is that the latent factors are oftenunidenti\ufb01able, and hence hard to interpret. Of course, we can combine graphical model structurelearning and latent variable learning, as we will show later in this chapter.\nIn some cases, we don\u2019t just want to model the observed correlation between variables;\ninstead, we want to model the causalstructure behind the data, so we can predict the effects\nof manipulating variables. This is a much more challenging task, which we brie\ufb02y discuss in", "938": "908 Chapter26. Graphicalmodelstructurelearning\nbible\nchristian\ngod\njesuscase\nfactchildren\ngovernment\nreligion computer\nscience\nuniversitycourse\nearth\norbitevidence\nhuman\nworldjews law president\nrights\nstatewargun israellaunch\nnasa\nshuttle\nspacelunar\nmoonmission\nsolar\nFigure 26.1 Part of a relevance network constructed from the 20-news data shown in Figure 1.2. We\nshow edges whose mutual information is greater than or equal to 20% of the maximum pairwise MI. For\nclarity, the graph has been cropped, so we only show a subset of the nodes and edges. Figure generatedbyrelevanceNetworkNewsgroupDemo .\nSection 26.6.\n26.2 Structure learning for knowledge discovery\nSince computing the MAP graph or the exact posterior edge marginals is in general computa-\ntionally intractable (Chickering 1996), in this section we discuss some \u201cquick and dirty\u201d methodsfor learning graph structures which can be used to visualize one\u2019s data. The resulting models donot constitute consistent joint probability distributions, so they cannot be used for prediction,and they cannot even be formally evaluated in terms of goodness of \ufb01t. Nevertheless, thesemethods are a useful ad hoc tool to have in one\u2019s data visualization toolbox, in view of theirspeed and simplicity.\n26.2.1 Relevance networks\nArelevance network is a way of visualizing the pairwise mutual information between multiple\nrandom variables: we simply choose a threshold and draw an edge from node ito nodejif\nI(Xi;Xj)is above this threshold. In the Gaussian case, I(Xi;Xj)=\u22121\n2log(1\u2212\u03c12\nij),w h e r e\n\u03c1ijis the correlation coefficient (see Exercise 2.13), so we are essentially visualizing \u03a3; this is\nknown as the covariance graph (Section 19.4.4.1).\nThis method is quite popular in systems biology (Margolin et al. 2006), where it is used to\nvisualize the interaction between genes. The trouble with biological examples is that they are\nhard for non-biologists to understand. So let us instead illustrate the idea using natural languagetext. Figure 26.1 gives an example, where we visualize the MI between words in the newsgroupdataset from Figure 1.2. The results seem intuitively reasonable.\nHowever, relevance networks suffer from a major problem: the graphs are usually very dense,\nsince most variables are dependent on most other variables, even after thresholding the MIs.For example, suppose X\n1directly in\ufb02uences X2which directly in\ufb02uences X3(e.g., these form\ncomponents of a signalling cascade, X1\u2212X2\u2212X3). ThenX1has non-zero MI with X3(and\nvice versa), so there will be a 1\u22123edge in the relevance network. Indeed, most pairs will be", "939": "26.2. Structurelearningforknowledgediscovery 909\naids\nhealthbaseball\nfans\ngames hit\nleague\nplayersbible\ngod\njesus\norbitbmw\nengine\nhonda\ncancer\ndisease\npatientscar\ndealerdrive\ndriver guninsurance oil card\ngraphics\nvideocase children\nfood christian\nmission\nreligion sciencecomputercourse\ndata\nsolar\nproblem evidencemedicinedisk\nfilesmemory scsi\nspacedisplay\nimageserverdoctor\nhelpdos\nwindows\nformatearth\nmarsmoon\nemail\nphonepower\nmsgfact\nhockey\nteam\nftphuman\nvitaminwater\nnumber\nversionnhl puck season\nwin\nwongovernment\njews president\nrightswar\nlaw systemisrael\nresearchstate launch\nshuttle\nlunarmac\npc\nstudiesnasa\nsatelliteprogramquestion\nuniversity software\ntechnologyworld\nFigure 26.2 A dependency network constructed from the 20-news data. We show all edges with regres-\nsion weight above 0.5 in the Markov blankets estimated by /lscript1penalized logistic regression. Undirected\nedges represent cases where a directed edge was found in both directions. From Figure 4.9 of (Schmidt\n2010). Used with kind permission of Mark Schmidt.\nconnected.\nA better approach is to use graphical models, which represent conditional independence,\nrather than dependence. In the above example, X1is conditionally independent of X3given\nX2, so there will not be a 1\u22123edge. Consequently graphical models are usually much sparser\nthan relevance networks, and hence are a more useful way of visualizing interactions between\nmultiple variables.\n26.2.2 Dependency networks\nA simple and efficient way to learn a graphical model structure is to independently \ufb01t Dsparse\nfull-conditional distributions p(xt|x\u2212t); this is called a dependency network (Heckerman et al.\n2000). The chosen variables constitute the inputs to the node, i.e., its Markov blanket. Wecan then visualize the resulting sparse graph. The advantage over relevance networks is thatredundant variables will not be selected as inputs.\nWe can use any kind of sparse regression or classi\ufb01cation method to \ufb01t each CPD. (Heckerman\net al. 2000) uses classi\ufb01cation/ regression trees, (Meinshausen and Buhlmann 2006) use /lscript\n1-\nregularized linear regression, (Wainwright et al. 2006) use /lscript1-regularized logistic regression (see\ndepnetFit for some code), (Dobra 2009) uses Bayesian variable selection, etc. (Meinshausen", "940": "910 Chapter26. Graphicalmodelstructurelearning\nand Buhlmann 2006) discuss theoretical conditions under which /lscript1-regularized linear regression\ncan recover the true graph structure, assuming the data was generated from a sparse Gaussian\ngraphical model.\nFigure 26.2 shows a dependency network that was learned from the 20-newsgroup data using\n/lscript1regularized logistic regression, where the penalty parameter \u03bbwas chosen by BIC. Many\nof the words present in these estimated Markov blankets represent fairly natural associations(aids:disease, baseball:fans, bible:god, bmw:car, cancer:patients, etc.). However, some of the esti-mated statistical dependencies seem less intuitive, such as baseball:windows and bmw:christian.We can gain more insight if we look not only at the sparsity pattern, but also the values of theregression weights. For example, here are the incoming weights for the \ufb01rst 5 words:\n\u2022aids: children (0.53), disease (0.84), fact (0.47), health (0.77), president (0.50), research (0.53)\n\u2022baseball:christian(-0.98),drive(-0.49), games (0.81), god(-0.46),government (-0.69), hit (0.62),\nmemory(-1.29), players (1.16), season (0.31), software(-0.68),windows(-1.45)\n\u2022bible:car(-0.72),card(-0.88), christian (0.49), fact (0.21), god (1.01), jesus (0.68), orbit (0.83),\nprogram(-0.56), religion (0.24), version (0.49)\n\u2022bmw: car (0.60), christian (-11.54), engine (0.69), god(-0.74),government (-1.01),help(-0.50),\nwindows(-1.43)\n\u2022cancer: disease (0.62), medicine (0.58), patients (0.90), research (0.49), studies (0.70)\nWords in italic red have negative weights, which represents a dissociative relationship. For\nexample, the model re\ufb02ects that baseball:windows is an unlikely combination. It turns out thatmost of the weights are negative (1173 negative, 286 positive, 8541 zero) in this model.\nIn addition to visualizing the data, a dependency network can be used for inference. However,\nthe only algorithm we can use is Gibbs sampling, where we repeatedly sample the nodes withmissing values from their full conditionals. Unfortunately, a product of full conditionals doesnot, in general, constitute a representation of any valid joint distribution (Heckerman et al.2000), so the output of the Gibbs sampler may not be meaningful. Nevertheless, the method cansometimes give reasonable results if there is not much missing data, and it is a useful methodfor data imputation (Gelman and Raghunathan 2001). In addition, the method can be used asan initialization technique for more complex structure learning methods that we discuss below.\n26.3 Learning tree structures\nFor the rest of this chapter, we focus on learning fully speci\ufb01ed joint probability models, whichcan be used for density estimation, prediction and knowledge discovery.\nSince the problem of structure learning for general graphs is NP-hard (Chickering 1996), we\nstart by considering the special case of trees. Trees are special because we can learn theirstructure efficiently, as we disuscs below, and because, once we have learned the tree, we canuse them for efficient exact inference, as discussed in Section 20.2.", "941": "26.3. Learningtreestructures 911\n4312\n(a)4312\n(b)4312\n(c)\nFigure 26.3 An undirected tree and two equivalent directed trees.\n26.3.1 Directed or undirected tree?\nBefore continuing, we need to discuss the issue of whether we should use directed or undirected\ntrees. A directed tree, with a single root node r, de\ufb01nes a joint distribution as follows:\np(x|T)=/productdisplay\nt\u2208Vp(xt|xpa(t)) (26.1)\nwhere we de\ufb01ne pa(r)=\u2205. For example, in Figure 26.3(b-c), we have\np(x1,x2,x3,x4|T)=p( x1)p(x2|x1)p(x3|x2)p(x4|x2) (26.2)\n=p(x2)p(x1|x2)p(x3|x2)p(x4|x2) (26.3)\nWe see that the choice of root does not matter: both of these models are equivalent.\nTo make the model more symmetric, it is preferable to use an undirected tree. This can be\nrepresented as follows:\np(x|T)=/productdisplay\nt\u2208Vp(xt)/productdisplay\n(s,t)\u2208Ep(xs,xt)\np(xs)p(xt)(26.4)\nwherep(xs,xt)is an edge marginal and p(xt)is a node marginal. For example, in Figure 26.3(a)\nwe have\np(x1,x2,x3,x4|T)=p(x1)p(x2)p(x3)p(x4)p(x1,x2)p(x2,x3)p(x2,x4)\np(x1)p(x2)p(x2)p(x3)p(x2)p(x4)(26.5)\nTo see the equivalence with the directed representation, let us cancel terms to get\np(x1,x2,x3,x4|T)=p( x1,x2)p(x2,x3)\np(x2)p(x2,x4)\np(x2)(26.6)\n=p(x1)p(x2|x1)p(x3|x2)p(x4|x2) (26.7)\n=p(x2)p(x1|x2)p(x3|x2)p(x4|x2) (26.8)\nwherep(xt|xs)=p(xs,xt)/p(xs).\nThus a tree can be represented as either an undirected or directed graph: the number of\nparameters is the same, and hence the complexity of learning is the same. And of course,inference is the same in both representations, too. The undirected representation, which issymmetric, is useful for structure learning, but the directed representation is more convenientfor parameter learning.", "942": "912 Chapter26. Graphicalmodelstructurelearning\n26.3.2 Chow-Liu algorithm for \ufb01nding the ML tree structure\nUsing Equation 26.4, we can write the log-likelihood for a tree as follows:\nlogp(D|\u03b8,T)=/summationdisplay\nt/summationdisplay\nkNtklogp(xt=k|\u03b8)\n+/summationdisplay\ns,t/summationdisplay\nj,kNstjklogp(xs=j,xt=k|\u03b8)\np(xs=j|\u03b8)p(xt=k|\u03b8)(26.9)\nwhereNstjkis the number of times node sis in state jand node tis in state k, andNtkis\nthe number of times node tis in state k. We can rewrite these counts in terms of the empirical\ndistribution: Nstjk=Npemp(xs=j,xt=k)andNtk=Npemp(xt=k). Setting \u03b8to the\nMLEs, this becomes\nlogp(D|\u03b8,T)\nN=/summationdisplay\nt\u2208V/summationdisplay\nkpemp(xt=k)logpemp(xt=k) (26.10)\n+/summationdisplay\n(s,t)\u2208E(T)I(xs,xt|\u02c6\u03b8st) (26.11)\nwhere I(xs,xt|\u02c6\u03b8st)\u22650is the mutual information between xsandxtgiven the empirical\ndistribution:\nI(xs,xt|\u02c6\u03b8st)=/summationdisplay\nj/summationdisplay\nkpemp(xs=j,xt=k)logpemp(xs=j,xt=k)\npemp(xs=j)pemp(xt=k)(26.12)\nSince the \ufb01rst term in Equation 26.11 is independent of the topology T, we can ignore it when\nlearning structure. Thus the tree topology that maximizes the likelihood can be found by\ncomputing the maximum weight spanning tree, where the edge weights are the pairwise mutualinformations, I(y\ns,yt|\u02c6\u03b8st). This is called the Chow-Liu algorithm (Chow and Liu 1968).\nThere are several algorithms for \ufb01nding a max spanning tree (MST). The two best known are\nPrim\u2019s algorithm and Kruskal\u2019s algorithm. Both can be implemented to run in O(ElogV)time,\nwhereE=V2is the number of edges and Vis the number of nodes. See e.g., (Sedgewick and\nWayne 2011, 4.3) for details. Thus the overall running time is O(NV2+V2logV), where the\n\ufb01rst term is the cost of computing the sufficient statistics.\nFigure 26.4 gives an example of the method in action, applied to the binary 20 newsgroups\ndata shown in Figure 1.2. The tree has been arbitrarily rooted at the node representing \u201cemail\u201d.The connections that are learned seem intuitively reasonable.\n26.3.3 Finding the MAP forest\nSince all trees have the same number of parameters, we can safely used the maximum likelihoodscore as a model selection criterion without worrying about over\ufb01tting. However, sometimes wemay want to \ufb01t a forestrather than a single tree, since inference in a forest is much faster than\nin a tree (we can run belief propagation in each tree in the forest in parallel). The MLE criterionwill never choose to omit an edge. However, if we use the marginal likelihood or a penalizedlikelihood (such as BIC), the optimal solution may be a forest. Below we give the details for themarginal likelihood case.", "943": "26.3. Learningtreestructures 913\naidsbaseball\nhit\nbiblebmw\ncancercar\ndealer engine hondacard\ngraphics video\ncasechildrenchristian\ncomputer coursedata\ndiseasedisk\ndrive memory system\ndisplay\nserver\ndoctordos\nscsidriver\nearth\ngodemail\nftp phone\noil\nevidencefact\nquestionfansfiles\nformat windows\nfood\nmsg waterimage\ngames\njesus religion\ngovernment\npower president rights state war\ngunhealth\ninsurance medicinehelp hockey\nnhl\nhumanisraeljewslaunch\nlawleague\nlunarmac\nmars\npatients studiesmissionmoon nasanumber\norbit\nsatellite solar\nvitaminpc\nsoftware\nplayersproblemprogram\nspace\npuck\nresearch scienceseason shuttle technology\nuniversityteamversion\nworldwin\nwon\nFigure 26.4 The MLE tree on the 20-newsgroup data. From Figure 4.11 of (Schmidt 2010). Used with kind\npermission of Mark Schmidt. (A topologically equivalent tree can be produced using chowliuTreeDemo .)\nIn Section 26.4.2.2, we explain how to compute the marginal likelihood of any DAG using a\nDirichlet prior for the CPTs. The resulting expression can be written as follows:\nlogp(D|T)=/summationdisplay\nt\u2208Vlog/integraldisplayN/productdisplay\ni=1p(xit|xi,pa(t)|\u03b8t)p(\u03b8t)d\u03b8t=/summationdisplay\ntscore(Nt,pa(t)) (26.13)\nwhereNt,pa(t)are the counts (sufficient statistics) for node tand its parents, and score is\nde\ufb01ned in Equation 26.28.\nNow suppose we only allow DAGs with at most one parent. Following (Heckerman et al. 1995,\np227), let us associate a weight with each s\u2192tedge,ws,t/definesscore(t|s)\u2212score(t|0),w h e r e\nscore(t|0)is the score when thas no parents. Note that the weights might be negative (unlike\nthe MLE case, where edge weights are aways non-negative because they correspond to mutual\ninformation). Then we can rewrite the objective as follows:\nlogp(D|T)=/summationdisplay\ntscore(t|pa(t)) =/summationdisplay\ntwpa(t),t+/summationdisplay\ntscore(t|0) (26.14)\nThe last term is the same for all trees T, so we can ignore it. Thus \ufb01nding the most probable\ntree amounts to \ufb01nding a maximal branching in the corresponding weighted directed graph.\nThis can be found using the algorithm in (Gabow et al. 1984).", "944": "914 Chapter26. Graphicalmodelstructurelearning\nIf the scoring function is prior and likelihood equivalent (these terms are explained in Sec-\ntion 26.4.2.3), we have\nscore(s|t)+score( t|0) =score(t|s)+score( s|0) (26.15)\nand hence the weight matrix is symmetric. In this case, the maximal branching is the same\nas the maximal weight forest. We can apply a slightly modi\ufb01ed version of the MST algorithmto \ufb01nd this (Edwards et al. 2010). To see this, let G=(V,E)be a graph with both positive\nand negative edge weights. Now let G\n/primebe a graph obtained by omitting all the negative edges\nfromG. This cannot reduce the total weight, so we can \ufb01nd the maximum weight forest of G\nby \ufb01nding the MST for each connected component of G/prime. We can do this by running Kruskal\u2019s\nalgorithm directly on G/prime: there is no need to \ufb01nd the connected components explicitly.\n26.3.4 Mixtures of trees\nA single tree is rather limited in its expressive power. Later in this chapter we discuss ways tolearn more general graphs. However, the resulting graphs can be expensive to do inference in.An interesting alternative is to learn a mixture of trees (Meila and Jordan 2000), where each\nmixture component may have a different tree topology. This is like an unsupervised version ofthe TAN classi\ufb01er discussed in Section 10.2.1. We can \ufb01t a mixture of trees by using EM: in theE step, we compute the responsibilities of each cluster for each data point, and in the M step,we use a weighted version of the Chow-Liu algorithm. See (Meila and Jordan 2000) for details.\nIn fact, it is possible to create an \u201cin\ufb01nite mixture of trees\u201d, by integrating out over all possible\ntrees. Remarkably, this can be done in V\n3time using the matrix tree theorem. This allows us to\nperform exact Bayesian inference of posterior edge marginals etc. However, it is not tractable touse this in\ufb01nite mixture for inference of hidden nodes. See (Meila and Jaakkola 2006) for details.\n26.4 Learning DAG structures\nIn this section, we discuss how to compute (functions of) p(G|D),w h e r eGis constrained to be\na DAG. This is often called Bayesian network structure learning. In this section, we assume\nthere is no missing data, and that there are no hidden variables. This is called the complete\ndata assumption. For simplicity, we will focus on the case where all the variables are categoricaland all the CPDs are tables, although the results generalize to real-valued data and other kindsof CPDs, such as linear-Gaussian CPDs.\nOur presentation is based in part on (Heckerman et al. 1995), although we will follow the\nnotation of Section 10.4.2. In particular, let x\nit\u2208{1,...,K t}be the value of node tin casei,\nwhereKtis the number of states for node t.L e t\u03b8tck/definesp(xt=k|xpa(t)=c),f o rk=1:K t,\nandc=1:C t,w h e r eCtis the number of parent combinations (possible conditioning cases).\nFor notational simplicity, we will often assume Kt=K, so all nodes have the same number of\nstates. We will also let dt=d i m ( p a ( t))be the degree or fan-in of node t, so that Ct=Kdt.\n26.4.1 Markov equivalence\nIn this section, we discuss some fundamental limits to our ability to learn DAG structures fromdata.", "945": "26.4. LearningDAGstructures 915\nX1\nX2X3\nX4X5X1\nX2X3\nX4X5X1\nX2X3\nX4X5G1 G2 G3\nFigure 26.5 Three DAGs. G1andG3are Markov equivalent, G2is not.\nConsider the following 3 DGMs: X\u2192Y\u2192Z,X\u2190Y\u2190ZandX\u2190Y\u2192Z. These all\nrepresent the same set of CI statements, namely\nX\u22a5Z|Y, X/negationslash\u22a5Z (26.16)\nWe say these graphs are Markov equivalent, since they encode the same set of CI assumptions.\nThat is, they all belong to the same Markov equivalence class. However, the v-structure\nX\u2192Y\u2190ZencodesX\u22a5ZandX/negationslash\u22a5Z|Y, which represents the opposite set of CI\nassumptions.\nOne can prove the following theorem.\nTheorem 26.4.1 (Verma and Pearl (Verma and Pearl 1990)). TwostructuresareMarkovequivalent\nifftheyhavethesameundirectedskeletonandthesamesetofv-structures.\nFor example, referring to Figure 26.5, we see that G1/negationslash\u2261G2, since reversing the 2\u21924arc\ncreates a new v-structure. However, G1\u2261G3, since reversing the 1\u21925arc does not create a\nnew v-structure.\nWe can represent a Markov equivalence class using a single partially directed acyclic graph\n(PDAG), also called an essential graph orpattern, in which some edges are directed and some\nundirected. The undirected edges represent reversible edges; any combination is possible so\nlong as no new v-structures are created. The directed edges are called compelled edges, since\nchanging their orientation would change the v-structures and hence change the equivalenceclass. For example, the PDAG X\u2212Y\u2212Zrepresents{X\u2192Y\u2192Z,X\u2190Y\u2190Z,X\u2190Y\u2192\nZ}which encodes X/negationslash\u22a5ZandX\u22a5Z|Y. See Figure 26.6.\nThe signi\ufb01cance of the above theorem is that, when we learn the DAG structure from data,\nwe will not be able to uniquely identify all of the edge directions, even given an in\ufb01nite amountof data. We say that we can learn DAG structure \u201cup to Markov equivalence\u201d. This also cautionsus not to read too much into the meaning of particular edge orientations, since we can oftenchange them without changing the model in any observable way.", "946": "916 Chapter26. Graphicalmodelstructurelearning\nZYX\nZYX\nZYX\nZYX\nZYX\nZYX\n\u2261\u2261\nFigure 26.6 PDAG representation of Markov equivalent DAGs.\n26.4.2 Exact structural inference\nIn this section, we discuss how to compute the exact posterior over graphs, p(G|D), ignoring\nfor now the issue of computational tractability.\n26.4.2.1 Deriving the likelihood\nAssuming there is no missing data, and that all CPDs are tabular, the likelihood can be written\nas follows:\np(D|G,\u03b8)=N/productdisplay\ni=1V/productdisplay\nt=1Cat(xit|xi,pa(t),\u03b8t) (26.17)\n=N/productdisplay\ni=1V/productdisplay\nt=1Ct/productdisplay\nc=1Cat(xit|\u03b8tc)I(xi,pa(t )=c)(26.18)\n=N/productdisplay\ni=1V/productdisplay\nt=1Ct/productdisplay\nc=1Kt/productdisplay\nk=1\u03b8I(xi,t=k,xi,pa(t)=c)\ntck(26.19)\n=V/productdisplay\nt=1Ct/productdisplay\nc=1Kt/productdisplay\nk=1\u03b8Ntck\ntck(26.20)\nwhereNtckis the number of times node tis in state kand its parents are in state c. (Technically\nthese counts depend on the graph structure G, but we drop this from the notation.)\n26.4.2.2 Deriving the marginal likelihood\nOf course, choosing the graph with the maximum likelihood will always pick a fully connectedgraph (subject to the acyclicity constraint), since this maximizes the number of parameters. Toavoid such over\ufb01tting, we will choose the graph with the maximum marginal likelihood, p(D|G);\nthe magic of the Bayesian Occam\u2019s razor will then penalize overly complex graphs.\nTo compute the marginal likelihood, we need to specify priors on the parameters. We will\nmake two standard assumptions. First, we assume global prior parameter independence,\nwhich means\np(\u03b8)=\nV/productdisplay\nt=1p(\u03b8t) (26.21)", "947": "26.4. LearningDAGstructures 917\nSecond, we assume local prior parameter independence, which means\np(\u03b8t)=Ct/productdisplay\nc=1p(\u03b8tc) (26.22)\nfor eacht. It turns out that these assumtions imply that the prior for each row of each CPT\nmust be a Dirichlet (Geiger and Heckerman 1997), that is,\np(\u03b8tc)=D i r (\u03b8tc|\u03b1tc) (26.23)\nGiven these assumptions, and using the results of Section 5.3.2.2, we can write down the\nmarginal likelihood of any DAG as follows:\np(D|G)=V/productdisplay\nt=1Ct/productdisplay\nc=1/integraldisplay\u23a1\n\u23a3/productdisplay\ni:xi,pa(t )=cCat(xit|\u03b8tc)\u23a4\u23a6Dir(\u03b8\ntc)d\u03b8tc (26.24)\n=V/productdisplay\nt=1Ct/productdisplay\nc=1B(Ntc+\u03b1tc)\nB(\u03b1tc)(26.25)\n=V/productdisplay\nt=1Ct/productdisplay\nc=1\u0393(Ntc)\n\u0393(Ntc+\u03b1tc)Kt/productdisplay\nk=1\u0393(Ntck+\u03b1G\ntck)\n\u0393(\u03b1G\nijk)(26.26)\n=V/productdisplay\nt=1score(Nt,pa(t)) (26.27)\nwhereNtc=/summationtext\nkNtck,\u03b1tc=/summationtext\nk\u03b1tck,Nt,pa(t)is the vector of counts (sufficient statistics) for\nnodetand its parents, and score() is a local scoring function de\ufb01ned by\nscore(Nt,pa(t))/definesCt/productdisplay\nc=1B(Ntc+\u03b1tc)\nB(\u03b1tc)(26.28)\nWe say that the marginal likelihood decomposes or factorizes according to the graph structure.\n26.4.2.3 Setting the prior\nHow should we set the hyper-parameters \u03b1tck? It is tempting to use a Jeffreys prior of the form\n\u03b1tck=1\n2(Equation 5.62). However, it turns out that this violates a property called likelihood\nequivalence, which is sometimes considered desirable. This property says that if G1andG2are\nMarkov equivalent (Section 26.4.1), they should have the same marginal likelihood, since they are\nessentially equivalent models. Geiger and Heckerman (1997) proved that, for complete graphs,the only prior that satis\ufb01es likelihood equivalence and parameter independence is the Dirichletprior, where the pseudo counts have the form\n\u03b1\ntck=\u03b1p0(xt=k,xpa(t)=c) (26.29)\nwhere\u03b1>0is called the equivalent sample size , andp0is some prior joint probability dis-\ntribution. This is called the BDeprior, which stands for Bayesian Dirichlet likelihood equivalent.", "948": "918 Chapter26. Graphicalmodelstructurelearning\nTo derive the hyper-parameters for other graph structures, Geiger and Heckerman (1997)\ninvoked an additional assumption called parameter modularity, which says that if node Xt\nhas the same parents in G1andG2, thenp(\u03b8t|G1)=p(\u03b8t|G2). With this assumption, we\ncan always derive \u03b1tfor a node tin any other graph by marginalizing the pseudo counts in\nEquation 26.29.\nTypically the prior distribution p0is assumed to be uniform over all possible joint con\ufb01gura-\ntions. In this case, we have\n\u03b1tck=\u03b1\nKtCt(26.30)\nsincep0(xt=k,xpa(t)=c)=1\nKtCt. Thus if we sum the pseudo counts over all Ct\u00d7Kt\nentries in the CPT, we get a total equivalent sample size of \u03b1. This is called the BDeuprior,\nwhere the \u201cu\u201d stands for uniform. This is the most widely used prior for learning Bayes net\nstructures. For advice on setting the global tuning parameter \u03b1, see (Silander et al. 2007).\n26.4.2.4 Simple worked example\nWe now give a very simple worked example from (Neapolitan 2003, p.438). Suppose we havejust 2 binary nodes, and the following 8 data cases:\nX\n1X2\n1112112211211122\nSuppose we are interested in two possible graphs: G\n1isX1\u2192X2andG2is the disconnected\ngraph. The empirical counts for node 1 in G1areN1=( 5,3)and for node 2 are\nX2=1X2=2\nX1=141\nX1=212\nThe BDeu prior for G1is\u03b11=(\u03b1/2,\u03b1/2),\u03b12|x 1=1=(\u03b1/4,\u03b1/4)and\u03b12|x 1=2=\n(\u03b1/4,\u03b1/4).F o rG2, the prior for \u03b81is the same, and for \u03b82it is\u03b12|x 1=1=(\u03b1/2,\u03b1/2)\nand\u03b12|x 1=2=(\u03b1/2,\u03b1/2). I fw es e t\u03b1 =4, and use the BDeu prior, we \ufb01nd p(D|G1)=\n7.2150\u00d710\u22126andp(D|G2)=6.7465\u00d710\u22126. Hence the posterior probabilites, under a\nuniform graph prior, are p(G1|D)=0.51678andp(G2|D)=0.48322.\n26.4.2.5 Example: analysis of the college plans dataset\nWe now consider a more interesting example from (Heckerman et al. 1997). Consider the dataset collected in 1968 by Sewell and Shah which measured 5 variables that might in\ufb02uence thedecision of high school students about whether to attend college. Speci\ufb01cally, the variables areas follows:", "949": "26.4. LearningDAGstructures 919\nFigure 26.7 The two most probable DAGs learned from the Sewell-Shah data. Source: (Heckerman et al.\n1997) . Used with kind permission of David Heckerman\n\u2022SexMale or female\n\u2022SESSocio economic status: low, lower middle, upper middle or high.\n\u2022IQIntelligence quotient: discretized into low, lower middle, upper middle or high.\n\u2022PEParental encouragment: low or high\n\u2022CPCollege plans: yes or no.\nThese variables were measured for 10,318 Wisconsin high school seniors. There are 2\u00d74\u00d7\n4\u00d72\u00d7= 128possible joint con\ufb01gurations.\nHeckerman et al. computed the exact posterior over all 29,281 possible 5 node DAGs, except\nfor ones in which SEX and/or SES have parents, and/or CP have children. (The prior probability\nof these graphs was set to 0, based on domain knowledge.) They used the BDeu score with\n\u03b1=5, although they said that the results were robust to any \u03b1in the range 3 to 40. The top\ntwo graphs are shown in Figure 26.7. We see that the most probable one has approximately all\nof the probability mass, so the posterior is extremely peaked.\nIt is tempting to interpret this graph in terms of causality (see Section 26.6). In particular,\nit seems that socio-economic status, IQ and parental encouragment all causally in\ufb02uence the\ndecision about whether to go to college, which makes sense. Also, sex in\ufb02uences college plans\nonly indirectly through parental encouragement, which also makes sense. However, the direct\nlink from socio economic status to IQ seems surprising; this may be due to a hidden common\ncause. In Section 26.5.1.4 we will re-examine this dataset allowing for the presence of hidden\nvariables.\n26.4.2.6 The K2 algorithm\nSuppose we know a total ordering of the nodes. Then we can compute the distribution over\nparents for each node independently, without the risk of introducing any directed cycles: we", "950": "920 Chapter26. Graphicalmodelstructurelearning\nsimply enumerate over all possible subsets of ancestors and compute their marginal likelihoods.1\nIf we just return the best set of parents for each node, we get the the K2 algorithm (Cooper\nand Herskovits 1992).\n26.4.2.7 Handling non-tabular CPDs\nIf all CPDs are linear Gaussian, we can replace the Dirichlet-multinomial model with the normal-\ngamma model, and thus derive a different exact expression for the marginal likelihood. See(Geiger and Heckerman 1994) for the details. In fact, we can easily combine discrete nodesand Gaussian nodes, as long as the discrete nodes always have discrete parents; this is called aconditional Gaussian DAG. Again, we can compute the marginal likelihood in closed form. See\n(Bottcher and Dethlefsen 2003) for the details.\nIn the general case (i.e., everything except Gaussians and CPTs), we need to approximate the\nmarginal likelihood. The simplest approach is to use the BIC approximation, which has the form\n/summationdisplay\ntlogp(Dt|\u02c6\u03b8t)\u2212KtCt\n2logN (26.31)\n26.4.3 Scaling up to larger graphs\nThe main challenge in computing the posterior over DAGs is that there are so many possiblegraphs. More precisely, (Robinson 1973) showed that the number of DAGs on Dnodes satis\ufb01es\nthe following recurrence:\nf(D)=D/summationdisplay\ni=1(\u22121)i+1/parenleftbiggD\ni/parenrightbigg\n2i(D\u2212i)f(D\u2212i) (26.32)\nforD>2. The base case is f(1) = 1. Solving this recurrence yields the following sequence:\n1, 3, 25, 543, 29281, 3781503, etc.2In view of the enormous size of the hypothesis space, we are\ngenerally forced to use approximate methods, some of which we review below.\n26.4.3.1 Approximating the mode of the posterior\nWe can use dynamic programming to \ufb01nd the globally optimal MAP DAG (up to Markov equiv-alence) (Koivisto and Sood 2004; Silander and Myllmaki 2006). Unfortunately this method takesV2\nVtime and space, making it intractable beyond about 16 nodes. Indeed, the general problem\nof \ufb01nding the globally optimal MAP DAG is provably NP-complete (Chickering 1996),\nConsequently, we must settle for \ufb01nding a locally optimal MAP DAG. The most common\nmethod is greedy hill climbing: at each step, the algorithm proposes small changes to thecurrent graph, such as adding, deleting or reversing a single edge; it then moves to the neigh-boring graph which most increases the posterior. The method stops when it reaches a lo-cal maximum. It is important that the method only proposes local changes to the graph,\n1. We can make this method more efficient by using /lscript1-regularization to select the parents (Schmidt et al. 2007). In this\ncase, we need to approximate the marginal likelhood as we discuss below.\n2. A longer list of values can be found at http://www .research .att.com/~njas/sequences/A003024 . Interest-\ningly, the number of DAGs is equal to the number of (0,1) matrices all of whose eigenvalues are positive real numbers(McKay et al. 2004).", "951": "26.4. LearningDAGstructures 921\naidsbaseball\nfans leaguebible bmw\ncancer\nhealthcar\ndealer\ndriverengine\nhonda\ninsuranceoil\ncard\ngraphicscase\nchildren\nfoodwaterchristian\njesus religion\ncomputer\nsciencecourse\ndataproblemdisease\nmedicinepatientsdisk\ndos\nfiles\nmacmemoryprogramspace\ndisplaydoctor\nhelpversion\nwindowsdrive\nformatscsi\nearth\nimagemarsmoon\nemail\nphonepowerevidence\nfact msg\nstudiesgod\nftp\nvitaminnumbergames\nhockey\nnhl puckseason\nteam\nwinwongovernment\njews law\npresidentrights\nwargun\nhit human\nisrael\nresearch statelaunch\nlunar\npcsoftware\nservervideomission\norbit\nsolarnasa\nsatelliteshuttle\nplayersquestion\nuniversitysystemtechnology\nworld\nFigure 26.8 A locally optimal DAG learned from the 20-newsgroup data. From Figure 4.10 of (Schmidt\n2010). Used with kind permission of Mark Schmidt.\nsince this enables the change in marginal likelihood (and hence the posterior) to be computed\nin constant time (assuming we cache the sufficient statistics). This is because all but oneor two of the terms in Equation 26.25 will cancel out when computing the log Bayes factor\u03b4(G\u2192G\n/prime)=l o gp(G/prime|D)\u2212logp(G|D).\nWe can initialize the search from the best tree, which can be found using exact methods\ndiscussed in Section 26.3. For speed, we can restrict the search so it only adds edges which arepart of the Markov blankets estimated from a dependency network (Schmidt 2010). Figure 26.8gives an example of a DAG learned in this way from the 20-newsgroup data.\nWe can use techniques such as multiple random restarts to increase the chance of \ufb01nding a\ngood local maximum. We can also use more sophisticated local search methods, such as geneticalgorithms or simulated annealing, for structure learning.\n26.4.3.2 Approximating other functions of the posterior\nIf our goal is knowledge discovery, the MAP DAG can be misleading, for reasons we discussed inSection 5.2.1. A better approach is to compute the probability that each edge is present, p(G\nst=\n1|D), of the probability there is a path from stot. We can do this exactly using dynamic\nprogramming (Koivisto 2006; Parviainen and Koivisto 2011). Unfortunately these methods takeV2\nVtime in the general case, making them intractable for graphs with more than about 16", "952": "922 Chapter26. Graphicalmodelstructurelearning\nnodes.\nAn approximate method is to sample DAGs from the posterior, and then to compute the\nfraction of times there is an s\u2192tedge or path for each (s,t)pair. The standard way to draw\nsamples is to use the Metropolis Hastings algorithm (Section 24.3), where we use the same local\nproposal as we did in greedy search (Madigan and Raftery 1994).\nA faster-mixing method is to use a collapsed MH sampler, as suggested in (Friedman and\nKoller 2003). This exploits the fact that, if a total ordering of the nodes is known, we canselect the parents for each node independently, without worrying about cycles, as discussed inSection 26.4.2.6. By summing over all possible choice of parents, we can marginalize out thispart of the problem, and just sample total orders. (Ellis and Wong 2008) also use order-space(collapsed) MCMC, but this time with a parallel tempering MCMC algorithm.\n26.5 Learning DAG structure with latent variables\nSometimes the complete data assumption does not hold, either because we have missing data,and/ or because we have hidden variables. In this case, the marginal likelihood is given by\np(D|G)=/integraldisplay/summationdisplay\nhp(D,h|\u03b8,G)p(\u03b8|G)d\u03b8=/summationdisplay\nh/integraldisplay\np(D,h|\u03b8,G)p(\u03b8|G)d\u03b8 (26.33)\nwherehrepresents the hidden or missing data.\nIn general this is intractable to compute. For example, consider a mixture model, where\nwe don\u2019t observe the cluster label. In this case, there are KNpossible completions of the\ndata (assuming we have Kclusters); we can evaluate the inner integral for each one of these\nassignments to h, but we cannot afford to evaluate all of the integrals. (Of course, most of these\nintegrals will correspond to hypotheses with little posterior support, such as assigning singledata points to isolated clusters, but we don\u2019t know ahead of time the relative weight of theseassignments.)\nIn this section, we discuss some ways for learning DAG structure when we have latent variables\nand/or missing data.\n26.5.1 Approximating the marginal likelihood when we have missing data\nThe simplest approach is to use standard structure learning methods for fully visible DAGs,but to approximate the marginal likelihood. In Section 24.7, we discussed some Monte Carlomethods for approximating the marginal likelihood. However, these are usually too slow to useinside of a search over models. Below we mention some faster deterministic approximations.\n26.5.1.1 BIC approximation\nA simple approximation is to use the BIC score, which is given by\nBIC(G)/defineslogp(D|\u02c6\u03b8,G)\u2212logN\n2dim(G) (26.34)\nwheredim(G)is the number of degrees of freedom in the model and \u02c6\u03b8is the MAP or ML\nestimate. However, the BIC score often severely underestimates the true marginal likelihood(Chickering and Heckerman 1997), resulting in it selecting overly simple models.", "953": "26.5. LearningDAGstructurewithlatentvariables 923\n26.5.1.2 Cheeseman-Stutz approximation\nWe now present a better method known as the Cheeseman-Stutz approximation (CS) (Cheese-\nman and Stutz 1996). We \ufb01rst compute a MAP estimate of the parameters \u02c6\u03b8(e.g., using EM).\nDenote the expected sufficient statistics of the data by D=D(\u02c6\u03b8); in the case of discrete\nvariables, we just \u201c\ufb01ll in\u201d the hidden variables with their expectation. We then use the exact\nmarginal likelihood equation on this \ufb01lled-in data:\np(D|G)\u2248p(D|G)=/integraldisplay\np(D|\u03b8,G)p(\u03b8|G)d\u03b8 (26.35)\nHowever, comparing this to Equation 26.33, we can see that the value will be exponentiallysmaller, since it does not sum over all values of h. To correct for this, we \ufb01rst write\nlogp(D|G)=l o g p(\nD|G)+log p(D|G)\u2212logp(D|G) (26.36)\nand then we apply a BIC approximation to the last two terms:\nlogp(D|G)\u2212logp(D|G)\u2248/bracketleftbigg\nlogp(D|\u02c6\u03b8,G)\u2212N\n2dim(\u02c6\u03b8)/bracketrightbigg\n(26.37)\n\u2212/bracketleftbigg\nlogp(D|\u02c6\u03b8,G)\u2212N\n2dim(\u02c6\u03b8)/bracketrightbigg\n(26.38)\n=l o gp(D|\u02c6\u03b8,G)\u2212logp(D|\u02c6\u03b8,G) (26.39)\nPutting it altogether we get\nlogp(D|G)\u2248logp(D|G)+log p(D|\u02c6\u03b8,G)\u2212logp(D|\u02c6\u03b8,G) (26.40)\nThe \ufb01rst term p(D|G)can be computed by plugging in the \ufb01lled-in data into the exact marginal\nlikelihood. The second term p(D|\u02c6\u03b8,G), which involves an exponential sum (thus matching the\n\u201cdimensionality\u201d of the left hand side) can be computed using an inference algorithm. The \ufb01naltermp(\nD|\u02c6\u03b8,G)can be computed by plugging in the \ufb01lled-in data into the regular likelihood.\n26.5.1.3 Variational Bayes EM\nAn even more accurate approach is to use the variational Bayes EM algorithm. Recall fromSection 21.6 that the key idea is to make the following factorization assumption:\np(\u03b8,z\n1:N|D)\u2248q(\u03b8)q(z)=q(\u03b8)/productdisplay\niq(zi) (26.41)\nwhereziare the hidden variables in case i. In the E step, we update the q(zi), and in the\nM step, we update q(\u03b8). The corresponding variational free energy provides a lower bound on\nthe log marginal likelihood. In (Beal and Ghahramani 2006), it is shown that this bound is amuch better approximation to the true log marginal likelihood (as estimated by a slow annealedimportance sampling procedure) than either BIC or CS. In fact, one can prove that the variationalbound will always be more accurate than CS (which in turn is always more accurate than BIC).", "954": "924 Chapter26. Graphicalmodelstructurelearning\np(male) = 0.48\nSESH\nSEX\nPE\nIQ\nCPp(H= 0) = 0.63\np(H= 1) = 0.37\nH\n0\n1\n0\n1PE\nlowlow\nhighhighp(IQ=high|PE,H)\n0.098\n0.220.210.49H\n01p(SES=high| H)\n0.088\n0.51\nSEX\nmale\nfemale\nmale\nfemaleSES\nlow\nlow\nhigh\nhighp(PE=high|SES,SEX)\n0.32\n0.166\n0.860.81SES\nlowlowlowlow\nhighhighhigh\nhighPE\nlow\nhigh\nlow\nhigh\nlow\nhigh\nlow\nhighIQ\nlow\nlow\nhighhigh\nlowlow\nhigh\nhighp(CP=yes|SES,IQ,PE)\n0.011\n0.1700.124\n0.53\n0.093\n0.390.24\n0.84\nFigure 26.9 The most probable DAG with a single binary hidden variable learned from the Sewell-Shah\ndata. MAP estimates of the CPT entries are shown for some of the nodes. Source: (Heckerman et al. 1997).\nUsed with kind permission of David Heckerman.\n26.5.1.4 Example: college plans revisited\nLet us revisit the college plans dataset from Section 26.4.2.5. Recall that if we ignore the\npossibility of hidden variables there was a direct link from socio economic status to IQ in theMAP DAG. Heckerman et al. decided to see what would happen if they introduced a hiddenvariableH, which they made a parent of both SES and IQ, representing a hidden common cause.\nThey also considered a variant in which Hpoints to SES, IQ and PE. For both such cases, they\nconsidered dropping none, one, or both of the SES-PE and PE-IQ edges. They varied the numberof states for the hidden node from 2 to 6. Thus they computed the approximate posterior over8\u00d75=4 0different models, using the CS approximation.\nThe most probable model which they found is shown in Figure 26.9. This is 2\u00b710\n10times\nmore likely than the best model containing no hidden variable. It is also 5\u00b7109times more\nlikely than the second most probable model with a hidden variable. So again the posterior isvery peaked.\nThese results suggests that there is indeed a hidden common cause underlying both the\nsocio-economic status of the parents and the IQ of the children. By examining the CPT entries,we see that both SES and IQ are more likely to be high when Htakes on the value 1. They\ninterpret this to mean that the hidden variable represents \u201cparent quality\u201d (possibly a geneticfactor). Note, however, that the arc between H and SES can be reversed without changing the v-structures in the graph, and thus without affecting the likelihood; this underscores the difficultyin interpreting hidden variables.\nInterestingly, the hidden variable model has the same conditional independence assumptions\namongst the visible variables as the most probable visible variable model. So it is not pos-sible to distinguish between these hypotheses by merely looking at the empirical conditionalindependencies in the data (which is the basis of the constraint-based approach to structure\nlearning (Pearl and Verma 1991; Spirtes et al. 2000)). Instead, by adopting a Bayesian approach,which takes parsimony into account (and not just conditional independence), we can discover", "955": "26.5. LearningDAGstructurewithlatentvariables 925\nthe possible existence of hidden factors. This is the basis of much of scienti\ufb01c and everday\nhuman reasoning (see e.g. (Griffiths and Tenenbaum 2009) for a discussion).\n26.5.2 Structural EM\nOne way to perform structural inference in the presence of missing data is to use a standardsearch procedure (deterministic or stochastic), and to use the methods from Section 26.5.1 toestimate the marginal likelihood. However, this approach is very efficient, because the marginallikelihood does not decompose when we have missing data, and nor do its approximations.For example, if we use the CS approximation or the VBEM approximation, we have to performinference in every neighboring model, just to evaluate the quality of a single move!\n(Friedman 1997b; Thiesson et al. 1998) presents a much more efficient approach called the\nstructural EM algorithm. The basic idea is this: instead of \ufb01tting each candidate neighboring\ngraph and then \ufb01lling in its data, \ufb01ll in the data once, and use this \ufb01lled-in data to evaluatethe score of all the neighbors. Although this might be a bad approximation to the marginallikelihood, it can be a good enough approximation of the difference in marginal likelihoodsbetween different models, which is all we need in order to pick the best neighbor.\nMore precisely, de\ufb01ne\nD(G0,\u02c6\u03b80)to be the data \ufb01lled in using model G0with MAP parameters\n\u02c6\u03b80. Now de\ufb01ne a modi\ufb01ed BIC score as follows:\nscoreBIC(G,D)/defineslogp(D|\u02c6\u03b8,G)\u2212logN\n2dim(G)+log p(G)+log p(\u02c6\u03b8|G) (26.42)\nwhere we have included the log prior for the graph and parameters. One can show (Friedman1997b) that if we pick a graph Gwhich increases the BIC score relative to G\n0on the expected\ndata, it will also increase the score on the actual data, i.e.,\nscoreBIC(G,D(G0,\u02c6\u03b80))\u2212scoreBIC(G0,D(G0,\u02c6\u03b80)\u2264scoreBIC(G,D)\u2212scoreBIC(G0,D)(26.43)\nTo convert this into an algorithm, we proceed as follows. First we initialize with some graph\nG0and some set of parameters \u03b80. Then we \ufb01ll-in the data using the current parameters \u2014 in\npractice, this means when we ask for the expected counts for any particular family, we perform\ninference using our current model. (If we know which counts we will need, we can precomputeall of them, which is much faster.) We then evaluate the BIC score of all of our neighbors usingthe \ufb01lled-in data, and we pick the best neighbor. We then re\ufb01t the model parameters, \ufb01ll-in thedata again, and repeat. For increased speed, we may choose to only re\ufb01t the model every fewsteps, since small changes to the structure hopefully won\u2019t invalidate the parameter estimatesand the \ufb01lled-in data too much.\nOne interesting application is to learn a phylogenetic tree structure. Here the observed leaves\nare the DNA or protein sequences of currently alive species, and the goal is to infer the topologyof the tree and the values of the missing internal nodes. There are many classical algorithms forthis task (see e.g., (Durbin et al. 1998)), but one that uses SEM is discussed in (Friedman et al.2002).\nAnother interesting application of this method is to learn sparse mixture models (Barash and\nFriedman 2002). The idea is that we have one hidden variable Cspecifying the cluster, and we\nhave to choose whether to add edges C\u2192X\ntfor each possible feature Xt. Thus some features\nwill be dependent on the cluster id, and some will be independent. (See also (Law et al. 2004)", "956": "926 Chapter26. Graphicalmodelstructurelearning\n\u0001\u0002\u0003\u0004\n\u0005\u0001\u0004\u0006\u0005\u0001\u0007\u0007\u0005\u0002\u0005\u0007\u0006\n\u0001\u000b\n\u0006\f \n\r\f\u0002\u0004\u000e\u0002\u0001\u000b \n\u000f\u0011\f\u0004\u0006\n\u0003\u0002\u0004\u0006\u0001\u0004\u0006\u0003\u000f\n\u000e\u000f\f \u0016\u0001\u000b\u0004\n\u0015\u0001\b\u0006\u0004 \u0015\u000f\u0003\r\u0006\u0001\u0007\u000e\r\r\u0002\u000e\n\r\u000f\n\u0012\u0006\u0013\u0002\u000b\u0004\u0011\f\u0001\u000b\n\u0006\n\u0017\u0006\u0004\u0011\u0004\u0007\u0006\u0001\u0015\u0011\u0006 \b\u0006\u0003\u0002\n\u0002\u000b\u0006\n\u000b\r\u0007 \u0010\u0001\u000e\u0002\u0006\u000b\u000e\u0004 \u0010\u0007\u0001\u0013\u0006\f\u0004\u0010\u000f\t\u0006\f \u0010\u0011\n\u0012\n\u0018\u0011\u0006\u0004\u000e\u0002\u000f\u000b \f\u0006\u0007\u0002\u0015\u0002\u000f\u000b\n\u0004\u0006\u0001\u0004\u000f\u000b\u0004\u000e\u0011\u0003\u0002\u0006\u0004\n\u000e\u0006\u0001\b\t\u0001\u000e\u0006\f\n\t\u0002\u000b \t\u000f\u000b\n\u0019\u001a\u0019\u0019\u001a\u001c\n\u0019\u001a\u001d\u0019\u001a\u001f\n\u0019\u001a\" \u0019\u0019\u001b \u0019\u0019\u001d \u0019\u001b\u0019\u0019\u001b\u001b \u0019\u001b\u001e\u0019\u001b\u001f\n\u0019\u001c\u001d\n\u0019\u001c\u001e\u0019\u001c\u001f\u0019\u001d\u001b \u0019\u001d\u001d\u0019\u001f\u001b\n\u0019\u001f\u001c\u0019\u001f\u001e \u0019\u001f!\u0019 \u001b \u0019 \u001f\u0019!\u001a \u0019!\u0019 \u0019!\u001b \u0019!\u001d\u0019! \u0019\"\u0019 \u0019\"\u001c\nFigure 26.10 Part of a hierarchical latent tree learned from the 20-newsgroup data. From Figure 2 of\n(Harmeling and Williams 2011). Used with kind permission of Stefan Harmeling.\nfor a different way to perform this task, using regular EM and a set of bits, one per feature, that\nare free to change across data cases.)\n26.5.3 Discovering hidden variables\nIn Section 26.5.1.4, we introduced a hidden variable \u201cby hand\u201d, and then \ufb01gured out the localtopology by \ufb01tting a series of different models and computing the one with the best marginallikelihood. How can we automate this process?\nFigure 11.1 provides one useful intuition: if there is a hidden variable in the \u201ctrue model\u201d,\nthen its children are likely to be densely connected. This suggest the following heuristic (Elidanet al. 2000): perform structure learning in the visible domain, and then look for structural\nsignatures, such as sets of densely connected nodes (near-cliques); introduce a hidden variableand connect it to all nodes in this near-clique; and then let structural EM sort out the details.Unfortunately, this technique does not work too well, since structure learning algorithms arebiased against \ufb01tting models with densely connected cliques.\nAnother useful intuition comes from clustering. In a \ufb02at mixture model, also called a latent\nclass model, the discrete latent variable provides a compressed representation of its children.Thus we want to create hidden variables with high mutual information with their children.\nOne way to do this is to create a tree-structured hierarchy of latent variables, each of which\nonly has to explain a small set of children. (Zhang 2004) calls this a hierarchical latent class\nmodel. They propose a greedy local search algorithm to learn such structures, based on addingor deleting hidden nodes, adding or deleting edges, etc. (Note that learning the optimal latent", "957": "26.5. LearningDAGstructurewithlatentvariables 927\nprogramearth\nlaunchlunar\nmarsmission moon\nnasaorbit satellite\nshuttlesolar\nspacetechnology\nh1h17\ncard\ncomputerdatadisk displaydos\ndrivedriveremail\nfiles formatftp\ngraphics\nhelpimage\nmacmemorynumber\npcphone\nproblemresearch sciencescsiserver software\nsystem\nuniversityversion\nvideowindowsh9\nh10h11h12\nh16h18\nh19\nh22\nh23 h24h25\nh26casechildren\ncourse evidence factgovernment\ngunhuman israel lawpower president\nquestionrights statewar\nworld\nh2h3\nh4 h20\nbaseball fans games\nhithockey league nhl playerspuck season team win\nwonh5\nh6 h7aids\ncancer disease doctorfoodhealth\nmedicine msg\npatientsstudies\nvitaminwater h13h21\nbmwcar\ndealer\nengine hondainsurance\noilh15bible\nchristian jesusreligion\nh8h14\ngodjews\nFigure 26.11 A partially latent tree learned from the 20-newsgroup data. Note that some words can\nhave multiple meanings, and get connected to different latent variables, representing different \u201ctopics\u201d. For\nexample, the word \u201cwin\u201d can refer to a sports context (represented by h5) or the Microsoft Windows context(represented by h25). From Figure 12 of (Choi et al. 2011). Used with kind permission of Jin Choi.\ntree is NP-hard (Roch 2006).)\nRecently (Harmeling and Williams 2011) proposed a faster greedy algorithm for learning such\nmodels based on agglomerative hierarchical clustering. Rather than go into details, we just give\nan example of what this system can learn. Figure 26.10 shows part of a latent forest learnedfrom the 20-newsgroup data. The algorithm imposes the constraint that each latent node hasexactly two children, for speed reasons. Nevertheless, we see interpretable clusters arising. Forexample, Figure 26.10 shows separate clusters concerning medicine, sports and religion. Thisprovides an alternative to LDA and other topic models (Section 4.2.2), with the added advantagethat inference in latent trees is exact and takes time linear in the number of nodes.\nAn alternative approach is proposed in (Choi et al. 2011), in which the observed data is not\nconstrained to be at the leaves. This method starts with the Chow-Liu tree on the observeddata, and then adds hidden variables to capture higher-order dependencies between internalnodes. This results in much more compact models, as shown in Figure 26.11. This model alsohas better predictive accuracy than other approaches, such as mixture models, or trees whereall the observed data is forced to be at the leaves. Interestingly, one can show that this methodcan recover the exact latent tree structure, providing the data is generated from a tree. See", "958": "928 Chapter26. Graphicalmodelstructurelearning\nFigure 26.12 Google\u2019s rephil model. Leaves represent presence or absence of words. Internal nodes\nrepresent clusters of co-occuring words, or \u201cconcepts\u201d. All nodes are binary, and all CPDs are noisy-OR.\nThe model contains 12 million word nodes, 1 million latent cluster nodes, and 350 million edges. Used\nwith kind permission of Brian Milch.\n(Choi et al. 2011) for details. Note, however, that this approach, unlike (Zhang 2004; Harmeling\nand Williams 2011), requires that the cardinality of all the variables, hidden and observed, be\nthe same. Furthermore, if the observed variables are Gaussian, the hidden variables must be\nGaussian also.\n26.5.4 Case study: Google\u2019s Rephil\nIn this section, we describe a huge DGM called Rephil, which was automatically learned from\ndata.3The model is widely used inside Google for various purposes, including their famous\nAdSense system.4\nThe model structure is shown in Figure 26.12. The leaves are binary nodes, and represent\nthe presence or absence of words or compounds (such as \u201cNew York City\u201d) in a text document\nor query. The latent variables are also binary, and represent clusters of co-occuring words. All\nCPDs are noisy-OR, since some leaf nodes (representing words) can have many parents. This\nmeans each edge can be augmented with a hidden variable specifying if the link was activated\nor not; if the link is not active, then the parent cannot turn the child on. (A very similar model\nwas proposed independently in (Singliar and Hauskrecht 2006).)\nParameter learning is based on EM, where the hidden activation status of each edge needs\nto be inferred (Meek and Heckerman 1997). Structure learning is based on the old neuroscience\n3. The original system, called \u201cPhil\u201d, was developed by Georges Harik and Noam Shazeer,. It has been published as US\nPatent #8024372, \u201cMethod and apparatus for learning a probabilistic generative model for text\u201d, \ufb01led in 2004. Rephil is\na more probabilistically sound version of the method, developed by Uri Lerner et al. The summary below is based on\nnotes by Brian Milch (who also works at Google).\n4. AdSense is Google\u2019s system for matching web pages with content-appropriate ads in an automatic way, by extracting\nsemantic keywords from web pages. These keywords play a role analogous to the words that users type in when\nsearching; this latter form of information is used by Google\u2019s AdWords system. The details are secret, but (Levy 2011)\ngives an overview.", "959": "26.5. LearningDAGstructurewithlatentvariables 929\nidea that \u201cnodes that \ufb01re together should wire together\u201d. To implement this, we run inference\nand check for cluster-word and cluster-cluster pairs that frequently turn on together. We thenadd an edge from parent to child if the link can signi\ufb01cantly increase the probability of thechild. Links that are not activated very often are pruned out. We initialize with one cluster per\u201cdocument\u201d (corresponding to a set of semantically related phrases). We then merge clusters A\nandBifAexplainsB\u2019s top words and vice versa. We can also discard clusters that are used\ntoo rarely.\nThe model was trained on about 100 billion text snippets or search queries; this takes several\nweeks, even on a parallel distributed computing architecture. The resulting model contains 12million word nodes and about 1 million latent cluster nodes. There are about 350 million linksin the model, including many cluster-cluster dependencies. The longest path in the graph haslength 555, so the model is quite deep.\nExact inference in this model is obviously infeasible. However note that most leaves will be\noff, since most words do not occur in a given query; such leaves can be analytically removed, asshown in Exercise 10.7. We an also prune out unlikely hidden nodes by following the strongestlinks from the words that are on up to their parents to get a candidate set of concepts. Wethen perform iterative conditional modes to \ufb01nd a good set of local maxima. At each step ofICM, each node sets its value to its most probable state given the values of its neighbors in itsMarkov blanket. This continues until it reaches a local maximum. We can repeat this processa few times from random starting con\ufb01gurations. At Google, this can be made to run in 15milliseconds!\n26.5.5 Structural equation models *\nAstructural equation model (Bollen 1989) is a special kind of directed mixed graph (Sec-\ntion 19.4.4.1), possibly cyclic, in which all CPDs are linear Gaussian, and in which all bidirectededges represent correlated Gaussian noise. Such models are also called path diagrams. SEMs\nare widely used, especially in economics and social science. It is common to interpret the edgedirections in terms of causality, where directed cycles are interpreted is in terms of feedback\nloops(see e.g., (Pearl 2000, Ch.5)). However, the model is really just a way of specifying a joint\nGaussian, as we show below. There is nothing inherently \u201ccausal\u201d about it at all. (We discusscausality in Section 26.6.)\nWe can de\ufb01ne an SEM as a series of full conditionals as follows:\nx\ni=\u03bci+/summationdisplay\nj/negationslash=iwijxj+/epsilon1i (26.44)\nwhere/epsilon1\u223cN(0,\u03a8). We can rewrite the model in matrix form as follows:\nx=Wx+\u03bc+/epsilon1\u21d2x=(I\u2212W)\u22121(/epsilon1+\u03bc) (26.45)\nHence the joint distribution is given by p(x)=N(\u03bc,\u03a3)where\n\u03a3=(I\u2212W)\u22121\u03a8(I\u2212W)\u2212T(26.46)\nWe draw an arc Xi\u2190Xjif|wij|>0.I fWis lower triangular then the graph is acyclic. If,\nin addition, \u03a8is diagonal, then the model is equivalent to a Gaussian DGM, as discussed in\nSection 10.2.5; such models are called recursive.I f \u03a8is not diagonal, then we draw a bidirected", "960": "930 Chapter26. Graphicalmodelstructurelearning\nY3Z3Z1 Z1Y1 Y2\nFigure 26.13 A cyclic directed mixed graphical model (non-recursive SEM). Note the Z1\u2192Z2\u2192Z3\u2192\nZ1feedback loop.\narcXi\u2194Xjfor each non-zero off-diagonal term. Such edges represent correlation, possibly\ndue to a hidden common cause.\nWhen using structural equation models, it is common to partition the variables into latent\nvariables, Zt, and observed or manifest variables Yt. For example, Figure 26.13 illustrates the\nfollowing model:\n\u239b\n\u239c\u239c\u239c\u239c\u239c\u239c\u239dX\n1\nX2\nX3\nX4\nX5\nX6\u239e\n\u239f\u239f\u239f\u239f\u239f\u239f\u23a0=\u239b\n\u239c\u239c\u239c\u239c\u239c\u239c\u239dZ\n1\nZ2\nZ3\nY1\nY2\nY3\u239e\n\u239f\u239f\u239f\u239f\u239f\u239f\u23a0=\u239b\n\u239c\u239c\u239c\u239c\u239c\u239c\u239d00 w\n13000\nw210 0 000\n0w320 000\nw410 0 000\n0w520 000\n00 w63000\u239e\n\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\u239b\n\u239c\u239c\u239c\u239c\u239c\u239c\u239dZ\n1\nZ2\nZ3\nY1\nY2\nY3\u239e\n\u239f\u239f\u239f\u239f\u239f\u239f\u23a0+\u239b\n\u239c\u239c\u239c\u239c\u239c\u239c\u239d/epsilon1\n1\n/epsilon12\n/epsilon13\n/epsilon14\n/epsilon15\n/epsilon16\u239e\n\u239f\u239f\u239f\u239f\u239f\u239f\u23a0, (26.47)\nwhere\n\u03a8=\u239b\n\u239c\u239c\u239c\u239c\u239c\u239c\u239d\u03a8\n1100000\n0\u03a8 220000\n00 \u03a8 33000\n000 \u03a8 44\u03a8450\n000 \u03a8 54\u03a8550\n00000 \u03a8 66\u239e\n\u239f\u239f\u239f\u239f\u239f\u239f\u23a0(26.48)\nThe presence of a feedback loop Z\n1\u2192Z2\u2192Z3is evident from the fact that Wis not lower\ntriangular. Also the presence of confounding between Y1andY2is evident in the off-diagonal\nterms in\u03a8.\nOften we assume there are multiple observations for each latent variable. To ensure identi\ufb01a-\nbility, we can set the mean of the latent variables Ztto 0, and we can set the regression weights\nofZt\u2192Ytto 1. This essentially de\ufb01nes the scale of each latent variable. (In addition to the\nZ\u2019s, there are the extra hidden variables implied by the presence of the bidirected edges.)\nThe standard practice in the SEM community, as exempli\ufb01ed by the popular commercial\nsoftware package called LISREL(available from http://www .ssicentral .com/lisrel/ ), is to", "961": "26.6. LearningcausalDAGs 931\nbuild the structure by hand, to estimate the parameters by maximum likelihood, and then to\ntest if any of the regression weights are signi\ufb01cantly different from 0, using standard frequentistmethods. However, one can also use Bayesian inference for the parameters (see e.g., (Dunsonet al. 2005)). Structure learning in SEMs is rare, but since recursive SEMs are equivalent toGaussian DAGs, many of the techniques we have been discussing in this section can be applied.\nSEMs are closely related to factor analysis (FA) models (Chapter 12). The basic difference is\nthat in an FA model, the latent Gaussian has a low-rank covariance matrix, and the observednoise has a diagonal covariance (hence no bidirected edges). In an SEM, the covariance of thelatent Gaussian has a sparse Cholesky decomposition (at least if Wis acyclic), and the observed\nnoise might have a full covariance matrix.\nNote that SEMs can be extended in many ways. For example, we can add covariates/ input\nvariables (possibly noisily observed), we can make some of the observations be discrete (e.g., byusing probit links), and so on.\n26.6 Learning causal DAGs\nCausal models are models which can predict the effects of interventions to, or manipulations\nof, a system. For example, an electronic circuit diagram implicitly provides a compact encodingof what will happen if one removes any given component, or cuts any wire. A causal medicalmodel might predict that if I continue to smoke, I am likely to get lung cancer (and hence ifI cease smoking, I am less likely to get lung cancer). Causal claims are inherently stronger,yet more useful, than purely associative claims, such as \u201cpeople who smoke often have lung\ncancer\u201d.\nCausal models are often represented by DAGs (Pearl 2000), although this is somewhat contro-\nversial (Dawid 2010). We explain this causal interpretation of DAGs below. We then show howto use a DAG to do causal reasoning. Finally, we brie\ufb02y discuss how to learn the structure ofcausal DAGs. A more detailed description of this topic can be found in (Pearl 2000) and (Kollerand Friedman 2009, Ch.21).\n26.6.1 Causal interpretation of DAGs\nIn this section, we de\ufb01ne a directed edge A\u2192Bin a DAG to mean that \u201cA directly causes B\u201d,\nso if we manipulate A, thenBwill change. This is known as the causal Markov assumption.\n(Of course, we have not de\ufb01ned the word \u201ccauses\u201d, and we cannot do that by appealing to aDAG, lest we end up with a cyclic de\ufb01nition; see (Dawid 2010) for further disussion of this point.)\nWe will also assume that all relevant variables are included in the model, i.e., there are no\nunknown confounders, re\ufb02ecting hidden common causes. This is called the causal sufficiency\nassumption. (If there are known to be confounders, they should be added to the model, althoughone can sometimes use mixed directed graphs (Section 26.5.5) as a way to avoid having to modelconfounders explicitly.)\nAssuming we are willing to make the causal Markov and causal sufficiency assumptions, we\ncan use DAGs to answer causal questions. The key abstraction is that of a perfect intervention;\nthis represents the act of setting a variable to some known value, say setting X\nitoxi. A real\nworld example of such a perfect intervention is a gene knockout experiment, in which a geneis \u201csilenced\u201d. We need some notational convention to distinguish this from observing that X\ni", "962": "932 Chapter26. Graphicalmodelstructurelearning\nX XGGdo(X=x)\nFigure 26.14 Surgical intervention on X. Based on (Pe\u2019er 2005).\nhappens to have value xi. We use Pearl\u2019s do calculus notation (as in the verb \u201cto do\u201d) and write\ndo(Xi=xi)to denote the event that we set Xitoxi. A causal model can be used to make\ninferences of the form p(x|do(Xi=xi)), which is different from making inferences of the form\np(x|Xi=xi).\nTo understand the difference between conditioning on interventions and conditioning on\nobservations (i.e., the difference between doing and seeing), consider a 2 node DGM S\u2192Y,i n\nwhichS=1if you smoke and S=0otherwise, and Y=1if you have yellow-stained \ufb01ngers,\nandY=0otherwise. If I observe you have yellow \ufb01ngers, I am licensed to infer that you are\nprobably a smoker (since nicotine causes yellow stains):\np(S=1|Y=1 )>p (S=1 ) (26.49)\nHowever, if I intervene and paintyour \ufb01ngers yellow, I am no longer licensed to infer this, since\nI have disrupted the normal causal mechanism. Thus\np(S=1|do(Y=1 ) )=p( S=1 ) (26.50)\nOne way to model perfect interventions is to use graph surgery: represent the joint distri-\nbution by a DGM, and then cut the arcs coming into any nodes that were set by intervention.\nSee Figure 26.14 for an example. This prevents any information \ufb02ow from the nodes that wereintervened on from being sent back up to their parents. Having perform this surgery, we canthen perform probabilistic inference in the resulting \u201cmutilated\u201d graph in the usual way to reasonabout the effects of interventions. We state this formally as follows.\nTheorem26.6.1 (Manipulationtheorem(Pearl2000;Spirtesetal.2000)). . Tocompute p(X\ni|do(Xj))\nforsetsofnodes i,j,wecanperformsurgicalinterventiononthe Xjnodesandthenusestandard\nprobabilisticinferenceinthemutilatedgraph.\nWe can generalize the notion of a perfect intervention by adding interventions as explicit\naction nodes to the graph. The result is like an in\ufb02uence diagram, except there are no utility\nnodes (Lauritzen 2000; Dawid 2002). This has been called the augmented DAG (Pearl 2000). We", "963": "26.6. LearningcausalDAGs 933\nxy\nFigure 26.15 Illustration of Simpson\u2019s paradox. Figure generated by simpsonsParadoxGraph .\ncan then de\ufb01ne the CPD p(Xi|do(Xi))to be anything we want. We can also allow an action to\naffect multiple nodes. This is called a fat hand intervention, a reference to someone trying to\nchange a single component of some system (e.g., an electronic circuit), but accidently touching\nmultiple components and thereby causing various side effects (see (Eaton and Murphy 2007) fora way to model this using augmented DAGs).\n26.6.2 Using causal DAGs to resolve Simpson\u2019s paradox\nIn this section, we assume we know the causal DAG. We can then do causal reasoning byapplying d-separation to the mutilated graph. In this section, we give an example of this, andshow how causal reasoning can help resolve a famous paradox, known as Simpon\u2019s paradox.\nSimpson\u2019s paradox says that any statistical relationship between two variables can be reversed\nby including additional factors in the analysis. For example, suppose some cause C(say, taking\na drug) makes some effect E(say getting better) more likely\nP(E|C)>P(E|\u00acC)\nand yet, when we condition on the gender of the patient, we \ufb01nd that taking the drug makesthe effect less likely in both females (F ) and males (\u00ac F):\nP(E|C,F)<P(E|\u00acC,F)\nP(E|C,\u00acF)<P(E|\u00acC,\u00acF)\nThis seems impossible, but by the rules of probability, this is perfectly possible, because theevent space where we condition on (\u00acC,F)or(\u00acC,\u00acF)can be completely different to the\nevent space when we just condition on \u00acC. The table of numbers below shows a concrete\nexample (from (Pearl 2000, p175)):\nCombined Male Female\nE\u00acE Total Rate E\u00acETotal Rate E\u00acETotal Rate\nC20 20 40 50% 18 12 30 60% 2 8 10 20%\n\u00acC16 24 40 40% 7 3 10 70% 9 21 30 30%\nTotal36 44 80 25 15 40 11 29 40", "964": "934 Chapter26. Graphicalmodelstructurelearning\nFrom this table of numbers, we see that\np(E|C)=2 0/40 = 0.5 >p(E|\u00acC)=1 6/40 = 0.4 (26.51)\np(E|C,F)=2/10 = 0.2 <p(E|\u00acC,F)=9/30 = 0.3 (26.52)\np(E|C,\u00acF)=1 8/30 = 0.6 <p(E|\u00ac,\u00acF)=7/10 = 0.7 (26.53)\nA visual representation of the paradox is given in in Figure 26.15. The line which goes up and\nto the right shows that the effect (y -axis) increases as the cause (x-axis) increases. However, the\ndots represent the data for females, and the crosses represent the data for males. Within eachsubgroup, we see that the effect decreases as we increase the cause.\nIt is clear that the effect is real, but it is still very counter-intuitive. The reason the paradox\narises is that we are interpreting the statements causally, but we are not using proper causalreasoning when performing our calculations. The statement that the drug Ccauses recovery E\nis\nP(E|do(C))>P(E|do(\u00acC)) (26.54)\nwhereas the data merely tell us\nP(E|C)>P(E|\u00acC) (26.55)\nThisis\nnot a contradiction. Observing Cis positive evidence for E, since more males than\nfemales take the drug, and the male recovery rate is higher (regardless of the drug). ThusEquation 26.55 does not imply Equation 26.54.\nNevertheless, we are left with a practical question: should we use the drug or not? It seems\nlike if we don\u2019t know the patient\u2019s gender, we should use the drug, but as soon as we discoverif they are male or female, we should stop using it. Obviously this conclusion is ridiculous.\nTo answer the question, we need to make our assumptions more explicit. Suppose reality can\nbe modeled by the causal DAG in Figure 26.16(a). To compute the causal effect of ConE,w e\nneed toadjust for (i.e., condition on) the confounding variable F. This is necessary because\nthere is a backdoor path fromCtoEviaF, so we need to check the C\u2192Erelationship for\neach value of Fseparately, to make sure the relationship between CandEis not affected by\nany value of F.\nSuppose that for each value of F, taking the drug is harmful, that is,\np(E|do(C),F)<p(E|do(\u00acC),F) (26.56)\np(E|do(C),\u00acF)<p(E|do(\u00acC),\u00acF) (26.57)\nThenw\ne can show that taking the drug is harmful overall:\np(E|do(C))<p(E|do(\u00acC)) (26.58)\nThe proof is as follows (Pearl 2000, p181). First, from our assumptions in Figure 26.16(a), we seethat drugs have no effect on gender\np(F|do(C)) =p(F|do(\u00acC)) =p(F) (26.59)\nNow using the law of total probability,\np(E|do(C)) =p(E|do(C),F)p(F|do(C))+p(E|do(C),\u00acF)p(\u00acF|do(C))(26.60)\n=p(E|do(C),F)p(F)+p(E|do(C),\u00acF)p(\u00acF) (26.61)", "965": "26.6. LearningcausalDAGs 935\nC F\nETreatment Gender\nRecovery\n(a)C F\nETreatment Blood Pressure\nRecovery\n(b)\nFigure26.16 Twodifferent modelsuses toillustrate Simpson\u2019sparadox. (a) Fisgender andis aconfounder\nfor C and E. (b) F is blood pressure and is caused by C.\nSimilarly,\np(E|do(\u00acC)) =p(E|do(\u00acC),F)p(F)+p(E|do(\u00acC),\u00acF)p(\u00acF) (26.62)\nSince every term in Equation 26.61 is less than the corresponding term in Equation 26.62, we\nconclude that\np(E|do(C))<p(E|do(\u00acC)) (26.63)\nSo if the model in Figure 26.16(a) is correct, we should not administer the drug, since it reducesthe probability of the effect.\nNow consider a different version of this example. Suppose we keep the data the same but\ninterpretFas something that is affected by C, such as blood pressure. See Figure 26.16(b). In\nthis case, we can no longer assume\np(F|do(C)) =p(F|do(\u00acC)) =p(F) (26.64)\nand the above proof breaks down. So p(E|do(C))\u2212p(E|do(\u00acC))may\n be positive or negaitve.\nIn the true model is Figure 26.16(b), then we should not condition on Fwhen assessing the\neffect of ConE, since there is no backdoor path in this case, because of the v-structure at\nF. That is, conditioning on Fmight block one of the causal pathways. In other words, by\ncomparing patients with the same post-treatment blood pressure (value of F), we may mask the\neffect of one of the two pathways by which the drug operates to bring about recovery.\nThus we see that different causal assumptions lead to different causal conclusions, and hence\ndifferent courses of action. This raises the question on whether we can learn the causal modelfrom data. We discuss this issue below.\n26.6.3 Learning causal DAG structures\nIn this section, we discuss some ways to learn causal DAG structures.", "966": "936 Chapter26. Graphicalmodelstructurelearning\n26.6.3.1 Learning from observational data\nIn Section 26.4, we discussed various methods for learning DAG structures from observational\ndata. It is natural to ask whether these methods can recover the \u201ctrue\u201d DAG structure that wasused to generate the data. Clearly, even if we have in\ufb01nite data, an optimal method can onlyidentify the DAG up to Markov equivalence (Section 26.4.1). That is, it can identify the PDAG(partially directed acylic graph), but not the complete DAG structure, because all DAGs which areMarkov equivalent have the same likelihood.\nThere are several algorithms (e.g., the greedy equivalence search method of (Chickering\n2002)) that are consistent estimators of PDAG structure, in the sense that they identify thetrue Markov equivalence class as the sample size goes to in\ufb01nity, assuming we observe all thevariables. However, we also have to assume that the generating distribution pisfaithfulto\nthe generating DAG G. This means that all the conditional indepence (CI) properties of pare\nexactly captured by the graphical structure, so I(p)=I(G); this means there cannot be any CI\nproperties in pthat are due to particular settings of the parameters (such as zeros in a regression\nmatrix) that are not graphically explicit. For this reason, a faithful distribution is also called astabledistribution.\nSuppose the assumptions hold and we learn a PDAG. What can we do with it? Instead of\nrecovering the full graph, we can focus on the causal analog of edge marginals, by computingthe magnitude of the causal effect of one node on another (say A on B). If we know the DAG, wecan do this using techniques described in (Pearl 2000). If the DAG is unknown, we can computea lower bound on the effect as follows (Maathuis et al. 2009): learn an equivalence class (PDAG)from data; enumerate all the DAGs in the equivalence class; apply Pearl\u2019s do-calculus to computethe magnitude of the causal effect of A on B in each DAG; \ufb01nally, take the minimum of theseeffects as the lower bound. It is usually computationally infeasible to compute all DAGs in theequivalence class, but fortunately one only needs to be able to identify the local neighborhoodofAandB, which can be esimated more efficiently, as described in (Maathuis et al. 2009). This\ntechnique is called IDA, which is short for \u201cintervention-calculus when the DAG is absent\u201d.\nIn (Maathuis et al. 2010), this technique was applied to some yeast gene expression data. Gene\nknockout data was used to estimate the \u201cground truth\u201d effect of each 234 single-gene deletionson the remaining 5,361 genes. Then the algorithm was applied to 63 unperturbed (wild-type)samples, and was used to rank order the likely targets of each of the 234 genes. The methodhad a precision of 66% when the recall was set to 10%; while low, this is substantially more thanrival variable-selection methods, such as lasso and elastic net, which were only slightly abovechance.\n26.6.3.2 Learning from interventional data\nIf we want to distinguish between DAGs within the equivalence class, we need to use interven-\ntional data, where certain variables have been set, and the consequences have been measured.An example of this is the dataset in Figure 26.17(a), where proteins in a signalling pathwaywere perturbed, and their phosphorylation status was measured using a technique called \ufb02owcytometry (Sachs et al. 2005).\nIt is straightforward to modify the standard Bayesian scoring criteria, such as the marginal\nlikelihood or BIC score, to handle learning from mixed observational and experimental data: we", "967": "26.6. LearningcausalDAGs 937\n(a)\nB2cAMPf\nerkakt\njnkPsitect AKT inh U0126\nPMA\np38G06967mek12rafpkc\npip3plcy\npip2\npka\nPresent\nMissing\nInt. edge\n(b)\nFigure 26.17 (a) A design matrix consisting of 5400 data points (rows) measuring the status (using \ufb02ow\ncytometry) of 11 proteins (columns) under different experimental conditions. The data has been discretized\ninto 3 states: low (black), medium (grey) and high (white). Some proteins were explicitly controlled using\nactivating or inhibiting chemicals. (b) A directed graphical model representing dependencies between\nvarious proteins (blue circles) and various experimental interventions (pink ovals), which was inferred from\nthis data. We plot all edges for which p(Gst=1|D)>0.5. Dotted edges are believed to exist in nature\nbut were not discovered by the algorithm (1 false negative). Solid edges are true positives. The light colored\nedges represent the effects of intervention. Source: Figure 6d of (Eaton and Murphy 2007) . This \ufb01gure can\nbe reproduced using the code at http://www .cs.ubc.ca/~murphyk/Software/BDAGL/index .html.", "968": "938 Chapter26. Graphicalmodelstructurelearning\njust compute the sufficient statistics for a CPD\u2019s parameter by skipping over the cases where that\nnode was set by intervention (Cooper and Yoo 1999). For example, when using tabular CPDs, wemodify the counts as follows:\nN\ntck/defines/summationdisplay\ni:xitnot setI(xi,t=k,xi,pa(t)=c) (26.65)\nThe justi\ufb01cation for this is that in cases where node tis set by force, it is not sampled from its\nusual mechanism, so such cases should be ignored when inferring the parameter \u03b8t. The mod-\ni\ufb01ed scoring criterion can be combined with any of the standard structure learning algorithms.(He and Geng 2009) discusses some methods for choosing which interventions to perform, soas to reduce the posterior uncertainty as quickly as possible (a form of active learning).\nThe preceeding method assumes the interventions are perfect. In reality, experimenters can\nrarely control the state of individual molecules. Instead, they inject various stimulant or inhibitorchemicals which are designed to target speci\ufb01c molecules, but which may have side effects. Wecan model this quite simply by adding the intervention nodes to the DAG, and then learninga larger augmented DAG structure, with the constraint that there are no edges between theintervention nodes, and no edges from the \u201cregular\u201d nodes back to the intervention nodes.\nFigure 26.17(b) shows the augmented DAG that was learned from the interventional \ufb02ow\ncytometry data depicted in Figure 26.17(a). In particular, we plot the median graph, whichincludes all edges for which p(G\nij=1|D)>0.5. These were computed using the exact\nalgorithm of (Koivisto 2006). It turns out that, in this example, the median model has exactlythe same structure as the optimal MAP model, argmax\nGp(G|D), which was computed using\nthe algorithm of (Koivisto and Sood 2004; Silander and Myllmaki 2006).\n26.7 Learning undirected Gaussian graphical models\nLearning the structured of undirected graphical models is easier than learning DAG structurebecause we don\u2019t need to worry about acyclicity. On the other hand, it is harder than learningDAG structure since the likelihood does not decompose (see Section 19.5). This precludes thekind of local search methods (both greedy search and MCMC sampling) we used to learn DAGstructures, because the cost of evaluating each neighboring graph is too high, since we have tore\ufb01t each model from scratch (there is no way to incrementally update the score of a model).\nIn this section, we discuss several solutions to this problem, in the context of Gaussian\nrandom \ufb01elds or undirected Gaussian graphical models (GGM)s. We consider structure learning\nfor discrete undirected models in Section 26.8.\n26.7.1 MLE for a GGM\nBefore discussing structure learning, we need to discuss parameter estimation. The task ofcomputing the MLE for a (non-decomposable) GGM is called covariance selection (Dempster\n1972).\nFrom Equation 4.19, the log likelihood can be written as\n/lscript(\u03a9)=l o gd e t \u03a9\u2212tr(S\u03a9) (26.66)", "969": "26.7. LearningundirectedGaussiangraphicalmodels 939\nwhere\u03a9=\u03a3\u22121is the precision matrix, and S=1\nN/summationtextN\ni=1(xi\u2212x)(xi\u2212x)Tis the empirical\ncovariance matrix. (For notational simplicity, we assume we have already estimated \u02c6\u03bc=x.)\nOne can show that the gradient of this is given by\n\u2207/lscript(\u03a9)=\u03a9\u22121\u2212S (26.67)\nHowever, we have to enforce the constraints that \u03a9st=0ifGst=0(structural zeros), and\nthat\u03a9is positive de\ufb01nite. The former constraint is easy to enforce, but the latter is somewhat\nchallenging (albeit still a convex constraint). One approach is to add a penalty term to the\nobjective if \u03a9leaves the positive de\ufb01nite cone; this is the approach used in ggmFitMinfunc\n(see also (Dahl et al. 2008)). Another approach is to use a coordinate descent method, describedin (Hastie et al. 2009, p633), and implemented in ggmFitHtf . Yet another approach is to use\niterative proportional \ufb01tting, described in Section 19.5.7. However, IPF requires identifying thecliques of the graph, which is NP-hard in general.\nInterestingly, one can show that the MLE must satisfy the following property: \u03a3\nst=Sstif\nGst=1ors=t, i.e., the covariance of a pair that are connected by an edge must match the\nempirical covariance. In addition, we have \u03a9st=0ifGst=0, by de\ufb01nition of a GGM, i.e.,\nthe precision of a pair that are not connected must be 0. We say that \u03a3is a positive de\ufb01nite\nmatrix completion ofS, since it retains as many of the entries in Sas possible, corresponding\nto the edges in the graph, subject to the required sparsity pattern on \u03a3\u22121, corresponding to the\nabsent edges; the remaining entries in \u03a3are \ufb01lled in so as to maximize the likelihood.\nLet us consider a worked example from (Hastie et al. 2009, p652). We will use the following\nadjacency matrix, representing the cyclic structure, X1\u2212X2\u2212X3\u2212X4\u2212X1, and the following\nempirical covariance matrix:\nG=\u239b\n\u239c\u239c\u239d0101\n101001011010\u239e\n\u239f\u239f\u23a0,S=\u239b\n\u239c\u239c\u239d1 0 154\n11 02 6\n5 2 10 3463 1 0\u239e\n\u239f\u239f\u23a0(26.68)\nThe MLE is given by\n\u03a3=\u239b\n\u239c\u239c\u239d10.00 1.001.314.00\n1.00 10.00 2.000.87\n1.312.00 10.00 3.00\n4.000.873.00 10.00\u239e\n\u239f\u239f\u23a0,\u03a9=\u239b\n\u239c\u239c\u239d0.12\u22120.010\u22120.05\n\u22120.010\n.11\u22120.020\n0\u22120.02 0.11\u22120.03\n\u22120.050\u22120.03 0.13\u239e\n\u239f\u239f\u23a0(26.69)\n(SeeggmFitDemo for the code to reproduce these numbers.) The constrained elements in \u03a9,\nand the free elements in \u03a3, both of which correspond to absent edges, have been highlighted.\n26.7.2 Graphical lasso\nWe now discuss one way to learn a sparse GRF structure, which exploits the fact that there is a\n1:1 correspondence between zeros in the precision matrix and absent edges in the graph. Thissuggests that we can learn a sparse graph structure by using an objective that encourages zerosin the precision matrix. By analogy to lasso (see Section 13.3), one can de\ufb01ne the following /lscript\n1\npenalized NLL:\nJ(\u03a9)=\u2212logdet\u03a9+tr(S\u03a9)+\u03bb||\u03a9||1 (26.70)", "970": "940 Chapter26. Graphicalmodelstructurelearning\nlambda=36.00, nedges=8\n(a)\nlambda=27.00, nedges=11\n(b)\nlambda=7.00, nedges=18\n(c)\nlambda=0.00, nedges=55\n(d)\nFigure 26.18 Sparse GGMs learned using graphical lasso applied to the \ufb02ow cytometry data. (a) \u03bb=3 6.\n(b)\u03bb=2 7. (c)\u03bb=7. (d)\u03bb=0. Figure generated by ggmLassoDemo .\nwhere||\u03a9||1=/summationtext\nj,k|\u03c9jk|is the 1-norm of the matrix. This is called the graphical lasso or\nGlasso.\nAlthough the objective is convex, it is non-smooth (because of the non-differentiable /lscript1\npenalty) and is constrained (because \u03a9must be a positive de\ufb01nite matrix). Several algorithms\nhave been proposed for optimizing this objective (Yuan and Lin 2007; Banerjee et al. 2008; Duchi\net al. 2008), although arguably the simplest is the one in (Friedman et al. 2008), which uses a\ncoordinate descent algorithm similar to the shooting algorithm for lasso. See ggmLassoHtf for\nan implementation. (See also (Mazumder and Hastie 2012) for a more recent version of this\nalgorithm.)\nAs an example, let us apply the method to the \ufb02ow cytometry dataset from (Sachs et al. 2005).\nA discretized version of the data is shown in Figure 26.17(a). Here we use the original continuous\ndata. However, we are ignoring the fact that the data was sampled under intervention. In\nFigure 26.18, we illustrate the graph structures that are learned as we sweep \u03bbfrom 0 to a large\nvalue. These represent a range of plausible hypotheses about the connectivity of these proteins.\nIt is worth comparing this with the DAG that was learned in Figure 26.17(b). The DAG has the\nadvantage that it can easily model the interventional nature of the data, but the disadvantage\nthat it cannot model the feedback loops that are known to exist in this biological pathway (see\nthe discussion in (Schmidt and Murphy 2009)). Note that the fact that we show many UGMs and\nonly one DAG is incidental: we could easily use BIC to pick the \u201cbest\u201d UGM, and conversely, we", "971": "26.7. LearningundirectedGaussiangraphicalmodels 941\ncould easily display several DAG structures, sampled from the posterior.\n26.7.3 Bayesian inference for GGM structure *\nAlthough the graphical lasso is reasonably fast, it only gives a point estimate of the structure.\nFurthermore, it is not model-selection consistent (Meinshausen 2005), meaning it cannot recoverthe true graph even as N\u2192\u221e. It would be preferable to integrate out the parameters, and\nperform posterior inference in the space of graphs, i.e., to compute p(G|D). We can then extract\nsummaries of the posterior, such as posterior edge marginals, p(G\nij=1|D), just as we did for\nDAGs. In this section, we discuss how to do this.\nNote that the situation is analogous to Chapter 13, where we discussed variable selection. In\nSection 13.2, we discussed Bayesian variable selection, where we integrated out the regressionweights and computed p(\u03b3|D)and the marginal inclusion probabilities p(\u03b3\nj=1|D). Then\nin Section 13.3, we discussed methods based on /lscript1regularization. Here we have the same\ndichotomy, but we are presenting them in the opposite order.\nIf the graph is decomposable, and if we use conjugate priors, we can compute the marginal\nlikelihood in closed form (Dawid and Lauritzen 1993). Furthermore, we can efficiently identifythe decomposable neighbors of a graph (Thomas and Green 2009), i.e., the set of legal edgeadditions and removals. This means that we can perform relatively efficient stochastic localsearch to approximate the posterior (see e.g. (Giudici and Green 1999; Armstrong et al. 2008;Scott and Carvalho 2008)).\nHowever, the restriction to decomposable graphs is rather limiting if one\u2019s goal is knowledge\ndiscovery, since the number of decomposable graphs is much less than the number of generalundirected graphs.\n5\nA few authors have looked at Bayesian inference for GGM structure in the non-decomposable\ncase (e.g., (Dellaportas et al. 2003; Wong et al. 2003; Jones et al. 2005)), but such methods cannotscale to large models because they use an expensive Monte Carlo approximation to the marginallikelihood (Atay-Kayis and Massam 2005). (Lenkoski and Dobra 2008) suggested using a Laplaceapproxmation. This requires computing the MAP estimate of the parameters for \u03a9under a G-\nWishart prior (Roverato 2002). In (Lenkoski and Dobra 2008), they used the iterative proportional\nscaling algorithm (Speed and Kiiveri 1986; Hara and Takimura 2008) to \ufb01nd the mode. However,this is very slow, since it requires knowing the maximal cliques of the graph, which is NP-hardin general.\nIn (Moghaddam et al. 2009), a much faster method is proposed. In particular, they modify\nthe gradient-based methods from Section 26.7.1 to \ufb01nd the MAP estimate; these algorithms donot need to know the cliques of the graph. A further speedup is obtained by just using adiagonal Laplace approximation, which is more accurate than BIC, but has essentially the samecost. This, plus the lack of restriction to decomposable graphs, enables fairly fast stochasticsearch methods to be used to approximate p(G|D)and its mode. This approach signi\ufb01cantly\noutperfomed graphical lasso, both in terms of predictive accuracy and structural recovery, for acomparable computational cost.\n5. The number of decomposable graphs on Vnodes, for V=2,...,8, is as follows ((Armstrong 2005, p158)): 2; 8; 61;\n822; 18,154; 61,7675; 30,888,596. If we divide these numbers by the number of undirected graphs, which is 2V(V\u22121)/2,\nwe \ufb01nd the ratios are: 1, 1, 0.95, 0.8, 0.55, 0.29, 0.12. So we see that decomposable graphs form a vanishing fraction of\nthe total hypothesis space.", "972": "942 Chapter26. Graphicalmodelstructurelearning\n26.7.4 Handling non-Gaussian data using copulas *\nThe graphical lasso and variants is inhertently limited to data that is jointly Gaussian, which is\na rather severe restriction. Fortunately the method can be generalized to handle non-Gaussian,but still continuous, data in a fairly simple fashion. The basic idea is to estimate a set of D\nunivariate monotonic transformations f\nj, one per variable j, such that the resulting transformed\ndata is jointly Gaussian. If this is possible, we say the data belongs to the nonparametricNormal distribution, or nonparanormal distribution (Liu et al. 2009). This is equivalent to the\nfamily of Gaussian copulas (Klaassen and Wellner 1997). Details on how to estimate the f\nj\ntransformations from the empirical cdf\u2019s of each variable can be found in (Liu et al. 2009). Aftertransforming the data, we can compute the correlation matrix and then apply glasso in the usualway. One can show, under various assumptions, that this is a consistent estimator of the graphstructure, representing the CI assumptions of the original distribution(Liu et al. 2009).\n26.8 Learning undirected discrete graphical models\nThe problem of learning the structure for UGMs with discrete variables is harder than theGaussian case, because computing the partition function Z(\u03b8), which is needed for parameter\nestimation, has complexity comparable to computing the permanent of a matrix, which ingeneral is intractable (Jerrum et al. 2004). By contrast, in the Gaussian case, computing Zonly\nrequires computing a matrix determinant, which is at most O(V\n3).\nSince stochastic local search is not tractable for general discrete UGMs, below we mention\nsome possible alternative approaches that have been tried.\n26.8.1 Graphical lasso for MRFs/CRFs\nIt is possible to extend the graphical lasso idea to the discrete MRF and CRF case. However, nowthere is a set of parameters associated with each edge in the graph, so we have to use the graphanalog of group lasso (see Section 13.5.1). For example, consider a pairwise CRF with ternarynodes, and node and edge potentials given by\n\u03c8\nt(yt,x)=\u239b\n\u239dvT\nt1x\nvT\nt2x\nvT\nt3x\u239e\u23a0,\u03c8\nst(ys,yt,x)=\u239b\u239dw\nT\nt11xwT\nst12xwT\nst13x\nwT\nst21xwT\nst22xwT\nst23x\nwT\nst31xwT\nst32xwT\nst33x\u239e\u23a0 (26.71)\nwhere we assume xbegins with a constant 1 term, to account for the offset. (If xonly contains\n1, the CRF reduces to an MRF.) Note that we may choose to set some of the v\ntkandwstjk\nweights to 0, to ensure identi\ufb01ability, although this can also be taken care of by the prior, as\nshown in Exercise 8.5.\nTo learn sparse structure, we can minimize the following objective:\nJ=\u2212N/summationdisplay\ni=1/bracketleftBigg/summationdisplay\ntlog\u03c8t(yit,xi,vt)+V/summationdisplay\ns=1V/summationdisplay\nt=s+1log\u03c8st(yis,yit,xi,wst)/bracketrightBigg\n+\u03bb1V/summationdisplay\ns=1V/summationdisplay\nt=s+1||wst||p+\u03bb2V/summationdisplay\nt=1||vt||2\n2 (26.72)", "973": "26.8. Learningundirecteddiscretegraphicalmodels 943\nbaseball\ngames\nleague\nplayersbible\nchristian\ngod\njesus\nquestioncar\ndealer drive enginecard\ndrivergraphics\npcproblem\nsystemvideo\nwindowscase\ncourse\nevidence\nfact\ngovernment\nhuman\nlaw number power\nrights\nstate\nworldchildren\npresident\nreligion warcomputer\ndataemail\nprogram\nsciencesoftware\nuniversitymemory\nresearch\nspacedisk\nfilesdisplay\nimage dos\nmac scsiearth\norbitformat\nftphelp\nphonejews\nfans\nhockey\nteam\nversionnhl\nseason\nwingunhealth\ninsurance\nisrael\nlaunch moonnasa\nshuttle\ntechnology\nwon\nFigure 26.19 An MRF estimated from the 20-newsgroup data using group /lscript1regularization with \u03bb= 256.\nIsolated nodes are not plotted. From Figure 5.9 of (Schmidt 2010). Used with kind permission of Mark\nSchmidt.\nwhere||wst||pis thep-norm; common choices are p=2orp=\u221e, as explained in Sec-\ntion 13.5.1. This method of CRF structure learning was \ufb01rst suggested in (Schmidt et al. 2008).\n(The use of /lscript1regularization for learning the structure of binary MRFs was proposed in (Lee\net al. 2006).)\nAlthough this objective is convex, it can be costly to evaluate, since we need to perform\ninference to compute its gradient, as explained in Section 19.6.3 (this is true also for MRFs). Weshould therefore use an optimizer that does not make too many calls to the objective functionor its gradient, such as the projected quasi-Newton method in (Schmidt et al. 2009). In addition,we can use approximate inference, such as convex belief propagation (Section 22.4.2), to computean approximate objective and gradient more quickly. Another approach is to apply the grouplasso penalty to the pseudo-likelihood discussed in Section 19.5.4. This is much faster, sinceinference is no longer required (Hoe\ufb02ing and Tibshirani 2009). Figure 26.19 shows the result ofapplying this procedure to the 20-newsgroup data, where y\nitindicates the presence of word t\nin document i, andxi=1(so the model is an MRF).", "974": "944 Chapter26. Graphicalmodelstructurelearning\n3\u0003\u000b& )\f 3\u000b& 7\f\n\u0013\u0011\u0018 \u0013\u0011\u0018\n& 3\u000b6 )\f 3\u000b6 7\f\n) \u0013\u0011\u0018 \u0013\u0011\u0018\n7 \u0013\u0011\u001c \u0013\u0011\u0014& 3\u000b5 )\f 3\u000b5 7\f\n) \u0013\u0011\u001b \u0013\u0011\u00157 \u0013\u0011\u0015 \u0013\u0011\u001b\n6\u0003\u00035 3\u000b: )\f 3\u000b: 7\f\n)\u0003\u0003) \u0014\u0011\u0013 \u0013\u0011\u00137\u0003\u0003) \u0013\u0011\u0014 \u0013\u0011\u001c)\u0003\u00037 \u0013\u0011\u0014 \u0013\u0011\u001c\n7\u0003\u00037 \u0013\u0011\u0013\u0014 \u0013\u0011\u001c\u001c&ORXG\\\n6SULQNOHU 5DLQ\n:HW\u0003\n*UDVV\nFigure 26.20 Water sprinkler DGM with corresponding binary CPTs. T and F stand for true and false.\n26.8.2 Thin junction trees\nSo far, we have been concerned with learning \u201csparse\u201d graphs, but these do not necessarily have\nlow treewidth. For example, a D\u00d7Dgrid is sparse, but has treewidth O(D). This means that\nthe models we learn may be intractable to use for inference purposes, which defeats one of thetwo main reasons to learn graph structure in the \ufb01rst place (the other reason being \u201cknowledgediscovery\u201d). There have been various attempts to learn graphical models with bounded treewidth(e.g., (Bach and Jordan 2001; Srebro 2001; Elidan and Gould 2008; Shahaf et al. 2009)), also knownasthin junction trees, but the exact problem in general is hard.\nAn alternative approach is to learn a model with low circuit complexity (Gogate et al.\n2010; Poon and Domingos 2011). Such models may have high treewidth, but they exploit context-speci\ufb01c independence and determinism to enable fast exact inference (see e.g., (Darwiche 2009)).\nExercises\nExercise 26.1 Causal reasoning in the sprinkler network\nConsider the causal network in Figure 26.20. Let Trepresent true and Frepresent false.\na. Suppose I perform a perfect intervention and make the grass wet. What is the probability the sprinkler\nis on,p(S=T|do(W=T))?\nb. Suppose I perform a perfect intervention and make the grass dry. What is the probability the sprinkler\nis on,p(S=T|do(W=F))?\nc. Suppose I perform a perfect intervention and make the clouds \u201cturn on\u201d (e.g., by seeding them). What\nis the probability the sprinkler is on, p(S=T|do(C=T))?", "975": "27 Latent variable models for discrete data\n27.1 Introduction\nIn this chapter, we are concerned with latent variable models for discrete data, such as bit vectors,\nsequences of categorical variables, count vectors, graph structures, relational data, etc. Thesemodels can be used to analyze voting records, text and document collections, low-intensityimages, movie ratings, etc. However, we will mostly focus on text analysis, and this will bere\ufb02ected in our terminology.\nSince we will be dealing with so many different kinds of data, we need some precise notation\nto keep things clear. When modeling variable-length sequences of categorical variables (i.e.,symbols or tokens), such as words in a document, we will let y\nil\u2208{1,...,V}represent\nthe identity of the l\u2019th word in document i,w h e r eVis the number of possible words in\nthe vocabulary. We assume l=1:Li,w h e r eLiis the (known) length of document i, and\ni=1:N,w h e r eN is the number of documents.\nWe will often ignore the word order, resulting in a bag of words. This can be reduced to\na \ufb01xed length vector of counts (a histogram). We will use niv\u2208{0,1,...,L i}to denote the\nnumber of times word voccurs in document i,f o rv=1:V. Note that the N\u00d7Vcount\nmatrixNis often large but sparse, since we typically have many documents, but most words\ndo not occur in any given document.\nIn some cases, we might have multiple different bags of words, e.g., bags of text words and\nbags of visual words. These correspond to different \u201cchannels\u201d or types of features. We willdenote these by y\nirl,f o rr=1:R(the number of responses) and l=1:L ir.I fLir=1,i t\nmeans we have a single token (a bag of length 1); in this case, we just write yir\u2208{1,...,V r}\nfor brevity. If every channel is just a single token, we write the \ufb01xed-size response vector asy\ni,1:R; in this case, the N\u00d7Rdesign matrix Ywill not be sparse. For example, in social\nscience surveys, yircould be the response of person ito ther\u2019th multi-choice question.\nOut goal is to build joint probability models of p(yi)orp(ni)using latent variables to capture\nthe correlations. We will then try to interpret the latent variables, which provide a compressedrepresentation of the data. We provide an overview of some approaches in Section 27.2, beforegoing into more detail in later sections.\nTowards the end of the chapter, we will consider modeling graphs and relations, which can\nalso be represented as sparse discrete matrices. For example, we might want to model the graphof which papers mycite which other papers. We will denote these relations by R, reserving the\nsymbolYfor any categorical data (e.g., text) associated with the nodes.", "976": "946 Chapter27. Latentvariablemodelsfordiscretedata\n27.2 Distributed state LVMs for discrete data\nIn this section, we summarize a variety of possible approaches for constructing models of the\nformp(yi,1:Li), for bags of tokens; p(y1:R), for vectors of tokens; and p(ni), for vectors of\ninteger counts.\n27.2.1 Mixture models\nThe simplest approach is to use a \ufb01nite mixture model (Chapter 11). This associates a singlediscrete latent variable, q\ni\u2208{1,...,K}, with every document, where Kis the number of\nclusters. We will use a discrete prior, qi\u223cCat(\u03c0). For variable length documents, we can\nde\ufb01nep(yil|qi=k)=bkv,w h e r ebkvis the probability that cluster kgenerates word v. The\nvalue ofqiis known as a topic, and the vector bkis thek\u2019th topic\u2019s word distribution. That is,\nthe likelihood has the form\np(yi,1:Li|qi=k)=Li/productdisplay\nl=1Cat(yil|bk) (27.1)\nThe induced distribution on the visible data is given by\np(yi,1:Li)=/summationdisplay\nk\u03c0k/bracketleftBiggLi/productdisplay\nl=1Cat(yil|bk)/bracketrightBigg\n(27.2)\nThe \u201cgenerative story\u201d which this encodes is as follows: for document i, pick a topic qifrom\n\u03c0, call itk, and then for each word l=1:L i, pick a word from bk. We will consider more\nsophisticated generative models later in this chapter.\nIf we have a \ufb01xed set of categorical observations, we can use a different topic matrix for each\noutput variable:\np(yi,1:R|qi=k)=R/productdisplay\nr=1Cat(yil|b(r)\nk) (27.3)\nThis is an unsupervised analog of naive Bayes classi\ufb01cation.\nWe can also model count vectors. If the sum Li=/summationtext\nvnivis known, we can use a\nmultinomial:\np(ni|Li,qi=k)=M u (ni|Li,bk) (27.4)\nIf the sum is unknown, we can use a Poisson class-conditional density to give\np(ni|qi=k)=V/productdisplay\nv=1Poi(niv|\u03bbvk) (27.5)\nIn this case, Li|qi=k\u223cPoi(/summationtext\nv\u03bbvk).", "977": "27.2. DistributedstateLVMsfordiscretedata 947\n27.2.2 Exponential family PCA\nUnfortunately, \ufb01nite mixture models are very limited in their expressive power. A more \ufb02exible\nmodel is to use a vector of real-valued continuous latent variables, similar to the factor analysis(FA) and PCA models in Chapter 12. In PCA, we use a Gaussian prior of the form p(z\ni)=\nN(\u03bc,\u03a3),w h e r ezi\u2208RK, and a Gaussian likelihood of the form p(yi|zi)=N(Wzi,\u03c32I).\nThis method can certainly be applied to discrete or count data. Indeed, the method knownaslatent semantic analysis (LSA)o rlatent semantic indexing (LSI) (Deerwester et al. 1990;\nDumais and Landauer 1997) is exactly equivalent to applying PCA to a term by document countmatrix.\nA better method for modeling categorical data is to use a multinoulli or multinomial distribu-\ntion. We just have to change the likelihood to\np(y\ni,1:Li|zi)=Li/productdisplay\nl=1Cat(yil|S(Wzi)) (27.6)\nwhereW\u2208RV\u00d7Kis a weight matrix and Sis the softmax function. If we have a \ufb01xed number\nof categorical responses, we can use\np(y1:R|zi)=R/productdisplay\nr=1Cat(yir|S(Wrzi)) (27.7)\nwhereWr\u2208RV\u00d7Kis the weight matrix for the r\u2019th response variable. This model is called\ncategorical PCA , and is illustrated in Figure 27.1(a); see Section 12.4 for further discussion. If we\nhave counts, we can use a multinomial model\np(ni|Li,zi)=M u (ni|Li,S(Wzi)) (27.8)\nor a Poisson model\np(ni|zi)=V/productdisplay\nv=1Poi(niv|exp(wT\nv,:zi)) (27.9)\nAll of these models are examples of exponential family PCA orePCA(Collins et al. 2002;\nMohamed et al. 2008), which is an unsupervised analog of GLMs. The corresponding induceddistribution on the visible variables has the form\np(y\ni,1:Li)=/integraldisplay/bracketleftBiggLi/productdisplay\nl=1p(yil|zi,W)/bracketrightBigg\nN(zi|\u03bc,\u03a3)dzi (27.10)\nFitting this model is tricky, due to the lack of conjugacy. (Collins et al. 2002) proposed acoordinate ascent method that alternates between estimating the z\niandW. This can be\nregarded as a degenerate version of EM, that computes a point estimate of ziin the E step. The\nproblem with the degenerate approach is that it is very prone to over\ufb01tting, since the numberof latent variables is proportional to the number of datacases (Welling et al. 2008). A true EMalgorithm would marginalize out the latent variables z\ni. A way to do this for categorical PCA,\nusing variational EM, is discussed in Section 12.4. For more general models, one can use MCMC(Mohamed et al. 2008).", "978": "948 Chapter27. Latentvariablemodelsfordiscretedata\n\u03b3W1,K,V WR,K,V...yi,1 yi,R ...zi\nN\u03bc \u03a3\n(a)BK,Vni,1...ni,V\u03c0i\u03b1\nLi\nN\n(b)\nFigure 27.1 Two LVMs for discrete data. Circles are scalar nodes, ellipses are vector nodes, squares are\nmatrix nodes. (a) Categorical PCA. (b) Multinomial PCA.\n27.2.3 LDA and mPCA\nIn ePCA, the quantity Wzirepresents the natural parameters of the exponential family. Some-\ntimes it is more convenient to use the dual parameters. For example, for the multinomial, the\ndual parameter is the probability vector, whereas the natural parameter is the vector of log odds.\nIf we want to use the dual parameters, we need to constrain the latent variables so they live\nin the appropriate parameter space. In the case of categorical data, we will need to ensure thelatent vector lives in S\nK, theK-dimensional probability simplex. To avoid confusion with ePCA,\nwe will denote such a latent vector by \u03c0i. In this case, the natural prior for the latent variables\nis the Dirichlet, \u03c0i\u223cDir(\u03b1). Typically we set \u03b1=\u03b11K.I fw es e t \u03b1/lessmuch1, we encourage \u03c0i\nto be sparse, as shown in Figure 2.14.\nWhen we have a count vector whose total sum is known, the likelihood is given by\np(ni|Li,\u03c0i)=M u (ni|Li,B\u03c0i) (27.11)\nThis model is called multinomial PCA ormPCA(Buntine 2002; Buntine and Jakulin 2004,\n2006). See Figure 27.1(b). Since we are assuming niv=/summationtext\nkbvk\u03c0iv, this can be seen as a form\nof matrix factorization for the count matrix. Note that we use bv,kto denote the parameter\nvector, rather than wv,k, since we impose the constraints that 0\u2264bv,k\u22641and/summationtext\nvbv,k=1.\nThe corresponding marginal distribution has the form\np(ni|Li)=/integraldisplay\nMu(ni|Li,B\u03c0i)Dir(\u03c0 i|\u03b1)d\u03c0i (27.12)\nUnfortunately, this integral cannot be computed analytically.\nIf we have a variable length sequence (of known length), we can use\np(yi,1:Li|\u03c0i)=Li/productdisplay\nl=1Cat(yil|B\u03c0i) (27.13)", "979": "27.2. DistributedstateLVMsfordiscretedata 949\nThis is called latent Dirichlet allocation orLDA(Blei et al. 2003), and will be described in\nmuch greater detail below. LDA can be thought of as a probabilistic extension of LSA, where the\nlatent quantities \u03c0ikare non-negative and sum to one. By contrast, in LSA, zikcan be negative\nwhich makes interpetation difficult.\nA predecessor to LDA, known as probabilistic latent semantic indexing orPLSI(Hofmann\n1999), uses the same model but computes a point estimate of \u03c0ifor each document (similar to\nePCA), rather than integrating it out. Thus in PLSI, there is no prior for \u03c0i.\nWe can modify LDA to handle a \ufb01xed number of different categorical responses as follows:\np(yi,1:R|\u03c0i)=R/productdisplay\nr=1Cat(yil|B(r)\u03c0i) (27.14)\nThis has been called the user rating pro\ufb01le (URP) model (Marlin 2003), and the simplex factor\nmodel(Bhattacharya and Dunson 2011).\n27.2.4 GaP model and non-negative matrix factorization\nNow consider modeling count vectors where we do not constrain the sum to be observed. Inthis case, the latent variables just need to be non-negative, so we will denote them by z\n+\ni. This\ncan be ensured by using a prior of the form\np(z+i)=K/productdisplay\nk=1Ga(z+\nik|\u03b1k,\u03b2k) (27.15)\nThe likelihood is given by\np(ni|z+i)=V/productdisplay\nv=1Poi(niv|bT\nv,:z+\ni) (27.16)\nThis is called the GaP(Gamma-Poisson) model (Canny 2004). See Figure 27.2(a).\nIn (Buntine and Jakulin 2006), it is shown that the GaP model, when conditioned on a \ufb01xed\nLi, reduces to the mPCA model. This follows since a set of Poisson random variables, when\nconditioned on their sum, becomes a multinomial distribution (see e.g., (Ross 1989)).\nIf we set \u03b1k=\u03b2k=0in the GaP model, we recover a method known as non-negative\nmatrix factorization orNMF(Lee and Seung 2001), as shown in (Buntine and Jakulin 2006).\nNMF is not a probabilistic generative model, since it does not specify a proper prior for z+i.\nFurthermore, the algorithm proposed in (Lee and Seung 2001) is another degenerate EM algo-\nrithm, so suffers from over\ufb01tting. Some procedures to \ufb01t the GaP model, which overcome theseproblems, are given in (Buntine and Jakulin 2006).\nTo encourage z\n+\nito be sparse, we can modify the prior to be a spike-and-Gamma type prior\nas follows:\np(z+\nik)=\u03c1kI(z+\nik=0 )+( 1\u2212\u03c1k)Ga(z+\nik|\u03b1k,\u03b2k) (27.17)\nwhere\u03c1kis the probability of the spike at 0. This is called the conditional Gamma Poisson\nmodel (Buntine and Jakulin 2006). It is simple to modify Gibbs sampling to handle this kind of\nprior, although we will not go into detail here.", "980": "950 Chapter27. Latentvariablemodelsfordiscretedata\n\u03b3Bni,1...ni,Vz+\ni,1...z+\ni,K\nN\u03b11\u03b21\n...\u03b1K\u03b2K\n(a)\u03b3Byilqil\nLi\u03c0i\nN\u03b1\n(b)\nFigure 27.2 (a) Gaussian-Poisson (GAP) model. (b) Latent Dirichlet allocation (LDA) model.\n27.3 Latent Dirichlet allocation (LDA)\nIn this section, we explain the latent Dirichlet allocation orLDA(Blei et al. 2003) model in\ndetail.\n27.3.1 Basics\nIn a mixture of multinoullis, every document is assigned to a single topic, qi\u2208{1,...,K},\ndrawn from a global distribution \u03c0. In LDA, every word is assigned to its own topic, qil\u2208\n{1,...,K}, drawn from a document-speci\ufb01c distribution \u03c0i. Since a document belongs to a\ndistribution over topics, rather than a single topic, the model is called an admixture mixture\normixed membership model (Erosheva et al. 2004). This model has many other applications\nbeyond text analysis, e.g., genetics (Pritchard et al. 2000), health science (Erosheva et al. 2007),\nsocial network analysis (Airoldi et al. 2008), etc.\nAdding conjugate priors to the parameters, the full model is as follows:1\n\u03c0i|\u03b1\u223cDir(\u03b11K) (27.18)\nqil|\u03c0i\u223cCat(\u03c0 i) (27.19)\nbk|\u03b3\u223cDir(\u03b31V) (27.20)\nyil|qil=k,B\u223cCat(b k) (27.21)\nThis is illustrated in Figure 27.2(b). We can marginalize out the qivariables, thereby creating a\n1. Our notation is similar to the one we use elsewhere in this book, but is different from that used by most LDA papers.\nThey typically use wndfor the identity of word nin document d,zndto represent the discrete indicator, \u03b8das the\ncontinuous latent vector for document d, and\u03b2kas thek\u2019th topic vector.", "981": "27.3. LatentDirichletallocation(LDA) 951\nP(word3)P(word1)\n01\n1\n1P(word2)=   observed\ndocument\n=   generated\ndocument=   topic\nFigure 27.3 Geometric interpretation of LDA. We have K=2topics and V=3words. Each document\n(white dots), and each topic (black dots), is a point in the 3d simplex. Source: Figure 5 of (Steyvers and\nGriffiths 2007). Used with kind permission of Tom Griffiths.\ndirect arc from \u03c0itoyil, with the following CPD:\np(yil=v|\u03c0i)=/summationdisplay\nkp(yil=v|qil=k)p(qil=k)=/summationdisplay\nk\u03c0ikbkv (27.22)\nAs we mentioned in the introduction, this is very similar to the multinomial PCA model proposed\nin (Buntine 2002), which in turn is closely related to categorical PCA, GaP, NMF, etc.\nLDA has an interesting geometric interpretation. Each vector bkde\ufb01nes a distribution over\nVwords; each kis known as a topic. Each document vector \u03c0ide\ufb01nes a distribution over K\ntopics. So we model each document as an admixture over topics. Equivalently, we can thinkof LDA as a form of dimensionality reduction (assuming K<V, as is usually the case), where\nwe project a point in the V-dimensional simplex (a normalized document count vector x\ni) onto\ntheK-dimensional simplex. This is illustrated in Figure 27.3, where we have V=3words and\nK=2topics. The observed documents (which live in the 3d simplex) are approximated as\nliving on a 2d simplex spanned by the 2 topic vectors, each of which lives in the 3d simplex.\nOne advantage of using the simplex as our latent space rather than Euclidean space is that\nthe simplex can handle ambiguity. This is importance since in natural language, words can oftenhave multiple meanings, a phenomomen known as polysemy. For example, \u201cplay\u201d might refer\nto a verb (e.g., \u201cto play ball\u201d or \u201cto play the coronet\u201d), or to a noun (e.g., \u201cShakespeare\u2019s play\u201d).In LDA, we can have multiple topics, each of which can generate the word \u201cplay\u201d, as shown inFigure 27.4, re\ufb02ecting this ambiguity.\nGiven word lin document i, we can compute p(q\nil=k|yi,\u03b8), and thus infer its most likely\ntopic. By looking at the word in isolation, it might be hard to know what sense of the word ismeant, but we can disambiguate this by looking at other words in the document. In particular,givenx\ni, we can infer the topic distribution \u03c0ifor the document; this acts as a prior for\ndisambiguating qil. This is illustrated in Figure 27.5, where we show three documents from the\nTASA corpus.2In the \ufb01rst document, there are a variety of music related words, which suggest\n2. The TASA corpus is a collection of 37,000 high-school level English documents, comprising over 10 million words,", "982": "952 Chapter27. Latentvariablemodelsfordiscretedata\nword prob. word prob. word prob.\nMUSIC .090 LITERATURE .031 PLAY .136\nDANCE .034 POEM .028 BALL .129\nSONG .033 POETRY .027 GAME .065\nPLAY .030 POET .020 PLAYING .042\nSING .026 PLAYS .019 HIT .032\nSINGING .026 POEMS .019 PLAYED .031\nBAND .026 PLAY .015 BASEBALL .027\nPLAYED .023 LITERARY .013 GAMES .025\nSANG .022 WRITERS .013 BAT .019\nSONGS .021 DRAMA .012 RUN .019\nDANCING .020 WROTE .012 THROW .016\nPIANO .017 POETS .011 BALLS .015\nPLAYING .016 WRITER .011 TENNIS .011\nRHYTHM .015 SHAKESPEARE .010 HOME .010\nALBERT .013 WRITTEN .009 CATCH .010\nMUSICAL .013 STAGE .009 FIELD .010Topic 166 Topic 82 Topic 77\nFigure 27.4 Three topics related to the word play. Source: Figure 9 of (Steyvers and Griffiths 2007).\nUsed with kind permission of Tom Griffiths.\nDocument #29795 \nBix beiderbecke, at  age060 fifteen207, sat174on the  slope071of a bluff055 overlooking027the mississippi137 river137.H e\nwas listening077to music077 coming009from a  passing043riverboat. The  music077had already  captured006his heart157\nas well as his  ear119. It was jazz077. Bix beiderbecke had already had  music077 lessons077.H e showed002 promise134on\nthe piano077, and his  parents035 hoped268he might  consider118becoming a  concert077 pianist077. But bix was\ninterested268in another  kind050of music077.H e wanted268to play077the cornet. And he wanted268to play077 jazz077...  \nDocument #1883 There is a  simple\n050 reason106why there are so few  periods078of really great  theater082in our whole  western046world.\nToo many  things300have to come right at the very same time. The dramatists must have the right  actors082,t h e\nactors082must have the right playhouses, the playhouses must have the right  audiences082.W em u s t  remember288that\nplays082 exist143to be performed077,n o t merely050to be read254. ( even when you  read254a play082to yourself,  try288to\nperform062it, to  put174it on a stage078, as you go along.) as  soon028as a play082has to be  performed082, then some \nkind126of theatrical082...\nDocument #21359 \nJim296has a  game166 book254. Jim296 reads254the book254. Jim296 sees081a game166for one.  Jim296 plays166the game166.\nJim296 likes081the game166for one. The  game166 book254 helps081 jim296. Don180 comes040into the  house038. Don180and\njim296 read254the game166 book254.T h e  boys020see a game166for two. The two  boys020 play166the game166.T h e\nboys020 play166the game166for two. The  boys020like the  game166. Meg282 comes040into the house282. Meg282and\ndon180and jim296 read254the book254.T h e ys e ea  game166for three.  Meg282and don180and jim296 play166the game166.\nThey play166...\nFigure 27.5 Three documents from the TASA corpus containing different senses of the word play.G r a y e d\nout words were ignored by the model, because they correspond to uninteresting stop words (such as \u201cand\u201d,\n\u201cthe\u201d, etc.) or very low frequency words. Source: Figure 10 of (Steyvers and Griffiths 2007). Used withkind permission of Tom Griffiths.", "983": "27.3. LatentDirichletallocation(LDA) 953\n\u03c0iwill put most of its mass on the music topic (number 77); this in turn makes the music\ninterpretation of \u201cplay\u201d the most likely, as shown by the superscript. The second document\ninterprets play in the theatrical sense, and the third in the sports sense. Note that is crucialthat\u03c0\nibe a latent variable, so information can \ufb02ow between the qil\u2019s, thus enabling local\ndisambiguation to use the full set of words.\n27.3.2 Unsupervised discovery of topics\nOne of the main purposes of LDA is discover topics in a large collection or corpusof docu-\nments (see Figure 27.12 for an example). Unfortunately, since the model is unidenti\ufb01able, theinterpertation of the topics can be difficult (Chang et al. 2009).. One approach, known as la-beled LDA (Ramage et al. 2009), exploits the existence of tags on documents as a way to ensureidenti\ufb01ability. In particular, it forces the topics to correspond to the tags, and then it learns adistribution over words for each tag. This can make the results easier to interpret.\n27.3.3 Quantitatively evaluating LDA as a language model\nIn order to evaluate LDA quantitatively, we can treat it as a language model , i.e., a probability\ndistribution over sequences of words. Of course, it is not a very good language model, since itignores word order and just looks at single words (unigrams), but it is interesting to compareLDA to other unigram-based models, such as mixtures of multinoullis, and pLSI. Such simplelanguage models are sometimes useful for information retrieval purposes. The standard way tomeasure the quality of a language model is to use perplexity, which we now de\ufb01ne below.\n27.3.3.1 Perplexity\nTheperplexity of language model qgiven a stochastic process\n3pis de\ufb01ned as\nperplexity( p,q)/defines2H(p,q)(27.23)\nwhereH(p,q)is the cross-entropy of the two stochastic processes, de\ufb01ned as\nH(p,q)/defineslim\nN\u2192\u221e\u22121\nN/summationdisplay\ny1:Np(y1:N)logq(y1:N) (27.24)\nThe cross entropy (and hence perplexity) is minimized if q=p; in this case, the model can\npredict as well as the \u201ctrue\u201d distribution.\nWe can approximate the stochastic process by using a single long test sequence (composed\nof multiple documents and multiple sentences, complete with end-of-sentence markers), callity\n\u2217\n1:N. (This approximation becomes more and more accurate as the sequence gets longer,\nprovided the process is stationary and ergodic (Cover and Thomas 2006).) De\ufb01ne the empiricaldistribution (an approximation to the stochastic process) as\np\nemp(y1:N)=\u03b4y\u2217\n1:N(y1:N) (27.25)\ncollated by a company formerly known as Touchstone Applied Science Associates, but now known as Questar Assessment\nIncwww.questarai .com.\n3. A stochastic process is one which can de\ufb01ne a joint distribution over an arbitrary number of random variables. Wecan think of natural language as a stochastic process, since it can generate an in\ufb01nite stream of words.", "984": "954 Chapter27. Latentvariablemodelsfordiscretedata\nIn this case, the cross-entropy becomes\nH(pemp,q)=\u22121\nNlogq(y\u2217\n1:N) (27.26)\nand the perplexity becomes\nperplexity( pemp,q)=2H(pemp,q)=q(y\u2217\n1:N)\u22121/N=N/radicaltp/radicalvertex/radicalvertex/radicalbtN/productdisplay\ni=11\nq(y\u2217\ni|y\u2217\n1:i\u22121)(27.27)\nWe see that this is the geometric mean of the inverse predictive probabilities, which is the usual\nde\ufb01nition of perplexity (Jurafsky and Martin 2008, p96).\nIn the case of unigram models, the cross entropy term is given by\nH=\u22121\nNN/summationdisplay\ni=11\nLiLi/summationdisplay\nl=1logq(yil) (27.28)\nwhereNis the number of documents and Liis the number of words in document i. Hence\nthe perplexity of model qis given by\nperplexity( pemp,p)=e x p/parenleftBigg\n\u22121\nNN/summationdisplay\ni=11\nLiLi/summationdisplay\nl=1logq(yil)/parenrightBigg\n(27.29)\nIntuitively, perplexity mesures the weighted average branching factor of the model\u2019s predic-\ntive distribution. Suppose the model predicts that each symbol (letter, word, whatever) is equallylikely, so p(y\ni|y1:i\u22121)=1/K. Then the perplexity is ((1/K)N)\u22121/N=K. If some symbols\nare more likely than others, and the model correctly re\ufb02ects this, its perplexity will be lowerthanK. Of course, H(p,p)=H (p)\u2264H(p,q), so we can never reduce the perplexity below\nthe entropy of the underlying stochastic process.\n27.3.3.2 Perplexity of LDA\nThe key quantity is p(v), the predictive distribution of the model over possible words. (It is\nimplicitly conditioned on the training set.) For LDA, this can be approximated by plugging inB(e.g., the posterior mean estimate) and approximately integrating out qusing mean \ufb01eld\ninference (see (Wallach et al. 2009) for a more accurate way to approximate the predictivelikelihood).\nIn Figure 27.6, we compare LDA to several other simple unigram models, namely MAP estima-\ntion of a multinoulli, MAP estimation of a mixture of multinoullis, and pLSI. (When performingMAP estimation, the same Dirichlet prior on Bwas used as in the LDA model.) The metric\nis perplexity, as in Equation 27.29, and the data is a subset of the TREC AP corpus containing16,333 newswire articles with 23,075 unique terms. We see that LDA signi\ufb01cantly outperformsthese other methods.", "985": "27.3. LatentDirichletallocation(LDA) 955\n0 20 40 60 80 100 120 140 160 180 2002500300035004000450050005500600065007000\nNumber of TopicsPerplexity\n  \nUnigram\nMixtures of Unigrams\nLDA\nFold in pLSI\nFigure 27.6 Perplexity vs number of topics on the TREC AP corpus for various language models. Based\non Figure 9 of (Blei et al. 2003). Figure generated by bleiLDAperplexityPlot .\n\u03b3b1...bKy1,1...y1,L1q1,1...q1,L1\u03c01\n......... ...\nyN,1...yN,LNqN,1...qN,LN\u03c0N\u03b1\n(a)\u03b3y1,1...y1,L1q1,1...q1,L1\n......yN,1...yN,LNqN,1...qN,LN\u03b1\n(b)\nFigure 27.7 (a) LDA unrolled for Ndocuments. (b) Collapsed LDA, where we integrate out the \u03c0iand\nthebk.\n27.3.4 Fitting using (collapsed) Gibbs sampling\nIt is straightforward to derive a Gibbs sampling algorithm for LDA. The full conditionals are as\nfollows:\np(qil=k|\u00b7)\u221dexp[log\u03c0ik+logbk,xil] (27.30)\np(\u03c0i|\u00b7) = Dir({\u03b1k+/summationdisplay\nlI(zil=k)}) (27.31)\np(bk|\u00b7) = Dir({\u03b3v+/summationdisplay\ni/summationdisplay\nlI(xil=v,zil=k)}) (27.32)\nHowever, one can get better performance by analytically integrating out the \u03c0i\u2019s and the bk\u2019s,", "986": "956 Chapter27. Latentvariablemodelsfordiscretedata\nboth of which have a Dirichlet distribution, and just sampling the discrete qil\u2019s. This approach\nwas \ufb01rst suggested in (Griffiths and Steyvers 2004), and is an example of collapsed Gibbs\nsampling. Figure 27.7(b) shows that now all the qilvariables are fully correlated. However, we\ncan sample them one at a time, as we explain below.\nFirst, we need some notation. Let civk=/summationtextLi\nl=1I(qil=k,yil=v)be the number of times\nwordvis assigned to topic kin document i.L e tcik=/summationtext\nvcivkbe the number of times any\nword from document ihas been assigned to topic k.L e tcvk=/summationtext\nicivkbe the number of times\nwordvhas been assigned to topic kin any document. Let niv=/summationtext\nkcivkbe the number of\ntimes word voccurs in document i; this is observed. Let ck=/summationtext\nvcvkbe the number of words\nassigned to topic k. Finally, let Li=/summationtext\nkcikbe the number of words in document i; this is\nobserved.\nWe can now derive the marginal prior. By applying Equation 5.24, one can show that\np(q|\u03b1)=/productdisplay\ni/integraldisplay/bracketleftBiggLi/productdisplay\nl=1Cat(qil|\u03c0i)/bracketrightBigg\nDir(\u03c0i|\u03b11K)d\u03c0i (27.33)\n=/parenleftbigg\u0393(K\u03b1)\n\u0393(\u03b1)K/parenrightbiggNN/productdisplay\ni=1/producttextK\nk=1\u0393(cik+\u03b1)\n\u0393(Li+K\u03b1)(27.34)\nBy similar reasoning, one can show\np(y|q,\u03b3)=/productdisplay\nk/integraldisplay\u23a1\n\u23a3/productdisplay\nil:qil=kCat(yil|bk)\u23a4\u23a6Dir(b\nk|\u03b31V)dbk (27.35)\n=/parenleftbigg\u0393(V\u03b2)\n\u0393(\u03b2)V/parenrightbiggKK/productdisplay\nk=1/producttextV\nv=1\u0393(cvk+\u03b2)\n\u0393(ck+V\u03b2)(27.36)\nFrom the above equations, and using the fact that \u0393(x+1)/\u0393(x)=x, we can derive the full\nconditional for p(qil|q\u2212i,l). De\ufb01nec\u2212\nivkto be the same as civkexcept it is compute by summing\nover all locations in document iexcept for qil. Also, let yil=v. Then\np(qi,l=k|q\u2212i,l,y,\u03b1,\u03b3)\u221dc\u2212v,k+\u03b3\nc\u2212k+V\u03b3c\u2212i,k+\u03b1\nLi+K\u03b1(27.37)\nWe see that a word in a document is assigned to a topic based both on how often that word is\ngenerated by the topic (\ufb01rst term), and also on how often that topic is used in that document(second term).\nGiven Equation 27.37, we can implement the collapsed Gibbs sampler as follows. We randomly\nassign a topic to each word, q\nil\u2208{1,...,K}. We can then sample a new topic as follows: for\na given word in the corpus, decrement the relevant counts, based on the topic assigned to thecurrent word; draw a new topic from Equation 27.37, update the count matrices; and repeat.This algorithm can be made efficient since the count matrices are very sparse.\n27.3.5 Example\nThis process is illustrated in Figure 27.8 on a small example with two topics, and \ufb01ve words.The left part of the \ufb01gure illustrates 16 documents that were sampled from the LDA model using", "987": "27.3. LatentDirichletallocation(LDA) 957\nRiver Stream Bank Money Loan\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n(a)River Stream Bank Money Loan\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n(b)\nFigure 27.8 Illustration of (collapsed) Gibbs sampling applied to a small LDA example. There are N=1 6\ndocuments, each containing a variable number of words drawn from a vocabulary of V=5words, There\nare two topics. A white dot means word the word is assigned to topic 1, a black dot means the word is\nassigned to topic 2. (a) The initial random assignment of states. (b) A sample from the posterior after 64steps of Gibbs sampling. Source: Figure 7 of (Steyvers and Griffiths 2007). Used with kind permission ofTom Griffiths.\np(money|k=1 )=p(loan| k=1 )=p(bank| k=1 )=1 /3andp(river|k=2 )=p(stream| k=\n2) =p(bank|k=2 )=1 /3. For example, we see that the \ufb01rst document contains the word\n\u201cbank\u201d 4 times (indicated by the four dots in row 1 of the \u201cbank\u201d column), as well as various\nother \ufb01nancial terms. The right part of the \ufb01gure shows the state of the Gibbs sampler after64 iterations. The \u201ccorrect\u201d topic has been assigned to each token in most cases. For example,in document 1, we see that the word \u201cbank\u201d has been correctly assigned to the \ufb01nancial topic,based on the presence of the words \u201cmoney\u201d and \u201cloan\u201d. The posterior mean estimate of theparameters is given by \u02c6p(money|k=1 )=0 .32,\u02c6p(loan|k=1 )=0 .29,\u02c6p(bank|k=1 )=\n0.39,\u02c6p(river|k=2 )=0 .25,\u02c6p(stream|k=2 )=0 .4, and\u02c6p(bank|k=2 )=0 .35, w\n hich is\nimpressively accurate, given that there are only 16 training examples.\n27.3.6 Fitting using batch variational inference\nA faster alternative to MCMC is to use variational EM. (We cannot use exact EM since exactinference of \u03c0\niandqiis intractable.) We give the details below.\n27.3.6.1 Sequence version\nFollowing (Blei et al. 2003), we will use a fully factorized (mean \ufb01eld) approximation of the form\nq(\u03c0i,qi) = Dir(\u03c0i|\u02dc\u03c0i)/productdisplay\nlCat(qil|\u02dcqil) (27.38)\nWe will follow the usual mean \ufb01eld recipe. For q(qil), we use Bayes\u2019 rule, but where we need to\ntake expectations over the prior:\n\u02dcqilk\u221dbyi,l,kexp(E[log\u03c0ik]) (27.39)\nwhere\nE[log\u03c0ik]=\u03c8k(\u02dc\u03c0i.)/defines\u03a8(\u02dc\u03c0ik)\u2212\u03a8(/summationdisplay\nk/prime\u02dc\u03c0ik/prime) (27.40)", "988": "958 Chapter27. Latentvariablemodelsfordiscretedata\nwhere\u03a8is the digamma function. The update for q(\u03c0i)is obtained by adding up the expected\ncounts:\n\u02dc\u03c0ik=\u03b1k+/summationdisplay\nl\u02dcqilk (27.41)\nThe M step is obtained by adding up the expected counts and normalizing:\n\u02c6bvk\u221d\u03b3v+N/summationdisplay\ni=1Li/summationdisplay\nl=1\u02dcqilkI(yil=v) (27.42)\n27.3.6.2 Count version\nNote that the E step takes O((/summationtext\niLi)VK)space to store the \u02dcqilk. It is much more space\nefficient to perform inference in the mPCA version of the model, which works with counts; these\nonly take O(NVK)space, which is a big savings if documents are long. (By contrast, the\ncollapsed Gibbs sampler must work explicitly with the qilvariables.)\nWe will focus on approximating p(\u03c0i,ci|ni,Li), where we write cias shorthand for ci...W e\nwill again use a fully factorized (mean \ufb01eld) approximation of the form\nq(\u03c0i,ci)=D i r (\u03c0i|\u02dc\u03c0i)/productdisplay\nvMu(civ.|niv,\u02dcciv.) (27.43)\nThe new E step becomes\n\u02dc\u03c0ik=\u03b1k+/summationdisplay\nvniv\u02dccivk (27.44)\n\u02dccivk\u221dbvkexp(E[log\u03c0ik]) (27.45)\nThe new M step becomes\n\u02c6bvk\u221d\u03b3v+/summationdisplay\niniv\u02dccivk (27.46)\n27.3.6.3 VB version\nWe now modify the algorithm to use VB instead of EM, so that we infer the parameters aswell as the latent variables. There are two advantages to this. First, by setting \u03b3/lessmuch1, VB will\nencourage Bto be sparse (as in Section 21.6.1.6). Second, we will be able to generalize this to\nthe online learning setting, as we discuss below.\nOur new posterior approximation becomes\nq(\u03c0\ni,ci,B)=D i r ( \u03c0i|\u02dc\u03c0i)/productdisplay\nvMu(civ.|niv,\u02dcciv.)/productdisplay\nkDir(b.k|\u02dcb.k) (27.47)\nThe update for \u02dccivkchanges, to the following:\n\u02dccivk\u221dexp(E[logbvk]+E[log\u03c0ik]) (27.48)", "989": "27.3. LatentDirichletallocation(LDA) 959\nAlgorithm 27.1: Batch VB for LDA\n1Input:niv,K,\u03b1k,\u03b3v;\n2Estimate\u02dcbvkusing EM for multinomial mixtures;\n3Initialize counts niv;\n4whilenotconverged do\n5/ /Es t e p;\n6svk=0// expected sufficient statistics;\n7foreachdocument i=1:Ndo\n8 (\u02dc\u03c0i,\u02dcci)=Estep(ni,\u02dcB,\u03b1);\n9 svk+=niv\u02dccivk;\n10/ /Ms t e p;\n11foreachtopic k=1:Kdo\n12 \u02dcbvk=\u03b3v+svk;\n13function(\u02dc\u03c0i,\u02dcci)=Estep(ni,\u02dcB,\u03b1);\n14Initialize \u02dc\u03c0ik=\u03b1k;\n15repeat\n16\u02dc\u03c0old\ni.=\u02dc\u03c0i.,\u02dc\u03c0ik=\u03b1k;\n17foreachword v=1:Vdo\n18 foreachtopic k=1:Kdo\n19 \u02dccivk=e x p/parenleftBig\n\u03c8k(\u02dcbv.)+\u03c8k(\u02dc\u03c0old\ni.)/parenrightBig\n;\n20 \u02dcciv.=normalize( \u02dcciv.);\n21 \u02dc\u03c0ik+=niv\u02dccivk\n22until1\nK/summationtext\nk|\u02dc\u03c0ik\u2212\u02dc\u03c0old\nik|<thresh;\nAlso, the M step becomes\n\u02dcbvk=\u03b3v+/summationdisplay\ni\u02dccivk (27.49)\nNo normalization is required, since we are just updating the pseudcounts. The overall algorithm\nis summarized in Algorithm 22.\n27.3.7 Fitting using online variational inference\nIn the bathc version, the E step clearly takes O(NKVT)time, where Tis the number of\nmean \ufb01eld updates (typically T\u223c5). This can be slow if we have many documents. This can\nbe reduced by using stochastic gradient descent (Section 8.5.2) to perform online variationalinference, as we now explain.\nWe can derive an online version, following (Hoffman et al. 2010). We perform an E step in the\nusual way. We then compute the variational parameters for Btreating the expected sufficient\nstatistics from the single data case as if the whole data set had those statistics. Finally, we make", "990": "960 Chapter27. Latentvariablemodelsfordiscretedata\nAlgorithm 27.2: Online variational Bayes for LDA\n1Input:niv,K,\u03b1k,\u03b3v,\u03c40,\u03ba;\n2Initialize\u02dcbvkrandomly;\n3fort=1:\u221edo\n4Set step size \u03c1t=(\u03c40+t)\u2212\u03ba;\n5Pick document i=i(t);;\n6(\u02dc\u03c0i,\u02dcci)=Estep(ni,\u02dcB,\u03b1);\n7\u02dcbnew\nvk=\u03b3v+Nniv\u02dccivk;\n8\u02dcbvk=( 1\u2212\u03c1t)\u02dcbvk+\u03c1t\u02dcbnewvk;\nDocuments seen (log scale)Perplexity\n600650700750800850900\n103.5104104.5105105.5106106.5Batch 98KOnline 98K\nOnline 3.3M\nFigure 27.9 Test perplexity vs number of training documents for batch and online VB-LDA. From Figure\n1 of (Hoffman et al. 2010). Used with kind permission of David Blei.\na partial update for the variational parameters for B, putting weight \u03c1ton the new estimate\nand weight 1\u2212\u03c1ton the old estimate. The step size \u03c1tdecays over time, as in Equation 8.83.\nThe overall algorithm is summarized in Algorithm 3. In practice, we should use mini-batches,\nas explained in Section 8.5.2.3. In (Hoffman et al. 2010), they used a batch of size 256\u20134096.\nFigure 27.9 plots the perplexity on a test set of size 1000 vs number of analyzed documents (E\nsteps), where the data is drawn from (English) Wikipedia. The \ufb01gure shows that online variationalinference is much faster than offline inference, yet produces similar results.\n27.3.8 Determining the number of topics\nChoosing K, the number of topics, is a standard model selection problem. Here are some\napproaches that have been taken:\n\u2022 Use annealed importance sampling (Section 24.6.2) to approximate the evidence (Wallach\net al. 2009).\n\u2022 Cross validation, using the log likelihood on a test set.", "991": "27.4. ExtensionsofLDA 961\n\u2022 Use the variational lower bound as a proxy for logp(D|K).\n\u2022 Use non-parametric Bayesian methods (Teh et al. 2006).\n27.4 Extensions of LDA\nMany extensions of LDA have been proposed since the \ufb01rst paper came out in 2003. We brie\ufb02y\ndiscuss a few of these below.\n27.4.1 Correlated topic model\nOne weakness of LDA is that it cannot capture correlation between topics. For example, if adocument has the \u201cbusiness\u201d topic, it is reasonable to expect the \u201c\ufb01nance\u201d topic to co-occcur.The source of the problem is the use of a Dirichlet prior for \u03c0\ni. The problem with the Dirichelt\nit that it is characterized by just a mean vector and a strength parameter, but its covariance is\ufb01xed (\u03a3\nij=\u2212\u03b1i\u03b1j), rather than being a free parameter.\nOne way around this is to replace the Dirichlet prior with the logistic normal distribution, as\nin categorical PCA (Section 27.2.2). The model becomes\nbk|\u03b3\u223cDir(\u03b31V) (27.50)\nzi\u223cN(\u03bc,\u03a3) (27.51)\n\u03c0i|zi=S(zi) (27.52)\nqil|\u03c0i\u223cCat(\u03c0 i) (27.53)\nyil|qil=k,B\u223cCat(b k) (27.54)\nThis is known as the correlated topic model (Blei and Lafferty 2007). This is very similar to\ncategorical PCA, but slightly different. To see the difference, let us marginalize out the qiland\n\u03c0i. Then in the CTM we have\nyil\u223cCat(BS(zi)) (27.55)\nwhereBis a stochastic matrix. By contrast, in catPCA we have\nyil\u223cCat(S(Wzi)) (27.56)\nwhereWis an unconstrained matrix.\nFitting this model is tricky, since the prior for \u03c0iis no longer conjugate to the multinomial\nlikelihood for qil. However, we can use any of the variational methods in Section 21.8.1.1, where\nwe discussed Bayesian multiclass logistic regression. In the CTM case, things are even hardersince the categorical response variables q\niare hidden, but we can handle this by using an\nadditional mean \ufb01eld approximation. See (Blei and Lafferty 2007) for details.\nHaving \ufb01t the model, one can then convert \u02c6\u03a3to a sparse precision matrix \u02c6\u03a3\u22121by pruning\nlow-strength edges, to get a sparse Gaussian graphical model. This allows you to visualize thecorrelation between topics. Figure 27.10 shows the result of applying this procedure to articlesfromSciencemagazine, from 1990-1999. (This corpus contains 16,351 documents, and 5.7M words\n(19,088 of them unique), after stop-word and low-frequency removal.) Nodes represent topics,with the top 5 words per topic listed inside. The font size re\ufb02ects the overall prevalence of thetopic in the corpus. Edges represent signi\ufb01cant elements of the precision matrix.", "992": "962 Chapter27. Latentvariablemodelsfordiscretedata\nwild type\nmutant\nmutations\nmutants\nmutation\nplants\nplant\ngene\ngenes\narabidopsisp53\ncell cycle\nactivity\ncyclin\nregulation\namino acids\ncdna\nsequence\nisolated\nprotein\ngene\ndisease\nmutations\nfamilies\nmutationrna\ndna\nrna polymerase\ncleavage\nsite\ncells\ncell\nexpression\ncell lines\nbone marrowunited states\nwomen\nuniversities\nstudents\neducationscience\nscientists\nsays\nresearch\npeopleresearch\nfunding\nsupport\nnih\nprogramsurface\ntip\nimage\nsample\ndevice\nlaser\noptical\nlight\nelectrons\nquantummaterials\norganic\npolymer\npolymers\nmolecules\nvolcanic\ndeposits\nmagma\neruption\nvolcanismmantle\ncrust\nupper mantle\nmeteorites\nratios\nearthquake\nearthquakes\nfault\nimages\ndataancient\nfound\nimpact\nmillion years ago\nafrica\nclimate\nocean\nice\nchanges\nclimate changecells\nproteins\nresearchers\nprotein\nfoundpatients\ndisease\ntreatment\ndrugs\nclinicalgenetic\npopulation\npopulations\ndifferences\nvariationfossil record\nbirds\nfossils\ndinosaurs\nfossilsequence\nsequences\ngenome\ndna\nsequencing\nbacteria\nbacterial\nhost\nresistance\nparasitedevelopment\nembryos\ndrosophila\ngenes\nexpressionspecies\nforest\nforests\npopulations\necosystemssynapses\nltp\nglutamate\nsynaptic\nneuronsneurons\nstimulus\nmotor\nvisual\ncortical\nozone\natmospheric\nmeasurements\nstratosphere\nconcentrationssun\nsolar wind\nearth\nplanets\nplanet\nco2\ncarbon\ncarbon dioxide\nmethane\nwaterreceptor\nreceptors\nligand\nligands\napoptosisproteins\nprotein\nbinding\ndomain\ndomainsactivated\ntyrosine phosphorylation\nactivation\nphosphorylation\nkinase\nmagnetic\nmagnetic \ufb01eld\nspin\nsuperconductivity\nsuperconductingphysicists\nparticles\nphysics\nparticle\nexperimentsurface\nliquid\nsurfaces\n\ufb02uid\nmodel reaction\nreactions\nmolecule\nmolecules\ntransition stateenzyme\nenzymes\niron\nactive site\nreduction\npressure\nhigh pressure\npressures\ncore\ninner corebrain\nmemory\nsubjects\nleft\ntask\ncomputer\nproblem\ninformation\ncomputers\nproblems\nstars\nastronomers\nuniverse\ngalaxies\ngalaxy\nvirus\nhiv\naids\ninfection\nvirusesmice\nantigen\nt cells\nantigens\nimmune response\nFigure 27.10 Output of the correlated topic model (with K=5 0topics) when applied to articles from\nScience. Nodes represent topics, with the 5 most probable phrases from each topic shown inside. Fontsize re\ufb02ects overall prevalence of the topic. See http://www .cs.cmu.edu/~lemur/science/ for an\ninteractive version of this model with 100 topics. Source: Figure 2 of (Blei and Lafferty 2007). Used withkind permission of David Blei.\n27.4.2 Dynamic topic model\nIn LDA, the topics (distributions over words) are assumed to be static. In some cases, it makes\nsense to allow these distributions to evolve smoothly over time. For example, an article mightuse the topic \u201cneuroscience\u201d, but if it was written in the 1900s, it is more likely to use wordslike \u201cnerve\u201d, whereas if it was written in the 2000s, it is more likely to use words like \u201ccalciumreceptor\u201d (this re\ufb02ects the general trend of neuroscience towards molecular biology).\nOne way to model this is use a dynamic logistic normal model, as illustrated in Figure 27.11.\nIn particular, we assume the topic distributions evolve according to a Gaussian random walk,and then we map these Gaussian vectors to probabilities via the softmax function:\nb\nt,k|bt\u22121,k\u223cN(bt\u22121,k,\u03c321V) (27.57)\n\u03c0t\ni\u223cDir(\u03b11K) (27.58)\nqt\nil|\u03c0ti\u223cCat(\u03c0ti) (27.59)\nyt\nil|qt\nil=k,Bt\u223cCat(S(btk)) (27.60)\nThis is known as a dynamic topic model (Blei and Lafferty 2006b).", "993": "27.4. ExtensionsofLDA 963\nbt\u22121\nkyt\u22121\nilqt\u22121\nil\u03c0t\u22121\ni\nN\nbt\nkyt\nilqt\nil\u03c0t\ni\nN\nbt+1\nkyt+1\nilqt+1\nil\u03c0t+1\ni\nN\nK\u03b1\nFigure 27.11 The dynamic topic model.\nOne can perform approximate infernece in this model using a structured mean \ufb01eld method\n(Section 21.4), that exploits the Kalman smoothing algorithm (Section 18.3.1) to perform exact\ninference on the linear-Gaussian chain between the bt,knodes (see (Blei and Lafferty 2006b) for\ndetails).\nFigure 27.12 illustrates a typical output of the system when applied to 100 years of articles\nfromScience. On the top, we visualize the top 10 words from a speci\ufb01c topic (which seems to\nbe related to neuroscience) after 10 year intervals. On the bottom left, we plot the probabilityof some speci\ufb01c words belonging to this topic. On the bottom right, we list the titles of somearticles that contained this topic.\nOne interesting application of this model is to perform temporally-corrected document re-\ntrieval. That is, suppose we look for documents about the inheritance of disease. Modernarticles will use words like \u201cDNA\u201d, but older articles (before the discovery of DNA) may use otherterms such as \u201cheritable unit\u201d. But both articles are likely to use the same topics. Similar ideascan be used to perform cross-language information retrieval, see e.g., (Cimiano et al. 2009).\n27.4.3 LDA-HMM\nThe LDA model assumes words are exchangeable, which is clearly not true. A simple wayto model sequential dependence between words is to use a hidden Markov model or HMM.The trouble with HMMs is that they can only model short-range dependencies, so they cannotcapture the overall gist of a document. Hence they can generate syntactically correct sentences(see e.g., Table 17.1). but not semantically plausible ones.\nIt is possible to combine LDA with HMM to create a model called LDA-HMM (Griffiths et al.", "994": "964 Chapter27. Latentvariablemodelsfordiscretedata\n\"Neuroscience\" 1881\nbrain\nmovement\naction\nright\neye\nhand\nleft\nmuscle\nnerve\nsound  1890\nmovement\neye\nright\nhand\nbrain\nleft\naction\nmuscle\nsound\nexperiment 1900\nbrain\neye\nmovement\nright\nleft\nhand\nnerve\nvision\nsound\nmuscle  1910\nmovement\nbrain\nsound\nnerve\nactive\nmuscle\nleft\neye\nright\nnervous  1920\nmovement\nsound\nmuscle\nactive\nnerve\nstimulate\n\ufb01ber\nreaction\nbrain\nresponse  1930\nstimulate\nmuscle\nsound\nmovement\nresponse\nnerve\nfrequency\n\ufb01ber\nactive\nbrain  1940\nrecord\nnerve\nstimulate\nresponse\nmuscle\nelectrode\nactive\nbrain\n\ufb01ber\npotential  1950\nrespons\nrecord\nstimulate\nnerve\nmuscle\nactive\nfrequency\nelectrode\npotential\nstudy  1960\nresponse\nstimulate\nrecord\ncondition\nactive\npotential\nstimulus\nnerve\nsubject\neye  1970\nrespons\ncell\npotential\nstimul\nneuron\nactive\nnerve\neye\nrecord\nabstract  1980\ncell\nneuron\nresponse\nactive\nbrain\nstimul\nmuscle\nsystem\nnerve\nreceptor  1990\ncell\nchannel\nneuron\nca2\nactive\nbrain\nreceptor\nmuscle\nrespons\ncurrent  2000\nneuron\nactive\nbrain\ncell\n\ufb01g\nresponse\nchannel\nreceptor\nsynapse\nsignal \n1880 1900 1920 1940 1960 1980 2000ca2\nneuronnerve1887 Mental Science\n1900 Hemianopsia in Migraine\n1912 A Defence of the ``New Phrenology''\n1921 The Synchronal Flashing of Fire\ufb02ies\n1932 Myoesthesis and Imageless Thought\n1943 Acetylcholine and the Physiology of the Nervous System\n1952 Brain Waves and Unit Discharge in Cerebral Cortex\n1963 Errorless Discrimination Learning in the Pigeon\n1974 Temporal Summation of Light by a Vertebrate Visual Receptor\n1983 Hysteresis in the Force-Calcium Relation in Muscle\n1993 GABA-Activated Chloride Channels in Secretory Nerve Endings\nFigure 27.12 Part of the output of the dynamic topic model when applied to articles from Science.W e\nshow the top 10 words for the neuroscience topic over time. We also show the probability of three words\nwithin this topic over time, and some articles that contained this topic. Source: Figure 4 of (Blei andLafferty 2006b). Used with kind permission of David Blei.\n2004). This model uses the HMM states to model function or syntactic words, such as \u201cand\u201d or\n\u201chowever\u201d, and uses the LDA to model content or semantic words, which are harder to predict.There is a distinguished HMM state which speci\ufb01es when the LDA model should be used togenerate the word; the rest of the time, the HMM generates the word.\nMore formally, for each document i, the model de\ufb01nes an HMM with states z\nil\u2208{0,...,C}.\nIn addition, each document has an LDA model associated with it. If zil=0, we generate word\nyilfrom the semantic LDA model, with topic speci\ufb01ed by qil; otherwise we generate word yil\nfrom the syntactic HMM model. The DGM is shown in Figure 27.13. The CPDs are as follows:\np(\u03c0i)=D i r ( \u03c0i|\u03b11K) (27.61)\np(qil=k|\u03c0i)=\u03c0 ik (27.62)\np(zil=c/prime|zi,l\u22121=c)=AHMM(c,c/prime) (27.63)\np(yil=v|qil=k,zil=c)=/braceleftbiggBLDA(k,v)ifc=0\nBHMM(c,v)ifc>0(27.64)\nwhereBLDAis the usual topic-word matrix, BHMMis the state-word HMM emission matrix\nandAHMMis the state-state HMM transition matrix.\nInference in this model can be done with collapsed Gibbs sampling, analytically integrating\nout all the continuous quantities. See (Griffiths et al. 2004) for the details.\nThe results of applying this model (with K= 200LDA topics and C=2 0HMM states) to the\ncombined Brown and TASA corpora4are shown in Table 27.1. We see that the HMM generally is\n4. The Brown corpus consists of 500 documents and 1,137,466 word tokens, with part-of-speech tags for each token.", "995": "27.4. ExtensionsofLDA 965\nAHMMBHMMBLDA\nzi,l\u22121 zi,l zi,l+1...yi,l\u22121 yi,l yi,l+1...qi,l\u22121 qi,l qi,l+1...\u03c0i\u03b1\nN\nFigure 27.13 LDA-HMM model.\n1.In contrast to this approach, we study here how the overall network activity can control single cell\nparameters such as input resistance, as well as time andspace constants, parameters that are crucial for\nexcitability andspariotemporal (sic) integration .\nThe integrated architecture in this paper combines feed forward control anderror feedback adaptive\ncontrol using neural networks.\n2.In other words, for our proof ofconvergence , we require the softassign algorithm toreturn adoubly\nstochastic matrix as *sinkhorn theorem guarantees that it will instead of a matrix which is merely close\nto being doubly stochastic based on some reasonable metric.\nThe aim is to construct a portfolio with a maximal expected return for a given risklevel andtime\nhorizon while simultaneously obeying *institutional or*legally required constraints.\n3.The left graph is the standard experiment the right from atraining with #samples .\nThe graphGis called the *guest graph, andHis called the host graph.\nFigure 27.14 Function and content words in the NIPS corpus, as distinguished by the LDA-HMM model.\nGraylevel indicates posterior probability of assignment to LDA component, with black being highest. The\nboxed word appears as a function word in one sentence, and as a content word in another sentence.Asterisked words had low frequency, and were treated as a single word type by the model. Source: Figure4 of (Griffiths et al. 2004). Used with kind permission of Tom Griffiths.", "996": "966 Chapter27. Latentvariablemodelsfordiscretedata\nthe the the the the a the the the\nblood , , of a the , , ,\n, and and , of of of a a\nof of of to , , a of in\nbody a in in in in and and game\nheart in land and to water in drink ball\nand trees to classes picture is story alcohol and\nin tree farmers government \ufb01lm and is to team\nto with for a image matter to bottle to\nis on farm state lens are as in play\nblood forest farmers government light water story drugs ball\nheart trees land state eye matter stories drug game\npressure forests crops federal lens molecules poem alcohol team\nbody land farm public image liquid characters people *\nlungs soil food local mirror particles poetry drinking baseball\noxygen areas people act eyes gas character person players\nvessels park farming states glass solid author effects football\narteries wildlife wheat national object substance poems marijuana player\n* area farms laws objects temperature life body \ufb01eld\nbreathing rain corn department lenses changes poet use basketball\nthe in he * be said can time ,\na for it new have made would way ;\nhis to you other see used will years (\nthis on they \ufb01rst make came could day :\ntheir with i same do went may part )\nthese at she great know found had number\nyour by we good get called must kind\nher from there small go do place\nmy as this little take have\nsome into who old \ufb01nd did\nTable 27.1 Upper row: Topics extracted by the LDA model when trained on the combined Brown and\nTASA corpora. Middle row: topics extracted by LDA part of LDA-HMM model. Bottom row: topics extracted\nby HMM part of LDA-HMM model. Each column represents a single topic/class, and words appear in orderof probability in that topic/class. Since some classes give almost all probability to only a few words, a listis terminated when the words account for 90% of the probability mass. Source: Figure 2 of (Griffiths et al.2004). Used with kind permission of Tom Griffiths.\nresponsible for syntactic words, and the LDA for semantics words. If we did not have the HMM,\nthe LDA topics would get \u201cpolluted\u201d by function words (see top of \ufb01gure), which is why suchwords are normally removed during preprocessing.\nThe model can also help disambiguate when the same word is being used syntactically or\nsemantically. Figure 27.14 shows some examples when the model was applied to the NIPScorpus.\n5We see that the roles of words are distinguished, e.g., \u201cwe require the algorithm to\nreturna matrix\u201d (verb) vs \u201cthe maximal expected return\u201d (noun). In principle, a part of speech\ntagger could disambiguate these two uses, but note that (1) the LDA-HMM method is fullyunsupervised (no POS tags were used), and (2) sometimes a word can have the same POS tag,but different senses, e.g., \u201cthe left graph\u201d (a synactic role) vs \u201cthe graph G\u201d (a semantic role).\nThe topic of probabilistic models for syntax and semantics is a vast one, which we do not\nThe TASA corpus is an untagged collection of educational materials consisting of 37,651 documents and 12,190,931 word\ntokens. Words appearing in fewer than 5 documents were replaced with an asterisk, but punctuation was included. The\ncombined vocabulary was of size 37,202 unique words.\n5. NIPS stands for \u201cNeural Information Processing Systems\u201d. It is one of the top machine learning conferences. The\nNIPS corpus volumes 1\u201312 contains 1713 documents.", "997": "27.4. ExtensionsofLDA 967\nByilqil\nci\u00afqi\nw\u03c0i\nN\u03b1\n(a)BA\nyilqilci\u03c0i\nN\u03b1\n(b)\nFigure 27.15 (a) Supervised LDA. (b) Discriminative LDA.\nhave space to delve into any more. See e.g., (Jurafsky and Martin 2008) for further information.\n27.4.4 Supervised LDA\nIn this section, we discuss extensions of LDA to handle side information of various kinds beyond\njust words.\n27.4.4.1 Generative supervised LDA\nSuppose we have a variable length sequence of words yil\u2208{1,...,V}as usual, but we also\nhave a class label ci\u2208{1,...,C}. How can we predict cifromyi? There are many possible\napproaches, but most are direct mappings from the words to the class. In some cases, suchassentiment analysis , we can get better performance by \ufb01rst performing inference, to try\nto disambiguate the meaning of words. For example, suppose the goal is to determine if adocument is a favorable review of a movie or not. If we encounter the phrase \u201cBrad Pitt wasexcellent until the middle of the movie\u201d, the word \u201cexcellent\u201d may lead us to think the review ispositive, but clearly the overall sentiment is negative.\nOne way to tackle such problems is to build a joint model of the form p(c\ni,yi|\u03b8). (Blei\nand McAuliffe 2010) proposes an approach, called supervised LDA, where the class label ciis\ngenerated from the topics as follows:\np(ci|qi) = Ber(sigm(wTqi)) (27.65)\nHereqiis the empirical topic distribution for document i:\nqik/defines1\nLiLi/summationdisplay\ni=1qilk (27.66)\nSee Figure 27.15(a) for an illustration.", "998": "968 Chapter27. Latentvariablemodelsfordiscretedata\nByilqil\u03c0ixi\nNW\n(a)Byilqil\u03c0ixi\nNWi\u03bc \u03a3\n(b)Byilqil\u03c0i\u03b1ixi\nNW\n(c)\nFigure 27.16 Discriminative variants of LDA. (a) Mixture of experts aka MR-LDA. The double ring denotes\na node that \u03c0ia deterministic function of its parents. (b) Mixture of experts with random effects. (c)\nDMR-LDA.\nWe can \ufb01t this model using Monte Carlo EM: run the collapsed Gibbs sampler in the E step, to\ncompute E[qik], and then use this as the input feature to a standard logistic regression package.\n27.4.4.2 Discriminative supervised LDA\nAn alternative approach, known as discriminative LDA (Lacoste-Julien et al. 2009), is shown in\nFigure 27.15(b). This is a discriminative model of the form p(yi|ci,\u03b8). The only change from\nregular LDA is that the topic prior becomes input dependent, as follows:\np(qil|\u03c0i,ci=c,\u03b8)=C a t (Ac\u03c0) (27.67)\nwhereAcis aK\u00d7Kstochastic matrix.\nSo far, we have assumed the \u201cside information\u201d is a single categorical variable ci.O f t e nw e\nhave high dimensional covariates xi\u2208RD. For example, consider the task of image tagging.\nThe idea is that yilrepresent correlated tags or labels, which we want to predict given xi.W e\nnow discuss several attempts to extend LDA so that it can generate tags given the inputs.\nThe simplest approach is to use a mixture of experts (Section 11.2.4) with multiple outputs.\nThis is just like LDA except we replace the Dirichlet prior on \u03c0iwith a deterministic function\nof the input:\n\u03c0i=S(Wxi) (27.68)\nIn (Law et al. 2010), this is called multinomial regression LDA. See Figure 27.16(a). Eliminating\nthe deterministic \u03c0iwe have\np(qil|xi,W)=C a t (S(Wxi)) (27.69)\nWe can \ufb01t this with EM in the usual way. However, (Law et al. 2010) suggest an alternative.\nFirst \ufb01t an unsupervised LDA model based only on yi; then treat the inferred \u03c0ias data, and", "999": "27.4. ExtensionsofLDA 969\n\ufb01t a multinomial logistic regression model mapping xito\u03c0i. Although this is fast, \ufb01tting LDA\nin an unsupervised fashion does not necessarily result in a discriminative set of latent variables,\nas discussed in (Blei and McAuliffe 2010).\nThere is a more subtle problem with this model. Since \u03c0iis a deterministic function of the\ninputs, it is effectively observed, rendering the qil(and hence the tags yil) independent. In other\nwords,\np(yi|xi,\u03b8)=Li/productdisplay\nl=1p(yil|xi,\u03b8)=Li/productdisplay\nl=1/summationdisplay\nkp(yil|qil=k,B)p(qil=k|xi,W) (27.70)\nThis means that if we observe the value of one tag, it will have no in\ufb02uence on any of theothers. This may explain why the results in (Law et al. 2010) only show negligible improvementover predicting each tag independently.\nOne way to induce correlations is to make Wa random variable. The resulting model is\nshown in Figure 27.16(b). We call this a random effects mixture of experts . We typically\nassume a Gaussian prior on W\ni.I fxi=1, thenp(qil|xi,wi)=C a t (S(wi)),s ow er e c o v e r\nthe correlated topic model. It is possible to extend this model by adding Markovian dynamicsto theq\nilvariables. This is called a conditional topic random \ufb01eld (Zhu and Xing 2010).\nA closely related approach, known as Dirichlet multinomial regression LDA (Mimno and\nMcCallum 2008), is shown in Figure 27.16(c). This is identical to standard LDA except we make\u03b1a function of the input\n\u03b1\ni=e x p (Wxi) (27.71)\nwhereWis aK\u00d7Dmatrix. Eliminating the deterministic \u03b1iwe have\n\u03c0i\u223cDir(exp(Wx i)) (27.72)\nUnlike (Law et al. 2010), this model allows information to \ufb02ow between tags via the latent \u03c0i.\nA variant of this model, where xicorresponds to a bag of discrete labels and \u03c0i\u223cDir(\u03b1\u2299\nxi), is known as labeled LDA (Ramage et al. 2009). In this case, the labels xiare in 1:1\ncorrespondence with the latent topics, which makes the resulting topics much more interpretable.An extension, known as partially labeled LDA (Ramage et al. 2011), allows each label to have\nmultiple latent sub-topics; this model includes LDA, labeled LDA and a multinomial mixturemodel as special cases.\n27.4.4.3 Discriminative categorical PCA\nAn alternative to using LDA is to expand the categorical PCA model with inputs, as shown inFigure 27.17(a). Since the latent space is now real-valued, we can use simple linear regressionfor the input-hidden mapping. For the hidden-output mapping, we use traditional catPCA:\np(z\ni|xi,V)=N (Vxi,\u03a3) (27.73)\np(yi|zi,W)=/productdisplay\nlCat(yil|S(Wzi)) (27.74)\nThis model is essentially a probabilistic neural network with one hidden layer, as shown inFigure 27.17(b), but with exchangeable output (e.g., to handle variable numbers of tags). The", "1000": "970 Chapter27. Latentvariablemodelsfordiscretedata\nyilzixi\nNWV\n(a)Wyi1... ...yiLizi1...ziKxi1... ...xiD\nNv1 vK\n(b)\nFigure 27.17 (a) Categorical PCA with inputs and exchangeable outputs. (b) Same as (a), but with the\nvector nodes expanded out.\nkey difference from a neural net is that information can \ufb02ow between the yil\u2019s via the latent\nbottleneck layer zi. This should work better than a conventional neural net when the output\nlabels are highly correlated, even after conditioning on the features; this problem frequently\narises in multi label classi\ufb01cation. Note that we could allow a direct xitoyiarc, but this would\nrequire too many parameters if the number of labels is large.6\nWe can \ufb01t this model with a small modi\ufb01cation of the variational EM algorithm in Section 12.4.\nIf we use this model for regression, rather than classi\ufb01cation, we can perform the E step exactly,by modifying the EM algorithm for factor analysis. (Ma et al. 1997) reports that this methodconverges faster than standard backpropagation.\nWe can also extend the model so that the prior on z\niis a mixture of Gaussians using input-\ndependent means. If the output is Gaussian, this corresponds to a mixture of discriminativefactor analysers (Fokoue 2005; Zhou and Liu 2008). If the output is categorical, this would bean (as yet unpublished) model, which we could call \u201cdiscriminative mixtures of categorical factoranalyzers\u201d.\n27.5 LVMs for graph-structured data\nAnother source of discrete data is when modeling graph or network structures. To see theconnection, recall that any graph on Dnodes can be represented as a D\u00d7Dadjacency\nmatrixG,w h e r e G(i,j)=1iff there is an edge from node ito nodej. Such matrices are\nbinary, and often very sparse. See Figure 27.19 for an example.\nGraphs arise in many application areas, such as modeling social networks, protein-protein\ninteraction networks, or patterns of disease transmission between people or animals. There areusually two primary goals when analysing such data: \ufb01rst, try to discover some \u201cinteresting\n6. A non-probabilistic version of this idea, using squared loss, was proposed in (Ji et al. 2010). This is similar to a linear\nfeed-forward neural network with an additional edge from xidirectly to yi.", "1001": "27.5. LVMsforgraph-structureddata 971\n789456123\n(a)978235164\n(b)\nFigure 27.18 (a) A directed graph. (b) The same graph, with the nodes partitioned into 3 groups, making\nthe block structure more apparent.\n1\n2\n3\n4\n5\n6\n7\n8\n9123456789\n(a)\n0.1\n 0.9\n 0.1\n0.1\n 0.1\n 0.9\n0.9\n 0.1\n 0.1\n1\n6\n4\n8\n2\n3\n5\n9\n7164823597\nRz97382\n5164\u03b7\n(b)\nFigure 27.19 (a) Adjacency matrix for the graph in Figure 27.18(a). (b) Rows and columns are shown\npermuted to show the block structure. We also sketch of how the stochastic block model can generate this\ngraph. From Figure 1 of (Kemp et al. 2006). Used with kind permission of Charles Kemp.\nstructure\u201d in the graph, such as clusters or communities; second, try to predict which links\nmight occur in the future (e.g., who will make friends with whom). Below we summarize some\nmodels that have been proposed for these tasks, some of which are related to LDA. Futher details\non these and other approaches can be found in e.g., (Goldenberg et al. 2009) and the references\ntherein.\n27.5.1 Stochastic block model\nIn Figure 27.18(a) we show a directed graph on 9 nodes. There is no apparent structure. However,\nif we look more deeply, we see it is possible to partition the nodes into three groups or blocks,\nB1={1,4,6},B2={2,3,5,8}, andB3={7,9}, such that most of the connections go from\nnodes in B1toB2,o rf r o m B2toB3,o rf r o m B3toB1. This is illustrated in Figure 27.18(b).", "1002": "972 Chapter27. Latentvariablemodelsfordiscretedata\nA\nD C B A\nDCBAE CD AB F G H\nA\nB\nC\nD\nE\nGF\nH\nCD\nD C B A\nDCB\nRelational system Sorted matrix\nE D C B A\nEDCBAB\nDA\nECE\nGHFBA\nCA\nDBC\nDBA\nFigure 27.20 Some examples of graphs generated using the stochastic block model with different kinds\nof connectivity patterns between the blocks. The abstract graph (between blocks) represent a ring, a\ndominance hierarchy, a common-cause structure, and a common-effect structure. From Figure 4 of (Kemp\net al. 2010). Used with kind permission of Charles Kemp.\nThe problem is easier to understand if we plot the adjacency matrices. Figure 27.19(a) shows\nthe matrix for the graph with the nodes in their original ordering. Figure 27.19(b) shows the\nmatrix for the graph with the nodes in their permtuted ordering. It is clear that there is block\nstructure.\nWe can make a generative model of block structured graphs as follows. First, for every\nnode, sample a latent block qi\u223cCat(\u03c0),w h e r e\u03c0kis the probability of choosing block k,f o r\nk=1:K. Second, choose the probability of connecting group ato group b, for all pairs of\ngroups; let us denote this probability by \u03b7a,b. This can come from a beta prior. Finally, generate\neach edge Rijusing the following model:\np(Rij=r|qi=a,qj=b,\u03b7)=B e r (r|\u03b7a,b) (27.75)\nThis is called the stochastic block model (Nowicki and Snijders 2001). Figure 27.21(a) illustrates\nthe model as a DGM, and Figure 27.19(c) illustrates how this model can be used to cluster the\nnodes in our example.\nNote that this is quite different from a conventional clustering problem. For example, we\nsee that all the nodes in block 3 are grouped together, even though there are no connections\nbetween them. What they share is the property that they \u201clike to\u201d connect to nodes in block 1,\nand to receive connections from nodes in block 2. Figure 27.20 illustrates the power of the model\nfor generating many different kinds of graph structure. For example, some social networks have\nhierarchical structure, which can be modeled by clustering people into different social strata,\nwhereas others consist of a set of cliques.\nUnlike a standard mixture model, it is not possible to \ufb01t this model using exact EM, because\nall the latent qivariables become correlated. However, one can use variational EM (Airoldi et al.", "1003": "27.5. LVMsforgraph-structureddata 973\nqi\n\u03b7a,bRi,j\nI\u03c0qj\nJ\n(a)qi\u2192j\n\u03b7a,bRi,j\nI\u03c0iqi\u2190j\u03c0j\u03b1\nJ\n(b)\nFigure 27.21 (a) Stochastic block model. (b) Mixed membership stochastic block model.\n2008), collapsed Gibbs sampling (Kemp et al. 2006), etc. We omit the details (which are similar\nto the LDA case).\nIn (Kemp et al. 2006), they lifted the restriction that the number of blocks Kbe \ufb01xed, by\nreplacing the Dirichlet prior on \u03c0by a Dirichlet process (see Section 25.2.2). This is known as\nthe in\ufb01nite relational model. See Section 27.6.1 for details.\nIf we have features associated with each node, we can make a discriminative version of this\nmodel, for example by de\ufb01ning\np(Rij=r|qi=a,qj=b,xi,xj,\u03b8)=B e r (r|wT\na,bf(xi,xj)) (27.76)\nwheref(xi,xj)is some way of combining the feature vectors. For example, we could use\nconcatenation, [xi,xj], or elementwise product xi\u2297xjas in supervised LDA. The overall\nmodel is like a relational extension of the mixture of experts model.\n27.5.2 Mixed membership stochastic block model\nIn (Airoldi et al. 2008), they lifted the restriction that each node only belong to one cluster. Thatis, they replaced q\ni\u2208{1,...,K}with\u03c0i\u2208SK. This is known as the mixed membership\nstochastic block model, and is similar in spirit to fuzzy clustering orsoft clustering.N o t e\nthat\u03c0ikis not the same as p(zi=k|D); the former represents ontological uncertainty (to\nwhat degree does each object belong to a cluster) wheras the latter represents epistemological\nuncertainty (which cluster does an object belong to). If we want to combine epistemological\nand ontological uncertainty, we can compute p(\u03c0i|D).\nIn more detail, the generative process is as follows. First, each node picks a distribution over\nblocks,\u03c0i\u223cDir(\u03b1). Second, choose the probability of connecting group ato group b, for all\npairs of groups, \u03b7a,b\u223c\u03b2(\u03b1,\u03b2). Third, for each edge, sample two discrete variables, one for\neach direction:\nqi\u2192j\u223cCat(\u03c0 i),qi\u2190j\u223cCat(\u03c0 j) (27.77)\nFinally, generate each edge Rijusing the following model:\np(Rij=1|qi\u2192j=a,qi\u2190j=b,\u03b7)=\u03b7a,b (27.78)", "1004": "974 Chapter27. Latentvariablemodelsfordiscretedata\n(a)   1    2   3\n   4   5   6   7\n   8   9\n   10   11   12\n   13\n   14   15\n   16   17\n   18Outcasts\nLoyal\nOpposition Young\nTurksWaverers1 Ambrose\n2 Boniface\n3 Mark4 Winfrid5 Elias6 Basil7 Simplicius8 Berthold9 John Bosco10 Victor11 Bonaventure12 Amand13 Louis14 Albert15Ramuald16 Peter17 Gregory18 Hugh\n(b)\nFigure 27.22 (a) Who-likes-whom graph for Sampson\u2019s monks. (b) Mixed membership of each monk in\none of three groups. From Figures 2-3 of (Airoldi et al. 2008). Used with kind permission of Edo Airoldi.\nSee Figure 27.21(b) for the DGM.\nUnlike the regular stochastic block model, each node can play a different role, depending on\nwho it is connecting to. As an illustration of this, we will consider a data set that is widely usedin the social networks analysis literature. The data concerns who-likes-whom amongst of groupof 18 monks. It was collected by hand in 1968 by Sampson (Sampson 1968) over a period ofmonths. (These days, in the era of social media such as Facebook, a social network with only 18people is trivially small, but the methods we are discussing can be made to scale.) Figure 27.22(a)plots the raw data, and Figure 27.22(b) plots E[\u03c0]\nifor each monk, where K=3. We see that\nmost of the monk belong to one of the three clusters, known as the \u201cyoung turks\u201d, the \u201coutcasts\u201dand the \u201cloyal opposition\u201d. However, some individuals, notably monk 15, belong to two clusters;Sampson called these monks the \u201cwaverers\u201d. It is interesting to see that the model can recoverthe same kinds of insights as Sampson derived by hand.\nOne prevalent problem in social network analysis is missing data. For example, if R\nij=0,\nit may be due to the fact that person iandjhave not had an opportunity to interact, or\nthat data is not available for that interaction, as opposed to the fact that these people don\u2019twant to interact. In other words, absence of evidence is not evidence of absence. We can model\nthis by modifying the observation model so that with probability \u03c1, we generate a 0 from the\nbackground model, and we only force the model to explain observed 0s with probability 1\u2212\u03c1.\nIn other words, we robustify the observation model to allow for outliers, as follows:\np(R\nij=r|qi\u2192j=a,qi\u2190j=b,\u03b7)=\u03c1\u03b40(r)+(1\u2212\u03c1)Ber(r|\u03b7a,b) (27.79)\nSee (Airoldi et al. 2008) for details.\n27.5.3 Relational topic model\nIn many cases, the nodes in our network have atttributes. For example, if the nodes representacademic papers, and the edges represent citations, then the attributes include the text of thedocument itself. It is therefore desirable to create a model that can explain the text and the linkstructure concurrently. Such a model can predict links given text, or even vice versa.\nTherelational topic model (RTM) (Chang and Blei 2010) is one way to do this. This is a", "1005": "27.6. LVMsforrelationaldata 975\nyilqil\u03c0i\n\u00afqiRij\nI\u00afqjqjlyjl\n\u03c0j\nJ\u03b1B\nw\nFigure 27.23 DGM for the relational topic model.\nsimple extension of supervised LDA (Section 27.4.4.1), where the response variable Rij(which\nrepresents whether there is an edge between nodes iandj) is modeled as follows:\np(Rij=1|qi,qj,\u03b8) = sigm(wT(qi\u2297qj)+w0) (27.80)\nRecall that qiis the empirical topic distribution for document i,qik/defines1\nLi/summationtextLi\ni=1qilk. See\nFigure 27.23\nNote that it is important that Rijdepend on the actual topics chosen, qiandqj, and not\non the topic distributions, \u03c0iand\u03c0j, otherwise predictive performance is not as good. The\nintuitive reason for this is as follows: if Rijis a child of \u03c0iand\u03c0j, it will be treated as just\nanother word, similar to the qil\u2019s andyil\u2019s; but since there are many more words than edges,\nthe graph structure information will get \u201cwashed out\u201d. By making Rija child of qiandqj, the\ngraph information can in\ufb02uence the choice of topics more directly.\nOne can \ufb01t this model in a manner similar to SLDA. See (Chang and Blei 2010) for details.\nThe method does better at predicting missing links than the simpler approach of \ufb01rst \ufb01tting an\nLDA model, and then using the qi\u2019s as inputs to a logistic regression problem. The reason is\nanalogous to the superiority of partial least squares (Section 12.5.2) to PCA+ linear regression,namely that the RTM learns a latent space that is forced to be predictive of the graph structureand words, whereas LDA might learn a latent space that is not useful for predicting the graph.\n27.6 LVMs for relational data\nGraphs can be used to represent data which represents the relation amongst variables of acertain type, e.g., friendship relationships between people. But often we have multiple types ofobjects, and multiple types of relations. For example, Figure 27.24 illustrates two relations, onebetween people and people, and one between people and movies.\nIn general, we de\ufb01ne a k-aryrelationRas a subset of k-tuples of the appropriate types:\nR\u2286T\n1\u00d7T2\u00d7\u00b7\u00b7\u00b7\u00d7T k (27.81)", "1006": "976 Chapter27. Latentvariablemodelsfordiscretedata\nFigure 27.24 Example of relational data. There are two types of objects, peopleandmovies; one 2-ary\nrelation,friends:people\u00d7people\u2192{0,1}and one 2-ary function, rates:people\u00d7movie\u2192R.Ageand\nsexare attributes (unary functions) of the peopleclass.\nwhereTiare sets or types. A binary, pairwise or dyadic relation is a relation de\ufb01ned on pairs\nof objects. For example, the seenrelation between people and movies might be represented as\nthe set of movies that people have seen. We can either represent this explicitly as a set, such as\nseen = { (Bob, StarWars), (Bob, TombRaider), (Alice, Jaws)}\nor implicitly, using an indicator function for the set:\nseen(Bob, StarWars)=1, seen(Bob, TombRaider)=1, seen(Alice, Jaws)=1\nA relation between two entities of types T1andT2can be represented as a binary function\nR:T1\u00d7T2\u2192{0,1}, and hence as a binary matrix. This can also be represented as a bipartite\ngraph, in which we have nodes of two types. If T1=T2, this becomes a regular directed graph,\nas in Section 27.5. However, there are some situations that are not so easily modelled by graphs,\nbut which can still be modelled by relations. For example, we might have a ternary relation,\nR:T1\u00d7T1\u00d7T2\u2192{0,1}, where, say, R(i,j,k)=1iff protein iinteracts with protein j\nwhen chemical kis present. This can be modelled by a 3d binary matrix. We will give some\nexamples of this in Section 27.6.1.\nMaking probabilistic models of relational data is called statistical relational learning (Getoor\nand Taskar 2007). One approach is to directly model the relationship between the variables using\ngraphical models; this is known as probabilistic relational modeling . Another approach is to\nuse latent variable models, as we discuss below.\n27.6.1 In\ufb01nite relational model\nIt is straightforward to extend the stochastic block model to model relational data: we just\nassociate a latent variable qt\ni\u2208{1,...,K t}with each entity iof each type t. We then de\ufb01ne\nthe probability of the relation holding between speci\ufb01c entities by looking up the probability of\nthe relation holding between entities of that type. For example, if R:T1\u00d7T1\u00d7T2\u2192{0,1},\nwe have\np(R(i,j,k)=1|q1\ni=a,q1\nj=b,q2\nk=c,\u03b7)=\u03b7a,b,c (27.82)\nIf we allow the number of clusters Ktfor each type to be unbounded, by using a Dirichlet pro-\ncess, the model is called the in\ufb01nite relational model (IRM) (Kemp et al. 2006). An essentially", "1007": "27.6. LVMsforrelationaldata 977\n/g4/g9/g6/g12/g10/g4/g2/g11/g16\n/g5/g10/g16/g6/g2/g16/g6/g16\n/g16/g10/g8/g13/g16 /g14/g15/g8/g2/g13/g10/g16/g12/g16/g3/g10/g14\u0001/g2/g4/g17/g10/g19/g6/g1/g16/g18/g3/g16/g17/g2/g13/g4/g6/g16\n/g3/g10/g14/g11/g14/g8/g10/g4/g2/g11/g1/g7/g18/g13/g4/g17/g10/g14/g13/g16\n/g2/g3/g13/g14/g15/g12/g2/g11/g10/g17/g10/g6/g16causes,\nresult ofaffects,\ncomplicates\ndisrupts\nresult of, manifestation ofaffects, process of,\naffects,\nprocess of\nassociated withcauses\nmanifestation ofaffects\naffects,process of,\nresult ofaffects,\nresult ofcomplicates,\nmanifestation ofinteract with\nprocess of,\nresult of\naffects,\nprocess ofcauses,\nmanifestation of,associated withcomplicatesaffects,causes\nmanifestation of,complicatesaffects,\nFigure 27.25 Illustration of an ontology learned by IRM applied to the Uni\ufb01ed Medical Language System.\nThe boxes represent 7 of the 14 concept clusters. Predicates that belong to the same cluster are grouped\ntogether, and associated with edges to which they pertain. All links with weight above 0.8 have beenincluded. From Figure 9 of (Kemp et al. 2010). Used with kind permission of Charles Kemp.\nidentical model, under the name in\ufb01nite hidden relational model (IHRM), was concurrently\nproposed in (Xu et al. 2006). We can \ufb01t this model with variational Bayes (Xu et al. 2006, 2007)\nor collapsed Gibbs sampling (Kemp et al. 2006). Rather than go into algorithmic detail, we justsketch some interesting applications.\n27.6.1.1 Learning ontologies\nAnontology refers to an organisation of knowledge. In AI, ontologies are often built by hand\n(see e.g., (Russell and Norvig 2010)), but it is interesting to try and learn them from data. In(Kemp et al. 2006), they show how this can be done using the IRM.\nThe data comes from the Uni\ufb01ed Medical Language System (McCray 2003), which de\ufb01nes\na semantic network with 135 concepts (such as \u201cdisease or syndrome\u201d, \u201cdiagnostic procedure\u201d,\u201canimal\u201d), and 49 binary predicates (such as \u201caffects\u201d, \u201cprevents\u201d). We can represent this as aternary relation R:T\n1\u00d7T1\u00d7T2\u2192{0,1},w h e r e T1is the set of concepts and T2is the\nset of binary predicates. The result is a 3d cube. We can then apply the IRM to partition thecube into regions of roughly homogoneous response. The system found 14 concept clusters and21 predicate clusters. Some of these are shown in Figure 27.25. The system learns, for example,that biological functions affect organisms (since \u03b7\na,b,c\u22481wherearepresents the biological\nfunction cluster, brepresents the organism cluster, and crepresents the affects cluster).\n27.6.1.2 Clustering based on relations and features\nWe can also use IRM to cluster objects based on their relations and their features. For example,(Kemp et al. 2006) consider a political dataset (from 1965) consisting of 14 countries, 54 binary", "1008": "978 Chapter27. Latentvariablemodelsfordiscretedata\njoint\nmembership\nof IGOs\njoint\nmembership\nof NGOs\nnegative\nbehavior\nnegative\ncommunications\n accusations\n protests\ntreaties\n conferences\ncommon bloc\nmembership\neconomic\naid\n emigration\nmilitary\nalliance\nsends\ntourists to\nexports\nbooks to\n exports to\nBrazil\nNetherlands\nUK\nUSA\nBurma\nIndonesia\nJordan\nEgypt\nIndia\nIsrael\nChina\nCuba\nPoland\nUSSR\nnoncommunist\nwestern bloc\nconstitutional govt\nfree elections\ncommunist bloc\ncommunists\ntotalitarian\nelitist\nhigh censorship\nno free elections\nilliteracy\ndomestic violence\npurges\nfar from US\nrainfall\nreligious books\nexports/GNP\ngovt education $\nmilitary personnel\nseaborne goods\ngovt crisis\nUN delinquent\nneutral bloc\nassassinations\ngovt revolution\nnum religions\nintervening military\nsome censorship\nenergy consumed\ntelephone\npopulation\ndefense $\nthreats\nGNP\nprotests\nCatholics\nUS aid taken\npopn. density\nland area\nrailroad length\nforeign students\nage of country\nlaw NGOs\nnum languages\naid $ taken\nfemale workers\nforeign mail sent\nprotein in diet\ninvestments\narts NGOs\nmonarchy\nroad length\narable\nemigrants\nunemployed\ncalories in dieta)\nc) d)\ne) g) h) i)b)\nbook\ntranslationsf)\nFigure 27.26 Illustration of IRM applied to some political data containing features and pairwise interac-\ntions. Top row (a). the partition of the countries into 5 clusters and the features into 5 clusters. Every\nsecond column is labelled with the name of the corresponding feature. Small squares at bottom (a-i): these\nare 8 of the 18 clusters of interaction types. From Figure 6 of (Kemp et al. 2006). Used with kind permission\nof Charles Kemp.\npredicates representing interaction types between countries (e.g., \u201csends tourists to\u201d, \u201ceconomic\naid\u201d), and 90 features (e.g., \u201ccommunist\u201d, \u201cmonarchy\u201d). To create a binary dataset, real-valued\nfeatures were thresholded at their mean, and categorical variables were dummy-encoded. The\ndata has 3 types: T1represents countries, T2represents interactions, and T3represents features.\nWe have two relations: R1:T1\u00d7T1\u00d7T2\u2192{0,1}, andR2:T1\u00d7T3\u2192{0,1}. (This problem\ntherefore combines aspects of both the biclustering model and the ontology discovery model.)\nWhen given multiple relations, the IRM treats them as conditionally independent. In this case,\nwe have\np(R1,R2,q1,q2,q3|\u03b8)=p(R1|q1,q2,\u03b8)p(R2|q1,q3,\u03b8) (27.83)\nThe results are shown in Figure 27.26. The IRM divides the 90 features into 5 clusters, the\n\ufb01rst of which contains \u201cnoncommunist\u201d, which captures one of the most important aspects of\nthis Cold-War era dataset. It also clusters the 14 countries into 5 clusters, re\ufb02ecting natural\ngeo-political groupings (e.g., US and UK, or the Communist Bloc), and the 54 predicates into 18\nclusters, re\ufb02ecting similar relationships (e.g., \u201cnegative behavior and \u201caccusations\u201d).", "1009": "27.6. LVMsforrelationaldata 979\n27.6.2 Probabilistic matrix factorization for collaborative \ufb01ltering\nAs discussed in Section 1.3.4.2, collaborative \ufb01ltering (CF) requires predicting entries in a matrix\nR:T1\u00d7T2\u2192R, where for example R(i,j)is the rating that user igave to movie j. Thus\nwe see that CF is a kind of relational learning problem (and one with particular commercialimportance).\nMuch of the work in this area makes use of the data that Net\ufb02ix made available in their\ncompetition. In particular, a large 17,770 \u00d7480,189 movie x user ratings matrix is provided. The\nfull matrix would have \u223c8.6\u00d710\n9entries, but only 100,480,507 (about 1%) of the entries are\nobserved, so the matrix is extremely sparse. In addition the data is quite imbalanced, with manyusers rating fewer than 5 movies, and a few users rating over 10,000 movies. The validationset is 1,408,395 (movie,user) pairs. Finally, there is a separate test set with 2,817,131 (movie,user)pairs, for which the ranking is known but withheld from contestants. The performance measureis root mean square error:\nRMSE=/radicaltp/radicalvertex/radicalvertex/radicalbt\n1\nNN/summationdisplay\ni=1(X(mi,ui)\u2212\u02c6X(mi,ui))2 (27.84)\nwhereX(mi,ui)is the true rating of user uion movie mi, and\u02c6X(mi,ui)is the prediction.\nThe baseline system, known as Cinematch, had an RMSE on the training set of 0.9514, and on\nthe test set of 0.9525. To qualify for the grand prize, teams needed to reduce the test RMSE by10%, i.e., get a test RMSE of 0.8563 or less. We will discuss some of the basic methods used bytthe winning team below.\nSince the ratings are drawn from the set {0,1,2,3,4,5}, it is tempting to use a categorical\nobservation model. However, this does not capture the fact that the ratings are ordered. Althoughwe could use an ordinal observation model, in practice people use a Gaussian observation modelfor simplicity. One way to make the model better match the data is to pass the model\u2019s predictedmean response through a sigmoid, and then to map the [0,1]interval to [0,5](Salakhutdinov\nand Mnih 2008). Alternatively we can make the data a better match to the Gaussian model bytransforming the data using R\nij=/radicalbig\n6\u2212Rij(Aggarwal and Merugu 2007).\nWe could use the IRM for the CF task, by associating a discrete latent variable for each user\nqu\niand for each movie or video qv\nj, and then de\ufb01ning\np(Rij=r|qu\ni=a,qv\nj=b,\u03b8)=N(r|\u03bca,b,\u03c32) (27.85)\nThis is just another example of co-clustering. We can also extend the model to generate sideinformation, such as attributes about each user and/or movie. See Figure 27.27 for an illustration.\nAnother possibility is to replace the discrete latent variables with continuous latent variables\n\u03c0\nu\ni\u2208SKuand\u03c0vj\u2208SKv. However, it has been found (see e.g., (Shan and Banerjee 2010)) that\none obtains much better results by using unconstrained real-valued latent factors for each user\nui\u2208RKand each movie vj\u2208RK.7We then use a likelihood of the form\np(Rij=r|ui,vj)=N(r|uT\nivj,\u03c32) (27.86)\n7. Good results with discrete latent variables have been obtained on some datasets that are smaller than Net\ufb02ix, such as\nMovieLens and EachMovie. However, these datasets are much easier to predict, because there is less imbalance between\nthe number of reviews performed by different users (in Net\ufb02ix, some users have rated more than 10,000 movies, whereas\nothers have rated less than 5).", "1010": "980 Chapter27. Latentvariablemodelsfordiscretedata\nFigure 27.27 Visualization of a small relational dataset, where we have one relation, likes(user, movie),\nand features for movies (here, genre) and users (here, occupation). From Figure 5 of (Xu et al. 2008). Used\nwith kind permission of Zhao Xu.\nRijui\u03bcu\n\u03a3uT1vj\nT2\u03bcv\u03a3v\n\u03c32\n(a)\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\nFactor Vector 1Factor Vector 2Freddy Got Fingered\nFreddy vs. JasonHalf Baked\nRoad Trip\nThe Sound of MusicSophie\u2019s Choice\nMoonstruck\nMaid in ManhattanThe Way We Were\nRunaway Bride Coyote UglyThe Royal TenenbaumsPunch\u2212Drunk Love\nI Heart Huckabees\nArmageddonCitizen Kane\nThe Waltons: Season 1\nStepmomJulien Donkey\u2212Boy\nSister Act The Fast and the FuriousThe Wizard of OzKill Bill: Vol. 1\nScarfaceNatural Born KillersAnnie Hall Belle de JourLost in Translation\nThe Longest YardBeing John Malkovich\nCatwoman\n(b)\nFigure 27.28 (a) A DGM for probabilistic matrix factorization. (b) Visualization of the \ufb01rst two factors in\nthe PMF model estimated from the Net\ufb02ix challenge data. Each movie jis plotted at the location speci\ufb01ed\n\u02c6vj. On the left we have low-brow humor and horror movies ( Half Baked ,Freddy vs Jason ), and on the\nright we have more serious dramas ( Sophie\u2019s Choice ,Moonstruck ). On the top we have critically acclaimed\nindependent movies ( Punch-Drunk Love ,I Heart Huckabees ), and on the bottom we have mainstream\nHollywood blockbusters ( Armageddon ,RunwayBride ). TheWizardofOz is right in the middle of these axes.\nFrom Figure 3 of (Koren et al. 2009). Used with kind permission of Yehuda Koren.\nThis has been called probabilistic matrix factorization (PMF) (Salakhutdinov and Mnih 2008).\nSee Figure 27.28(a) for the DGM. The intuition behind this method is that each user and each\nmovie get embedded into the same low-dimensional continuous space (see Figure 27.28(b)). If a\nuser is close to a movie in that space, they are likely to rate it highly. All of the best entries in\nthe Net\ufb02ix competition used this approach in one form or another.8\nPMF is closely related to the SVD. In particular, if there is no missing data, then computing\nthe MLE for the ui\u2019s and the vj\u2019s is equivalent to \ufb01nding a rank Kapproximation to R.\nHowever, as soon as we have missing data, the problem becomes non-convex, as shown in\n8. The winning entry was actually an ensemble of different methods, including PMF, nearest neighbor methods, etc.", "1011": "27.6. LVMsforrelationaldata 981\n0 5 10 15 20 25 30 35 40 45 50 55 600.90.910.920.930.940.950.960.97\nEpochsRMSE\nPMF\nConstrained \nPMFNetflix Baseline Score\nSVD\n(a)10 100 1000 10000 1000000.8750.880.8850.890.8950.90.9050.9140\n60\n90\n128\n180 50\n100200\n50\n100\n200\n50\n100200500 1000\n1500\nMillions of ParametersRMSEPlain\nw/Biases\nw/Implicit feedback\nw/Temporal dynamics\n(b)\nFigure 27.29 (a) RMSE on the validation set for different PMF variants vs number of passes through\nthe data. \u201cSVD\u201d is the unregularized version, \u03bbU=\u03bbV=0. \u201cPMF1\u201d corresponds to \u03bbU=0.01and\n\u03bbV=0.001, while \u201cPMF2\u201d corresponds to \u03bbU=0.001and\u03bbV=0.0001. \u201cPMFA1\u201d corresponds to a\nversion where the mean and diagonal covariance of the Gaussian prior were learned from data. From\nFigure 2 of (Salakhutdinov and Mnih 2008). Used with kind permission of Ruslan Salakhutdinov. (b) RMSEon the test set (quiz portion) vs number of parameters for several different models. \u201cPlain\u201d is the baselinePMF with suitably chosen \u03bb\nU,\u03bbV. \u201cWith biases\u201d adds fiandgjoffset terms. \u201cWith implicit feedback\u201d\n\u201cWith temporal dynamics\u201d allows the offset terms to change over time. The Net\ufb02ix baseline system achievesan RMSE of 0.9514, and the grand prize\u2019s required accuracy is 0.8563 (which was obtained on 21 September2009). Figure generated by netflixResultsPlot . From Figure 4 of (Koren et al. 2009). Used with kind\npermission of Yehuda Koren.\n(Srebro and Jaakkola 2003), and standard SVD methods cannot be applied. (Recall that in the\nNet\ufb02ix challenge, only about 1% of the matrix is observed.)\nThe most straightforward way to \ufb01t the PMF model is to minimize the overall NLL:\nJ(U,V)=\u2212logp(R|U,V,O)=\u2212log\u239b\n\u239dN/productdisplay\ni=1M/productdisplay\nj=1/bracketleftbig\nN(Rij|uT\nivj,\u03c32)/bracketrightbigI(Oij=1)\u239e\n\u23a0(27.87)\nwhereOij=1if userihas seen movie j. Since this is non-convex, we can just \ufb01nd a locally\noptimal MLE. Since the Net\ufb02ix data is so large (about 100 million observed entries), it is common\nto use stochastic gradient descent (Section 8.5.2) for this task. The gradient for uiis given by\ndJ\ndui=d\ndui1\n2/summationdisplay\nijI(Oij=1 ) (Rij\u2212uT\nivj)2=\u2212/summationdisplay\nj:Oij=1eijvj (27.88)\nwhereeij=Rij\u2212uT\nivjis the error term. By stochastically sampling a single movie jthat user\nihas watched, the update takes the following simple form:\nui=ui+\u03b7eijvj (27.89)\nwhere\u03b7is the learning rate. The update for vjis similar.", "1012": "982 Chapter27. Latentvariablemodelsfordiscretedata\nOf course, just maximizing the likelihood results in over\ufb01tting, as shown in Figure 27.29(a).\nWe can regularize this by imposing Gaussian priors:\np(U,V)=/productdisplay\niN(ui|\u03bcu,\u03a3u)/productdisplay\njN(vj|\u03bcv,\u03a3v) (27.90)\nIf we use \u03bcu=\u03bcv=0,\u03a3u=\u03c32\nUIK, and\u03a3v=\u03c32\nVIK, the new objective becomes\nJ(U,V)=\u2212 logp(R,U,V|O,\u03b8) (27.91)\n=/summationdisplay\ni/summationdisplay\njI(Oij=1 ) (Rij\u2212uT\nivj)2\n+\u03bbU/summationdisplay\ni||ui||22+\u03bbV/summationdisplay\nj||vj||22+const (27.92)\nwhere we have de\ufb01ned \u03bbU=\u03c32/\u03c32\nUand\u03bbV=\u03c32/\u03c32\nV. By varying the regularizers, we can\nreduce the effect of over\ufb01tting, as shown in Figure 27.29(a). We can \ufb01nd MAP estimates using\nstochastic gradient descent. We can also compute approximate posteriors using variational Bayes(Ilin and Raiko 2010).\nIf we use diagonal covariances for the priors, we can penalize each latent dimension by a\ndifferent amount. Also, if we use non-zero means for the priors, we can account for offset terms.Optimizing the prior parameters (\u03bc\nu,\u03a3u,\u03bcv,\u03a3v)at the same time as the model parameters\n(U,V,\u03c32)is a way to create an adaptive prior. This avoids the need to search for the optimal\nvalues of \u03bbUand\u03bbV, and gives even better results, as shown in Figure 27.29(a).\nIt turns out that much of the variation in the data can be explained by movie-speci\ufb01c or\nuser-speci\ufb01c effects. For example, some movies are popular for all types of users. And someusers give low scores for all types of movies. We can model this by allowing for user and moviespeci\ufb01c offset or bias terms as follows:\np(R\nij=r|ui,vj,\u03b8)=N(r|uT\nivj+\u03bc+fi+gj,\u03c32) (27.93)\nwhere\u03bcis the overall mean, fiis the user bias, gjis the movie bias, and uT\nivjis the\ninteraction term. This is equivalent to applying PMF just to the residual matrix, and gives much\nbetter results, as shown in Figure 27.29(b). We can estimate the fi,gjand\u03bcterms using\nstochastic gradient descent, just as we estimated U,Vand\u03b8.\nWe can also allow the bias terms to evolve over time, to re\ufb02ect the changing preferences of\nusers (Koren 2009b). This is important since in the Net\ufb02ix competition, the test data was morerecent than the training data. Figure 27.29(b) shows that allowing for temporal dynamics canhelp a lot.\nOften we also have side information of various kinds. In the Net\ufb02ix competition, entrants\nknew which movies the user had rated in the test set, even though they did not know thevalues of these ratings. That is, they knew the value of the (dense) Omatrix even on the\ntest set. If a user chooses to rate a movie, it is likely because they have seen it, which inturns means they thought they would like it. Thus the very act of rating reveals information.Conversely, if a user chooses not rate a movie, it suggests they knew they would not like it.So the data is not missing at random (see e.g., (Marlin and Zemel 2009)). Exploiting this canimprove performance, as shown in Figure 27.29(b). In real problems, information on the test setis not available. However, we often know which movies the user has watched or declined to", "1013": "27.7. RestrictedBoltzmannmachines(RBMs) 983\nwatch, even if they did not rate them (this is called implicit feedback), and this can be used as\nuseful side information.\nAnother source of side information concerns the content of the movie, such as the movie\ngenre, the list of the actors, or a synopsis of the plot. This can be denoted by xv, the features\nof the video. (In the case where we just have the id of the video, we can treat xvas a|V|-\ndimensional bit vector with just one bit turned on.) We may also know features about the user,\nwhich we can denote by xu. In some cases, we only know if the user clicked on the video or\nnot, that is, we may not have a numerical rating. We can then modify the model as follows:\np(R(u,v)|xu,xv,\u03b8)=B e r (R(u,v)|(Uxu)T(Vxv)) (27.94)\nwhereUis a|U|\u00d7Kmatrix, and Vis a|V|\u00d7Kmatrix (we can incorporate an offset term\nby appending a 1 to xuandxvin the usual way). A method for computing the approximate\nposterior p(U,V|D)in an online fashion, using ADF and EP, was described in (Stern et al.\n2009). This was implemented by Microsoft and has been deployed to predict click through rateson all the ads used by Bing.\nUnfortunately, \ufb01tting this model just from positive binary data can result in an over prediction\nof links, since no negative examples are included. Better performance is obtained if one hasaccess to the set of all videos shown to the user, of which at most one was picked; data of thisform is known as an impression log. In this case, we can use a multinomial model instead of\na binary model; in (Yang et al. 2011), this was shown to work much better than a binary model.To understand why, suppose some is presented with a choice of an action movie starring ArnoldSchwarzenegger, an action movie starring Vin Diesel, and a comedy starring Hugh Grant. Ifthe user picks Arnold Schwarzenegger, we learn not only that they like prefer action movies tocomedies, but also that they prefer Schwarzenegger to Diesel. This is more informative than justknowing that they like Schwarzenegger and action movies.\n27.7 Restricted Boltzmann machines (RBMs)\nSo far, all the models we have proposed in this chapter have been representable by directedgraphical models. But some models are better represented using undirected graphs. For example,theBoltzmann machine (Ackley et al. 1985) is a pairwise MRF with hidden nodes hand visible\nnodesv, as shown in Figure 27.30(a). The main problem with the Boltzmann machine is that\nexact inference is intractable, and even approximate inference, using e.g., Gibbs sampling, canbe slow. However, suppose we restrict the architecture so that the nodes are arranged in layers,and so that there are no connections between nodes within the same layer (see Figure 27.30(b)).Then the model has the form\np(h,v|\u03b8)=1\nZ(\u03b8)R/productdisplay\nr=1K/productdisplay\nk=1\u03c8rk(vr,hk) (27.95)\nwhereRis the number of visible (response) variables, Kis the number of hidden variables, and\nvplays the role of yearlier in this chapter. This model is known as a restricted Boltzmann\nmachine (RBM) (Hinton 2002), or a harmonium (Smolensky 1986).\nAn RBM is a special case of a product of experts (PoE) (Hinton 1999), which is so-called\nbecause we are multiplying together a set of \u201cexperts\u201d (here, potential functions on each edge)", "1014": "984 Chapter27. Latentvariablemodelsfordiscretedata\n(a)H\nV\n(b)\nFigure 27.30 (a) A general Boltzmann machine, with an arbitrary graph structure. The shaded (visible)\nnodes are partitioned into input and output, although the model is actually symmetric and de\ufb01nes a joint\ndensity on all the nodes. (b) A restricted Boltzmann machine with a bipartite structure. Note the lack of\nintra-layer connections.\nand then normalizing, whereas in a mixture of experts, we take a convex combination of\nnormalized distributions. The intuitive reason why PoE models might work better than a mixture\nis that each expert can enforce a constraint (if the expert has a value which is /greatermuch1or/lessmuch1)\nor a \u201cdon\u2019t care\u201d condition (if the expert has value 1). By multiplying these experts together\nin different ways we can create \u201csharp\u201d distributions which predict data which satis\ufb01es the\nspeci\ufb01ed constraints (Hinton and Teh 2001). For example, consider a distributed model of text.\nA given document might have the topics \u201cgovernment\u201d, \u201cma\ufb01a\u201d and \u201cplayboy\u201d. If we \u201cmultiply\u201d\nthe predictions of each topic together, the model may give very high probability to the word\n\u201cBerlusconi\u201d9(Salakhutdinov and Hinton 2010). By contrast, adding together experts can only\nmake the distribution broader (see Figure 14.17).\nTypically the hidden nodes in an RBM are binary, so hspeci\ufb01es which constraints are active.\nIt is worth comparing this with the directed models we have discussed. In a mixture model, we\nhave one hidden variable q\u2208{1,...,K}. We can represent this using a set of Kbits, with the\nrestriction that exactly one bit is on at a time. This is called a localist encoding , since only\none hidden unit is used to generate the response vector. This is analogous to the hypothetical\nnotion of grandmother cells in the brain, that are able to recognize only one kind of object.\nBy contrast, an RBM uses a distributed encoding , where many units are involved in generating\neach output. Models that used vector-valued hidden variables, such as \u03c0\u2208SK, as in mPCA/\nLDA, orz\u2208RK, as in ePCA also use distributed encodings.\nThe main difference between an RBM and directed two-layer models is that the hidden\nvariables are conditionally independent given the visible variables, so the posterior factorizes:\np(h|v,\u03b8)=/productdisplay\nkp(hk|v,\u03b8) (27.96)\nThis makes inference much simpler than in a directed model, since we can estimate each hk\n9. Silvio Berlusconi is the current (2011) prime minister of Italy.", "1015": "27.7. RestrictedBoltzmannmachines(RBMs) 985\nVisible Hidden Name Reference\nBinary Binary Binary RBM (Hinton 2002)\nGaussian Binary Gaussian RBM (Welling and Sutton 2005)\nCategorical Binary Categorical RBM (Salakhutdinov et al. 2007)\nMultiple categorical Binary Replicated softmax/ undirected LDA (Salakhutdinov and Hinton 2010)\nGaussian Gaussian Undirected PCA (Marks and Movellan 2001)\nBinary Gaussian Undirected binary PCA (Welling and Sutton 2005)\nTable 27.2 Summary of different kinds of RBM.\nindependently and in parallel, as in a feedforward neural network. The disadvantage is that\ntraining undirected models is much harder, as we discuss below.\n27.7.1 Varieties of RBMs\nIn this section, we describe various forms of RBMs, by de\ufb01ning different pairwise potentialfunctions. See Table 27.2 for a summary. All of these are special cases of the exponential\nfamily harmonium (Welling et al. 2004).\n27.7.1.1 Binary RBMs\nThe most common form of RBM has binary hidden nodes and binary visible nodes. The jointdistribution then has the following form:\np(v,h|\u03b8)=1\nZ(\u03b8)exp(\u2212E(v,h;\u03b8)) (27.97)\nE(v,h;\u03b8)/defines\u2212R/summationdisplay\nr=1K/summationdisplay\nk=1vrhkWrk\u2212R/summationdisplay\nr=1vrbr\u2212K/summationdisplay\nk=1hkck (27.98)\n=\u2212(vTWh+vTb+hTc) (27.99)\nZ(\u03b8)=/summationdisplay\nv/summationdisplay\nhexp(\u2212E(v,h;\u03b8)) (27.100)\nwhereEis the energy function, Wis aR\u00d7Kweight matrix, bare the visible bias terms, care\nthe hidden bias terms, and \u03b8=(W,b,c)are all the parameters. For notational simplicity, we\nwill absorb the bias terms into the weight matrix by clamping dummy units v0=1andh0=1\nand setting w0,:=candw:,0=b. Note that naively computing Z(\u03b8)takesO(2R2K)time\nbut we can reduce this to O(min{R2K,K2R})time (Exercise 27.1).\nWhen using a binary RBM, the posterior can be computed as follows:\np(h|v,\u03b8)=K/productdisplay\nk=1p(hk|v,\u03b8)=/productdisplay\nkBer(hk|sigm(wT\n:,kv)) (27.101)\nBy symmetry, one can show that we can generate data given the hidden variables as follows:\np(v|h,\u03b8)=/productdisplay\nrp(vr|h,\u03b8)=/productdisplay\nrBer(vr|sigm(wT\nr,:h)) (27.102)", "1016": "986 Chapter27. Latentvariablemodelsfordiscretedata\nWe can write this in matrix-vetor notation as follows:\nE[h|v\u03b8]=s i g m ( WTv) (27.103)\nE[v|h,\u03b8]=s i g m ( Wh) (27.104)\nThe weights in Ware called the generative weights, since they are used to generate the\nobservations, and the weights in WTare called the recognition weights, since they are used\nto recognize the input.\nFrom Equation 27.101, we see that we activate hidden node kin proportion to how much the\ninput vector v\u201clooks like\u201d the weight vector w:,k(up to scaling factors). Thus each hidden node\ncaptures certain features of the input, as encoded in its weight vector, similar to a feedforward\nneural network.\n27.7.1.2 Categorical RBM\nWe can extend the binary RBM to categorical visible variables by using a 1-of-C encoding,\nwhereCis the number of states for each vir. We de\ufb01ne a new energy function as follows\n(Salakhutdinov et al. 2007; Salakhutdinov and Hinton 2010):\nE(v,h;\u03b8)/defines\u2212R/summationdisplay\nr=1K/summationdisplay\nk=1C/summationdisplay\nc=1vc\nrhkWc\nrk\u2212R/summationdisplay\nr=1C/summationdisplay\nc=1vc\nrbc\nr\u2212K/summationdisplay\nk=1hkck (27.105)\nThe full conditionals are given by\np(vr|h,\u03b8)=C a t (S({bcr+/summationdisplay\nkhkWc\nrk}Cc=1)) (27.106)\np(hk=1|c,\u03b8) = sigm(c k+/summationdisplay\nr/summationdisplay\ncvc\nrWc\nrk) (27.107)\n27.7.1.3 Gaussian RBM\nWe can generalize the model to handle real-valued data. In particular, a Gaussian RBM has the\nfollowing energy function:\nE(v,h|\u03b8)=\u2212R/summationdisplay\nr=1K/summationdisplay\nk=1Wrkhkvr\u22121\n2R/summationdisplay\nr=1(vr\u2212br)2\u2212K/summationdisplay\nk=1akhk (27.108)\nThe parameters of the model are \u03b8=(wrk,ak,br). (We have assumed the data is standardized,\nso we \ufb01x the variance to \u03c32=1.) Compare this to a Gaussian in information form:\nNc(v|\u03b7,\u039b)\u221dexp(\u03b7Tv\u22121\n2vT\u039bv) (27.109)\nwhere\u03b7=\u039b\u03bc. We see that we have set \u039b=I, and\u03b7=/summationtext\nkhkw:,k. Thus the mean is\ngiven by\u03bc=\u039b\u22121\u03b7=/summationtext\nkhkw:,k. The full conditionals, which are needed for inference and", "1017": "27.7. RestrictedBoltzmannmachines(RBMs) 987\nlearning, are given by\np(vr|h,\u03b8)=N (vr|br+/summationdisplay\nkwrkhk,1) (27.110)\np(hk=1|v,\u03b8)=s i g m/parenleftBigg\nck+/summationdisplay\nrwrkvr/parenrightBigg\n(27.111)\nWe see that each visible unit has a Gaussian distribution whose mean is a function of the\nhidden bit vector. More powerful models, which make the (co)variance depend on the hiddenstate, can also be developed (Ranzato and Hinton 2010).\n27.7.1.4 RBMs with Gaussian hidden units\nIf we use Gaussian latent variables and Gaussian visible variables, we get an undirected versionof factor analysis. However, it turns out that it is identical to the standard directed version(Marks and Movellan 2001).\nIf we use Gaussian latent variables and categorical observed variables, we get an undirected\nversion of categorical PCA (Section 27.2.2). In (Salakhutdinov et al. 2007), this was applied to theNet\ufb02ix collaborative \ufb01ltering problem, but was found to be signi\ufb01cantly inferior to using binarylatent variables, which have more expressive power.\n27.7.2 Learning RBMs\nIn this section, we discuss some ways to compute ML parameter estimates of RBMs, usinggradient-based optimizers. It is common to use stochastic gradient descent, since RBMs oftenhave many parameters and therefore need to be trained on very large datasets. In addition, it isstandard to use /lscript\n2regularization, a technique that is often called weight decay in this context.\nThis requires a very small change to the objective and gradient, as discussed in Section 8.3.6.\n27.7.2.1 Deriving the gradient using p(h,v|\u03b8)\nTo compute the gradient, we can modify the equations from Section 19.5.2, which show how to\ufb01t a generic latent variable maxent model. In the context of the Boltzmann machine, we haveone feature per edge, so the gradient is given by\n\u2202/lscript\n\u2202wrk=1\nNN/summationdisplay\ni=1E[vrhk|vi,\u03b8]\u2212E[vrhk|\u03b8] (27.112)\nWe can write this in matrix-vector form as follows:\n\u2207w/lscript=Epemp(\u00b7|\u03b8)/bracketleftbig\nvhT/bracketrightbig\n\u2212Ep(\u00b7|\u03b8)/bracketleftbig\nvhT/bracketrightbig\n(27.113)\nwherepemp(v,h|\u03b8)/definesp(h|v,\u03b8)pemp(v), andpemp(v)=1\nN/summationtextN\ni=1\u03b4vi(v)is the empirical\ndistribution. (We can derive a similar expression for the bias terms by setting vr=1or\nhk=1.)\nThe \ufb01rst term on the gradient, when vis \ufb01xed to a data case, is sometimes called the\nclamped phase, and the second term, when vis free, is sometimes called the unclamped", "1018": "988 Chapter27. Latentvariablemodelsfordiscretedata\nphase. When the model expectations match the empirical expectations, the two terms cancel\nout, the gradient becomes zero and learning stops. This algorithm was \ufb01rst proposed in (Ackleyet al. 1985). The main problem is efficiently computing the expectations. We discuss some waysto do this below.\n27.7.2.2 Deriving the gradient using p(v|\u03b8)\nWe now present an alternative way to derive Equation 27.112, which also applies to other energybased models. First we marginalize out the hidden variables and write the RBM in the formp(v|\u03b8)=\n1\nZ(\u03b8)exp(\u2212F(v;\u03b8)),w h e r eF (v;\u03b8)is thefree energy:\nF(v)/defines/summationdisplay\nhE(v,h)=/summationdisplay\nhexp/parenleftBiggR/summationdisplay\nr=1K/summationdisplay\nk=1vrhkWrk/parenrightBigg\n(27.114)\n=/summationdisplay\nhK/productdisplay\nk=1exp/parenleftBiggR/summationdisplay\nr=1vrhkWrk/parenrightBigg\n(27.115)\n=K/productdisplay\nk=1/summationdisplay\nhr\u2208{0,1}exp/parenleftBiggR/summationdisplay\nr=1vrhrWrk/parenrightBigg\n(27.116)\n=K/productdisplay\nk=1/parenleftBigg\n1+exp(R/summationdisplay\nr=1vrWrk)/parenrightBigg\n(27.117)\nNext we write the (scaled) log-likelihood in the following form:\n/lscript(\u03b8)=1\nNN/summationdisplay\ni=1logp(vi|\u03b8)=\u22121\nNN/summationdisplay\ni=1F(vi|\u03b8)\u2212logZ(\u03b8) (27.118)\nUsing the fact that Z(\u03b8)=/summationtext\nvexp(\u2212F(v;\u03b8))we have\n\u2207/lscript(\u03b8)=\u22121\nNN/summationdisplay\ni=1\u2207F(vi)\u2212\u2207Z\nZ(27.119)\n=\u22121\nNN/summationdisplay\ni=1\u2207F(vi)+/summationdisplay\nv\u2207F(v)exp(\u2212F(v))\nZ(27.120)\n=\u22121\nNN/summationdisplay\ni=1\u2207F(vi)+E[\u2207F(v)] (27.121)\nPlugging in the free energy (Equation 27.117), one can show that\n\u2202\n\u2202wrkF(v)=\u2212vrE[hk|v,\u03b8]=\u2212E[vrhk|v,\u03b8] (27.122)\nHence\n\u2202\n\u2202wrk/lscript(\u03b8)=1\nNN/summationdisplay\ni=1E[vrhk|v,\u03b8]\u2212E[vrhk|\u03b8] (27.123)\nwhich matches Equation 27.112.", "1019": "27.7. RestrictedBoltzmannmachines(RBMs) 989\n+M\n;L\u001f;L+M! \u001f;L+M! \u001f;L+M! \u0013\u0014LQILQLW\\\n7\u0003 \u0003\u0013\u001d\u0003GDWD7\u0003 \u0003\u0014\u001d\u0003\u0014\u0010VWHS\u0003\nUHFRQVWUXFWLRQV7\u0003 \u0003LQILQLW\\\u001d\u0003HTXLOLEULXP\u0003\nVDPSOHV\nFigure 27.31 Illustration of Gibbs sampling in an RBM. The visible nodes are initialized at a datavector,\nthen we sample a hidden vector, then another visible vector, etc. Eventually (at \u201cin\ufb01nity\u201d) we will be\nproducing samples from the joint distribution p(v,h|\u03b8).\n27.7.2.3 Approximating the expectations\nWe can approximate the expectations needed to evaluate the gradient by performing block\nGibbs sampling, using Equations 27.101 and 27.102. In more detail, we can sample from thejoint distribution p(v,h|\u03b8)as follows: initialize the chain at vv\n1(e.g. by setting v1=vifor\nsome data vector), and then sample from h1\u223cp(h|v1), then from v2\u223cp(v|h1), then from\nh2\u223cp(h|v2), etc. See Figure 27.31 for an illustration. Note, however, that we have to wait until\nthe Markov chain reaches equilibrium (i.e., until it has \u201cburned in\u201d) before we can interpret thesamples as coming from the joint distribution of interest, and this might take a long time.\nA faster alternative is to use mean \ufb01eld, where we make the approximation E[v\nrhk]\u2248\nE[vr]E[hk]. However, since p(v,h)is typically multimodal, this is usually a very poor approx-\nimation, since it will average over different modes (see Section 21.2.2). Furthermore, there is amore subtle reason not to use mean \ufb01eld: since the gradient has the form E[v\nrhk|v]\u2212E[vrhk],\nwe see that the negative sign in front means that the method will try to make the variationalbound as loose as possible (Salakhutdinov and Hinton 2009). This explains why earlier attemptsto use mean \ufb01eld to learn Boltzmann machines (e.g., (Kappen and Rodriguez 1998)) did not work.\n27.7.2.4 Contrastive divergence\nThe problem with using Gibbs sampling to compute the gradient is that it is slow. We nowpresent a faster method known as contrastive divergence orCD(Hinton 2002). CD was\noriginally derived by approximating an objective function de\ufb01ned as the difference of two KLdivergences, rather than trying to maximize the likelihood itself. However, from an algorithmicpoint of view, it can be thought of as similar to stochastic gradient descent, except it approxi-mates the \u201cunclamped\u201d expectations with \u201cbrief\u201d Gibbs sampling where we initialize each Markovchain at the data vectors. That is, we approximate the gradient for one datavector as follows:\n\u2207\nw/lscript\u2248E/bracketleftbig\nvhT|vi/bracketrightbig\n\u2212Eq/bracketleftbig\nvhT/bracketrightbig\n(27.124)\nwhereqcorresponds to the distribution generated by Kup-down Gibbs sweeps, started at vi,\nas in Figure 27.31. This is known as CD-K . In more detail, the procedure (for K=1)i sa s", "1020": "990 Chapter27. Latentvariablemodelsfordiscretedata\nfollows:\nhi\u223cp(h|vi,\u03b8) (27.125)\nv/prime\ni\u223cp(v|hi,\u03b8) (27.126)\nh/prime\ni\u223cp(h|v/prime\ni,\u03b8) (27.127)\nWe then make the approximation\nEq/bracketleftbig\nvhT/bracketrightbig\n\u2248vi(h/primei)T(27.128)\nSuch samples are sometimes called fantasy data. We can think of v/prime\nias the model\u2019s best\nattempt at reconstructing viafter being coded and then decoded by the model. This is similar\nto the way we train auto-encoders, which are models which try to \u201csqueeze\u201d the data through a\nrestricted parametric \u201cbottleneck\u201d (see Section 28.3.2).\nIn practice, it is common to use E[h|v/prime\ni]instead of a sampled value h/prime\niin the \ufb01nal upwards\npass, since this reduces the variance. However, it is not valid to use E[h|vi]instead of sampling\nhi\u223cp(h|vi)in the earlier upwards passes, because then each hidden unit would be able to\npass more than 1 bit of information, so it would not act as much of a bottleneck.\nThe whole procedure is summarized in Algorithm 3. (Note that we follow the positive gradient\nsince we are maximizing likelihood.) Various tricks can be used to speed this algorithm up, such\nas using a momentum term (Section 8.3.2), using mini-batches, averaging the updates, etc. Suchdetails can be found in (Hinton 2010; Swersky et al. 2010).\nAlgorithm 27.3: CD-1 training for an RBM with binary hidden and visible units\n1Initialize weights W\u2208RR\u00d7Krandomly;\n2t:= 0;\n3foreachepoch do\n4t:=t+1;\n5foreachminibatchofsize Bdo\n6 Set minibatch gradient to zero, g:=0;\n7 foreachcase viintheminibatch do\n8 Compute \u03bci=E[h|vi,W];\n9 Samplehi\u223cp(h|vi,W);\n10 Samplev/prime\ni\u223cp(v|hi,W);\n11 Compute \u03bc/prime\ni=E[h|v/prime\ni,W];\n12 Compute gradient \u2207W=(vi)(\u03bci)T\u2212(v/prime\ni)(\u03bc/primei)T;\n13 Accumulate g:=g+\u2207W;\n14 Update parameters W:=W+(\u03b1t/B)g\n27.7.2.5 Persistent CD\nIn Section 19.5.5, we presented a technique called stochastic maximum likelihood (SML) for\n\ufb01tting maxent models. This avoids the need to run MCMC to convergence at each iteration,", "1021": "27.7. RestrictedBoltzmannmachines(RBMs) 991\nby exploiting the fact that the parameters are changing slowly, so the Markov chains will not\nbe pushed too far from equilibrium after each update (Younes 1989). In other words, there aretwo dynamical processes running at different time scales: the states change quickly, and theparameters change slowly. This algorithm was independently rediscovered in (Tieleman 2008),who called it persistent CD. See Algorithm 3 for the pseudocode.\nPCD often works better than CD (see e.g., (Tieleman 2008; Marlin et al. 2010; Swersky et al.\n2010)), although CD can be faster in the early stages of learning.\nAlgorithm 27.4: Persistent CD for training an RBM with binary hidden and visible units\n1Initialize weights W\u2208RD\u00d7Lrandomly;\n2Initialize chains (vs,hs)S\ns=1randomly ;\n3fort=1,2,...do\n4// Mean \ufb01eld updates ;\n5foreachcase i=1:Ndo\n6 \u03bcik=s i g m (vT\niw:,k)\n7// MCMC updates ;\n8foreachsample s=1:Sdo\n9 Generate (vs,hs)by brief Gibbs sampling from old (vs,hs)\n10// Parameter updates ;\n11g=1\nN/summationtextN\ni=1vi(\u03bci)T\u22121\nS/summationtextSs=1vs(hs)T;\n12W:=W+\u03b1tg;\n13Decrease \u03b1t\n27.7.3 Applications of RBMs\nThe main application of RBMs is as a building block for deep generative models, which we\ndiscuss in Section 28.2. But they can also be used as substitutes for directed two-layer models.They are particularly useful in cases where inference of the hidden states at test time must befast. We give some examples below.\n27.7.3.1 Language modeling and document retrieval\nWe can use a categorical RBM to de\ufb01ne a generative model for bag-of-words, as an alternativeto LDA. One subtlety is that the partition function in an undirected models depends on howbig the graph is, and therefore on how long the document is. A solution to this was proposedin (Salakhutdinov and Hinton 2010): use a categorical RBM with tied weights, but multiply thehidden activation bias terms c\nkby the document length Lto compensate form the fact that the\nobserved word-count vector vis larger in magnitude:\nE(v,h;\u03b8)/defines\u2212K/summationdisplay\nk=1C/summationdisplay\nc=1vchkWc\nk\u2212C/summationdisplay\nc=1vcbc\nr\u2212LK/summationdisplay\nk=1hkck (27.129)", "1022": "992 Chapter27. Latentvariablemodelsfordiscretedata\nData set Number of docs K \u00afD St. Dev. Avg. Test perplexity per word (in nats)\nTrain Test LDA-50 LDA-200 R. Soft-50 Unigram\nNIPS 1,690 50 13,649 98.0 245.3 3576 3391 3405 4385\n20-news 11,314 7,531 2,000 51.8 70.8 1091 1058 953 1335\nReuters 794,414 10,000 10,000 94.6 69.3 1437 1142 988 2208\nFigure 27.32 Comparison of RBM (replicated softmax) and LDA on three corpora. Kis the number of\nwords in the vocabulary, Dis the average document length, and St. Dev. is the standard deviation of the\ndocument length. Source: (Salakhutdinov and Hinton 2010) .\n0.02    0.1     0.4     1.6     6.4     25.6    100 102030405060\nRecall (%) Precision (%)Replicated \nSoftmax 50\u2212D\nLDA 50\u2212D\n0.001     0.006     0.051     0.4        1.6       6.4       25.6      100 1020304050\nRecall (%) Precision (%)Replicated \nSoftmax 50\u2212D\nLDA 50\u2212D20-newsgroups Reuters\nFigure 27.33 Precision-recall curves for RBM (replicated softmax) and LDA on two corpora. From Figure\n3 of (Salakhutdinov and Hinton 2010). Used with kind permission of Ruslan Salakhutdinov.\nwherevc=/summationtextL\nl=1I(yil=c). This is like having a single multinomial node (so we have dropped\nthersubscript) with Cstates, where Cis the number of words in the vocabulary. This is\ncalled the replicated softmax model (Salakhutdinov and Hinton 2010), and is an undirected\nalternative to mPCA/ LDA.\nWe can compare the modeling power of RBMs vs LDA by measuring the perplexity on a test\nset. This can be approximated using annealing importance sampling (Section 24.6.2). The results\nare shown in Figure 27.32. We see that the LDA is signi\ufb01cantly better than a unigram model,\nbut that an RBM is signi\ufb01cantly better than LDA.\nAnother advantage of the LDA is that inference is fast and exact: just a single matrix-vector\nmultiply followed by a sigmoid nonlinearity, as in Equation 27.107. In addition to being faster,\nthe RBM is more accurate. This is illustrated in Figure 27.33, which shows precision-recall curves\nfor RBMs and LDA on two different corpora. These curves were generated as follows: a query\ndocument from the test set is taken, its similarity to all the training documents is computed,\nwhere the similarity is de\ufb01ned as the cosine of the angle between the two topic vectors, and\nthen the top Mdocuments are returned for varying M. A retrieved document is considered\nrelevant if it has the same class label as that of the query\u2019s (this is the only place where labels\nare used).", "1023": "27.7. RestrictedBoltzmannmachines(RBMs) 993\n27.7.3.2 RBMs for collaborative \ufb01ltering\nRBMs have been applied to the Net\ufb02ix collaborative \ufb01ltering competition (Salakhutdinov et al.\n2007). In fact, an RBM with binary hidden nodes and categorical visible nodes can slightlyoutperform SVD. By combining the two methods, performance can be further improved. (Thewinning entry in the challenge was an ensemble of many different types of model (Koren 2009a).)\nExercises\nExercise 27.1 Partition function for an RBM\nShow how to compute Z(\u03b8)for an RBM with Kbinary hidden nodes and Rbinary observed nodes in\nO(R2K)time, assuming K<R.", "1024": "", "1025": "28 Deep learning\n28.1 Introduction\nMany of the models we have looked at in this book have a simple two-layer architecture of\nthe form z\u2192yfor unsupervised latent variable models, or x\u2192yfor supervised models.\nHowever, when we look at the brain, we seem many levels of processing. It is believed that eachlevel is learning features or representations at increasing levels of abstraction. For example, thestandard model of the visual cortex (Hubel and Wiesel 1962; Serre et al. 2005; Ranzato et al.\n2007) suggests that (roughly speaking) the brain \ufb01rst extracts edges, then patches, then surfaces,then objects, etc. (See e.g., (Palmer 1999; Kandel et al. 2000) for more information about howthe brain might perform vision.)\nThis observation has inspired a recent trend in machine learning known as deep learning\n(see e.g., (Bengio 2009), deeplearning .net, and the references therein), which attempts to\nreplicate this kind of architecture in a computer. (Note the idea can be applied to non-visionproblems as well, such as speech and language.)\nIn this chapter, we give a brief overview of this new \ufb01eld. However, we caution the reader\nthat the topic of deep learning is currently evolving very quickly, so the material in this chaptermay soon be outdated.\n28.2 Deep generative models\nDeep models often have millions of parameters. Acquiring enough labeled data to train suchmodels is diffcult, despite crowd sourcing sites such as Mechanical Turk. In simple settings, suchas hand-written character recognition, it is possible to generate lots of labeled data by makingmodi\ufb01ed copies of a small manually labeled training set (see e.g., Figure 16.13), but it seemsunlikely that this approach will scale to complex scenes.\n1\nTo overcome the problem of needing labeled training data, we will focus on unsupervised\nlearning. The most natural way to perform this is to use generative models. In this section, wediscuss three different kinds of deep generative models: directed, undirected, and mixed.\n1. There have been some attempts to use computer graphics and video games to generate realistic-looking images of\ncomplex scenes, and then to use this as training data for computer vision systems. However, often graphics programs\ncut corners in order to make perceptually appealing images which are not re\ufb02ective of the natural statistics of real-world\nimages.", "1026": "996 Chapter28. Deeplearning\n(a) (b)\n (c)\nFigure 28.1 Some deep multi-layer graphical models. Observed variables are at the bottom. (a) A directed\nmodel. (b) An undirected model (deep Boltzmann machine). (c) A mixed directed-undirected model (deep\nbelief net).\n28.2.1 Deep directed networks\nPerhaps the most natural way to build a deep generative model is to construct a deep directed\ngraphical model, as shown in Figure 28.1(a). The bottom level contains the observed pixels (or\nwhatever the data is), and the remaining layers are hidden. We have assumed just 3 layers for\nnotational simplicity. The number and size of layers is usually chosen by hand, although one\ncan also use non-parametric Bayesian methods (Adams et al. 2010) or boosting (Chen et al. 2010)\nto infer the model structure.\nWe shall call models of this form deep directed networks or DDNs. If all the nodes are\nbinary, and all CPDs are logistic functions, this is called a sigmoid belief net (Neal 1992). In\nthis case, the model de\ufb01nes the following joint distribution:\np(h1,h2,h3,v|\u03b8)=/productdisplay\niBer(vi|sigm(hT\n1w0i))/productdisplay\njBer(h1j|sigm(hT\n2w1j)) (28.1)\n/productdisplay\nkBer(h2k|sigm(hT\n3w2k))/productdisplay\nlBer(h3l|w3l) (28.2)\nUnfortunately, inference in directed models such as these is intractable because the posterior\non the hidden nodes is correlated due to explaining away. One can use fast mean \ufb01eld approxi-\nmations (Jaakkola and Jordan 1996a; Saul and Jordan 2000), but these may not be very accurate,\nsince they approximate the correlated posterior with a factorial posterior. One can also use\nMCMC inference (Neal 1992; Adams et al. 2010), but this can be quite slow because the variables\nare highly correlated. Slow inference also results in slow learning.\n28.2.2 Deep Boltzmann machines\nA natural alternative to a directed model is to construct a deep undirected model. For example,\nwe can stack a series of RBMs on top of each other, as shown in Figure 28.1(b). This is known\nas adeep Boltzmann machine orDBM(Salakhutdinov and Hinton 2009). If we have 3 hidden\nlayers, the model is de\ufb01ned as follows:\np(h1,h2,h3,v|\u03b8)=1\nZ(\u03b8)exp\u239b\n\u239d/summationdisplay\nijvih1jW1ij+/summationdisplay\njkh1jh2jW2jk+/summationdisplay\nklh2kh3lW3kl\u239e\n\u23a0(28.3)", "1027": "28.2. Deepgenerativemodels 997\nwhere we are ignoring constant offset or bias terms.\nThe main advantage over the directed model is that one can perform efficient block (layer-\nwise) Gibbs sampling, or block mean \ufb01eld, since all the nodes in each layer are conditionally\nindependent of each other given the layers above and below (Salakhutdinov and Larochelle2010). The main disadvantage is that training undirected models is more difficult, because of thepartition function. However, below we will see a greedy layer-wise strategy for learning deepundirected models.\n28.2.3 Deep belief networks\nAn interesting compromise is to use a model that is partially directed and partially undirected.In particular, suppose we construct a layered model which has directed arrows, except at thetop, where there is an undirected bipartite graph, as shown in Figure 28.1(c). This model isknown as a deep belief network (Hinton et al. 2006) or DBN.\n2If we have 3 hidden layers, the\nmodel is de\ufb01ned as follows:\np(h1,h2,h3,v|\u03b8)=/productdisplay\niBer(vi|sigm(hT\n1w1i)/productdisplay\njBer(h1j|sigm(hT2w2j) (28.4)\n1\nZ(\u03b8)exp/parenleftBigg/summationdisplay\nklh2kh3lW3kl/parenrightBigg\n(28.5)\nEssentially the top two layers act as an associative memory, and the remaining layers then\ngenerate the output.\nThe advantage of this peculiar architecture is that we can infer the hidden states in a\nfast, bottom-up fashion. To see why, suppose we only have two hidden layers, and thatW\n2=WT\n1, so the second level weights are tied to the \ufb01rst level weights (see Figure 28.2(a)).\nThis de\ufb01nes a model of the form p(h1,h2,v|W1). One can show that the distribution\np(h1,v|W1)=/summationtext\nh2p(h1,h2,v|W1)has the form p(h1,v|W1)=1\nZ(W 1)exp(vTW1h1),\nwhich is equivalent to an RBM. Since the DBN is equivalent to the RBM as far as p(h1,v|W1)\nis concerned, we can infer the posterior p(h1|v,W1)in the DBN exactly as in the RBM. This\nposterior is exact, even though it is fully factorized.\nNow the only way to get a factored posterior is if the prior p(h1|W1)is acomplementary\nprior. This is a prior which, when multiplied by the likelihood p(v|h1), results in a perfectly\nfactored posterior. Thus we see that the top level RBM in a DBN acts as a complementary priorfor the bottom level directed sigmoidal likelihood function.\nIf we have multiple hidden levels, and/or if the weights are not tied, the correspondence\nbetween the DBN and the RBM does not hold exactly any more, but we can still use the factoredinference rule as a form of approximate bottom-up inference. Below we show that this is a validvariational lower bound. This bound also suggests a layer-wise training strategy, that we willexplain in more detail later. Note, however, that top-down inference in a DBN is not tractable,so DBNs are usually only used in a feedforward manner.\n2. Unforuntately the acronym \u201cDBN\u201d also stands for \u201cdynamic Bayes net\u201d (Section 17.6.7). Geoff Hinton, who invented\ndeep belief networks, has suggested the acronyms DeeBNsandDyBNsfor these two different meanings. However, this\nterminology is non-standard.", "1028": "998 Chapter28. Deeplearning\n(a)\n (b)\n (c)\nFigure 28.2 (a) A DBN with two hidden layers and tied weights that is equivalent to an RBM. Source:\nFigure 2.2 of (Salakhutdinov 2009). (b) A stack of RBMs trained greedily. (c) The corresponding DBN.\nSource: Figure 2.3 of (Salakhutdinov 2009). Used with kind permission of Ruslan Salakhutdinov.\n28.2.4 Greedy layer-wise learning of DBNs\nThe equivalence between DBNs and RBMs suggests the following strategy for learning a DBN.\n\u2022 Fit an RBM to learn W1using methods described in Section 27.7.2.\n\u2022 Unroll the RBM into a DBN with 2 hidden layers, as in Figure 28.2(a). Now \u201cfreeze\u201d the\ndirected weights W1and letW2be \u201cuntied\u201d so it is no longer forced to be equal to WT\n1.\nWe will now learn a better prior for p(h1|W2)by \ufb01tting a second RBM. The input data to\nthis new RBM is the activation of the hidden units E[h1|v,W1]which can be computed\nusing a factorial approximation.\n\u2022 Continue to add more hidden layers until some stopping criterion is satisi\ufb01ed, e.g., you run\nout of time or memory, or you start to over\ufb01t the validation set. Construct the DBN from\nthese RBMs, as illustrated in Figure 28.2(c).\nOne can show (Hinton et al. 2006) that this procedure always increases a lower bound the\nobserved data likelihood. Of course this procedure might result in over\ufb01tting, but that is a\ndifferent matter.\nIn practice, we want to be able to use any number of hidden units in each level. This means\nwe will not be able to initialize the weights so that W/lscript=WT\n/lscript\u22121. This voids the theoretical\nguarantee. Nevertheless the method works well in practice, as we will see. The method can also\nbe extended to train DBMs in a greedy way (Salakhutdinov and Larochelle 2010).\nAfter using the greedy layer-wise training strategy, it is standard to \u201c\ufb01ne tune\u201d the weights,\nusing a technique called back\ufb01tting . This works as follows. Perform an upwards sampling pass\nto the top. Then perform brief Gibbs sampling in the top level RBM, and perform a CD update\nof the RBM parameters. Finally, perform a downwards ancestral sampling pass (which is an\napproximate sample from the posterior), and update the logistic CPD parameters using a small\ngradient step. This is called the up-down procedure (Hinton et al. 2006). Unfortunately this\nprocedure is very slow.", "1029": "28.3. Deepneuralnetworks 999\n28.3 Deep neural networks\nGiven that DBNs are often only used in a feed-forward, or bottom-up, mode, they are effectively\nacting like neural networks. In view of this, it is natural to dispense with the generative storyand try to \ufb01t deep neural networks directly, as we discuss below. The resulting training methodsare often simpler to implement, and can be faster.\nNote, however, that performance with deep neural nets is sometimes not as good as with\nprobabilistic models (Bengio et al. 2007). One reason for this is that probabilistic models supporttop-down inference as well as bottom-up inference. (DBNs do not support efficient top-downinference, but DBMs do, and this has been shown to help (Salakhutdinov and Larochelle 2010).)Top-down inference is useful when there is a lot of ambiguity about the correct interpretationof the signal.\nIt is interesting to note that in the mammalian visual cortex, there are many more feedback\nconnections than there are feedforward connections (see e.g., (Palmer 1999; Kandel et al. 2000)).The role of these feedback connections is not precisely understood, but they presumably providecontextual prior information (e.g., coming from the previous \u201cframe\u201d or retinal glance) whichcan be used to disambiguate the current bottom-up signals (Lee and Mumford 2003).\nOf course, we can simulate the effect of top-down inference using a neural network. However\nthe models we discuss below do not do this.\n28.3.1 Deep multi-layer perceptrons\nMany decision problems can be reduced to classi\ufb01cation, e.g., predict which object (if any) ispresent in an image patch, or predict which phoneme is present in a given acoustic featurevector. We can solve such problems by creating a deep feedforward neural network or multi-layer perceptron (MLP), as in Section 16.5, and then \ufb01tting the parameters using gradient descent(aka back-propagation).\nUnfortunately, this method does not work very well. One problem is that the gradient becomes\nweaker the further we move away from the data; this is known as the \u201cvanishing gradient\u201dproblem (Bengio and Frasconi 1995). A related problem is that there can be large plateaus inthe error surface, which cause simple \ufb01rst-order gadient-based methods to get stuck (Glorot andBengio 2010).\nConsequently early attempts to learn deep neural networks proved unsuccesful. Recently there\nhas been some progress, due to the adoption of GPUs (Ciresan et al. 2010) and second-orderoptimization algorithms (Martens 2010). Nevertheless, such models remain difficult to train.\nBelow we discuss a way to initialize the parameters using unsupervised learning; this is called\ngenerative pre-training. The advantage of performing unsupervised learning \ufb01rst is that themodel is forced to model a high-dimensional response, namely the input feature vector, ratherthan just predicting a scalar response. This acts like a data-induced regularizer, and helpsbackpropagation \ufb01nd local minima with good generalization properties (Erhan et al. 2010; Glorotand Bengio 2010).", "1030": "1000 Chapter28. Deeplearning\n/g21/g21\n/g21/g1\u0001/g21\n/g21/g21\n/g21\n/g21/g1\u0001\n/g21/g1\u0001/g21/g1\u0001/g21/g21/g1\u0001/g21/g1\u0001/g21/g1\u0001\n/g1\u0001\n/g21/g21/g21/g21/g21/g21\n/g3/g4/g2/g2/g2/g18/g11/g16/g4\n/g4/g2/g2/g2/g3/g2/g2/g2/g7/g2/g2/g7/g2/g2\n/g3/g2/g2/g2/g3/g2/g2/g2\n/g7/g2/g2\n/g3/g3/g4/g2/g2/g2\n/g4/g2/g2/g2/g7/g2/g2 /g7/g2/g2\n/g3/g2/g2/g2/g3/g2/g2/g2/g4/g2/g2/g2\n/g7/g2/g2\n/g4/g2/g2/g2/g19\n/g6/g19\n/g18/g11/g16\n/g17/g32/g25/g33/g32/g22/g27/g29/g27/g29/g26 /g20/g29/g32/g30/g28/g28/g27/g29/g26/g3/g2/g2/g2/g18/g11/g16/g5/g6/g5/g2\n/g5/g2\n/g15/g27/g29/g25\u0001/g33/g34/g29/g27/g29/g26/g6/g6\n/g4/g4/g5/g5/g6/g19\n/g7/g5/g19\n/g8/g4/g19\n/g9/g3/g19\n/g10\n/g14/g29/g23/g30/g24/g25/g32/g3/g4/g5/g5/g2\n/g6/g5/g4/g19/g3/g19\n/g12/g30/g24/g25/g1/g28/g22/g35/g25/g32/g13/g25/g23/g30/g24/g25/g32\n/g18/g11/g16/g19/g30/g31\nFigure 28.3 Training a deep autoencoder. (a) First we greedily train some RBMs. (b) Then we construct\nthe auto-encoder by replicating the weights. (c) Finally we \ufb01ne-tune the weights using back-propagation.\nFrom Figure 1 of (Hinton and Salakhutdinov 2006). Used with kind permission of Ruslan Salakhutdinov.\n28.3.2 Deep auto-encoders\nAnauto-encoder is a kind of unsupervised neural network that is used for dimensionality\nreduction and feature discovery. More precisely, an auto-encoder is a feedforward neural network\nthat is trained to predict the input itself. To prevent the system from learning the trivial identity\nmapping, the hidden layer in the middle is usually constrained to be a narrow bottleneck . The\nsystem can minimize the reconstruction error by ensuring the hidden units capture the most\nrelevant aspects of the data.\nSuppose the system has one hidden layer, so the model has the form v\u2192h\u2192v. Further,\nsuppose all the functions are linear. In this case, one can show that the weights to the K\nhidden units will span the same subspace as the \ufb01rst Kprincipal components of the data\n(Karhunen and Joutsensalo 1995; Japkowicz et al. 2000). In other words, linear auto-encoders are\nequivalent to PCA. However, by using nonlinear activation functions, one can discover nonlinear\nrepresentations of the data.\nMore powerful representations can be learned by using deep auto-encoders . Unfortunately\ntraining such models using back-propagation does not work well, because the gradient signal\nbecomes too small as it passes back through multiple layers, and the learning algorithm often\ngets stuck in poor local minima.\nOne solution to this problem is to greedily train a series of RBMs and to use these to initialize\nan auto-encoder, as illustrated in Figure 28.3. The whole system can then be \ufb01ne-tuned using\nbackprop in the usual fashion. This approach, \ufb01rst suggested in (Hinton and Salakhutdinov", "1031": "28.4. Applicationsofdeepnetworks 1001\n2000 top-level units\n500 units \n500 units \n28 x 28    \npixel \nimage10 label units\nThis could be the \ntop level of \nanother sensory \npathway\n(a)\n (b)\nFigure 28.4 (a) A DBN architecture for classifying MNIST digits. Source: Figure 1 of (Hinton et al. 2006).\nUsed with kind permission of Geoff Hinton. (b) These are the 125 errors made by the DBN on the 10,000\ntest cases of MNIST. Above each image is the estimated label. Source: Figure 6 of (Hinton et al. 2006).\nUsed with kind permission of Geoff Hinton. Compare to Figure 16.15.\n2006), works much better than trying to \ufb01t the deep auto-encoder directly starting with random\nweights.\n28.3.3 Stacked denoising auto-encoders\nA standard way to train an auto-encoder is to ensure that the hidden layer is narrower than the\nvisible layer. This prevents the model from learning the identity function. But there are other\nways to prevent this trivial solution, which allow for the use of an over-complete representation.\nOne approach is to impose sparsity constraints on the activation of the hidden units (Ranzato\net al. 2006). Another approach is to add noise to the inputs; this is called a denoising auto-\nencoder (Vincent et al. 2010). For example, we can corrupt some of the inputs, for example\nby setting them to zero, so the model has to learn to predict the missing entries. This can be\nshown to be equivalent to a certain approximate form of maximum likelihood training (known\nas score matching) applied to an RBM (Vincent 2011).\nOf course, we can stack these models on top of each other to learn a deep stacked denoising\nauto-encoder, which can be discriminatively \ufb01ne-tuned just like a feedforward neural network,\nif desired.\n28.4 Applications of deep networks\nIn this section, we mention a few applications of the models we have been discussing.\n28.4.1 Handwritten digit classi\ufb01cation using DBNs\nFigure 28.4(a) shows a DBN (from (Hinton et al. 2006)) consisting of 3 hidden layers. The visible\nlayer corresponds to binary images of handwritten digits from the MNIST data set. In addition,\nthe top RBM is connected to a softmax layer with 10 units, representing the class label.", "1032": "1002 Chapter28. Deeplearning\n(a)Legal/Judicial Leading          \nEconomic         Indicators       European Community Monetary/Economic  \nAccounts/Earnings Interbank Markets\nGovernment Borrowings Disasters and Accidents     Energy Markets\n(b)\nFigure 28.5 2d visualization of some bag of words data from the Reuters RCV1-v2 corpus. (a) Results of\nusing LSA. (b) results of using a deep auto-encoder. Source: Figure 4 of (Hinton and Salakhutdinov 2006).\nUsed with kind permission of Ruslan Salakhutdinov.\nThe \ufb01rst 2 hidden layers were trained in a greedy unsupervised fashion from 50,000 MNIST\ndigits, using 30 epochs (passes over the data) and stochastic gradient descent, with the CD\nheuristic. This process took \u201ca few hours per layer\u201d (Hinton et al. 2006, p1540). Then the toplayer was trained using as input the activations of the lower hidden layer, as well as the classlabels. The corresponding generative model had a test error of about 2.5%. The network weightswere then carefully \ufb01ne-tuned on all 60,000 training images using the up-down procedure. Thisprocess took \u201cabout a week\u201d (Hinton et al. 2006, p1540). The model can be used to classifyby performing a deterministic bottom-up pass, and then computing the free energy for thetop-level RBM for each possible class label. The \ufb01nal error on the test set was about 1.25%. Themisclassi\ufb01ed examples are shown in Figure 28.4(b).\nThis was the best error rate of any method on the permutation-invariant version of MNIST\nat that time. (By permutation-invariant, we mean a method that does not exploit the fact thatthe input is an image. Generic methods work just as well on permuted versions of the input(see Figure 1.5), and can therefore be applied to other kinds of datasets.) The only other methodthat comes close is an SVM with a degree 9 polynomial kernel, which has achieved an errorrate of 1.4% (Decoste and Schoelkopf 2002). By way of comparison, 1-nearest neighbor (usingall 60,000 examples) achieves 3.1% (see mnist1NNdemo ). This is not as good, although 1-NN is\nmuch simpler.\n3\n28.4.2 Data visualization and feature discovery using deep auto-encoders\nDeep autoencoders can learn informative features from raw data. Such features are often usedas input to standard supervised learning methods.\nTo illustrate this, consider \ufb01tting a deep auto-encoder with a 2d hidden bottleneck to some\n3. One can get much improved performance on this task by exploiting the fact that the input is an image. One way to do\nthis is to create distorted versions of the input, adding small shifts and translations (see Figure 16.13 for some examples).\nApplying this trick reduced the SVM error rate to 0.56%. Similar error rates can be achieved using convolutional neural\nnetworks (Section 16.5.1) trained on distorted images ((Simard et al. 2003) got 0.4%). However, the point of DBNs is that\nthey offer a way to learn such prior knowledge, without it having to be hand-crafted.", "1033": "28.4. Applicationsofdeepnetworks 1003\n/g6/g5/g7/g1 /g6/g5/g8/g1 /g6/g5/g10/g1 /g6/g5/g13/g1 /g7/g5/g12/g1 /g9/g5/g8/g1 /g12/g5/g10/g1 /g7/g8/g5/g13 /g8/g11/g5/g12 /g11/g7/g5/g8 /g7/g6/g6/g1/g7/g6/g8/g6/g9/g6/g10/g6/g11/g6\n/g18/g23/g21/g20/g27/g27/g1/g3/g2/g4/g1/g17/g31/g23/g21/g26/g32/g26/g29/g28/g1/g3/g2/g4/g14/g34/g33/g29/g23/g28/g21/g29/g22/g23/g31/g1/g7/g6/g15\n/g16/g19/g14/g1/g7/g6/g15\n/g16/g19/g14/g1/g11/g6/g15\n/g14/g34/g33/g29/g23/g28/g21/g29/g22/g23/g31/g1/g7/g6/g15\n/g30/g31/g26/g29/g31/g1/g33/g29/g1/g24/g26/g28/g23 \u0001/g33/g34/g28/g26/g28/g25\nFigure 28.6 Precision-recall curves for document retrieval in the Reuters RCV1-v2 corpus. Source: Figure\n3.9 of (Salakhutdinov 2009). Used with kind permission of Ruslan Salakhutdinov.\ntext data. The results are shown in Figure 28.5. On the left we show the 2d embedding produced\nby LSA (Section 27.2.2), and on the right, the 2d embedding produced by the auto-encoder. It isclear that the low-dimensional representation created by the auto-encoder has captured a lot ofthe meaning of the documents, even though class labels were not used.\n4\nNote that various other ways of learning low-dimensional continuous embeddings of words\nhave been proposed. See e.g., (Turian et al. 2010) for details.\n28.4.3 Information retrieval using deep auto-encoders (semantic hashing)\nIn view of the sucess of RBMs for information retrieval discussed in Section 27.7.3.1, it is naturalto wonder if deep models can do even better. In fact they can, as is shown in Figure 28.6.\nMore interestingly, we can use a binary low-dimensional representation in the middle layer\nof the deep auto-encoder, rather than a continuous representation as we used above. Thisenables very fast retrieval of related documents. For example, if we use a 20-bit code, wecan precompute the binary representation for all the documents, and then create a hash-tablemapping codewords to documents. This approach is known as semantic hashing, since the\nbinary representation of semantically similar documents will be close in Hamming distance.\nFor the 402,207 test documents in Reuters RCV1-v2, this results in about 0.4 documents per\nentry in the table. At test time, we compute the codeword for the query, and then simply retrievethe relevant documents in constant time by looking up the contents of the relevant address in\nmemory. To \ufb01nd other other related documents, we can compute all the codewords within a\n4. Some details. Salakhutdinov and Hinton used the Reuters RCV1-v2 data set, which consists of 804,414 newswire\narticles, manually classi\ufb01ed into 103 topics. They represent each document by counting how many times each of the top\n2000 most frequent words occurs. They trained a deep auto-encoder with 2000-500-250-125-2 layers on half of the data.\nThe 2000 visible units use a replicated softmax distribution, the 2 hidden units in the middle layer have a Gaussian\ndistribution, and the remaining units have the usual Bernoulli-logistic distribution. When \ufb01ne tuning the auto-encoder,\na cross-entropy loss function (equivalent to maximum likelihood under a multinoulli distribution) was used. See (Hinton\nand Salakhutdinov 2006) for further details.", "1034": "1004 Chapter28. Deeplearning\nx1 x2 x3 x4h1\n1h12h13 h21h22h23\nw1w1w1\nw2w2w2\nFigure 28.7 A small 1d convolutional RBM with two groups of hidden units, each associated with a \ufb01lter\nof size 2. h1\n1andh21are two different \u201cviews\u201d of the data in the \ufb01rst window, (x1,x2). The \ufb01rst view is\ncomputed using the \ufb01lter w1, the second view using \ufb01lter w2. Similarly, h12andh22are the views of the\ndata in the second window, (x2,x3), computed using w1andw2respectively.\nHamming distance of, say, 4. This results in retrieving about 6196\u00d70.4\u22482500documents5.\nThe key point is that the total time is independent of the size of the corpus.\nOf course, there are other techniques for fast document retrieval, such as inverted indices.\nThese rely on the fact that individual words are quite informative, so we can simply intersect all\nthe documents that contain each word. However, when performing image retrieval, it is clear thatwe do not want to work at the pixel level. Recently (Krizhevsky and Hinton 2010) showed thata deep autoencoder could learn a good semantic hashing function that outperformed previoustechniques (Torralba et al. 2008; Weiss et al. 2008) on the 80 million tiny images dataset. Itis hard to apply inverted indexing techniques to real-valued data (although one could imaginevector quantizing image patches).\n28.4.4 Learning audio features using 1d convolutional DBNs\nTo apply DBNs to time series of unbounded length, it is necessary to use some form of parametertying. One way to do this is to use convolutional DBNs (Lee et al. 2009; Desjardins and Bengio\n2008), whichuseconvolutional RBMsas theirbasic unit. These modelsare agenerative versionofconvolutional neural nets discussed in Section 16.5.1. The basic idea is illustrated in Figure 28.7.The hidden activation vector for each group is computed by convolving the input vector withthat group\u2019s \ufb01lter (weight vector or matrix). In other words, each node within a hidden groupis a weighted combination of a subset of the inputs. We compute the activaton of all thehidden nodes by \u201csliding\u201d this weight vector over the input. This allows us to model translation\ninvariance, since we use the same weights no matter where in the input vector the patternoccurs.\n6Each group has its own \ufb01lter, corresponding to its own pattern detector.\n5. Note that 6196 =/summationtext4\nk=0/parenleftbig20\nk/parenrightbig\nis the number of bit vectors that are up to a Hamming distance of 4 away.\n6. It is often said that the goal of deep learnng is to discover invariant features, e.g., a representation of an object\nthat does not change even as nuisance variables, such as the lighting, do change. However, sometimes these so-called\n\u201cnuisance variables\u201d may be the variables of interest. For example if the task is to determine if a photograph was taken\nin the morning or the evening, then lighting is one of the more salient features, and object identity may be less relevant.\nAs always, one task\u2019s \u201csignal\u201d is another task\u2019s \u201cnoise\u201d, so it unwise to \u201cthrow away\u201d apparently irrelevant information", "1035": "28.5. Discussion 1005\nMore formally, for binary 1d signals, we can de\ufb01ne the full conditionals in a convolutional\nRBM as follows (Lee et al. 2009):\np(hk\nt=1|v)=s i g m ( ( wk\u2297v)t+bt) (28.6)\np(vs=1|h) = sigm(/summationdisplay\nk(wk\u2297hk)s+cs) (28.7)\nwherewkis the weight vector for group k,btandcsare bias terms, and a\u2297brepresents the\nconvolution of vectors aandb.\nIt is common to add a max pooling layer as well as a convolutional layer, which computes\na local maximum over the \ufb01ltered response. This allows for a small amount of translation\ninvariance. It also reduces the size of the higher levels, which speeds up computation consider-ably. De\ufb01ning this for a neural network is simple, but de\ufb01ning this in a way which allows forinformation \ufb02ow backwards as well as forwards is a bit more involved. The basic idea is similarto a noisy-OR CPD (Section 10.2.3), where we de\ufb01ne a probabilistic relationship between the maxnode and the parts it is maxing over. See (Lee et al. 2009) for details. Note, however, that thetop-down generative process will be difficult, since the max pooling operation throws away somuch information.\n(Lee et al. 2009) applies 1d convolutional DBNs of depth 2 to auditory data. When the input\nconsists of speech signals, the method recovers a representation that is similar to phonemes.When applied to music classi\ufb01cation and speaker identi\ufb01cation, their method outperforms tech-niques using standard features such as MFCC. (All features were fed into the same discriminativeclassi\ufb01er.)\nIn (Seide et al. 2011), a deep neural net was used in place of a GMM inside a conventional\nHMM. The use of DNNs signi\ufb01cantly improved performance on conversational speech recogni-tion. In an interview, the tech lead of this project said \u201chistorically, there have been very fewindividual technologies in speech recognition that have led to improvements of this magnitude\u201d.\n7\n28.4.5 Learning image features using 2d convolutional DBNs\nWe can extend a convolutional DBN from 1d to 2d in a straightforward way (Lee et al. 2009), asillustrated in Figure 28.8. The results o fa3l a y e rs y s t e mt r ained on four classes of visual objects\n(cars, motorbikes, faces and airplanes) from the Caltech 101 dataset are shown in Figure 28.9.We only show the results for layers 2 and 3, because layer 1 learns Gabor-like \ufb01lters that arevery similar to those learned by sparse coding, shown in Figure 13.21(b). We see that layer 2 haslearned some generic visual parts that are shared amongst object classes, and layer 3 seems tohave learned \ufb01lters that look like grandmother cells, that are speci\ufb01c to individual object classes,and in some cases, to individual objects.\n28.5 Discussion\nSo far, we have been discussing models inspired by low-level processing in the brain. Thesemodels have produced useful features for simple classi\ufb01cation tasks. But can this pure bottom-up\ntoo early.\n7. Source: http://research .microsoft .com/en-us/news/features/speechrecognition-082911 .aspx.", "1036": "1006 Chapter28. Deeplearning\nFigure 28.8 A 2d convolutional RBM with max-pooling layers. The input signal is a stack of 2d images\n(e.g., color planes). Each input layer is passed through a different set of \ufb01lters. Each hidden unit is\nobtained by convolving with the appropriate \ufb01lter, and then summing over the input planes. The \ufb01nal layer\nis obtained by computing the local maximum within a small window. Source: Figure 1 of (Chen et al.\n2010) . Used with kind permission of Bo Chen.\nfaces, cars, airplanes, motorbikes\n(a)\n (b)\nFigure 28.9 Visualization of the \ufb01lters learned by a convolutional DBN in layers two and three. Source:\nFigure 3 of (Lee et al. 2009). Used with kind permission of Honglak Lee.\napproach scale to more challenging problems, such as scene interpretation or natural language\nunderstanding?\nTo put the problem in perspective, consider the DBN for handwritten digit classi\ufb01cation in\nFigure 28.4(a). This has about 1.6M free parameters ( 28\u00d728\u00d7500+500\u00d7500+510\u00d72000 =\n1,662,000). Although this is a lot, it is tiny compared to the number of neurons in the brain.\nAs Hinton says,\nThis is about as many parameters as 0.002 cubic millimetres of mouse cortex, and several\nhundred networks of this complexity could \ufb01t within a single voxel of a high-resolution\nfMRI scan. This suggests that much bigger networks may be required to compete with\nhuman shape recognition abilities. \u2014 (Hinton et al. 2006, p1547).\nTo scale up to more challenging problems, various groups are using GPUs (see e.g., (Raina\net al. 2009)) and/or parallel computing. But perhaps a more efficient approach is to work at a\nhigher level of abstraction, where inference is done in the space of objects or their parts, rather", "1037": "28.5. Discussion 1007\nthan in the space of bits and pixels. That is, we want to bridge the signal-to-symbol divide,\nwhere by \u201csymbol\u201d we mean something atomic, that can be combined with other symbols in a\ncompositional way.\nThe question of how to convert low level signals into a more structured/ \u201csemantic\u201d represen-\ntation is known as the symbol grounding problem (Harnard 1990). Traditionally such symbols\nare associated with words in natural language, but it seems unlikely we can jump directly fromlow-level signals to high-level semantic concepts. Instead, what we need is an intermediate levelof symbolic or atomic parts.\nA very simple way to create such parts from real-valued signals, such as images, is to apply\nvector quantization. This generates a set of visual words. These can then be modelled using\nsome of the techniques from Chapter 27 for modeling bags of words. Such models, however, arestill quite \u201cshallow\u201d.\nIt is possible to de\ufb01ne, and learn, deep models which use discrete latent parts. Here we just\nmention a few recent approaches, to give a \ufb02avor of the possibilites. (Salakhutdinov et al. 2011)combine RBMs with hierarchical latent Dirichlet allocation methods, trained in an unsupervisedway. (Zhu et al. 2010) use latent and-or graphs, trained in a manner similar to a latent structuralSVM. A similar approach, based on grammars, is described in (Girshick et al. 2011). What isinteresting about these techniques is that they apply data-driven machine learning methodsto rich structured/symbolic \u201cAI-style\u201d models. This seems like a promising future direction formachine learning.", "1038": "", "1039": "Notation\nIntroduction\nIt is very difficult to come up with a single, consistent notation to cover the wide variety of\ndata, models and algorithms that we discuss. Furthermore, conventions differ between machinelearning and statistics, and between different books and papers. Nevertheless, we have triedto be as consistent as possible. Below we summarize most of the notation used in this book,although individual sections may introduce new notation. Note also that the same symbol mayhave different meanings depending on the context, although we try to avoid this where possible.\nGeneral math notation\nSymbol Meaning\n\u230ax\u230b Floor ofx, i.e., round down to nearest integer\n\u2308x\u2309 Ceiling of x, i.e., round up to nearest integer\nx\u2297y Convolution of xandy\nx\u2299y Hadamard (elementwise) product of xandy\na\u2227b logical AND\na\u2228b logical OR\n\u00aca logical NOT\nI(x) Indicator function, I(x)=1ifxis true, else I(x)=0\n\u221e In\ufb01nity\n\u2192 Tends towards, e.g., n\u2192\u221e\n\u221d Proportional to, so y=axcan be written as y\u221dx\n|x| Absolute value\n|S| Size (cardinality) of a set\nn! Factorial function\n\u2207 Vector of \ufb01rst derivatives\n\u22072Hessian matrix of second derivatives\n/defines De\ufb01ned as\nO(\u00b7) Big-O: roughly means order of magnitude\nR The real numbers\n1:n Range (Matlab convention): 1:n={1,2,...,n}\n\u2248 Approximately equal to\nargmaxxf(x)Argmax: the value xthat maximizes f", "1040": "1010 Notation\nB(a,b) Beta function, B(a,b)=\u0393(a)\u0393(b)\n\u0393(a+b)\nB(\u03b1) Multivariate beta function,/producttext\nk\u0393(\u03b1k)\n\u0393(/summationtext\nk\u03b1k) /parenleftbign\nk/parenrightbig\nnchoosek, equal to n!/(k!(n\u2212k)!)\n\u03b4(x) Dirac delta function, \u03b4(x)=\u221eifx=0, else\u03b4(x)=0\n\u03b4ij Kronecker delta, equals 1 if i=j, otherwise equals 0\n\u03b4x(y) Kronecker delta, equals 1 if x=y, otherwise equals 0\nexp(x) Exponential function ex\n\u0393(x) Gamma function, \u0393(x)=/integraltext\u221e\n0ux\u22121e\u2212udu\n\u03a8(x) Digamma function, \u03a8(x)=d\ndxlog\u0393(x)\nX A set from which values are drawn (e.g., X=RD)\nLinear algebra notation\nWe use boldface lowercase to denote vectors, such as a, and boldface uppercase to denote\nmatrices, such as A. Vectors are assumed to be column vectors, unless noted otherwise.\nSymbol Meaning\nA/follows0 Ais a positive de\ufb01nite matrix\ntr(A) Trace of a matrix\ndet(A) Determinant of matrix A\n|A| Determinant of matrix A\nA\u22121I n v e r s eo fam a t r i x\nA\u2020Pseudo-inverse of a matrix\nATTranspose of a matrix\naTTranspose of a vector\ndiag(a) Diagonal matrix made from vector a\ndiag(A)Diagonal vector extracted from matrix A\nIorId Identity matrix of size d\u00d7d(ones on diagonal, zeros off)\n1or1d Vector of ones (of length d)\n0or0d Vector of zeros (of length d)\n||x||=||x||2Euclidean or /lscript2norm/radicalBig/summationtextd\nj=1x2\nj\n||x||1 /lscript1norm/summationtextd\nj=1|xj|\nA:,j j\u2019th column of matrix\nAi,: transpose of i\u2019th row of matrix (a column vector)\nAij Element(i,j)of matrix A\nx\u2297y Tensor product of xandy\nProbability notation\nWe denote random and \ufb01xed scalars by lower case, random and \ufb01xed vectors by bold lower case,\nand random and \ufb01xed matrices by bold upper case. Occastionally we use non-bold upper caseto denote scalar random variables. Also, we use p()for both discrete and continuous random\nvariables.", "1041": "Notation 1011\nSymbol Meaning\nX\u22a5YX is independent of Y\nX/negationslash\u22a5YX is not independent of Y\nX\u22a5Y|ZX is conditionally independent of YgivenZ\nX/negationslash\u22a5Y|ZX is not conditionally independent of YgivenZ\nX\u223cpX is distributed according to distribution p\n\u03b1 Parameters of a Beta or Dirichlet distribution\ncov[x] Covariance of x\nE[X] Expected value of X\nEq[X] Expected value of Xwrt distribution q\nH(X)orH(p)Entropy of distribution p(X)\nI(X;Y) Mutual information between XandY\nKL(p||q) KL divergence from distribution ptoq\n/lscript(\u03b8) Log-likelihood function\nL(\u03b8,a) Loss function for taking action awhen true state of nature is \u03b8\n\u03bb Precision (inverse variance) \u03bb=1/\u03c32\n\u039b Precision matrix \u039b=\u03a3\u22121\nmode[X] Most probable value of X\n\u03bc Mean of a scalar distribution\n\u03bc Mean of a multivariate distribution\np(x) Probability density or mass function\np(x|y) Conditional probability density of xgiveny\n\u03a6 cdf of standard normal\n\u03c6 pdf of standard normal\n\u03c0 multinomial parameter vector, Stationary distribution of Markov chain\n\u03c1 Correlation coefficient\nsigm(x) Sigmoid (logistic) function,1\n1+e\u2212x\n\u03c32Variance\n\u03a3 Covariance matrix\nvar[x] Variance of x\n\u03bd Degrees of freedom parameter\nZ Normalization constant of a probability distribution\nMachine learning/statistics notation\nIn general, we use upper case letters to denote constants, such as C,D,K,N,S,T, etc. We\nuse lower case letters as dummy indexes of the appropriate range, such as c=1:Cto index\nclasses,j=1:D to index input features, k=1:K to index states or clusters, s=1:Sto\nindex samples, t=1:Tto index time, etc. To index data cases, we use the notation i=1:N,\nalthough the notation n=1:Nis also widely used.\nWe usexto represent an observed data vector. In a supervised problem, we use yoryto\nrepresent the desired output label. We use zto represent a hidden variable. Sometimes we also\nuseqto represent a hidden discrete variable.", "1042": "1012 Notation\nSymbol Meaning\nC Number of classes\nD Dimensionality of data vector (number of features)\nR Number of outputs (response variables)\nD Training dataD={xi|i=1:N}orD={(xi,yi)|i=1:N}\nDtest Test data\nJ(\u03b8) Cost function\nK Number of states or dimensions of a variable (often latent)\n\u03ba(x,y)Kernel function\nK Kernel matrix\n\u03bb Strength of /lscript2or/lscript1regularizer\nN Number of data cases\nNc Number of examples of class c,Nc=/summationtextN\nn=1I(yn=c)\n\u03c6(x) Basis function expansion of feature vector x\n\u03a6 Basis function expansion of design matrix X\nq() Approximate or proposal distribution\nQ(\u03b8,\u03b8old)Auxiliary function in EM\nS Number of samples\nT Length of a sequence\nT(D)Test statistic for data\nT Transition matrix of Markov chain\n\u03b8 Parameter vector\n\u03b8(s)s\u2019th sample of parameter vector\n\u02c6\u03b8 Estimate (usually MLE or MAP) of \u03b8\n\u02c6\u03b8ML Maximum likelihood estimate of \u03b8\n\u02c6\u03b8MAP MAP estimate of \u03b8\n\u03b8 Estimate (usually posterior mean) of \u03b8\nw Vector of regression weights (called \u03b2in statistics)\nW Matrix of regression weights\nxij Component (i.e., feature) jof data case i,f o ri=1:N,j=1:D\nxi Training case, i=1:N\nX Design matrix of size N\u00d7D\nx Empirical mean x=1\nN/summationtextNi=1xi\n\u02dcx Future test case\nx\u2217 Future test case\ny Vector of all training labels y=(y1,...,y N)\nzij Latent component jfor casei\nGraphical model notation\nIn graphical models, we index nodes by s,t,u\u2208V, and states by i,j,k\u2208X.", "1043": "Notation 1013\nSymbol Meaning\ns\u223ctNodesis connected to node t\nbel Belief function\nC Cliques of a graph\nchjChild of node ji naD A G\ndescjDescendants of node ji naD A G\nG A graph\nE Edges of a graph\nmbtMarkov blanket of node t\nnbdtNeighborhood of node t\npatParents of node ti naD A G\npredtPredecessors of node tin a DAG wrt some ordering\n\u03c8c(xc)Potential function for clique c\nS Separators of a graph\n\u03b8sjkprob. node sis in state kgiven its parents are in states j\nV Nodes of a graph", "1044": "1014 Notation\nList of commonly used abbreviations\nAbbreviation Meaning\ncdf Cumulative distribution function\nCPD Conditional probability distributionCPT Conditional probability tableCRF Conditional random \ufb01eldDAG Directed acyclic graphicDGM Directed graphical modelEB Empirical BayesEM Expectation maximization algorithmEP Expectation propagationGLM Generalized linear modelGMM Gaussian mixture modelHMM Hidden Markov modeliid Independent and identically distributediff If and only ifKL Kullback Leibler divergenceLDS Linear dynamical systemLHS Left hand side (of an equation)MAP Maximum A Posterior estimateMCMC Markov chain Monte CarloMH Metropolis HastingsMLE Maximum likelihood estimateMPM Maximum of Posterior MarginalsMRF Markov random \ufb01eldMSE Mean squared errorNLL Negative log likelihoodOLS Ordinary least squarespd Positive de\ufb01nite (matrix)pdf Probability density functionpmf Probability mass functionRBPF Rao-Blackwellised particle \ufb01lterRHS Right hand side (of an equation)RJMCMC Reversible jump MCMCRSS Residual sum of squaresSLDS Switching linear dynamical systemSSE Sum of squared errorsUGM Undirected graphical modelVB Variational Bayeswrt With respect to", "1045": "Bibliography\nAbend, K., T. J. Harley, and L. N. Kanal\n(1965). Classi\ufb01cation of Binary Ran-\ndom Patterns. IEEETransactionson\nInformationTheory 11(4), 538\u2013544.\nAckley, D., G. Hinton, and T. Sejnowski\n(1985). A learning algorithm forboltzmann machines. Cognitive\nScience 9, 147\u2013169.\nAdams, R. P., H. Wallach, and\nZ. Ghahramani (2010). Learning thestructure of deep sparse graphicalmodels. InAI/Statistics.\nAggarwal, D. and S. Merugu (2007).\nPredictive discrete latent factormodels for large scale dyadic data.\nInProc. of the Int\u2019l Conf. on Knowl-\nedgeDiscoveryandDataMining.\nAhmed, A. and E. Xing (2007).\nOn tight approximate inference of\nthe logistic-normal topic admixturemodel. InAI/Statistics.\nAhn, J.-H. and J.-H. Oh (2003). A Con-\nstrained EM Algorithm for Princi-\npal Component Analysis. Neural\nComputation 15, 57\u201365.\nAhn, S., A. Korattikara, and M. Welling\n(2012). Bayesian Posterior Sam-\npling via Stochastic Gradient Fisher\nScoring. In Intl. Conf. on Machine\nLearning.\nAiroldi, E., D. Blei, S. Fienberg, and\nE. Xing (2008). Mixed-membershipstochastic blockmodels. J. of Ma-\nchine Learning Research 9, 1981\u2013\n2014.\nAitchison, J. (1982). The statistical\nanalysis of compositional data. J.of\nRoyal Stat. Soc. Series B 44 (2), 139\u2013\n177.Aji, S. M. and R. J. McEliece (2000,\nMarch). The generalized distribu-\ntive law. IEEE Trans. Info. The-\nory 46(2), 325\u2013343.\nAlag, S. and A. Agogino (1996). In-\nference using message propoga-\ntion and topology transformationin vector Gaussian continuous net-works. InUAI.\nAlbers, C., M. Leisink, and H. Kap-\npen (2006). The Cluster VariationMethod for Efficient Linkage Anal-ysis on Extended Pedigrees. BMC\nBioinformatics 7.\nAlbert, J. and S. Chib (1993). Bayesian\nanalysis of binary and polychoto-\nmous response data. J. of the Am.\nStat.Assoc. 88 (422), 669\u2013679.\nAllwein, E., R. Schapire, and Y. Singer\n(2000). Reducing multiclass to bi-\nnary: A unifying approach for mar-\ngin classi\ufb01ers. J.ofMachineLearn-\ningResearch , 113\u2013141.\nAloise, D., A. Deshpande, P. Hansen,\nand P. Popat (2009). NP-hardness\nof Euclidean sum-of-squares clus-\ntering.Machine Learning 75, 245\u2013\n249.\nAlpaydin, E. (2004). Introduction to\nmachinelearning. MIT Press.\nAltun, Y., T. Hofmann, and I. Tsochan-\ntaridis (2006). Large Margin Meth-\nodsf\nor Structured and Interde-\npendent Output Variables. InG. Bakir, T. Hofmann, B. Scholkopf,A. Smola, B. Taskar, and S. Vish-wanathan (Eds.), Machine Learning\nwithStructuredOutputs. MIT Press.\nAmir, E. (2010). Approximation Al-\ngorithms for Treewidth. Algorith-\nmica 56(4), 448.\nAmir, E. and S. McIlraith (2005).\nPartition-based logical reason-ing for \ufb01rst-order and propo-\nsitional theories. Arti\ufb01cial\nIntelligence 162 (1), 49\u201388.\nAndo, R. and T. Zhang (2005). A\nframework for learning predictive\nstructures from multiple tasks and\nunlabeled data. J. of Machine\nLearningResearch 6, 1817\u20131853.\nAndrews, D. and C. Mallows (1974).\nScale mixtures of Normal distribu-\ntions.J. of Royal Stat. Soc. Series\nB3 6, 99\u2013102.Andrieu, C., N. de Freitas, and\nA. Doucet (2000). SequentialBayesian estimation and model se-lection for dynamic kernel ma-chines. Technical report, Cam-bridge Univ.\nAndrieu, C., N. de Freitas, and\nA. Doucet (2001). Robust FullBayesian Learning for Radial Ba-\nsis Networks. Neural Computa-\ntion 13(10), 2359\u20132407.\nAndrieu, C., N. de Freitas, A. Doucet,\nand M. Jordan (2003). An introduc-\ntion to MCMC for machine learn-ing.MachineLearning 50, 5\u201343.\nAndrieu, C., A. Doucet, and V. Tadic\n(2005). Online EM for parameterestimation in nonlinear-non Gaus-sian state-space models. In Proc.\nIEEECDC.\nAndrieu, C. and J. Thoms (2008). A\ntutorial on adaptive MCMC. Statis-\nticalComputing 18, 343\u2013373.\nAoki, M. (1987). Statespacemodelingof\ntimeseries. Springer.\nArchambeau, C. and F. Bach (2008).\nSparse probabilistic projections. InNIPS.\nArgyriou, A., T. Evgeniou, and M. Pon-\ntil (2008). Convex multi-task fea-ture learning. Machine Learn-\ning 73(3), 243\u2013272.\nArmagan, A., D. Dunson, and J. Lee\n(2011). Generalized double paretoshrinkage. Technical report, Duke.\nArmstrong, H. (2005). Bayesian esti-\nmation of decomposable Gaussian\ngraphical models. Ph.D. thesis,\nUNSW.\nArmstrong, H., C. Carter, K. Wong,\nand R. Kohn (2008). Bayesian Co-variance Matrix Estimation using aMixture of Decomposable Graphi-\ncal Models. Statistics and Comput-\ning, 1573\u20131375.\nArnborg, S., D. G. Corneil, and\nA. Proskurowski (1987). Complex-\nity of \ufb01nding embeddings in a k-\ntree.SIAM J. on Algebraic and Dis-\ncreteMethods 8 , 277\u2013284.\nArora, S. and B. Barak (2009). Com-\nplexity\nTheory: AModernApproach.\nCambridge.\nArthur, D. and S. Vassilvitskii (2007). k-\nmeans++: the advantages of carefulseeding. In Proc. 18th ACM-SIAM\nsymp. on Discrete algorithms, pp.\n1027\u00e2\u02d8A\u00b8S1035.", "1046": "1016 BIBLIOGRAPHY\nArulampalam, M., S. Maskell, N. Gor-\ndon, and T. Clapp (2002, Febru-\nary). A Tutorial on Particle Fil-ters for Online Nonlinear/Non-Gaussian Bayesian Tracking. IEEE\nTrans. on Signal Processing 50 (2),\n174\u2013189.\nAsavathiratham, C. (2000). The In\ufb02u-\nenceModel: ATractableRepresenta-\ntionfortheDynamicsofNetworkedMarkov Chains. Ph.D. thesis, MIT,\nDept. EECS.\nAtay-Kayis, A. and H. Massam (2005).\nA Monte Carlo method for comput-ing the marginal likelihood in non-decomposable Gaussian graphical\nmodels.Biometrika 92, 317\u2013335.\nAttenberg, J., K. Weinberger, A. Smola,\nA. Dasgupta, and M. Zinkevich\n(2009). Collaborative spam \ufb01lter-ing with the hashing trick. In Virus\nBulletin.\nAttias, H. (1999). Independent factor\nanalysis.Neural Computation 11,\n803\u2013851.\nAttias, H. (2000). A variational\nBayesian framework for graphicalmodels. InNIPS-12.\nBach, F. (2008). Bolasso: Model Con-\nsistent Lasso Estimation through\nthe Bootstrap. In Intl.Conf.onMa-\nchineLearning.\nBach, F. and M. Jordan (2001). Thin\njunction trees. In NIPS.\nBach, F. and M. Jordan (2005). A prob-\nabilistic interpretation of canonical\ncorrelation analysis. Technical Re-port 688, U. C. Berkeley.\nBach, F. and E. Moulines (2011). Non-\nasymptotic analysis of stochasticapproximation algorithms for ma-chine learning. In NIPS.\nBahmani, B., B. Moseley, A. Vattani,\nR. Kumar, and S. Vassilvitskii (2012).Scalable k-Means++. In VLDB.\nBakker, B. and T. Heskes (2003). Task\nClustering and Gating for Bayesian\nMultitask Learning. J. of Machine\nLearningResearch 4, 83\u201399.\nBaldi, P. and Y. Chauvin (1994).\nSmooth online learning algorithmsfor hidden Markov models. Neural\nComputation 6 , 305\u2013316.\nBalding, D. (2006). A tutorial on sta-\ntistical methods for population as-\nsociation studies. Nature Reviews\nGenetics 7 , 81\u201391.Banerjee, O., L. E. Ghaoui, and\nA. d\u2019Aspremont (2008). Model se-lection through sparse maximumlikelihood estimation for multivari-ate gaussian or binary data. J. of\nMachineLearningResearch 9, 485\u2013516.\nBar-Shalom, Y. and T. Fortmann\n(1988).Tracking and data associa-\ntion. Academic Press.\nBar-Shalom, Y. and X. Li (1993). Es-\ntimation and Tracking: Principles,\nTechniques and Software. Artech\nHouse.\nBarash, Y. and N. Friedman (2002).\nContext-speci\ufb01c Bayesian cluster-ing for gene expression data. J.\nCom\np.Bio. 9, 169\u2013191.\nBarber, D. (2006). Expectation Cor-\nrection for Smoothed Inference inSwitching Linear Dynamical Sys-\ntems.J. of Machine Learning Re-\nsearch 7, 2515\u20132540.\nBarber, D. and C. Bishop (1998).\nEnsemble Learning in Bayesian\nNeural Networks. In C. Bishop\n(Ed.),NeuralNetworksandMachine\nLearning, pp. 215\u2013237. Springer.\nBarber, D. and S. Chiappa (2007).\nUni\ufb01ed inference for variational\nbayesian linear gaussian statespace models. In NIPS.\nBarbieri, M. and J. Berger (2004). Op-\ntimal predictive model selection.\nAnnalsofStatistics 32, 870\u2013897.\nBartlett, P., M. Jordan, and J. McAuliffe\n(2006). Convexity, Classi\ufb01cation,and Risk Bounds. J.oftheAm.Stat.\nAssoc. 101(473), 138\u2013156.\nBaruniak, R. (2007). Compressive sens-\ning.IEEE Signal Processing Maga-\nzine.\nBarzilai, J. and J. Borwein (1988). Two\npoint step size gradient methods.IMAJ.ofNumericalAnalysis 8, 141\u2013\n148.\nBasu, S., T. Choudhury, B. Clarkson,\nand A. Pentland (2001). Learn-ing human interactions withthe in\ufb02uence model. Techni-cal Report 539, MIT Media Lab.ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-539-ABSTRACT.html.\nBaum, L. E., T. Petrie, G. Soules, and\nN. Weiss (1970). A maximizationtechnique occuring in the statisti-cal analysis of probabalistic func-\ntions in markov chains. TheAnnals\nof Mathematical Statistics 41, 164\u2013\n171.\nBeal, M. (2003). VariationalAlgorithms\nforApproximateBayesianInference.Ph.D. thesis, Gatsby Unit.\nBeal, M. and Z. Ghahramani (2006).\nVariational Bayesian Learning ofDirected Graphical Models with\nHidden Variables. Bayesian Anal-\nysis 1(4).\nBeal, M. J., Z. Ghahramani, and C. E.\nRasmussen (2002). The in\ufb01nite hid-\nden Markov model. In NIPS-14.\nBeck, A. and M. Teboulle (2009). A fast\niterative shrinkage-thresholding al-gorothm for linear inverse prob-lems.SIAM J. on Imaging Sci-\nences 2(1), 183\u2013202.\nBeinlich, I., H. Suermondt, R. Chavez,\nand G. Cooper (1989). The ALARMmonitoring system: A case studywith two probabilistic inferencetechniques for belief networks. In\nProc. of the Second European Conf.\nonAIinMedicine, pp. 247\u2013256.\nBekkerman, R., M. Bilenko, and\nJ. Langford (Eds.) (2011). ScalingUp\nMachineLearning. Cambridge.\nBell, A. J. and T. J. Sejnowski (1995).\nAn information maximisation ap-\nproach to blind separation and\nblind deconvolution. Neural Com-\nputation 7 (6), 1129\u20131159.\nBengio,Y\n. (2009). Learning deep ar-\nchitectures for AI. Foundationsand\nTrendsinMachineLearning 2 (1), 1\u2013\n127.\nBengio, Y. and S. Bengio (2000).\nModeling high-dimensional dis-\ncrete data with multi-layer neuralnetworks. In NIPS.\nBengio, Y., O. Delalleau, N. Roux,\nJ. Paiement, P. Vincent, andM. Ouimet (2004). Learning eigen-functions links spectral embedding\nand kernel PCA. Neural Computa-\ntion 16, 2197\u20132219.\nBengio, Y. and P. Frasconi (1995). Dif-\nfusion of context and credit infor-\nmation in markovian models. J. of\nAIResearch 3, 249\u2013270.\nBengio, Y. and P. Frasconi (1996).\nInput/output HMMs for sequence\nprocessing. IEEE Trans. on Neural\nNetworks 7 (5), 1231\u20131249.", "1047": "BIBLIOGRAPHY 1017\nBengio, Y., P. Lamblin, D. Popovici,\nand H. Larochelle (2007). Greedy\nlayer-wise training of deep net-works. InNIPS.\nBerchtold, A. (1999). The double chain\nmarkov model. Comm. Stat. Theor.\nMethods 28, 2569\u20132589.\nBerger, J. (1985). Bayesian salesman-\nship. In P. K. Goel and A. Zellner\n(Eds.),Bayesian Inference and De-\ncisionTechniqueswithApplications:\nEssays in Honor of Bruno deFinetti.\nNorth-Holland.\nBerger, J. and R. Wolpert (1988). The\nLikelihood Principle. The Instituteof Mathematical Statistics. 2nd edi-tion.\nBerkhin, P. (2006). A survey of\nclustering datamining techniques.In J. Kogan, C. Nicholas, and\nM. Teboulle (Eds.), Grouping Multi-\ndimensional Data: Recent Advances\ninClustering, pp. 25\u201371. Springer.\nBernardo, J. and A. Smith (1994).\nBayesianTheory. John Wiley.\nBerrou, C., A. Glavieux, and P. Thiti-\nmajashima (1993). Near Shannonlimit error-correcting coding anddecoding: Turbo codes. Proc. IEEE\nIntl.Comm.Conf..\nBerry, D. and Y. Hochberg (1999).\nBayesian perspectives on multiple\ncomparisons. J. Statist. Planning\nandInference 82, 215\u2013227.\nBertele, U. and F. Brioschi (1972). Non-\nserialDynamicProgramming. Aca-\ndemic Press.\nBertsekas, D. (1997). Parallel and Dis-\ntribution Computation: Numerical\nMethods. Athena Scienti\ufb01c.\nBertsekas, D. (1999). Nonlinear Pro-\ngramming (Second ed.). Athena\nScienti\ufb01c.\nBertsekas, D. and J. Tsitsiklis (2008).\nIntroduction to Probability. Athena\nScienti\ufb01c. 2nd Edition.\nBesag, J. (1975). Statistical analysis of\nnon-lattice data. TheStatistician24 ,\n179\u2013196.\nBhatnagar, N., C. Bogdanov, and\nE. Mossel (2010). The compu-tational complexity of estimatingconvergence time. Technical re-port, .Bhattacharya, A. and D. B. Dunson\n(2011). Simplex factor models formultivariate unordered categorical\ndata.J. of the Am. Stat. Assoc..T o\nappear.\nBickel, P. and E. Levina (2004). Some\ntheory for Fisher\u2019s linear discrimi-\nnant function, \"Naive Bayes\", andsome alternatives when there aremany more variables than obser-\nvations.Bernoulli 10, 989\u20131010.\nBickson, D. (2009). Gaussian Belief\nPropagation: Theory and Applica-\ntion. Ph.D. thesis, Hebrew Univer-sity of Jerusalem.\nBilmes, J. (2000). Dynamic Bayesian\nmultinets. In UAI.\nBilmes, J. A. (2001). Graphical models\nandaut\nomatic speech recognition.\nTechnical Report UWEETR-2001-0005, Univ. Washington, Dept. ofElec. Eng.\nBinder, J., D. Koller, S. J. Russell, and\nK. Kanazawa (1997). Adaptive prob-abilistic networks with hidden vari-ables.Machine Learning 29, 213\u2013\n244.\nBinder, J., K. Murphy, and S. Russell\n(1997). Space-efficient inference indynamic probabilistic networks. In\nIntl.JointConf.onAI.\nBirnbaum, A. (1962). On the founda-\ntions of statistical infernece. J. of\ntheAm.Stat.Assoc. 57, 269\u2013326.\nBishop, C. (1999). Bayesian PCA. In\nNIPS.\nBishop, C. (2006a). Patternrecognition\nandmachinelearning. Springer.\nBishop, C. (2006b). Patternrecognition\nandmachinelearning. Springer.\nBishop, C. and G. James (1993). Analy-\nsis of multiphase \ufb02ows using dual-\nenergy densitometry and neural\nnetworks.NuclearInstrumentsand\nMethods in Physics Research A327,\n580\u2013593.\nBishop, C. and M. Svens\u00e9n (2003).\nBayesian hierarchical mixtures ofexperts. InUAI.\nBishop, C. and M. Tipping (2000).\nVariational relevance vector ma-chines. InUAI.\nBishop, C. M. (1994). Mixture density\nnetworks. Technical Report NCRG4288, Neural Computing ResearchGroup, Department of ComputerScience, Aston University.Bishop, C. M. (1995). Neural Networks\nfor Pattern Recognition. ClarendonPress.\nBishop, Y., S. Fienberg, and P. Holland\n(1975).Discrete Multivariate Analy-\nsis: TheoryandPractice. MIT Press.\nBistarelli, S., U. Montanari, and\nF. Rossi (1997). Semiring-basedconstraint satisfaction and opti-\nmization.J. of the ACM 44 (2), 201\u2013\n236.\nBlake, A., P. Kohli, and C. Rother (Eds.)\n(2011).AdvancesinMarkovRandom\nFieldsforVisionandImageProcess-\ning. MIT Press.\nBlei, D. and J. Lafferty (2006a). Corre-\nlated topic models. In NIPS.\nBlei, D. and J. Lafferty (2006b). Dy-\nnamic topic models. In Intl. Conf.\nonMachineLearning, pp. 113\u2013120.\nBlei, D. and J. Lafferty (2007). A Corre-\nlated Topic Model of \"Science\". An-\nnalsofAppliedStat. 1 (1), 17\u201335.\nBlei, D. and J. McAuliffe (2010, March).\nSupervised topic models. Technicalreport, Princeton.\nBlei, D., A. Ng, and M. Jordan (2003).\nLatent dirichlet allocation. J. of\nMachineLearningResearch 3, 993\u20131022.\nBlumensath, T. and M. Davies (2007).\nOnthe\ndifference between Orthog-\nonal Matching Pursuit and Orthog-onal Least Squares. Technical re-port, U. Edinburgh.\nBo, L., C. Sminchisescu, A. Kanaujia,\nand D. Metaxas (2008). Fast Algo-rithms for Large Scale Conditional3D Prediction. In CVPR.\nBohning, D. (1992). Multinomial logis-\ntic regression algorithm. Annals of\ntheInst.ofStatisticalMath. 44, 197\u2013200.\nBollen, K. (1989). Structural Equation\nModels with Latent Variables. JohnWiley & Sons.\nBordes, A., L. Bottou, and P. Galli-\nnari (2009, July). Sgd-qn: Care-ful quasi-newton stochastic gradi-\nent descent. J.ofMachineLearning\nResearch 10, 1737\u20131754.\nBordes, A., L. Bottou, P. Gallinari,\nJ. Chang, and S. A. Smith (2010). Er-\nratum: SGDQN is Less Careful than\nExpected.J. of Machine Learning\nResearch 11, 2229\u20132240.", "1048": "1018 BIBLIOGRAPHY\nBoser, B. E., I. M. Guyon, and V. N.\nVapnik (1992). A training algorithm\nfor optimal margin classi\ufb01ers. In\nProc.oftheWorkshoponComputa-\ntionalLearningTheory.\nBottcher, S. G. and C. Dethlefsen\n(2003). deal: A package for learning\nbayesian networks. J. of Statistical\nSoftware 8 (20).\nBottolo, L. and S. Richardson (2010).\nEvolutionary stochastic search.BayesianAnalysis 5 (3), 583\u2013618.\nBottou, L. (1998). Online algorithms\nand stochastic approximations. InD. Saad (Ed.), Online Learning and\nNeuralNetworks. Cambridge.\nBottou, L. (2007). Learning with large\ndatasets (nips tutorial).\nBottou, L., O. Chapelle, D. DeCoste,\nand J. Weston (Eds.) (2007). Large\nScaleKernelMachines. MIT Press.\nBouchard, G. (2007). Efficient bounds\nfor the softmax and applications\nto approximate inference in hybrid\nmodels. In NIPS 2007 Workshop\nonApproximateInferenceinHybrid\nModels.\nBouchard-Cote, A. and M. Jordan\n(2009). Optimization of structured\nmean \ufb01eld objectives. In UAI.\nBowman, A.andA.Azzalini(1997). Ap-\npliedSmoothingTechniquesforData\nAnalysis. Oxford.\nBox, G. and N. Draper (1987). Empir-\nical Model-Building and ResponseSurfaces. Wiley.\nBox, G. and G. Tiao (1973). Bayesian\ninference in statistical analysis.\nAddison-Wesley.\nBoyd, S. and L. Vandenberghe (2004).\nConvexoptimization. Cambridge.\nBoyen, X. and D. Koller (1998).\nTractable inference for complexstochastic processes. In UAI.\nBoykov, Y., O. Veksler, and R. Zabih\n(2001). Fast approximate energyminimization via graph cuts. IEEE\nTrans. on Pattern Analysis and Ma-\nchineIntelligence 23 (11).\nBrand, M. (1996). Coupled hidden\nMarkov models for modeling inter-\nacting processes. Technical Report405, MIT Lab for Perceptual Com-puting.Brand, M. (1999). Structure learning\nin conditional probability modelsvia an entropic prior and param-\neter extinction. Neural Computa-\ntion 11, 1155\u20131182.\nBraun, M. and J. McAuliffe (2010). Vari-\national Inference for Large-ScaleModels of Discrete Choice. J.ofthe\nAm.Stat.Assoc. 105 (489), 324\u2013335.\nBreiman, L. (1996). Bagging predictors.\nMachineLearning 24 , 123\u2013140.\nBreiman, L. (1998). Arcing classi\ufb01ers.\nAnnalsofStatistics 26, 801\u2013849.\nBreiman, L. (2001a). Random forests.\nMachineLearning 45 (1), 5\u201332.\nBreiman, L. (2001b). Statistical mod-\neling: the two cultures. Statistical\nScience 16 (3), 199\u2013231.\nBreiman, L., J. Friedman, and R. Ol-\nshen(1984).Clas\nsi\ufb01cation and re-\ngressiontrees. Wadsworth.\nBreslow, N. E. and D. G. Clayton (1993).\nApproximate inference in general-ized linear mixed models. J. of the\nAm.Stat.Assoc. 88 (421), 9\u201325.\nBriers, M., A. Doucet, and S. Maskel\n(2010). Smoothing algorithms forstate-space models. Annals of\ntheInstituteofStatisticalMathemat-\nics 62(1), 61\u201389.\nBrochu, E., M. Cora, and N. de Fre-\nitas (2009, November). A tutorialon Bayesian optimization of expen-sive cost functions, with applica-tion to active user modeling andhierarchical reinforcement learn-ing. Technical Report TR-2009-23,Department of Computer Science,University of British Columbia.\nBrooks, S. and G. Roberts (1998).\nAssessing convergence of MarkovChain Monte Carlo algorithms.\nStatistics and Computing 8, 319\u2013\n335.\nBrown, L., T. Cai, and A. DasGupta\n(2001). Interval estimation for a bi-nomial proportion. Statistical Sci-\nence 16(2), 101\u2013133.\nBrown, M. P., R. Hughey, A. Krogh, I. S.\nMian, K. Sj\u00f6lander, and D. Haus-sler (1993). Using dirichlet mixturespriors to derive hidden Markovmodels for protein families. In\nIntl. Conf. on Intelligent Systems for\nMolecularBiology, pp. 47\u201355.Brown, P., M. Vannucci, and T. Fearn\n(1998). Multivariate Bayesian vari-\nable selection and prediction. J. of\nthe Royal Statistical Society B 60 (3),\n627\u2013641.\nBruckstein, A., D. Donoho, and\nM. Elad (2009). From sparse so-\nlutions of systems of equations tosparse modeling of signals and im-ages.SIAMReview 51 (1), 34\u201381.\nBryson, A. and Y.-C. Ho (1969). Applied\noptimal control: optimization, esti-\nmation, andcontrol. Blaisdell Pub-\nlishing Company.\nBuhlmann, P. and T. Hothorn (2007).\nBoosting Algorithms: Regulariza-tion, Prediction and Model Fitting.\nStatisticalScience 22 (4), 477\u2013505.\nBuhlmann, P. and S. van de\nGeer (2011). Statistics for High-\nDimensional Data: Methodology,\nTheoryandApplications. Springer.\nBuhlmann, P. and B. Yu (2003). Boost-\ning with the L2 loss: Regression\nand classi\ufb01cation. J.oftheAm.Stat.\nAssoc. 98(462), 324\u2013339.\nBuhlmann, P. and B. Yu (2006). Sparse\nboosting.J. of Machine Learning\nResearch 7 , 1001\u20131024.\nBui, H., S. Venkatesh, and G. West\n(2002). Policy Recognition in the\nAbstract Hidden Markov Model. J.\nofAIResearch 17, 451\u2013499.\nBuntine, W. (2002). Variational Exten-\nsions to EM and Multinomial PCA.InIntl.Conf.onMachineLearning.\nBuntine, W. and A. Jakulin (2004). Ap-\nplying Discrete PCA in Data Analy-sis. InUAI.\nBuntine,W\n. and A. Jakulin (2006). Dis-\ncrete Component Analysis. In Sub-\nspace, Latent Structure and Feature\nSelection: Statistical and Optimiza-tionPerspectivesWorkshop.\nBuntine, W. and A. Weigend (1991).\nBayesian backpropagation. Com-\nplexSystems 5, 603\u2013643.\nBurges, C. J., T. Shaked, E. Renshaw,\nA. Lazier, M. Deeds, N. Hamilton,\nand G. Hullender (2005). Learningto rank using gradient descent. In\nIntl.Conf.onMachineLearning, pp.\n89\u201396.\nBurkard, R., M. Dell\u2019Amico, and\nS. Martello (2009). Assignment\nProblems. SIAM.", "1049": "BIBLIOGRAPHY 1019\nByran, K. and T. Leise (2006). The\n25,000,000,000 Eigenvector: The\nLinear Algebra behind Google.SIAMReview 48 (3).\nCalvetti, D. and E. Somersalo (2007).\nIntroduction to Bayesian Scienti\ufb01cComputing. Springer.\nCandes, E., J. Romberg, and T. Tao\n(2006). Robust uncertainty prin-ciples: Exact signal reconstructionfrom highly incomplete frequency\ninformation. IEEE. Trans. Inform.\nTheory 52 (2), 489\u2013509.\nCandes, E. and M. Wakin (2008,\nMarch). An introduction to com-\npressive sampling. IEEESignalPro-\ncessingMagazine 21.\nCandes, E., M. Wakin, and S. Boyd\n(2008). Enhancing sparsity by\nreweighted l1 minimization. J. of\nFourierAnalysisandApplications 1,\n877\u2013905.\nCannings, C., E. A. Thompson, and\nM. H. Skolnick (1978). Probabil-ity functions in complex pedigrees.\nAdvances in Applied Probability 10,\n26\u201361.\nCanny, J. (2004). Gap: a factor model\nfor discrete data. In Proc. An-\nnual Intl. ACM SIGIR Conference,pp. 122\u2013129.\nCao, Z., T. Qin, T.-Y. Liu, M.-F. Tsai,\nand H. Li (2007). Learning to rank:From pairwise approach to listwise\napproach. In Intl.Conf.onMachine\nLearning, pp. 129\u00e2 \u02d8A\u00b8S136.\nCappe, O. (2010). Online Expectation\nMaximisation. In K. Mengersen,\nM. Titterington, and C. Robert(Eds.),Mixtures.\nCappe, O. and E. Mouline (2009, June).\nOnline EM Algorithm for Latent\nData Models. J. of Royal Stat. Soc.\nSeriesB 71 (3), 593\u2013613.\nCappe, O., E. Moulines, and T. Ryden\n(2005).Inference in Hidden Markov\nModels. Springer.\nCarbonetto, P. (2003). Unsupervised\nstatistical models for general object\nrecognition. Master\u2019s thesis, Uni-versity of British Columbia.\nCarlin, B. P. and T. A. Louis (1996).\nBayesandEmpiricalBayesMethods\nfor Data Analysis. Chapman and\nHall.Caron, F. and A. Doucet (2008).\nSparse Bayesian nonparametric re-\ngression. In Intl. Conf. on Machine\nLearning.\nCarreira-Perpinan, M. and C. Williams\n(2003). An isotropic gaussian mix-\nture can have more modes thancomponents. Technical ReportEDI-INF-RR-0185, School of Infor-matics, U. Edinburgh.\nCarter, C. and R. Kohn (1994). On\nGibbs sampling for state space\nmodels.Biometrika 81 (3), 541\u2013553.\nCarterette, B., P. Bennett, D. Chicker-\ning, and S. Dumais (2008). Here\nor There: Preference Judgments forRelevance. In Proc.ECIR.\nCaruana, R. (1998). A dozen tricks with\nmultitask learning. In G. Orr and\nK.-R. Mueller (Eds.), Neural Net-\nwork\ns: TricksoftheTrade. Springer-\nVerlag.\nCaruana, R. and A. Niculescu-Mizil\n(2006). An empirical comparison\nof supervised learning algorithms.\nInIntl.Conf.onMachineLearning.\nCarvahlo, C., N. Polson, and J. Scott\n(2010). The horseshoe estimator for\nsparse signals. Biometrika 97 (2),\n465.\nCarvahlo, L. and C. Lawrence (2007).\nCentroid estimation in discretehigh-dimensional spaces with ap-\nplications in biology. Proc. of\nthe National Academy of Science,\nUSA 105(4).\nCarvalho, C. M. and M. West (2007).\nDynamic matrix-variate graphical\nmodels.BayesianAnalysis 2 (1), 69\u2013\n98.\nCasella, G. and R. Berger (2002). Statis-\ntical inference. Duxbury. 2nd edi-\ntion.\nCastro, M., M. Coates, and R. D.\nNowak (2004). Likelihood based hi-erarchical clustering. IEEETrans.in\nSignalProcessing 52 (8), 230.\nCeleux, G. and J. Diebolt (1985).\nThe SEM algorithm: A probabilis-tic teacher derive from the EMalgorithm for the mixture prob-\nlem.ComputationalStatisticsQuar-\nterly 2, 73\u201382.\nCemgil, A. T. (2001). A technique for\npainless derivation of kalman \ufb01lter-\ning recursions. Technical report, U.Nijmegen.Cesa-Bianchi, N. and G. Lugosi (2006).\nPrediction, learning, and games.Cambridge University Press.\nCevher, V. (2009). Learning with com-\npressible priors. In NIPS.\nChai, K. M. A. (2010). Multi-tasklearn-\ning with Gaussian processes. Ph.D.thesis, U. Edinburgh.\nChang, H., Y. Weiss, and W. Freeman\n(2009). Informative Sensing. Tech-nical report, Hebrew U. Submittedto IEEE Transactions on Info. The-ory.\nChang, J. and D. Blei (2010). Hierar-\nchical relational models for docu-ment networks. The Annals of Ap-\npliedStatistics 4 (1), 124\u2013150.\nChang, J., J. Boyd-Graber, S. Gerrish,\nC. Wang, and D. Blei (2009). Read-ing tea leaves: How humans inter-pret topic models. In NIPS.\nChapelle, O. and L. Li (2011). An empir-\nical evaluation of Thompson sam-pling. InNIPS.\nChartrand, R. and W. Yin (2008). It-\neratively reweighted algorithms for\ncompressive sensing. In Intl. Conf.\non Acoustics, Speech and Signal\nProc.\nChechik, G., A. G. N. Tishby, and\nY. Weiss (2005). Information bot-tleneck for gaussian variables. J.\nof Machine Learning Research 6,\n165\u00e2\u02d8A\u00b8S188.\nCheeseman, P\n., J. Kelly, M. Self, J. Stutz,\nW. Taylor, and D. Freeman (1988).\nAutoclass: A Bayesian classi\ufb01cation\nsystem. In Proc. of the Fifth Intl.\nWorkshoponMachineLearning.\nCheeseman, P. and J. Stutz (1996).\nBayesian classi\ufb01cation (autoclass):\nTheory and results. In Fayyad,Pratetsky-Shapiro, Smyth, and\nUthurasamy (Eds.), Advances in\nKnowledgeDiscoveryandDataMin-\ning. MIT Press.\nChen, B., K. Swersky, B. Marlin, and\nN. de Freitas (2010). Sparsity priorsand boosting for learning localizeddistributed feature representations.Technical report, UBC.\nChen, B., J.-A. Ting, B. Marlin, and\nN. de Freitas (2010). Deep learningof invariant spatio-temporal fea-\ntures from video. In NIPSWorkshop\nonDeepLearning.", "1050": "1020 BIBLIOGRAPHY\nChen, M., D. Carlson, A. Zaas,\nC. Woods, G. Ginsburg, A. Hero,\nJ. Lucas, and L. Carin (2011,March). The Bayesian ElasticNet: Classifying Multi-Task Gene-Expression Data. IEEE Trans.\nBiomed.Eng. 58 (3), 468\u201379.\nChen, R. and S. Liu (2000). Mixture\nKalman \ufb01lters. J.RoyalStat.Soc.B .\nChen, S. and J. Goodman (1996). An\nempirical study of smoothing tech-niques for language modeling. In\nProc.34thACL, pp. 310\u2013318.\nChen, S. and J. Goodman (1998).\nAn empirical study of smoothing\ntechniques for language modeling.Technical Report TR-10-98, Dept.Comp. Sci., Harvard.\nChen, S. and J. Wigger (1995, July). Fast\northogonal least squares algorithmforefficientsubset modelselection.IEEE Trans. Signal Processing 3 (7),\n1713\u20131715.\nChen, S. S., D. L. Donoho, and M. A.\nSaunders (1998). Atomic decompo-sition by basis pursuit. SIAM Jour-\nnal on Scienti\ufb01c Computing 20 (1),\n33\u201361.\nChen, X., S. Kim, Q. Lin, J. G.\nCarbonell, and E. P. Xing (2010).Graph-Structured Multi-task Re-gression and an Efficient Optimiza-tion Method for General FusedLasso. Technical report, CMU.\nChib, S. (1995). Marginal likelihood\nfrom the Gibbs output. J. of the\nAm.Stat.Assoc. 90, 1313\u20131321.\nChickering, D. (1996). Learning\nBayesian networks is NP-Complete.InAI/StatsV.\nChickering, D. and D. Heckerman\n(1997). Efficient approximations forthe marginal likelihood of incom-plete data given a Bayesian net-\nwork.Machine Learning 29, 181\u2013\n212.\nChickering, D. M. (2002). Optimal\nstructure identi\ufb01cation with greedysearch.Journal of Machine Learn-\ningResearch 3, 507\u2013554.\nChipman, H., E. George, and R. Mc-\nCulloch (1998). Bayesian CARTmodel search. J. of the Am. Stat.\nAssoc. 93, 935\u2013960.\nChipman, H., E. George, and R. Mc-\nCulloch (2001). The practical imple-\nmentation of Bayesian Model Se-\nlection.Model Selection . IMS Lec-\nture Notes.Chipman, H., E. George, and R. Mc-\nCulloch (2006). Bayesian Ensemble\nLearning. In NIPS.\nChipman, H., E. George, and R. Mc-\nCulloch (2010). BART: Bayesian ad-\nditive regression trees. Ann. Appl.\nStat. 4(1), 266\u2013298.\nChoi, M., V. Tan, A. Anandkumar, and\nA. Willsky (2011). Learning latenttree graphical models. J. of Ma-\nchineLearningResearch .\nChoi, M. J. (2011). Trees and Be-\nyond: Exploiting and Improving\nTree-Structured Graphical Models.\nPh.D. thesis, MIT.\nChoset, H. and K. Nagatani (2001).\nTopological simultaneous localiza-tion and mapping (SLAM): towardex\nact localization without explicit\nlocalization. IEEE Trans. Robotics\nandAutomation 17 (2).\nChow, C. K. and C. N. Liu (1968).\nApproximating discrete probabil-ity distributions with dependence\ntrees.IEEETrans.onInfo.Theory14,\n462\u201367.\nChristensen, O., G. Roberts, and\nM. Sk\u00c3\u02dduld (2006). Robust Markov\nchain Monte Carlo methods for\nspatial generalized linear mixed\nmodels.J. of Computational and\nGraphicalStatistics 15, 1\u201317.\nChung, F. (1997). Spectral Graph The-\nory. AMS.\nCimiano, P., A. Schultz, S. Sizov,\nP. Sorg, and S. Staab (2009). Ex-\nplicit versus latent concept modelsfor cross-language information re-\ntrieval. InIntl.JointConf.onAI.\nCipra, B. (2000). The Ising Model Is\nNP-Complete. SIAMNews 33 (6).\nCiresan, D. C., U. Meier, L. M.\nGambardella, and J. Schmidhuber\n(2010). Deep big simple neural netsfor handwritten digit recognition.\nNeural Computation 22 (12), 3207\u2013\n3220.\nClarke, B. (2003). Bayes model av-\neraging and stacking when model\napproximation error cannot be ig-\nnored.J. of Machine Learning Re-\nsearch, 683\u2013712.\nClarke, B., E. Fokoue, and H. H. Zhang\n(2009).Principles and Theory for\nData Mining and Machine Learn-\ning. Springer.Cleveland, W. and S. Devlin (1988).\nLocally-weighted regression: Anapproach to regression analysis by\nlocal \ufb01tting. J. of the Am. Stat. As-\nsoc. 83(403), 596\u2013610.\nCollins, M. (2002). Discrimina-\ntive Training Methods for Hidden\nMarkov Models: Theory and Exper-iments with Perceptron Algorithms.InEMNLP.\nCollins, M., S. Dasgupta, and R. E.\nSchapire (2002). A generalizationof principal components analysisto the exponential family. In NIPS-\n14.\nCollins, M. and N. Duffy (2002). Con-\nvolution kernels for natural lan-guage. InNIPS.\nCollobert, R. and J. Weston (2008).\nA Uni\ufb01ed Architecture for NaturalLanguage Processing: Deep NeuralNetworks with Multitask Learning.\nInIntl.Conf.onMachineLearning.\nCombettes, P. and V. Wajs (2005). Sig-\nnal recovery by proximal forward-backward splitting. SIAM J. Multi-\nscaleModel.Simul. 4 (4), 1168\u20131200.\nCook, J. (2005). Exact Calculation\nof Beta Inequalities. Technical re-\nport, M. D. Anderson Cancer Cen-ter, Dept. Biostatistics.\nCooper, G. and E. Herskovits (1992).\nA Bayesian method for the induc-tion of probabilistic networks from\ndata.MachineLearning 9, 309\u2013347.\nCoope\nr, G. and C. Yoo (1999). Causal\ndiscovery from a mixture of exper-\nimental and observational data. InUAI.\nCover, T. and P. Hart (1967). Near-\nest neighbor pattern classi\ufb01cation.\nIEEETrans.Inform.Theory 13 (1), 21\u2013\n27.\nCover, T. M. and J. A. Thomas\n(1991).ElementsofInformationThe-\nory. John Wiley.\nCover, T. M. and J. A. Thomas\n(2006).Elements of Information\nTheory. John Wiley. 2nd edition.\nCowles, M. and B. Carlin (1996).\nMarkov chain monte carlo conver-\ngence diagnostics: A comparative\nreview.J. of the Am. Stat. Assoc. 91,\n883\u2013904.", "1051": "BIBLIOGRAPHY 1021\nCrisan, D., P. D. Moral, and T. Lyons\n(1999). Discrete \ufb01ltering using\nbranching and interacting particle\nsystems.Markov Processes and Re-\nlatedFields 5 (3), 293\u2013318.\nCui, Y., X. Z. Fern, and J. G. Dy\n(2010). Learning multiple nonre-\ndundant clusterings. ACMTransac-\ntions on Knowledge Discovery fromData 4(3).\nCukier, K. (2010, February). Data, data\neverywhere.\nDagum, P. and M. Luby (1993). Ap-\nproximating probabilistic inferencein Bayesian belief networks is NP-\nhard.Arti\ufb01cialIntelligence 60, 141\u2013\n153.\nDahl, J., L. Vandenberghe, and V. Roy-\nchowdhury (2008, August). Co-\nvariance selection for non-chordalgraphs via chordal embedding.\nOptimization Methods and Soft-\nware 23(4), 501\u2013502.\nDahlhaus, R. and M. Eichler (2000).\nCausality and graphical models fortime series. In P. Green, N. Hjort,\nand S. Richardson (Eds.), Highly\nstructured stochastic systems. Ox-\nford University Press.\nDallal, S. and W. Hall (1983). Approxi-\nmating priors by mixtures of natu-\nral conjugate priors. J. of Royal Stat.\nSoc.SeriesB 45 , 278\u2013286.\nDarwiche, A. (2009). Modeling and\nReasoning with Bayesian Networks.\nCambridge.\nDaume, H. (2007a). Fast search for\nDirichlet process mixture models.InAI/Statistics.\nDaume, H. (2007b). Frustratingly easy\ndomain adaptation. In Proc.theAs-\nsoc.forComp.Ling.\nDawid, A. P. (1992). Applications of a\ngeneral propagation algorithm forprobabilistic expert systems. Statis-\nticsandComputing 2, 25\u201336.\nDawid, A. P. (2002). In\ufb02uence dia-\ngrams for causal modelling and in-\nference.Intl. Stat. Review 70, 161\u2013\n189. Corrections p437.\nDawid, A. P. (2010). Beware of the DAG!\nJ. of Machine Learning Research 6,\n59\u201386.Dawid, A. P. and S. L. Lauritzen\n(1993). Hyper-markov laws in thestatistical analysis of decompos-\nable graphical models. The Annals\nofStatistics 3, 1272\u20131317.\nde Freitas, N., R. Dearden, F. Hut-\nter, R. Morales-Menendez, J. Mutch,\nand D. Poole (2004). Diagnosis bya waiter and a mars explorer. Proc.\nIEEE 92(3).\nde Freitas, N., M. Niranjan, and A. Gee\n(2000). Hierarchical Bayesian mod-els for regularisation in sequential\nlearning.NeuralComputation12 (4),\n955\u2013993.\nDechter, R. (1996). Bucket elimination:\na unifying framework for proba-\nbilistic inference. In UAI.\nDechter, R. (2003). Constraint Process-\ning. Morgan Kaufmann.\nDecoste, D. and B. Schoelkopf (2002).\nTr\naining invariant support vector\nmachines.Machinelearnng 41, 161\u2013\n190.\nDeerwester, S., S. Dumais, G. Fur-\nnas, T. Landauer, and R. Harshman(1990). Indexing by latent semantic\nanalysis.J. of the American Society\nfor Information Science 41 (6), 391\u2013\n407.\nDeGroot, M. (1970). OptimalStatistical\nDecisions. McGraw-Hill.\nDeisenroth, M., C. Rasmussen, and\nJ. Peters (2009). Gaussian Process\nDynamic Programming. Neurocom-\nputing 72 (7), 1508\u20131524.\nDellaportas, P., P. Giudici, and\nG. Roberts (2003). Bayesian infer-ence for nondecomposable graphi-\ncal gaussian models. Sankhya, Ser.\nA6 5, 43\u201355.\nDellaportas, P. and A. F. M. Smith\n(1993). Bayesian Inference for Gen-\neralized Linear and ProportionalHazards Models via Gibbs Sam-pling.J. of the Royal Statisti-\ncal Society. Series C (Applied Statis-tics) 42(3), 443\u2013459.\nDelyon, B., M. Lavielle, and\nE. Moulines (1999). Convergence ofa stochastic approximation version\nof the EM algorithm. Annals of\nStatistics 27 (1), 94\u2013128.\nDempster, A. (1972). Covariance selec-\ntion.Biometrics 28 (1).Dempster, A. P., N. M. Laird, and D. B.\nRubin (1977). Maximum likelihood\nfrom incomplete data via the EM\nalgorithm.J. of the Royal Statistical\nSociety,SeriesB 34, 1\u201338.\nDenison, D., C. Holmes, B. Mallick,\nand A. Smith (2002). Bayesian\nmethodsfornonlinearclassi\ufb01cation\nandregression. Wiley.\nDenison, D., B. Mallick, and A. Smith\n(1998). A Bayesian CART algorithm.\nBiometrika 85, 363\u2013377.\nDesjardins, G. and Y. Bengio (2008).\nEmpirical evaluation of convolu-\ntional RBMs for vision. TechnicalReport 1327, U. Montreal.\nDey, D., S. Ghosh, and B. Mallick (Eds.)\n(2000).Generalized Linear Models:\nABayesianPerspective. Chapman &Hall/CRC Biostatistics Series.\nDiaconis, P., S. Holmes, and R. Mont-\ngomery (2007). Dynamical Bias inthe Coin Toss. SIAM Review 49 (2),\n211\u2013235.\nDiaconis, P. and D. Ylvisaker (1985).\nQuantifying prior opinion. InBayesianStatistics2.\nDietterich, T. G. and G. Bakiri (1995).\nSolving multiclass learning prob-\nlems via ECOCs. J.ofAIResearch2,\n263\u2013286.\nDiggle, P. and P. Ribeiro (2007). Model-\nbasedGeostatistics. Springer.\nDing, Y. and R. Harrison (2010). A\nsparse multinomial probit modelfor classi\ufb01cation. Pattern Analysis\nandApplications , 1\u20139.\nDobra, A. (2009). Dependency net-\nworks for genome-wide data. Tech-\nnicalr\neport, U. Washington.\nDobra, A. and H. Massam (2010). The\nmode oriented stochastic search(MOSS) algorithm for log-linearmodels with conjugate priors. Sta-\ntisticalMethodology 7, 240\u2013253.\nDomingos, P. and D. Lowd (2009).\nMarkovLogic: AnInterfaceLayerforAI. Morgan & Claypool.\nDomingos, P. and M. Pazzani (1997).\nOn the optimality of the simplebayesian classi\ufb01er under zero-one\nloss.Machine Learning 29, 103\u2013\n130.", "1052": "1022 BIBLIOGRAPHY\nDomke, J., A. Karapurkar, and Y. Aloi-\nmonos (2008). Who killed the di-\nrected model? In CVPR.\nDoucet, A., N.deFreitas, andN.J.Gor-\ndon (2001). SequentialMonteCarlo\nMethods in Practice. Springer Ver-lag.\nDoucet, A., N. Gordon, and V. Krish-\nnamurthy (2001). Particle Filters forState Estimation of Jump MarkovLinear Systems. IEEETrans.onSig-\nnalProcessing 49 (3), 613\u2013624.\nDow, J. and J. Endersby (2004). Multi-\nnomial probit and multinomiallogit: a comparison of choice mod-\nels for voting research. Electoral\nStudies 23 (1), 107\u2013122.\nDrineas, P., A. Frieze, R. Kannan,\nS. Vempala, and V. Vinay (2004).\nClustering large graphs via the sin-gular value decomposition. Ma-\nchineLearning 56, 9\u201333.\nDrugowitsch, J. (2008). Bayesian lin-\near regression. Technical report, U.Rochester.\nDruilhet, P. and J.-M. Marin (2007). In-\nvariant HPD credible sets and MAPestimators. Bayesian Analysis 2 (4),\n681\u2013692.\nDuane, S., A. Kennedy, B. Pendle-\nton, and D. Roweth (1987). Hy-\nbrid Monte Carlo. Physics Letters\nB 195(2), 216\u2013222.\nDuchi, J., S. Gould, and D. Koller\n(2008). Projected subgradient\nmethods for learning sparse gaus-sians. InUAI.\nDuchi, J., E. Hazan, and Y. Singer\n(2010). Adaptive Subgradient Meth-ods for Online Learning andStochastic Optimization. In Proc.\nof the Workshop on Computational\nLearningTheory.\nDuchi, J., S. Shalev-Shwartz, Y. Singer,\nand T. Chandra (2008). Efficient\nprojections onto the L1-ball forlearning in high dimensions. In\nIntl.Conf.onMachineLearning.\nDuchi, J. and Y. Singer (2009). Boost-\ning with structural sparsity. In Intl.\nConf.onMachineLearning.\nDuchi, J., D. Tarlow, G. Elidan, and\nD. Koller (2007). Using combi-\nnatorial optimization within max-product belief propagation. InNIPS.Duda, R. O., P. E. Hart, and D. G. Stork\n(2001).PatternClassi\ufb01cation. Wiley\nInterscience. 2nd edition.\nDumais, S. and T. Landauer (1997). A\nsolution to Plato\u2019s problem: Thelatent semantic analysis theory ofacquisition, induction and repre-\nsentation of knowledge. Psycholog-\nicalReview 104, 211\u2013240.\nDunson, D., J. Palomo, and K. Bollen\n(2005). Bayesian Structural Equa-\ntion Modeling. Technical Report2005-5, SAMSI.\nDurbin, J. and S. J. Koopman (2001).\nTime Series Analysis by State Space\nMethods. Oxford University Press.\nDurbin, R., S. Eddy, A. Krogh, and\nG. Mitchison (1998). Biological Se-\nquence Analysis: Probabilistic Mod-els of Proteins and Nucleic Acids.\nCambridge: Cambridge UniversityPress.\nEarl, D. and M. Deem (2005). Paral-\nleltempering: Theory, applications,\nand new perspectives. Phy\n s.Chem.\nChem.Phys. 7, 3910.\nEaton, D. and K. Murphy (2007). Exact\nBayesian structure learning from\nuncertain interventions. In AI/S-\ntatistics.\nEdakunni, N., S. Schaal, and S. Vi-\njayakumar (2010). Probabilistic in-cremental locally weighted learn-ing using randomly varying coeffi-cient model. Technical report, USC.\nEdwards, D., G. de Abreu, and\nR. Labouriau (2010). Selecting high-dimensional mixed graphical mod-els using minimal AIC or BIC\nforests.BMCBioinformatics 11 (18).\nEfron, B. (1986). Why Isn\u2019t Everyone\na Bayesian? The American Statisti-\ncian 40(1).\nEfron, B. (2010). Large-Scale Infer-\nence: Empirical Bayes Methods for\nEstimation, Testing, and Prediction.\nCambridge.\nEfron, B., I. Johnstone, T. Hastie, and\nR. Tibshirani (2004). Least angle re-\ngression.Annals of Statistics 32 (2),\n407\u2013499.\nEfron, B. and C. Morris (1975). Data\nanalysis using stein\u2019s estimator andits generalizations. J.oftheAm.Stat.\nAssoc. 70(350), 311\u2013319.Elad, M. and I. Yavnch (2009). A plu-\nrality of sparse representations is\nbetter than the sparsest one alone.\nIEEE Trans. on Info. Theory 55 (10),\n4701\u20134714.\nElidan, G. and S. Gould (2008). Learn-\ning Bounded Treewidth BayesianNetworks.J. of Machine Learning\nResearch, 2699\u20132731.\nElidan, G., N. Lotner, N. Friedman, and\nD. Koller (2000). Discovering hid-\nden variables: A structure-basedapproach. In NIPS.\nElidan, G., I. McGraw, and D. Koller\n(2006). Residual belief propa-gation: Informed scheduling forasynchronous message passing. InUAI.\nElkan, C. (2003). Using the triangle in-\nequality to accelerate k-means. In\nIntl.Conf.onMachineLearning.\nElkan, C. (2005). Deriving TF-IDF as\na Fisher kernel. In Proc.Intl.Symp.\non String Processing and Informa-\ntionRetrieval(SPIRE), pp. 296\u2013301.\nElkan, C. (2006). Clustering docu-\nments with an exponential fmaily\napproximation of the Dirichletcompoind multinomial model. In\nIntl.Conf.onMachineLearning.\nEllis, B. and W. H. Wong (2008). Learn-\ning causal bayesian network struc-tures from experimental data. J. of\nthe Am. Stat. Assoc. 103 (482), 778\u2013\n789.\nEngel, Y., S. Mannor, and R. Meir\n(2005). Reinforcement Learningwith Gaussian Processes. In Intl.\nConf.onMachineLearning.\nErhan, D., Y. Bengio, A. Courville, P.-A.\nManzagol, P. Vincent, and S. Ben-\ngio (2010). Why Does UnsupervisedPre-training Help Deep Learning?\nJ. of Machine Learning Research 11,\n625\u2013660.\nEroshe\nva, E., S. Fienberg, and\nC. Joutard (2007). Describingdisability through individual-levelmixture models for multivariate bi-nary data.AnnalsofAppliedStatis-\ntics.\nErosheva, E., S. Fienberg, and J. Laf-\nferty (2004). Mixed-membershipmodels of scienti\ufb01c publications.\nProc. of the National Academy of\nScience,USA 101, 5220\u20132227.", "1053": "BIBLIOGRAPHY 1023\nEscobar, M. D. and M. West (1995).\nBayesian density estimation and\ninference using mixtures. J. of the\nAm.Stat.Assoc. 90 (430), 577\u2013588.\nEwens, W. (1990). Population genet-\nics theory - the past and the fu-ture. In S.Lessard (Ed.), Mathemeti-\ncal and Statistica Developments of\nEvolutionary Theory, pp. 177\u2013227.\nReidel.\nFan, J. and R. Z. Li (2001). Variable se-\nlection via non-concave penalizedlikelihood and its oracle properties.\nJ. of the Am. Stat. Assoc. 96 (456),\n1348\u20131360.\nFearnhead, P. (2004). Exact bayesian\ncurve \ufb01tting and signal segmen-tation.IEEE Trans. Signal Process-\ning 53, 2160\u20132166.\nFelzenszwalb, P. and D. Huttenlocher\n(2006). Efficient belief propagationfor early vision. Intl. J. Computer\nVision 70(1), 41\u201354.\nFerrucci, D., E. Brown, J. Chu-Carroll,\nJ. Fan, D. Gondek, A. Kalyanpur,\nA. Lally, J. W. Murdock, E. N.amd J. Prager, N. Schlaefter, andC. Welty (2010). Building Wat-son: An Overview of the DeepQAProject.AIMagazine , 59\u201379.\nFienberg, S. (1970). An iterative pro-\ncedure for estimation in contin-gency tables. Annals of Mathemat-\nicalStatistics 41 (3), 907\u00e2\u02d8A\u00b8S917.\nFigueiredo, M. (2003). Adaptive\nsparseness for supervised learn-\ning.IEEE Trans. on Pattern Anal-\nysisandMachineIntelligence 25 (9),\n1150\u20131159.\nFigueiredo, M., R. Nowak, and\nS. Wright (2007). Gradient pro-\njection for sparse reconstruction:application to compressed sensingand other inverse problems. IEEE.\nJ. on Selected Topics in Signal Pro-cessing.\nFigueiredo, M. A. T. and A. K. Jain\n(2002). Unsupervised learning of \ufb01-nite mixture models. IEEETrans.on\nPatternAnalysisandMachineIntel-\nligence 24 (3), 381\u2013396. Matlab code\nat http://www.lx.it.pt/ mtf/mixture-\ncode.zip.\nFine, S., Y. Singer, and N. Tishby\n(1998). The hierarchical HiddenMarkov Model: Analysis and appli-\ncations.MachineLearning 32, 41.Finkel, J. and C. Manning (2009). Hier-\narchical bayesian domain adapta-\ntion. InProc.NAACL, pp. 602\u2013610.\nFischer, B. and J. Schumann (2003).\nAutobayes: A system for generatingdata analysis programs from sta-\ntistical models. J. Functional Pro-\ngramming 13 (3), 483\u2013508.\nFishelson, M. and D. Geiger (2002).\nExact genetic linkage computations\nfor general pedigrees. BMC Bioin-\nformatics 18.\nFletcher, R. (2005). On the Barzilai-\nBorwein Method. Applied Opti-\nmization 96, 235\u2013256.\nFokoue, E. (2005). Mixtures of factor\nanalyzers: an extension with co-\nvariates.J.MultivariateAnalysis 95,\n370\u2013384.\nForbes,\nJ., T. Huang, K. Kanazawa, and\nS. Russell (1995). The BATmobile:\nTowards a Bayesian automated taxi.\nInIntl.JointConf.onAI.\nForsyth, D. and J. Ponce (2002). Com-\nputer vision: a modern approach.\nPrentice Hall.\nFraley, C. and A. Raftery (2002).\nModel-based clustering, discrimi-nant analysis, and density estima-\ntion.J. of the Am. Stat. Assoc. (97),\n611\u2013631.\nFraley, C. and A. Raftery (2007).\nBayesian Regularization for Normal\nMixture Estimation and Model-Based Clustering. J. of Classi\ufb01ca-\ntion 24, 155\u2013181.\nFranc, V., A. Zien, and B. Schoelkopf\n(2011). Support vector machines as\nprobabilistic models. In Intl. Conf.\nonMachineLearning.\nFrank, I. and J. Friedman (1993). A\nstatistical view of some chemomet-\nrics regression tools. Technomet-\nrics 35(2), 109\u2013135.\nFraser, A. (2008). HiddenMarkovMod-\nels and Dynamical Systems. SIAMPress.\nFreund, Y. and R. R. Schapire (1996).\nExperiments with a new boosting\nalgorithm. In Intl.Conf.onMachine\nLearning.\nFrey, B. (1998). Graphical Models for\nMachineLearningandDigitalCom-\nmunication. MIT Press.Frey, B. (2003). Extending factor\ngraphs so as to unify directed andundirected graphical models. InUAI.\nFrey, B. and D. Dueck (2007, Febru-\nary). Clustering by Passing Mes-sages Between Data Points. Sci-\nence 315, 972\u00e2 \u02d8A\u00b8S976.\nFriedman, J. (1991). Multivariate\nadaptive regression splines. Ann.\nStatist. 19, 1\u201367.\nFriedman, J. (1997a). On bias, variance,\n0-1 loss and the curse of dimen-sionality.J.DataMiningandKnowl-\nedgeDiscovery 1, 55\u201377.\nFriedman, J. (2001). Greedy function\napproximation: a gradient boost-\ning machine. AnnalsofStatistics29,\n1189\u20131232.\nFriedman, J., T. Hastie, and R. Tibshi-\nrani (2000). Additive logistic regres-\nsion: a statistical view of boosting.\nAnnalsofstatistics 28 (2), 337\u2013374.\nFriedman, J., T. Hastie, and R. Tib-\nshirani (2008). Sparse inverse co-\nvariance estimation the graphicallasso.Biostatistics 9 (3), 432\u2013441.\nFriedman, J., T. Hastie, and R. Tibshi-\nrani (2010, Februrary). Regulariza-tion Paths for Generalized LinearModels via Coordinate Descent. J.\nofStatisticalSoftware 33 (1).\nFriedman,\nN. (1997b). Learning\nBayesian networks in the presenceof missing values and hidden vari-ables. InUAI.\nFriedman, N., D. Geiger, and M. Gold-\nszmidt (1997). Bayesian network\nclassi\ufb01ers.Machine Learning J. 29,\n131\u2013163.\nFriedman, N., D. Geiger, and N. Lot-\nner (2000). Likelihood computation\nwith value abstraction. In UAI.\nFriedman, N. and D. Koller (2003). Be-\ning Bayesian about Network Struc-ture: A Bayesian Approach toStructure Discovery in Bayesian\nNetworks. Machine Learning 50,\n95\u2013126.\nFriedman, N., M. Ninion, I. Pe\u2019er, and\nT. Pupko (2002). A Structural EM\nAlgorithm for Phylogenetic Infer-ence.J.Comp.Bio. 9, 331\u2013353.\nFriedman, N. and Y. Singer (1999). Ef-\n\ufb01cient Bayesian parameter estima-tion in large discrete domains. InNIPS-11.", "1054": "1024 BIBLIOGRAPHY\nFruhwirth-Schnatter, S. (2007). Fi-\nnite Mixture and Markov Switching\nModels. Springer.\nFruhwirth-Schnatter, S. and R. Fruh-\nwirth (2010). Data Augmentation\nand MCMC for Binary and Multi-nomial Logit Models. In T. Kneib\nand G. Tutz (Eds.), Statistical Mod-\nellingandRegressionStructures, pp.\n111\u2013132. Springer.\nFu, W. (1998). Penalized regressions:\nthe bridge verus the lasso. J.Com-\nputationalandgraphicalstatistics .\nFukushima, K. (1975). Cognitron: a\nself-organizing multilayered neu-\nral network. Biological Cybernet-\nics 20(6), 121\u2013136.\nFung, R. and K. Chang (1989). Weight-\ning and integrating evidence for\nstochastic simulation in Bayesiannetworks. In UAI.\nGabow, H., Z. Galil, and T. Spencer\n(1984). Efficient implementationof graph algorithms using contrac-\ntion. InIEEE Symposium on the\nFoundationsofComputerScience.\nGales, M. (2002). Maximum like-\nlihood multiple subspace projec-\ntions for hidden Markov models.IEEE. Trans. on Speech and AudioProcessing 10 (2), 37\u201347.\nGales, M. J. F. (1999). Semi-tied covari-\nance matrices for hidden Markovmodels.IEEETrans.onSpeechand\nAudioProcessing 7 (3), 272\u2013281.\nGamerman, D. (1997). Efficient sam-\npling from the posterior distribu-tion in generalized linear mixed\nmodels.StatisticsandComputing 7,\n57\u201368.\nGeiger, D. and D. Heckerman (1994).\nLearning Gaussian networks. In\nUAI, Volume 10, pp. 235\u2013243.\nGeiger, D. and D. Heckerman (1997).\nA characterization of Dirchlet dis-tributions through local and global\nindependence. Annals of Statis-\ntics 25, 1344\u20131368.\nGelfand, A. (1996). Model determina-\ntion using sampling-based meth-\nods. In Gilks, Richardson, andSpiegelhalter (Eds.), Markov Chain\nMonte Carlo in Practice. Chapman& Hall.\nGelfand, A. and A. Smith (1990).\nSampling-based approaches to cal-\nculating marginal densities. J.ofthe\nAm.Stat.Assoc. 85, 385\u2013409.Gelman, A., J. Carlin, H. Stern, and\nD. Rubin (2004). Bayesian data\nanalysis. Chapman and Hall. 2nd\nedition.\nGelman, A. and J. Hill (2007). Data\nanalysis using regression and mul-\ntilevel/ hierarchical models. Cam-\nbridge.\nGelman, A. and X.-L. Meng (1998).\nSimulating normalizing constants:from importance sampling tobridge sampling to path sampling.\nStatisicalScience 13, 163\u2013185.\nGelman, A. and T. Raghunathan (2001).\nUsing conditional distributions formissing-data imputation. Statistical\nScience.\nGelman,A\n. and D. Rubin (1992). Infer-\nence from iterative simulation us-\ning multiple sequences. Statistical\nScience 7, 457\u2013511.\nGeman, S., E. Bienenstock, and\nR. Doursat (1992). Neural networksand the bias-variance dilemma.NeuralComputing 4, 1\u201358.\nGeman, S. and D. Geman (1984).\nStochastic relaxation, Gibbs distri-butions, and the Bayesian restora-tion of images. IEEE Trans. on Pat-\ntern Analysis and Machine Intelli-gence 6(6).\nGeoffrion, A. (1974). Lagrangian\nrelaxation for integer program-\nming.Mathematical Programming\nStudy 2, 82\u2013114.\nGeorge, E. and D. Foster (2000). Cal-\nibration and empirical bayes vari-able selection. Biometrika 87 (4),\n731\u2013747.\nGetoor, L. and B. Taskar (Eds.) (2007).\nIntroductiontoRelationalStatistical\nLearning. MIT Press.\nGeyer, C. (1992). Practical markov\nchain monte carlo. Statistical Sci-\nence 7, 473\u2013483.\nGhahramani, Z. and M. Beal (2000).\nVariational inference for Bayesianmixtures of factor analysers. InNIPS-12.\nGhahramani, Z. and M. Beal (2001).\nPropagation algorithms for varia-tional Bayesian learning. In NIPS-\n13.\nGhahramani, Z. and G. Hinton (1996a).\nThe EM algorithm for mixtures offactor analyzers. Technical report,Dept. of Comp. Sci., Uni. Toronto.Ghahramani, Z. and G. Hinton (1996b).\nParameter estimation for linear dy-namical systems. Technical Re-port CRG-TR-96-2, Dept. Comp.Sci., Univ. Toronto.\nGhahramani, Z. and M. Jordan (1997).\nFactorial hidden Markov models.MachineLearning 29, 245\u2013273.\nGilks, W. and C. Berzuini (2001).\nFollowing a moving target \u2013Monte Carlo infernece for dynamic\nBayesian models. J. of Royal Stat.\nSoc.SeriesB 63, 127\u2013146.\nGilks, W., N. Best, and K. Tan (1995).\nAdaptive rejection Metropolis sam-pling.AppliedStatistics44, 455\u2013472.\nGilks, W. and P. Wild (1992). Adaptive\nrejection sampling for Gibbs sam-pling.AppliedStatistics41, 337\u2013348.\nGirolami, M., B. Calderhead, and\nS. Chin (2010). Riemannian Man-\nifold Hamiltonian Monte Carlo. J.\nof Royal Stat. Soc. Series B . To ap-\npear.\nGirolami, M. and S. Rogers (2005). Hi-\nerarchic bayesian models for kernel\nlearning. In Intl. Conf. on Machine\nLearning, pp. 241\u2013248.\nGirolami, M. and S. Rogers (2006).\nVariational Bayesian multinomial\nprobit regression with Gaussian\nprocess priors. Neural Comptua-\ntion 18(8), 1790 \u2013 1817.\nGirshick, R., P. Felzenszwalb, and\nD.McAllest\ner (2011). Object de-\ntection with grammar models. In\nNIPS.\nGittins, J. (1989). Multi-armed Bandit\nAllocationIndices. Wiley.\nGiudici, P. and P. Green (1999).\nDecomposable graphical gaus-sian model determination.Biometrika 86 (4), 785\u2013801.\nGivoni, I. E. and B. J. Frey (2009, June).\nA binary variable model for affin-\nity propagation. Neural Computa-\ntion 21(6), 1589\u20131600.\nGloberson, A. and T. Jaakkola (2008).\nFixing max-product: Convergent\nmessage passing algorithms forMAP LP-relaxations. In NIPS.\nGlorot, X. and Y. Bengio (2010, May).\nUnderstanding the difficulty oftraining deep feedforward neuralnetworks. In AI/Statistics, Volume 9,\npp. 249\u2013256.", "1055": "BIBLIOGRAPHY 1025\nGogate, V., W. A. Webb, and P. Domin-\ngos (2010). Learning efficient\nMarkov networks. In NIPS.\nGoldenberg, A., A.X.Zheng, S.E.Fien-\nberg, and E. M. Airoldi (2009). ASurvey of Statistical Network Mod-\nels.FoundationsandTrendsinMa-\nchineLearning , 129\u2013233.\nGolub, G. and C. F. van Loan (1996).\nMatrix computations. Johns Hop-\nkins University Press.\nGonen, M., W. Johnson, Y. Lu, and\nP. Westfall (2005, August). The\nBayesian Two-Sample t Test. The\nAmerican Statistician 59 (3), 252\u2013\n257.\nGonzales, T. (1985). Clustering to\nminimize the maximum interclus-\nter distance. Theor. Comp. Sci. 38,\n293\u2013306.\nGorder, P. F. (2006, Nov/Dec). Neu-\nral networks show new promise formachine vision. Computing in sci-\nence&engineering 8 (6), 4\u20138.\nGordon, N. (1993). Novel ap-\nproach to nonlinear/non-GaussianBayesian state estimation. IEEPro-\nceedings(F) 140 (2), 107\u2013113.\nGraepel, T., J. Quinonero-Candela,\nT. Borchert, and R. Herbrich(2010). Web-Scale Bayesian Click-\nThrough Rate Prediction for Spon-\nsored Search Advertising in Mi-\ncrosoft\u00e2\u02d8A\u00b4Zs Bing Search Engine. In\nIntl.Conf.onMachineLearning.\nGrauman, K. and T. Darrell (2007,\nApril). The Pyramid Match Kernel:\nEfficient Learning with Sets of Fea-\ntures.J. of Machine Learning Re-\nsearch 8, 725\u2013760.\nGreen, P. (1998). Reversible jump\nMarkov chain Monte Carlo compu-\ntation and Bayesian model deter-\nmination.Biometrika 82, 711\u2013732.\nGreen, P. (2003). Tutorial on trans-\ndimensional MCMC. In P. Green,\nN. Hjort, and S. Richardson (Eds.),\nHighlyStructuredStochasticSystems.\nOUP.\nGreen, P. and B. Silverman (1994). Non-\nparametric regression and general-\nized linear models. Chapman and\nHall.Greenshtein, E. and J. Park (2009). Ap-\nplication of Non Parametric Empir-ical Bayes Estimation to High Di-\nmensional Classi\ufb01cation. J. of Ma-\nchine Learning Research 10, 1687\u2013\n1704.\nGreig, D., B. Porteous, and A. Seheult\n(1989). Exact maximum a posteriori\nestimation for binary images. J. of\nRoyal Stat. Soc. Series B 51 (2), 271\u2013\n279.\nGriffin, J. and P. Brown (2007).\nBayesian adaptive lassos with non-\nconvex penalization. Technical re-port, U. Kent.\nGriffin, J. and P. Brown (2010). In-\nference with normal-gamma priordistributions in regression prob-\nlems.Bayesian Analysis 5 (1), 171\u2013\n188.\nGriffiths, T. . and J. Tenenbaum (2009).\nTheory\n-Based Causal Induction.\nPsychological Review 116 (4), 661\u2013\n716.\nGriffiths, T. and M. Steyvers (2004).\nFinding scienti\ufb01c topics. Proc. of\nthe National Academy of Science,\nUSA 101, 5228\u20135235.\nGriffiths, T., M. Steyvers, D. Blei, and\nJ. Tenenbaum (2004). Integratingtopics and syntax. In NIPS.\nGriffiths, T. and J. Tenenbaum (2001).\nUsing vocabulary knowledge inbayesian multinomial estimation.InNIPS, pp. 1385\u20131392.\nGriffiths, T. and J. Tenenbaum (2005).\nStructure and strength in causal\ninduction.Cognitive Psychology 51,\n334\u2013384.\nGrimmett, G. and D. Stirzaker (1992).\nProbability and Random Processes.\nOxford.\nGuan, Y., J. Dy, D. Niu, and Z. Ghahra-\nmani (2010). Variational Inferencefor Nonparametric Multiple Clus-\ntering. In 1st Intl. Workshop on\nDiscovering, Summarizing and Us-\ningMultipleClustering(MultiClust).\nGuedon, Y. (2003). Estimating hidden\nsemi-markov chains from discrete\nsequences.J.ofComputationaland\nGraphicalStatistics 12, 604\u2013639.\nGuo, Y. (2009). Supervised exponential\nfamily principal component anal-ysis via convex optimization. InNIPS.Gustafsson, M. (2001). A proba-\nbilistic derivation of the partial\nleast-squares algorithm. Journal of\nChemical Information and Model-\ning 41, 288\u2013294.\nGuyon, I., S. Gunn, M. Nikravesh, and\nL. Zadeh (Eds.) (2006). Feature Ex-\ntraction: Foundations and Applica-tions. Springer.\nHacker, J. and P. Pierson (2010).\nWinner-Take-All Politics: HowWashington Made the Rich Richer\u2013\nand Turned Its Back on the Middle\nClass. Simon & Schuster.\nHalevy, A., P. Norvig, and F. Pereira\n(2009). The unreasonable effective-ness of data. IEEE Intelligent Sys-\ntems 24(2), 8\u201312.\nHall, P., J. T. Ormerod, and M. P. Wand\n(2011). Theory of Gaussian Varia-tional Approximation for a Gener-alised Linear Mixed Model. Statis-\nticaSinica 21, 269\u2013389.\nHamilton, J. (1990). Analysis of time\nseries subject to changes in regime.J.Econometrics 45 , 39\u201370.\nHans, C. (2009). Bayesian Lasso re-\ngression.Biometrika 96 (4), 835\u2013\n845.\nHansen, M. and B. Yu (2001). Model\nselection and the principle of min-\nimum description length. J. of the\nAm.Stat.Assoc..\nHara, H. and A. Takimura (2008).\nA Localization Approach to Im-\nprove Iterative Proportional Scal-ing in Gaussian Graphical Models.\nCommunications in Statistics - The-\noryandMethod . to appear.\nHardin,\nJ. and J. Hilbe (2003). Gener-\nalizedEstimatingEquations. Chap-man and Hall/CRC.\nHarmeling, S. and C. K. I. Williams\n(2011). Greedy learning of binarylatent trees. IEEE Trans. on Pat-\ntern Analysis and Machine Intelli-gence 33(6), 1087\u20131097.\nHarnard, S. (1990). The symbol\ngrounding problem. Physica D 42,\n335\u2013346.\nHarvey, A. C. (1990). Forecasting,Struc-\ntural Time Series Models, and theKalman Filter. Cambridge Univer-ity Press.", "1056": "1026 BIBLIOGRAPHY\nHastie, T., S. Rosset, R. Tibshirani, and\nJ. Zhu (2004). The entire regular-\nization path for the support vector\nmachine.J. of Machine Learning\nResearch 5, 1391\u20131415.\nHastie, T. and R. Tibshirani (1990).\nGeneralizedadditivemodels. Chap-\nman and Hall.\nHastie, T., R. Tibshirani, and J. Fried-\nman (2001). The Elements of Statis-\nticalLearning. Springer.\nHastie, T., R. Tibshirani, and J. Fried-\nman (2009). TheElementsofStatisti-\ncalLearning. Springer. 2nd edition.\nHastings, W. (1970). Monte carlo\nsampling methods using markovchains and their applications.\nBiometrika 57 (1), 97\u2013109.\nHaykin, S. (1998). Neural Networks: A\nComprehensive Foundation. Pren-\ntice Hall. 2nd Edition.\nHaykin, S. (Ed.) (2001). Kalman Filter-\ningandNeuralNetworks. Wiley.\nHazan, T. and A. Shashua (2008).\nConvergent message-passing algo-rithms for inference over generalgraphs with convex free energy. InUAI.\nHazan, T. and A. Shashua (2010).\nNorm-product belief propagation:primal-dual message passing forapproximate inference. IEEE Trans.\nonInfo.Theory 56 (12), 6294\u20136316.\nHe, Y.-B. and Z. Geng (2009). Active\nlearning of causal networks withintervention experiments and opti-\nmal designs. J.ofMachineLearning\nResearch 10, 2523\u20132547.\nHeaton, M. and J. Scott (2009).\nBayesian computation and the lin-\near model. Technical report, Duke.\nHeckerman, D., D. Chickering,\nC. Meek, R. Rounthwaite, andC. Kadie (2000). Dependencynetworks for density estimation,collaborative \ufb01ltering, and data vi-\nsualization. J. of Machine Learning\nResearch 1, 49\u201375.\nHeckerman, D., D. Geiger, and\nM. Chickering (1995). Learning\nBayesian networks: the combina-tion of knowledge and statistical\ndata.MachineLearning 20 (3), 197\u2013\n243.Heckerman, D., C. Meek, and\nG. Cooper (1997, February). A\nBayesian approach to causal dis-covery. Technical Report MSR-TR-97-05, Microsoft Research.\nHeckerman, D., C. Meek, and D. Koller\n(2004). Probabilistic models forrelational data. Technical Re-port MSR-TR-2004-30, MicrosoftResearch.\nHeller, K. and Z. Ghahramani (2005).\nBayesianHierarchicalClustering. In\nIntl.Conf.onMachineLearning.\nHenrion, M. (1988). Propagation of\nuncertainty by logic sampling in\nBayes\u2019 networks. In UAI, pp. 149\u2013\n164.\nHerbrich, R., T. Minka, and T. Graepel\n(2007). TrueSkill: A Bayesian skillrating system. In NIPS.\nHertz, J., A. Krogh, and R. G. Palmer\n(1991).AnIntroductiontotheTheory\nof Neural Comptuation. Addison-\nWesley.\nHillar, C., J. Sohl-Dickstein, and\nK. Koepsell (2012, April). Efficientandop\ntimal binary hop\ufb01eld asso-\nciative memory storage using min-imum probability \ufb02ow. Technicalreport.\nHinton, G. (1999). Products of experts.\nInProc.9thIntl.Conf.onArtif.Neu-\nralNetworks(ICANN), Volume 1, pp.1\u20136.\nHinton, G. (2002). Training products of\nexperts by minimizing contrastive\ndivergence. NeuralComputation14,\n1771\u20131800.\nHinton, G. (2010). A Practical Guide\nto Training Restricted Boltzmann\nMachines. Technical report, U.Toronto.\nHinton, G. and D. V. Camp (1993).\nKeeping neural networks simple byminimizing the description length\nof the weights. In in Proc. of the\n6th Ann. ACM Conf. on Computa-\ntional Learning Theory, pp. 5\u201313.\nACM Press.\nHinton, G., S. Osindero, and Y. Teh\n(2006). A fast learning algorithm\nfor deep belief nets. Neural Com-\nputation 18, 1527\u20131554.\nHinton, G.andR.Salakhutdinov(2006,\nJuly). Reducing the dimensionality\nof data with neural networks. Sci-\nence 313(5786), 504\u2013507.Hinton, G. E., P. Dayan, and M. Revow\n(1997). Modeling the manifolds ofimages of handwritten digits. IEEE\nTrans.onNeuralNetworks8, 65\u201374.\nHinton, G. E. and Y. Teh (2001).\nDiscovering multiple constraintsthat are frequently approximatelysatis\u00ef\u02c7n\u02dbAed. InUAI.\nHjort, N., C. Holmes, P. Muller, and\nS. Walker (Eds.) (2010). Bayesian\nNonparametrics. Cambridge.\nHoe\ufb02ing, H. (2010). A Path Algorithm\nfor the Fused Lasso Signal Approx-imator. Technical report, Stanford.\nHoe\ufb02ing, H. and R. Tibshirani\n(2009). Estimation of Sparse Bi-naryPairwiseMarkovNetworksus-\ning Pseudo-likelihoods. J. of Ma-\nchineLearningResearch 10.\nHoeting, J., D. Madigan, A. Raftery,\nand C. Volinsky (1999). Bayesian\nmodel averaging: A tutorial. Statis-\nticalScience 4 (4).\nHoff, P. D. (2009, July). A First\nCourse in Bayesian Statistical Meth-\nods. Springer.\nHoffman, M., D. Blei, and F. Bach\n(2010). Online learning for latent\ndirichlet allocation. In NIPS.\nHoffman, M. and A. Gelman (2011).\nThe no-U-turn sampler: Adaptivelysetting path lengths in Hamilto-nianMonteCarlo. Technicalreport,Columbia U.\nHofmann, T. (1999). Probabilistic la-\ntent semantic indexing. Research\nand Development in Information\nRetrieval, 50\u201357.\nHolmes, C. and L. Held (2006).\nBayesian auxiliary variable models\nfor binary and multinomial regres-\nsion.Bayesian Analysis 1 (1),145\u2013\n168.\nHonk\nela, A. and H. Valpola (2004).\nVariational Learning and Bits-Back\nCoding: An Information-TheoreticView to Bayesian Learning. IEEE.\nTrans.onNeuralNetworks 15 (4).\nHonkela, A., H. Valpola, and\nJ. Karhunen (2003). Accelerat-ing Cyclic Update Algorithms forParameter Estimation by Pattern\nSearches. Neural Processing Let-\nters 17, 191\u2013203.", "1057": "BIBLIOGRAPHY 1027\nHop\ufb01eld, J. J. (1982, April). Neu-\nral networks and physical systems\nwith emergent collective computa-\ntional abilities. Proc.oftheNational\nAcademy of Science, USA 79 (8),\n2554\u00e2\u02d8A\u00b8S2558.\nHornik, K. (1991). Approximation ca-\npabilities of multilayer feedforwardnetworks. Neural Networks 4 (2),\n251\u00e2\u02d8A\u00b8S257.\nHorvitz, E., J. Apacible, R. Sarin, and\nL. Liao (2005). Prediction, Expecta-\ntion, and Surprise: Methods, De-signs, and Study of a DeployedTraffic Forecasting Service. In UAI.\nHoward, R. and J. Matheson (1981). In-\n\ufb02uence diagrams. In R. Howard\nand J. Matheson (Eds.), Readingson\nthe Principles and Applications of\nDecisionAnalysis,volumeII.S t r a t e -\ngic Decisions Group.\nHoyer, P. (2004). Non-negative matrix\nfactorizaton with sparseness con-\nstraints.J.ofMachineLearningRe-\nsearch 5, 1457\u20131469.\nHsu, C.-W., C.-C. Chang, and C.-J. Lin\n(2009). A practical guide to sup-\nport vector classi\ufb01cation. Technicalreport, Dept. Comp. Sci., NationalTaiwan University.\nHu, D., L. van der Maaten, Y. Cho,\nL. Saul, and S. Lerner (2010). LatentVariable Models for Predicting FileDependencies in Large-Scale Soft-ware Development. In NIPS.\nHu, M., C. Ingram, M.Sirski, C. Pal,\nS. Swamy, and C. Patten (2000).A Hierarchical HMM Implementa-tion for Vertebrate Gene Splice SitePrediction. Technical report, Dept.Computer Science, Univ. Waterloo.\nHuang, J., Q. Morris, and B. Frey\n(2007). Bayesian inference of Mi-croRNA targets from sequence andexpression data. J.Comp.Bio..\nHubel, D. and T. Wiesel (1962). Recep-\ntive \ufb01elds, binocular itneraction,and functional architecture in thecat\u2019s visual cortex. J.Physiology 160,\n106\u2013154.\nHuber, P. (1964). Robust estimation\nof a location parameter. Annals of\nStatistics 53, 73\u00e2 \u02d8A\u00b8S101.\nHubert, L. and P. Arabie (1985). Com-\nparing partitions. J. of Classi\ufb01ca-\ntion 2, 193\u2013218.Hunter, D. and R. Li (2005). Variable\nselection using MM algorithms.\nAnnalsofStatistics 33, 1617\u20131642.\nHunter, D. R. and K. Lange (2004). A\nTutorial on MM Algorithms. The\nAmericanStatistician 58, 30\u201337.\nHya\ufb01l, L. and R. Rivest (1976). Con-\nstructing Optimal Binary DecisionTrees is NP-complete. Information\nProcessingLetters 5 (1), 15\u201317.\nHyvarinen, A., J. Hurri, and P. Hoyer\n(2009).Natural Image Statistics: a\nprobabilisticapproachtoearlycom-\nputationalvision. Springer.\nHyvarinen,\nA. and E. Oja (2000). In-\ndependent component analysis: al-\ngorithms and applications. Neural\nNetworks 13, 411\u2013430.\nIlin, A. and T. Raiko (2010). Practi-\ncal Approaches to Principal Com-\nponent Analysis in the Presence\nof Missing Values. J. of Machine\nLearningResearch 11, 1957\u20132000.\nInsua, D. R. and F. Ruggeri (Eds.)\n(2000).Robust Bayesian Analysis.\nSpringer.\nIsard, M. (2003). PAMPAS: Real-Valued\nGraphical Models for Computer Vi-\nsion. InCVPR, Volume 1, pp. 613.\nIsard, M. and A. Blake (1998). CON-\nDENSATION - conditional densitypropagation for visual tracking.\nIntl. J. of Computer Vision 29 (1), 5\u2013\n18.\nJaakkola, T. (2001). Tutorial on varia-\ntional approximation methods. InM. Opper and D. Saad (Eds.), Ad-\nvanced mean \ufb01eld methods. MIT\nPress.\nJaakkola, T. and D. Haussler (1998). Ex-\nploiting generative models in dis-criminative classi\ufb01ers. In NIPS, pp.\n487\u2013493.\nJaakkola, T. and M. Jordan (1996a).\nComputing upper and lowerbounds on likelihoods in in-tractable networks. In UAI.\nJaakkola, T. and M. Jordan (1996b).\nA variational approach to Bayesianlogistic regression problems andtheir extensions. In AI+Statistics.\nJaakkola, T. S. and M. I. Jordan (2000).\nBayesian parameter estimation via\nvariational methods. Statistics and\nComputing 10, 25\u201337.Jacob, L., F. Bach, and J.-P. Vert (2008).\nClustered Multi-Task Learning: a\nConvex Formulation. In NIPS.\nJain, A. and R. Dubes (1988). Algo-\nrithmsforClusteringData. PrenticeHall.\nJames, G. and T. Hastie (1998). The\nerror coding method and PICTS.\nJ. of Computational and Graphical\nStatistics 7 (3), 377\u2013387.\nJapkowicz, N., S. Hanson, and\nM. Gluck (2000). Nonlinear autoas-sociation is not equivalent to PCA.\nNeuralComputation 12, 531\u2013545.\nJaynes, E. T. (2003). Probability the-\nory: thelogicofscience. Cambridge\nuniversity press.\nJebara, T., R. Kondor, and A. Howard\n(2004). Probability product kernels.\nJ. of Machine Learning Research 5,\n819\u2013844.\nJeffreys, H. (1961). TheoryofProbability.\nOxford.\nJelinek, F. (1997). Statisticalmethodsfor\nspeechrecognition. MIT Press.\nJensen, C. S., A. Kong, and U. Kjaerulff\n(1995). Blocking-gibbs samplingin very large probabilistic expert\nsystems.Intl.J.\n Human-Computer\nStudies, 647\u2013666.\nJermyn, I. (2005). Invariant bayesian\nestimation on manifolds. Annalsof\nStatistics 33 (2), 583\u2013605.\nJerrum, M. and A. Sinclair (1993).\nPolynomial-time approximation al-\ngorithms for the Ising model. SIAM\nJ.onComputing 22, 1087\u20131116.\nJerrum, M. and A. Sinclair (1996).\nThe markov chain monte carlomethod: an approach to approxi-mate counting and integration. InD. S. Hochbaum (Ed.), Approxima-\ntion Algorithms for NP-hard prob-\nlems. PWS Publishing.\nJerrum, M., A. Sinclair, and E. Vigoda\n(2004). A polynomial-time approx-\nimation algorithm for the perma-nent of a matrix with non-negative\nentries.Journal of the ACM , 671\u2013\n697.\nJi, S., D. Dunson, and L. Carin\n(2009). Multi-task compressivesensing.IEEETrans.SignalProcess-\ning 57(1).", "1058": "1028 BIBLIOGRAPHY\nJi, S., L. Tang, S. Yu, and J. Ye (2010).\nA shared-subspace learning frame-\nwork for multi-label classi\ufb01cation.ACMTrans.onKnowledgeDiscovery\nfrom Data 4 (2).\nJirousek, R. and S. Preucil (1995). On\nthe effective implementation of the\niterative proportional \ufb01tting proce-\ndure.Computational Statistics &\nDataAnalysis 19, 177\u2013189.\nJoachims, T. (2006). Training Linear\nSVMs in Linear Time. In Proc. of\ntheInt\u2019lConf.onKnowledgeDiscov-\neryandDataMining.\nJoachims, T., T. Finley, and C.-N. Yu\n(2009). Cutting-Plane Training of\nStructural SVMs. Machine Learn-\ning 77(1), 27\u201359.\nJohnson, J. K., D. M. Malioutov, and\nA. S. Willsky (2006). Walk-sum in-\nterpretation and analysis of gaus-sian belief propagation. In NIPS,\npp. 579\u2013586.\nJohnson, M. (2005). Capacity and\ncomplexity of HMM duration mod-\neling techniques. Signal Processing\nLetters 12(5), 407\u2013410.\nJohnson, N. (2009). A study of the\nNIPS feature selection challenge.\nTechnical report, Stanford.\nJohnson, V. and J. Albert (1999). Ordi-\nnaldatamodeling. Springer.\nJones, B., A. Dobra, C. Carvalho,\nC. Hans, C. Carter, and M. West(2005). Experiments in stochasticcomputation for high-dimensional\ngraphical models. Statistical Sci-\nence 20, 388\u2013400.\nJordan, M. I. (2007). An introduction to\nprobabilistic graphical models. In\npreparation.\nJordan, M. I. (2011). The era of big\ndata. InISBA Bulletin, Volume 18,\npp. 1\u20133.\nJordan, M. I., Z. Ghahramani, T. S.\nJaakkola, and L. K. Saul (1998). Anintroduction to variational meth-ods for graphical models. In M. Jor-\ndan (Ed.),Learning in Graphical\nModels. MIT Press.\nJordan, M. I. and R. A. Jacobs (1994).\nHierarchical mixtures of expertsand the EM algorithm. Neural\nComputation 6, 181\u2013214.Journee, M., Y. Nesterov, P. Richtarik,\nand R. Sepulchre (2010). General-\nized power method for sparse prin-\ncipal components analysis. J. of\nMachineLearningResearch 11, 517\u2013\n553.\nJulier, S. and J. Uhlmann (1997). A\nnew extension of the Kalman \ufb01l-ter to nonlinear systems. In Proc.\nofAeroSense: The11thIntl.Symp.on\nAerospace/Defence Sensing, Simula-tionandControls.\nJurafsky, D. and J. H. Martin (2000).\nSpeech and language processing:An Introduction to Natural Lan-\nguage Processing, Computational\nLinguistics,andSpeechRecognition.\nPrentice-Hall.\nJurafsky, D. and J. H. Martin (2008).\nSpeech and language processing:\nAn Introduction to Natural Lan-\nguage Processing, Computational\nLinguistics,andSpeechRecognition.\nPrentice-Hall. 2nd edition.\nKaariainen,\nM. and J. Langford (2005).\nA Comparison of Tight Generaliza-\ntion Bounds. In Intl. Conf. on Ma-\nchineLearning.\nKaelbling, L., M. Littman, and\nA. Moore (1996). Reinforcement\nlearning: A survey. J. of AI Re-\nsearch 4, 237\u2013285.\nKaelbling, L. P., M. Littman, and\nA. Cassandra (1998). Planningand acting in partially observable\nstochastic domains. Arti\ufb01cialIntel-\nligence 101.\nKaiser, H. (1958). The varimax crite-\nrion for analytic rotation in factoranalysis.Psychometrika 23 (3).\nKakade, S., Y. W. Teh, and S. Roweis\n(2002). An alternate objective func-tion for markovian \ufb01elds. In Intl.\nConf.onMachineLearning.\nKanazawa, K., D. Koller, and S. Rus-\nsell (1995). Stochastic simulation al-\ngorithms for dynamic probabilisticnetworks. In UAI.\nKandel, E., J. Schwarts, and T. Jessell\n(2000).PrinciplesofNeuralScience.\nMcGraw-Hill.\nKappen, H. and F. Rodriguez (1998).\nBoltzmann machine learning usingmean \ufb01eld theory and linear re-sponse correction. In NIPS.Karhunen, J. and J. Joutsensalo\n(1995). Generalizations of princi-pal component analysis, optimiza-tion problems, and neural net-\nworks.Neural Networks 8 (4), 549\u2013\n562.\nKass, R. and L. Wasserman (1995). A\nreference bayesian test for nested\nhypotheses and its relationship to\nthe schwarz criterio. J. of the Am.\nStat.Assoc. 90 (431), 928\u2013934.\nKatayama, T. (2005). SubspaceMethods\nfor Systems Identi\ufb01cation. Springer\nVerlag.\nKaufman, L. and P. Rousseeuw (1990).\nFinding Groups in Data: An Intro-\nductiontoClusterAnalysis. Wiley.\nKawakatsu, H. and A. Largey (2009).\nEM algorithms for ordered probit\nmodels with endogenous regres-\nsors.TheEconometricsJournal12 (1),\n164\u2013186.\nKearns, M. J. and U. V. Vazirani (1994).\nAn Introduction to Computational\nLearningTheory. MIT Press.\nKelley, J. E. (1960). The cutting-plane\nmethod for solving convex pro-\ngrams.J. of the Soc. for Industrial\nandAppliedMath. 8, 703\u2013712.\nKemp, C., J. Tenenbaum, S. Niyogi, and\nT. Griffiths (2010). A probabilistic\nmodel of theory formation. Cogni-\ntion 114, 165\u2013196.\nKemp, C., J. Tenenbaum, T. Y. T. Grif-\n\ufb01ths and, and N. Ueda (2006).Learning systems of concepts withan in\ufb01nite relational model. InAAAI.\nKersting, K., S. Natarajan, and D. Poole\n(2011). Statistical Relational AI:Logic, Probability and Computa-tion. Technical report, UBC.\nKhan,M.\nE., B. Marlin, G. Bouchard,\nand K. P. Murphy (2010). Varia-tional bounds for mixed-data fac-tor analysis. In NIPS.\nKhan, Z., T. Balch, and F. Dellaert\n(2006). MCMC Data Associationand Sparse Factorization Updatingfor Real Time Multitarget Trackingwith Merged and Multiple Mea-surements. IEEE Trans. on Pat-\ntern Analysis and Machine Intelli-gence 28(12).\nKirkpatrick, S., C. G. Jr., and M. Vecchi\n(1983). Optimization by simulatedannealing.Science 220, 671\u2013680.", "1059": "BIBLIOGRAPHY 1029\nKitagawa, G. (2004). The two-\ufb01lter for-\nmula for smoothing and an im-\nplementation of the Gaussian-sum\nsmoother.AnnalsoftheInstituteof\nStatistical Mathematics 46 (4), 605\u2013\n623.\nKjaerulff, U. (1990). Triangulation of\ngraphs \u2013 algorithms giving small\ntotal state space. Technical ReportR-90-09, Dept. of Math. and Comp.Sci., Aalborg Univ., Denmark.\nKjaerulff, U. and A. Madsen (2008).\nBayesian Networks and In\ufb02uence\nDiagrams: A Guide to ConstructionandAnalysis. Springer.\nKlaassen, C. and J. A. Wellner (1997).\nEfficient estimation in the bivari-\nate noramal copula model: Nor-mal margins are least favorable.\nBernoulli 3 (1), 55\u201377.\nKlami, A. and S. Kaski (2008). Proba-\nbilistic approach to detecting de-\npendencies between data sets.Neurocomputing 72, 39\u201346.\nKlami, A., S. Virtanen, and S. Kaski\n(2010). Bayesian exponential fam-ily projections for coupled datasources. InUAI.\nKleiner, A., A. Talwalkar, P. Sarkar, and\nM. I. Jordan (2011). A scalable boot-strap for massive data. Technicalreport, UC Berkeley.\nKneser, R. and H. Ney (1995). Im-\nproved backing-off for n-gram lan-\nguage modeling. In Intl. Conf. on\nAcoustics, Speech and Signal Proc.,\nVolume 1, pp. 181\u2013184.\nKo, J. and D. Fox (2009). GP-\nBayesFilters: Bayesian Filtering Us-ing Gaussian Process Predictionand Observation Models. Au-\ntonomousRobotsJournal .\nKohn, R., M. Smith, and D. Chan\n(2001). Nonparametric regressionusing linear combinations of basis\nfunctions.Statistical Computing 11,\n313\u2013322.\nKoivisto, M. (2006). Advances in ex-\nact Bayesian structure discovery in\nBayesian networks. In UAI.\nKoivisto, M. and K. Sood (2004). Ex-\nact Bayesian structure discovery in\nBayesian networks. J. of Machine\nLearningResearch 5, 549\u2013573.\nKoller, D. and N. Friedman (2009).\nProbabilistic Graphical Models:\nPrinciples and Techniques . MIT\nPress.Koller, D. and U. Lerner (2001). Sam-\npling in Factored Dynamic Sys-\ntems. In A. Doucet, N. de Fre-itas, and N. Gordon (Eds.), Sequen-\ntial Monte Carlo Methods in Prac-tice. Springer.\nKolmogorov, V. (2006, October). Con-\nvergent Tree-reweighted MessagePassing for Energy Minimization.\nIEEETrans.onPatternAnalysisand\nMachine Intelligence 28 (10), 1568\u2013\n1583.\nKolmogorov, V. and M. Wainwright\n(2005). On optimality properties\nof tree-reweighted message pass-ing. InUAI, pp. 316\u2013322.\nKolmogorov, V. and R. Zabin (2004).\nWhat energy functions can be min-imized via graph cuts? IEEETrans.\non Pattern Analysis and Machine\nIntelligence 26 (2), 147\u2013159.\nKomodakis, N., N. Paragios, and\nG. Tziritas (2011). MRF Energy Mini-\nmization and Beyond via Dual De-composition. IEEET\n rans. on Pat-\ntern Analysis and Machine Intelli-gence 33(3), 531\u2013552.\nKoo, T., A. M. Rush, M. Collins,\nT. Jaakkola, and D. Sontag (2010).Dual Decomposition for Parsingwith Non-Projective Head Au-tomata. In Proc. EMNLP, pp.\n1288\u00e2\u02d8A\u00b8S1298.\nKoren, Y. (2009a). The bellkor solution\nto the net\ufb02ix grand prize. Techni-cal report, Yahoo! Research.\nKoren, Y. (2009b). Collaborative \ufb01l-\ntering with temporal dynamics. In\nProc.oftheInt\u2019lConf.onKnowledge\nDiscoveryandDataMining.\nKoren, Y., R. Bell, and C. Volinsky\n(2009). Matrix factorization tech-\nniques for recommender systems.IEEEComputer 42 (8), 30\u201337.\nKrishnapuram, B., L. Carin,\nM. Figueiredo, and A. Hartemink(2005). Learning sparse bayesianclassi\ufb01ers: multi-class formulation,fast algorithms, and generalizationbounds. IEEE Transaction on\nPattern Analysis and Machine\nIntelligence .\nKrizhevsky, A. and G. Hinton (2010).\nUsing Very Deep Autoencoders\nfor Content-Based Image Retrieval.Submitted.Kschischang, F., B. Frey, and H.-A.\nLoeliger (2001, February). Factorgraphs and the sum-product algo-\nrithm.IEEETransInfo.Theory .\nKuan, P., G. Pan, J. A. Thomson,\nR. Stewart, and S. Keles (2009).\nA hierarchical semi-Markov modelfor detecting enrichment with ap-plication to ChIP-Seq experiments.Technical report, U. Wisconsin.\nKulesza, A. and B. Taskar (2011). Learn-\ning Determinantal Point Processes.InUAI.\nKumar, N. and A. Andreo (1998). Het-\neroscedastic discriminant analysisand reduced rank HMMs for im-proved speech recognition. Speech\nCommunication 26, 283\u2013297.\nKumar, S. and M. Hebert (2003). Dis-\ncriminative random \ufb01elds: A dis-criminative framework for contex-tual interaction in classi\ufb01cation. InIntl.Conf.onComputerVision.\nKuo, L. and B. Mallick (1998). Vari-\nable selection for regression mod-\nels.SankhyaSeriesB 60, 65\u201381.\nKurihara, K., M. Welling, and N. Vlas-\nsis (2006). Accelerated variational\nDP mixture models. In NIPS.\nKushner, H. and G. Yin (2003).\nStochasticapproximationandrecur-\nsive algorithms and applications.\nSpringer.\nKuss and C. Rasmussen (2005). As-\nsessing approximate inference forbinary gaussian process classi\ufb01ca-\ntion.J. of Machine Learning Re-\nsearch 6, 1679\u20131704.\nKwon, J. and K. Murphy (2000). Mod-\neling freeway traffic with coupled\nHMMs. Technical report, Univ. Cal-ifornia, Berkeley.\nKyung, M., J. Gill, M. Ghosh, and\nG. Casella (2010). Penalized Regres-sion, Standard Errors and Bayesian\nLassos.BayesianAnalysis5 (2), 369\u2013\n412.\nLacoste-Julien, S., F. Huszar, and\nZ. Ghahramani (2011). Approximate\ninference for the loss-calibratedBay\nesian. InAI/Statistics.\nLacoste-Julien, S., F. Sha, and M. I. Jor-\ndan (2009). DiscLDA: Discrimina-tive learning for dimensionality re-duction and classi\ufb01cation. In NIPS.", "1060": "1030 BIBLIOGRAPHY\nLafferty, J., A. McCallum, and\nF. Pereira (2001). Conditional ran-\ndom \ufb01elds: Probabilistic modelsfor segmenting and labeling se-\nquence data. In Intl. Conf. on Ma-\nchineLearning.\nLange, K., R. Little, and J. Taylor (1989).\nRobust statistical modeling usingthe t disribution. J. of the Am. Stat.\nAssoc. 84(408), 881\u2013896.\nLangville, A. and C. Meyer (2006).\nUpdating Markov chains with an\neye on Google\u2019s PageRank. SIAM\nJ. on Matrix Analysis and Applica-tions 27(4), 968\u2013987.\nLarranaga, P., C. M. H. Kuijpers,\nM. Poza, and R. H. Murga (1997).Decomposing bayesian networks:triangulation of the moral graphwith genetic algorithms. Statistics\nandComputing(UK) 7 (1), 19\u201334.\nLashkari, D. and P. Golland (2007).\nConvex clustering with examplar-based models. In NIPS.\nLasserre, J., C. Bishop, and T. Minka\n(2006). Principled hybrids of gen-erative and discriminative models.InCVPR.\nLau, J. and P. Green (2006).\nBayesian model-based clustering\nprocedures. Journal of Computa-\ntional and Graphical Statistics 12,\n351\u2013357.\nLauritzen, S. (1996). Graphical Models.\nOUP.\nLauritzen, S. (2000). Causal infer-\nence from graphical models. InD. R. C. O. E. Barndoff-Nielsenand C. Klueppelberg (Eds.), Com-\nplex stochastic systems. Chapmanand Hall.\nLauritzen, S. and D. Nilsson (2001).\nRepresenting and solving decisionproblems with limited information.ManagementScience 47, 1238\u20131251.\nLauritzen, S. L. (1992, December).\nPropagation of probabilities, meansand variances in mixed graphical\nassociation models. J. of the Am.\nStat.Assoc. 87 (420), 1098\u20131108.\nLauritzen, S. L. (1995). The EM al-\ngorithm for graphical association\nmodels with missing data. Com-\nputational Statistics and Data Anal-ysis 19, 191\u2013201.Lauritzen, S. L. and D. J. Spiegelhal-\nter (1988). Local computations withprobabilities on graphical struc-tures and their applications to ex-pert systems. J.R.Stat.Soc.B B (50),\n127\u2013224.\nLaw, E., B. Settles, and T. Mitchell\n(2010). Learning to tag from openvocabulary labels. In Proc. Euro-\npeanConf.onMachineLearning.\nLaw, M., M. Figueiredo, and A. Jain\n(2004). Simultaneous Feature Se-lection and Clustering Using Mix-ture Models. IEEE Trans. on Pat-\ntern Analysis and Machine Intelli-gence 26(4).\nLawrence, N. D. (2005). Probabilis-\ntic non-linear principal componentanalysis with gaussian process la-\ntent variable models. J.ofMachine\nLearningResearch 6, 1783\u20131816.\nLawrence, N. D. (2012). A unifying\nprobabilistic\nperspective for spec-\ntral dimensionality reduction: in-sights and new models. J. of Ma-\nchine Learning Research 13, 1609\u2013\n1638.\nLearned-Miller, E. (2004). Hyperspac-\nings and the estimation of infor-mation theoretic quantities. Tech-nical Report 04-104, U. Mass.Amherst Comp. Sci. Dept.\nLeCun, Y., B. Boser, J. S. Denker,\nD. Henderson, R. E. Howard,W. Hubbard, and L. D. Jackel (1989,Winter). Backpropagation appliedto handwritten zip code recogni-\ntion.NeuralComputation 1 (4), 541\u2013\n551.\nLeCun, Y., L. Bottou, Y. Bengio,\nand P. Haffner (1998, November).\nGradient-based learning applied to\ndocument recognition. Proceedings\noftheIEEE 86 (11), 2278\u20132324.\nLeCun, Y., S. Chopra, R. Hadsell, F.-J.\nHuang, and M.-A. Ranzato (2006).\nA tutorial on energy-based learn-\ning. In B. et al. (Ed.), Predicting\nStructuredOutputs. MIT press.\nLedoit, O. and M. Wolf (2004a). Honey,\nI Shrunk the Sample CovarianceMatrix.J. of Portfolio Manage-\nment 31(1).\nLedoit, O. and M. Wolf (2004b). A well-\nconditioned estimator for large-\ndimensional covariance matrices. J.\nof Multivariate Analysis 88 (2), 365\u2013\n411.Lee, A., F. Caron, A. Doucet, and\nC. Holmes (2010). A hierarchicalbayesian framework for construct-ing sparsity-inducing priors. Tech-nical report, U. Oxford.\nLee, A., F. Caron, A. Doucet, and\nC.Holmes(2011). BayesianSparsity-Path-Analysis of Genetic Associ-ation Signal using Generalized tPrior. Technical report, U. Oxford.\nLee, D. and S. Seung (2001). Algo-\nrithms for non-negative matrix fac-torization. In NIPS.\nLee, H., R. Grosse, R. Ranganath, and\nA. Ng (2009). Convolutional deepbelief networks for scalable un-supervised learning of hierarchical\nrepresentations. In Intl. Conf. on\nMachineLearning.\nLee, H., Y. Largman, P. Pham, and\nA. Ng (2009). Unsupervised feature\nlearning for audio classi\ufb01cation us-ing convolutional deep belief net-works. InNIPS.\nLee, S.-I., V. Ganapathi, and D. Koller\n(2006). Efficient structure learn-ing of Markov networks using L1-regularization. In NIPS.\nLee, T. S. and D. Mumford (2003). Hi-\nerarchical Bayesian inference in the\nvisual cortex. J.ofOpticalSocietyof\nAmericaA 20 (7), 1434\u20131448.\nLenk, P., W. S. DeSarbo, P. Green,\nand M. Young (1996). Hierarchi-\ncal Bayes Conjoint Analysis: Re-covery of Partworth Heterogeneityfrom Reduced Experimental De-\nsigns.MarketingScience 15 (2), 173\u2013\n191.\nLenkoski, A. and A. Dobra (2008).\nBayesian structural learning and\nestimation in Gaussian graphicalmodels. Technical Report 545, De-partment of Statistics, University ofWashington.\nLepar, V. and P. P. Shenoy (1998).\nA Comparison of Lauritzen-Spiegelhalter, Hugin and Shenoy-Shafer Architectures for ComputingMarginals\nof Probability Distribu-\ntions. In G. Cooper and S. Moral(Eds.),UAI, pp. 328\u2013337. Morgan\nKaufmann.\nLerner, U. and R. Parr (2001). Infer-\nence in hybrid networks: Theoreti-cal limits and practical algorithms.InUAI.", "1061": "BIBLIOGRAPHY 1031\nLeslie, C., E. Eskin, A. Cohen, J. We-\nston, and W. Noble (2003). Mis-\nmatch string kernels for discrimi-native protein classi\ufb01cation. Bioin-\nformatics 1 , 1\u201310.\nLevy, S. (2011). In The Plex: How\nGoogle Thinks, Works, and ShapesOurLives. Simon & Schuster.\nLi, L., W. Chu, J. Langford, and\nX. Wang (2011). Unbiased offlineevaluation of contextual-bandit-based news article recommenda-tion algorithms. In WSDM.\nLiang, F., S. Mukherjee, and M. West\n(2007). Understanding the use ofunlabelled data in predictive mod-\nelling.Statistical Science 22, 189\u2013\n205.\nLiang, F., R. Paulo, G. Molina,\nM. Clyde, and J. Berger (2008). Mix-\ntures of g-priors for Bayesian Vari-\nable Selection. J. of the Am. Stat.\nAssoc. 103 (481), 410\u2013423.\nLiang, P. and M. I. Jordan (2008).\nAn asymptotic analysis of gen-\nerative, discriminative, and pseu-dolikelihood estimators. In In-\nternational Conference on MachineLearning(ICML).\nLiang, P. and D. Klein. Online EM\nfor Unsupervised Models. In Proc.\nNAACLConference.\nLiao, L., D. J. Patterson, D. Fox, and\nH. Kautz (2007). Learning andInferring Transportation Routines.\nArti\ufb01cialIntelligence 171 (5), 311\u2013331.\nLindley, D. (1982). Scoring rules and\nthe inevetability of probability. ISI\nReview 50, 1\u201326.\nLindley, D. V. (1972). BayesianStatistics:\nAReview. SIAM.\nLindley, D. V. and L. D. Phillips (1976).\nInference for a Bernoulli Process\n(A Bayesian View). The American\nStatistician 30 (3), 112\u2013119.\nLindsay, B. (1988). Composite like-\nlihood methods. Contemporary\nMathematics 80 (1), 221\u2013239.\nLipton, R. J. and R. E. Tarjan (1979).\nA separator theorem for planar\ngraphs.SIAM Journal of Applied\nMath 36, 177\u2013189.\nLittle., R. J. and D. B. Rubin (1987). Sta-\ntistical Analysis with Missing Data.\nNew York: Wiley and Son.Liu, C. and D. Rubin (1995). ML Esti-\nmation of the T distribution usingEM and its extensions, ECM andECME.StatisticaSinica 5, 19\u201339.\nLiu, H., J. Lafferty, and L. Wasserman\n(2009). The nonparanormal: Semi-parametric estimation of high di-mensional undirected graphs. J.\nof Machine Learning Research 10,2295\u20132328.\nLiu, J. (2001). MonteCarloStrategiesin\nScienti\ufb01cComputation. Springer.\nLiu, J. S., W. H. Wong, and A. Kong\n(1994). Covariance structure of thegibbs sampler with applicationsto the comparisons of estima-tors and augmentation schemes.\nBiometrika 81 (1), 27\u201340.\nLiu,T\n.-Y. (2009). Learning to rank\nfor information retrieval. Founda-\ntionsandTrendsinInformationRe-\ntrieval 3(3), 225\u2013331.\nLizotte, D. (2008). Practical Bayesian\noptimization. Ph.D. thesis, U. Al-\nberta.\nLjung, L. (1987). System Identi\ufb01ciation:\nTheoryfortheUser. Prentice Hall.\nLo, C. H. (2009). Statistical methods\nforhighthroughputgenomics. Ph.D.thesis, UBC.\nLo, K., F. Hahne, R. Brinkman, R. Ryan,\nand R. Gottardo (2009, May). \ufb02ow-clust: a bioconductor package forautomated gating of \ufb02ow cytome-\ntry data.BMC Bioinformatics 10,\n145+.\nLopes, H. and M. West (2004).\nBayesian model assessment in fac-\ntor analysis. StatisicaSinica 14, 41\u2013\n67.\nLowe, D. G. (1999). Object recognition\nfrom local scale-invariant features.InProc. of the International Con-\nference on Computer Vision ICCV,\nCorfu, pp. 1150\u20131157.\nLuce, R. (1959). Individual choice be-\nhavior: A theoretical analysis.W i -\nley.\nLunn, D., N. Best, and J. Whit-\ntaker (2009). Generic reversiblejump MCMC using graphical mod-\nels.Statistics and Computing 19 (4),\n395\u2013408.Lunn, D., A. Thomas, N. Best, and\nD. Spiegelhalter (2000). WinBUGS\n\u2013 a Bayesian modelling framework:concepts, structure, and extensibil-\nity.Statistics and Computing 10,\n325\u2013337.\nMa, H., H. Yang, M. Lyu, and I. King\n(2008). SoRec: Social recommenda-\ntion using probabilistic matrix fac-\ntorization. In Proc. of17th Conf.on\nInformation and Knowledge Man-\nagement.\nMa, S., C. Ji, and J. Farmer (1997). An\nefficient EM-based training algo-rithm for feedforward neural net-works.NeuralNetworks 10 (2), 243\u2013\n256.\nMaathuis, M., D. Colombo, M. Kalisch,\nand P. B\u00c3ijhlmann (2010). Pre-dicting causal effects in large-scalesystems from observational data.\nNatureMethods 7, 247\u2013248.\nMaathuis, M., M. Kalisch, and\nP. B\u00c3ijhlmann (2009). Estimating\nhigh-dimensional intervention ef-fects from observational data. An-\nnalsofStatistics 37 , 3133\u20133164.\nMacKay, D. (1992). Bayesian interpo-\nlation.Neural Computation 4, 415\u2013\n447.\nMacKay, D. (1995a). Developments in\nprobabilistic modeling with neuralnetworks \u2014 ensemble learning. In\nProc. 3rd Ann. Symp. Neural Net-\nworks.\nMacKay, D. (1995b). Probable net-\nworks and plausible predictions\n\u2014 a review of practical Bayesianmethods\nfor supervised neural net-\nworks.Network.\nMacKay, D. (1997). Ensemble learning\nfor Hidden Markov Models. Tech-nical report, U. Cambridge.\nMacKay, D. (1999). Comparision of\napproximate methods for handling\nhyperparameters. NeuralComputa-\ntion 11(5), 1035\u20131068.\nMacKay, D. (2003). InformationTheory,\nInference,andLearningAlgorithms.\nCambridge University Press.\nMacnaughton-Smith, P., W. T.\nWilliams, M. B. Dale, and G. Mock-ett (1964). Dissimilarity analysis:a new technique of hierarchicalsub-division. Nature 202, 1034 \u2013\n1035.", "1062": "1032 BIBLIOGRAPHY\nMadeira, S. C. and A. L. Oliveira\n(2004). Biclustering algorithms for\nbiological data analysis: A survey.IEEE/ACM Transactions on Compu-\ntational Biology and Bioinformat-\nics 1(1), 24\u201345.\nMadigan, D. and A. Raftery (1994).\nModel selection and accountingfor model uncertainty in graphical\nmodelsusingOccam\u2019swindow. J.of\ntheAm.Stat.Assoc. 89, 1535\u20131546.\nMadsen, R., D. Kauchak, and C. Elkan\n(2005). Modeling word burstiness\nusing the Dirichlet distribution. In\nIntl.Conf.onMachineLearning.\nMairal, J., F. Bach, J. Ponce, and\nG. Sapiro (2010). Online learning\nfor matrix factorization and sparse\ncoding.J. of Machine Learning Re-\nsearch 11, 19\u201360.\nMairal, J., M. Elad, and G. Sapiro\n(2008). Sparse representation for\ncolor image restoration. IEEETrans.\nonImageProcessing 17 (1), 53\u201369.\nMalioutov, D., J. Johnson, and A. Will-\nsky (2006). Walk-sums and beliefpropagation in gaussian graphical\nmodels.J.ofMachineLearningRe-\nsearch 7, 2003\u20132030.\nMallat, S., G. Davis, and Z. Zhang\n(1994, July). Adaptive time-\nfrequency decompositions. SPIE\nJournal of Optical Engineering 33,2183\u20132919.\nMallat, S. and Z. Zhang (1993). Match-\ning pursuits with time-frequencydictionaries. IEEE Transactions on\nSignalProcessing 41 (12), 3397\u20133415.\nMalouf, R. (2002). A comparison of\nalgorithms for maximum entropyparameter estimation. In Proc.\nSixth Conference on Natural Lan-guage Learning (CoNLL-2002), pp.49\u201355.\nManning, C., P. Raghavan, and\nH. Schuetze (2008). Introduction\ntoInformationRetrieval. CambridgeUniversity Press.\nManning, C. and H. Schuetze (1999).\nFoundations of statistical natural\nlanguageprocessing. MIT Press.\nMansinghka, V., D. Roy, R. Rifkin,\nand J. Tenenbaum (2007). AClass:\nAn online algorithm for generativeclassi\ufb01cation. In AI/Statistics.Mansinghka, V., P. Shafto, E. Jonas,\nC. Petschulat, and J. Tenenbaum(2011). Cross-Categorization: ANonparametric Bayesian Methodfor Modeling Heterogeneous, HighDimensional Data. Technical re-port, MIT.\nMargolin, A., I. Nemenman, K. Basso,\nC. Wiggins, G. Stolovitzky, and R. F.abd A. Califano (2006). ARACNE:An Algorithm for the Reconstruc-tion of Gene Regulatory Networksin a Mammalian Cellular Context.BMCBionformatics 7 .\nMarin, J.-M. and C. Robert (2007).\nBayesianCore: apracticalapproach\ntocomputationalBayesianstatistics.\nSpringer.\nMarks, T. K. and J. R. Movellan (2001).\nDiffusion networks, products of ex-perts, and factor analysis. Techni-cal report, University of CaliforniaSan Diego.\nMarlin, B. (2003). Modeling user rat-\ning pro\ufb01les for collaborative \ufb01lter-ing. InNIPS.\nMarlin, B. (2008). Missing\n Data Prob-\nlems in Machine Learning. Ph.D.thesis, U. Toronto.\nMarlin, B., E. Khan, and K. Murphy\n(2011). Piecewise Bounds for Es-timating Bernoulli-Logistic Latent\nGaussian Models. In Intl. Conf. on\nMachineLearning.\nMarlin, B. and R. Zemel (2009). Collab-\norative prediction and ranking with\nnon-random missing data. In Proc.\nof the 3rd ACM Conference on Rec-\nommenderSystems.\nMarlin, B. M., K. Swersky, B. Chen, and\nN. de Freitas (2010). Inductive prin-\nciples for restricted boltzmann ma-chine learning. In AI/Statistics.\nMarroquin, J., S. Mitter, and T. Pog-\ngio (1987). Probabilistic solutionof ill-posed problems in computa-\ntional vision. J. of the Am. Stat. As-\nsoc. 82(297), 76\u201389.\nMartens, J. (2010). Deep learning via\nhessian-free optimization. In Intl.\nConf.onMachineLearning.\nMaruyama, Y. and E. George (2008). A\ng-prior extension for p>n. Tech-\nnical report, U. Tokyo.\nMason, L., J. Baxter, P. Bartlett, and\nM. Frean (2000). Boosting algo-\nrithms as gradient descent. InNIPS, Volume 12, pp. 512\u2013518.Matthews, R. (1998). Bayesian Critique\nof Statistics in Health: The GreatHealth Hoax.\nMaybeck, P. (1979). Stochastic models,\nestimation, and control. AcademicPress.\nMazumder, R. and T. Hastie (2012).\nThe Graphical Lasso: New Insightsand Alternatives. Technical report.\nMcAuliffe, J., D. Blei, and M. Jordan\n(2006). Nonparametric empiricalbayes for the dirichlet process mix-\nture model. Statistics and Comput-\ning 16(1), 5\u201314.\nMcCallum, A. (2003). Efficiently induc-\ning features of conditional random\n\ufb01elds. InUAI.\nMcCallum, A., D. Freitag, and\nF. Pereira (2000). Maximum En-tropy Markov Models for Informa-tion Extraction and Segmentation.\nInIntl.Conf.onMachineLearning.\nMcCallum, A. and K. Nigam (1998).\nA comparison of event models for\nnaive Bayes text classi\ufb01cation. In\nAAAI/ICML workshop on Learning\nforTextCategorization.\nMcCray, A. (2003). An upper level\nontology for the biomedical do-\nmain.Comparative and Functional\nGenomics 4, 80\u201384.\nMcCullagh, P. and J. Nelder (1989).\nGeneralized linear models. Chap-\nman and Hall. 2nd edition.\nMcCullich, W. and W. Pitts (1943). A\nlogical calculus of the ideas imma-\nnent in nervous activity. Bulletinof\nMathematicalBiophysics 5, 115\u2013137.\nMcDonald, J. and W. Newey (1988).\nPartially Adaptive Estimation of Re-\ngression Models via the General-ized t Distribution. Econometric\nTheory 4(3),428\u2013445.\nMcEliece,\nR. J., D. J. C. MacKay, and\nJ. F. Cheng (1998). Turbo decod-ing as an instance of Pearl\u2019s \u2019beliefpropagation\u2019 algorithm. IEEE J. on\nSelectedAreasinComm. 16 (2), 140\u2013\n152.\nMcFadden, D. (1974). Conditional logit\nanalysis of qualitative choice be-havior. In P. Zarembka (Ed.), Fron-\ntiers in econometrics, pp. 105\u2013142.Academic Press.", "1063": "BIBLIOGRAPHY 1033\nMcGrayne, S. B. (2011). The the-\nory that would not die: how\nBayes\u2019rulecrackedtheenigmacode,hunted down Russian submarines,\nand emerged triumphant from two\ncenturies of controversy. Yale Uni-\nversity Press.\nMcKay, B. D., F. E. Oggier, G. F. Royle,\nN. J. A. Sloane, I. M. Wanless, andH. S. Wilf (2004). Acyclic digraphsand eigenvalues of (0,1)-matrices. J.\nIntegerSequences 7 (04.3.3).\nMcKay, D. and L. C. B. Peto (1995).\nA hierarchical dirichlet language\nmodel.NaturalLanguageEngineer-\ning 1(3), 289\u2013307.\nMcLachlan, G. J. and T. Krishnan\n(1997).The EM Algorithm and Ex-\ntensions. Wiley.\nMeek, C. and D. Heckerman (1997).\nStructure and parameter learn-\ning for causal independence andcausal interaction models. In UAI,\npp. 366\u2013375.\nMeek, C., B. Thiesson, and D. Hecker-\nman (2002). Staged mixture mod-elling and boosting. In UAI, San\nFrancisco, CA, pp. 335\u2013343. Mor-gan Kaufmann.\nMeila, M. (2001). A random walks view\nof spectral segmentation. In AI/S-\ntatistics.\nMeila, M. (2005). Comparing cluster-\nings: an axiomatic view. In Intl.\nConf.onMachineLearning.\nMeila, M. and T. Jaakkola (2006).\nTractable Bayesian learning of tree\nbelief networks. StatisticsandCom-\nputing 16, 77\u201392.\nMeila, M. and M. I. Jordan (2000).\nLearning with mixtures of trees. J.\nof Machine Learning Research 1,1 \u2013\n48.\nMeinshausen, N. (2005). A note on the\nlasso for gaussian graphical modelselection. Technical report, ETHSeminar fur Statistik.\nMeinshausen, N. and P. Buhlmann\n(2006). High dimensional graphsand variable selection with thelasso.The Annals of Statistics 34,\n1436\u20131462.\nMeinshausen, N. and P. B\u00c3ijhlmann\n(2010). Stability selection. J.ofRoyal\nStat.Soc.SeriesB 72, 417\u2013473.Meltzer, T., C. Yanover, and Y. Weiss\n(2005). Globally optimal solu-tions for energy minimization instereo vision using reweighted be-lief propagation. In ICCV, pp. 428\u2013\n435.\nMeng, X. L. and D. van Dyk (1997).\nThe EM algorithm \u2014 an old folksong sung to a fast new tune (with\nDiscussion). J.RoyalStat.Soc.B 59,\n511\u2013567.\nMesot, B. and D. Barber (2009). A Sim-\nple Alternative Derivation of the\nExpectation Correction Algorithm.\nIEEE Signal Processing Letters 16 (1),\n121\u2013124.\nMetropolis, N., A. Rosenbluth,\nM. Rosenbluth, A. Teller, and\nE. Teller (1953). Equation of statecalculations by fast computing\nmachines.J.ofChemicalPhysics 21,\n1087\u20131092.\nMetz,\nC. (2010). Google behavioral ad\ntargeter is a Smart Ass. The Regis-\nter.\nMiller, A. (2002). Subsetselectioninre-\ngression. Chapman and Hall. 2nd\nedition.\nMimno, D. and A. McCallum (2008).\nTopic models conditioned on ar-bitrary features with dirichlet-multinomial regression. In UAI.\nMinka, T. (1999). Pathologies of ortho-\ndox statisics. Technical report, MITMedia Lab.\nMinka, T. (2000a). Automatical choice\nof dimensionality for PCA. Techni-cal report, MIT.\nMinka, T. (2000b). Bayesian linear re-\ngression. Technical report, MIT.\nMinka, T. (2000c). Bayesian model av-\neraging is not model combination.Technical report, MIT Media Lab.\nMinka, T. (2000d). Empirical risk min-\nimization is an incomplete induc-tive principle. Technical report,MIT.\nMinka, T. (2000e). Estimating a Dirich-\nlet distribution. Technical report,MIT.\nMinka, T. (2000f). Inferring a Gaussian\ndistribution. Technical report, MIT.\nMinka, T. (2001a). Bayesian inference\nof a uniform distribution. Techni-cal report, MIT.Minka, T. (2001b). Empirical Risk Min-\nimization is an incomplete induc-tive principle. Technical report,MIT.\nMinka, T. (2001c). Expectation propa-\ngationforapproximateBayesianin-ference. InUAI.\nMinka, T. (2001d). A family of algo-\nrithmsforapproximateBayesianin-\nference. Ph.D. thesis, MIT.\nMinka, T. (2001e). Statistical ap-\nproaches to learning and discovery\n10-602: Homework assignment 2,question 5. Technical report, CMU.\nMinka, T. (2003). A comparison of nu-\nmerical optimizers for logistic re-gression. Technical report, MSR.\nMinka, T. (2005). Divergence measures\nand message passing. Technical re-port, MSR Cambridge.\nMinka, T. and Y. Qi (2003). Tree-\nstructured approximations by ex-pectation propagation. In NIPS.\nMinka, T., J. Winn, J. Guiver, and\nD. Knowles (2010). Infer.NET 2.4.Microsoft Research Cambridge.http://research.microsoft.com/infernet.\nMinsky, M. and S. Papert (1969). Per-\nceptrons. MIT Press.\nMitchell, T. (1997). Machine Learning.\nMcGraw Hill.\nMitchell, T. and J. Beauchamp (1988).\nBayesian Variable Selection in Lin-\near Regression. J. of the Am. Stat.\nAssoc. 83, 1023\u20131036.\nMobahi, H., R. Collobert, and J. We-\nston (2009). Deep learning from\ntemporal coherence in video. In\nIntl.Conf.onMachineLearning.\nMockus, J., W. Eddy, A. Mockus,\nL. Mockus, and G. Reklaitis (1996).\nBayesianHeuristicApproachtoDis-\ncrete and Global Optimization: Al-gorithms, Visualization, Software,andApplications. Kluwer.\nMoghaddam, B., A. Gruber, Y. Weiss,\nand S. Avidan (2008). Sparse re-\ngression as a sparse eigenvalue\nproblem. In Information Theory &\nApplicationsWorkshop(ITA\u201908).\nMoghaddam, B., B. Marlin, E. Khan,\nandK.\nMurphy (2009). Accel-\nerating bayesian structural infer-\nence for non-decomposable gaus-sian graphical models. In NIPS.", "1064": "1034 BIBLIOGRAPHY\nMoghaddam, B. and A. Pentland\n(1995). Probabilistic visual learning\nfor object detection. In Intl. Conf.\nonComputerVision.\nMohamed, S., K. Heller, and\nZ. Ghahramani (2008). Bayesian\nExponential Family PCA. In NIPS.\nMoler, C. (2004). NumericalComputing\nwithMATLAB. SIAM.\nMorris, R. D., X. Descombes, and\nJ. Zerubia (1996). The Ising/Pottsmodel is not well suited to seg-\nmentation tasks. In IEEEDSPWork-\nshop.\nMosterman, P. J. and G. Biswas (1999).\nDiagnosis of continuous valued\nsystems in transient operating re-gions.IEEETrans.onSystems,Man,\nand Cybernetics, Part A 29 (6), 554\u2013\n565.\nMoulines, E., J.-F. Cardoso, and E. Gas-\nsiat (1997). Maximum likelihoodfor blind separation and deconvo-lution of noisy signals using mix-ture models. In Proc. IEEE Int.\nConf. on Acoustics, Speech and Sig-\nnal Processing (ICASSP\u201997), Munich,\nGermany, pp. 3617\u20133620.\nMuller, P., G. Parmigiani, C. Robert,\nand J. Rousseau (2004). Optimalsample size for multiple testing:the case of gene expression mi-\ncroarrays. J. of the Am. Stat. As-\nsoc. 99, 990\u20131001.\nMumford, D. (1994). Neuronal archi-\ntectures for pattern-theoretic prob-\nlems. In C. Koch and J. Davis (Eds.),LargeScaleNeuronalTheoriesoftheBrain. MIT Press.\nMurphy, K. (2000). Bayesian map\nlearning in dynamic environments.InNIPS, Volume 12.\nMurphy, K. and M. Paskin (2001). Lin-\near time inference in hierarchicalHMMs. InNIPS.\nMurphy, K., Y. Weiss, and M. Jordan\n(1999). Loopy belief propagation forapproximate inference: an empiri-cal study. In UAI.\nMurphy, K. P. (1998). Filtering and\nsmoothing in linear dynamical sys-tems using the junction tree algo-rithm. Technical report, U.C. Berke-ley, Dept. Comp. Sci.\nMurray, I. and Z. Ghahramani (2005).\nA note on the evidence andbayesian occam\u2019s razor. Technicalreport, Gatsby.Musso, C., N. Oudjane, and F. LeGland\n(2001). Improving regularized par-ticle \ufb01lters. In A. Doucet, J. F. G.de Freitas, and N. Gordon (Eds.),Sequential Monte Carlo Methods inPractice. Springer.\nNabney, I. (2001). NETLAB: algorithms\nforpatternrecognition. Springer.\nNeal, R. (1992). Connectionist learning\nof belief networks. Arti\ufb01cialIntelli-\ngence 56, 71\u2013113.\nNeal, R. (1993). Probabilistic Inference\nUsing Markov Chain Monte CarloMethods. Technical report, Univ.Toronto.\nNeal, R. (1996). Bayesian learning for\nneuralnetworks. Springer.\nNeal, R. (1997). Monte Carlo Im-\nplementation of Gaussian ProcessModels for Bayesian Regressionand Classi\ufb01cation. Technical Re-port\n9702, U. Toronto.\nNeal, R. (1998). Erroneous Results\nin \u2019Marginal Likelihood from theGibbs Output\u2019. Technical report, U.Toronto.\nNeal, R. (2000). Markov Chain Sam-\npling Methods for Dirichlet Process\nMixture Models. J. of Computa-\ntional and Graphical Statistics 9 (2),\n249\u2013265.\nNeal, R. (2003a). Slice sampling. An-\nnalsofStatistics 31 (3), 7\u20135\u2013767.\nNeal, R. (2010). MCMC using Hamil-\ntonian Dynamics. In S. Brooks,\nA. Gelman, G. Jones, and X.-L.Meng (Eds.), Handbook of Markov\nChain Monte Carlo. Chapman &Hall.\nNeal, R. and D. MacKay (1998).\nLikelihood-based boosting. Techni-cal report, U. Toronto.\nNeal, R. and J. Zhang (2006). High\ndimensional classi\ufb01cation Bayesianneural networks and Dirichlet dif-fusion trees. In I. Guyon, S. Gunn,M. Nikravesh, and L. Zadeh (Eds.),FeatureExtraction. Springer.\nNeal, R. M. (2001). Annealed impor-\ntance sampling. StatisticsandCom-\nputing 11, 125\u2013139.\nNeal, R. M. (2003b). Density Model-\ning and Clustering using DirichletDiffusion Trees. In J. M. Bernardoet al. (Eds.),BayesianStatistics7, pp.\n619\u2013629. Oxford University Press.Neal, R. M. and G. E. Hinton (1998).\nA new view of the EM algorithmthat justi\ufb01es incremental and othervariants. In M. Jordan (Ed.), Learn-\ninginGraphicalModels. MIT Press.\nNeapolitan, R. (2003). Learning\nBayesianNetworks. Prentice Hall.\nNe\ufb01an, A., L. Liang, X. Pi, X. Liu,\nand K. Murphy (2002). DynamicBayesian Networks for Audio-\nVisual Speech Recognition. J. Ap-\npliedSignalProcessing .\nNemirovski, A. and D. Yudin (1978). On\nCezari\u2019s convergence of the steep-est descent method for approxi-mating saddle points of convex-\nconcave functions. Soviet Math.\nDokl. 19.\nNesterov, Y. (2004). Introductory Lec-\ntures on Convex Optimization. A\nbasiccourse. Kluwer.\nNewton, M., D. Noueiry, D. Sarkar,\nand P. Ahlquist (2004). Detecting\ndifferential gene expression witha semiparametric hierarchical mix-ture method. Biostatistics 5, 155\u2013\n176.\nNewton, M. and A. Raftery (1994). Ap-\nproximate Bayesian Inference withthe Weighted Likelihood Bootstrap.\nJ. of Royal Stat. Soc. Series B 56 (1),\n3\u201348.\nNg, A., M. Jordan, and Y. Weiss (2001).\nOn Spectral Clustering: Analysis\nand an algorithm. In NIPS.\nNg, A. Y. and M. I. Jordan (2002). On\ndiscriminative vs. generative classi-\ufb01ers: A comparison of logistic re-gression and naive bayes. In NIPS-\n14.\nNickisch, H. and C. Rasmussen (2008).\nApproximations for binary gaus-\nsian process classi\ufb01cation. J.ofMa-\nchineLear\nning Research 9, 2035\u2013\n2078.\nNilsson, D. (1998). An efficient algo-\nrithm for \ufb01nding the M most prob-\nable con\ufb01gurations in a probabilis-\ntic expert system. Statistics and\nComputing 8 , 159\u2013173.\nNilsson, D. and J. Goldberger (2001).\nSequentially \ufb01nding the N-Best Listin Hidden Markov Models. In Intl.\nJointConf.onAI, pp. 1280\u20131285.\nNocedal, J. and S. Wright (2006). Nu-\nmericalOptimization. Springer.", "1065": "BIBLIOGRAPHY 1035\nNowicki, K. and T. A. B. Snijders\n(2001). Estimation and prediction\nfor stochastic blockstructures. Jour-\nnaloftheAmericanStatisticalAsso-ciation 96 (455), 1077\u2013??\nNowlan, S. and G. Hinton (1992). Sim-\nplifying neural networks by soft\nweight sharing. Neural Computa-\ntion 4(4), 473\u2013493.\nNummiaro, K., E. Koller-Meier, and\nL. V. Gool (2003). An adaptive\ncolor-based particle \ufb01lter. Image\nandVisionComputing 21 (1), 99\u2013110.\nObozinski, G., B. Taskar, and M. I. Jor-\ndan (2007). Joint covariate selectionfor grouped classi\ufb01cation. Techni-cal report, UC Berkeley.\nOh, M.-S. and J. Berger (1992). Adap-\ntive importance sampling in Monte\nCarlo integration. J. of Statistical\nComputation and Simulation 41 (3),\n143 \u2013 168.\nOh, S., S. Russell, and S. Sastry (2009).\nMarkov Chain Monte Carlo Data\nAssociation for Multi-Target Track-ing.IEEETrans.onAutomaticCon-\ntrol 54(3), 481\u2013497.\nO\u2019Hagan, A. (1978). Curve \ufb01tting and\noptimal design for prediction. J. of\nRoyalStat.Soc.SeriesB 40, 1\u201342.\nO\u2019Hara, R. and M. Sillanpaa (2009).\nA Review of Bayesian Variable Se-lection Methods: What, How andWhich.Bayesian Analysis 4 (1), 85\u2013\n118.\nOlshausen, B. A. and D. J. Field (1996).\nEmergence of simple cell recep-tive \ufb01eld properties by learning asparse code for natural images. Na-\nture 381, 607\u2013609.\nOpper, M. (1998). A Bayesian approach\nto online learning. In D. Saad\n(Ed.),On-line learning in neural\nnetworks. Cambridge.\nOpper, M. and C. Archambeau (2009).\nThe variational Gaussian approxi-mation revisited. Neural Computa-\ntion 21(3), 786\u2013792.\nOpper, M. and D. Saad (Eds.) (2001).\nAdvanced mean \ufb01eld methods: the-\noryandpractice. MIT Press.\nOsborne, M. R., B. Presnell, and B. A.\nTurlach (2000a). A new approach to\nvariable selection in least squares\nproblems.IMA Journal of Numeri-\ncalAnalysis 20 (3), 389\u2013403.Osborne, M. R., B. Presnell, and B. A.\nTurlach (2000b). On the lasso and\nits dual. J. Computational and\ngraphicalstatistics 9, 319\u2013337.\nOstendorf, M., V. Digalakis, and\nO. Kimball (1996). From HMMs tosegment models: a uni\ufb01ed viewof stochastic modeling for speech\nrecognition. IEEE Trans. on Speech\nand Audio Processing 4 (5), 360\u2013\n378.\nOverschee, P. V. and B. D. Moor\n(1996).Subspace Identi\ufb01cation for\nLinear Systems: Theory, Implemen-\ntation, Applications . Kluwer Aca-\ndemic Publishers.\nPaatero, P. and U. Tapper (1994). Pos-\nitive matrix factorization: A non-\nnegative factor model with opti-mal utilization of error estimates ofdata values. Environmetrics 5 , 111\u2013\n126.\nPadadimit\nriou, C. and K. Steiglitz\n(1982).Combinatorialoptimization:\nAlgorithms and Complexity. Pren-tice Hall.\nPaisley, J. and L. Carin (2009). Non-\nparametric factor analysis with\nbeta process priors. In Intl. Conf.\nonMachineLearning.\nPalmer, S. (1999). Vision Science: Pho-\ntonstoPhenomenology. MIT Press.\nParise, S. and M. Welling (2005).\nLearning in Markov Random\nFields: An Empirical Study. In Joint\nStatisticalMeeting.\nPark, T. and G. Casella (2008). The\nBayesian Lasso. J. of the Am. Stat.\nAssoc. 103 (482), 681\u2013686.\nParviainen, P. and M. Koivisto (2011).\nAncestor relations in the presenceof unobserved variables. In Proc.\nEuropean Conf. on Machine Learn-ing.\nPaskin, M. (2003). Thin junction tree\n\ufb01lters for simultaneous localizationand mapping. In Intl.JointConf.on\nAI.\nPearl, J. (1988). ProbabilisticReasoning\nin Intelligent Systems: Networks of\nPlausible Inference. Morgan Kauf-\nmann.\nPearl, J. (2000). Causality: Models,Rea-\nsoning and Inference. CambridgeUniv. Press.Pearl, J. and T. Verma (1991). A theory\nof inferred causation. In Knowledge\nRepresentation, pp. 441\u2013452.\nPe\u2019er, D. (2005, April). Bayesian net-\nwork analysis of signaling net-works: a primer. ScienceSTKE 281,\n14.\nPeng, F., R. Jacobs, and M. Tan-\nner (1996). Bayesian Inferencein Mixtures-of-Experts and Hier-archical Mixtures-of-Experts Mod-els With an Application to Speech\nRecognition. J. of the Am. Stat. As-\nsoc. 91(435), 953\u2013960.\nPetris, G., S. Petrone, and P. Campag-\nnoli (2009). Dynamiclinearmodels\nwithR. Springer.\nPham, D.-T. and P. Garrat (1997). Blind\nseparation of mixture of inde-\npendent sources through a quasi-maximum likelihood approach.\nIEEE Trans. on Signal Process-\ning 45(7), 1712\u20131725.\nPietra, S. D., V. D. Pietra, and J. Laf-\nferty (1997). Inducing features ofrandom \ufb01elds. IEEE Trans. on Pat-\ntern Analysis and Machine Intelli-gence 19(4).\nPlackett, R. (1975). The analysis of per-\nmutations. Applied Stat. 24, 193\u2013\n202.\nPlatt, J. (1998). Using analytic QP\nand sparseness to speed training ofsupport vector machines. In NIPS.\nPlatt, J. (2000). Probabilities for sv ma-\nchines. In A. Smola, P. Bartlett,B. Schoelkopf, and D. Schuurmans\n(Eds.),Advances in Large Margin\nClassi\ufb01ers. MIT Press.\nPla\ntt, J., N. Cristianini, and J. Shawe-\nTaylor (2000). Large margin DAGs\nfor multiclass classi\ufb01cation. In\nNIPS, Volume 12, pp. 547\u2013553.\nPlummer, M. (2003). JAGS: A Program\nfor Analysis of Bayesian Graphi-cal Models Using Gibbs Sampling.\nInProc. 3rd Intl. Workshop on Dis-\ntributedStatisticalComputing.\nPolson, N. and S. Scott (2011). Data\naugmentation for support vectormachines. Bayesian Analysis 6 (1),\n1\u2013124.\nPontil, M., S. Mukherjee, and F. Girosi\n(1998). On the Noise Model of Sup-\nport Vector Machine Regression.Technical report, MIT AI Lab.\nPoon, H. and P. Domingos (2011). Sum-\nproduct networks: A new deep ar-chitecture. In UAI.", "1066": "1036 BIBLIOGRAPHY\nPourahmadi, M. (2004). Simultaneous\nModelling of Covariance Matrices:\nGLM, Bayesian and Nonparamet-ric Perspectives. Technical report,Northern Illinois University.\nPrado, R. and M. West (2010). Time\nSeries: Modelling, Computation and\nInference. CRC Press.\nPress, S. J. (2005). Applied mul-\ntivariate analysis, using Bayesianand frequentist methods of infer-\nence. Dover. Second edition.\nPress, W., W. Vetterling, S. Teukolosky,\nand B. Flannery (1988). Numeri-\ncal Recipes in C: The Art of Scien-\nti\ufb01cComputing (Second ed.). Cam-\nbridge University Press.\nPrince, S. (2012). Computer Vision:\nModels, Learning and Inference.\nCambridge.\nPritchard, J., M. M. Stephens, and\nP. Donnelly (2000). Inference ofpopulation structure using multi-locus genotype data. Genetics 155,\n945\u2013959.\nQi, Y. and T. Jaakkola (2008). Param-\neter Expanded Variational BayesianMethods. In NIPS.\nQi, Y., M. Szummer, and T. Minka\n(2005). Bayesian Conditional Ran-\ndom Fields. In 10th Intl. Workshop\nonAI/Statistics.\nQuinlan, J. (1990). Learning logical def-\ninitions from relations. Machine\nLearning 5, 239\u2013266.\nQuinlan, J. R. (1986). Induction of de-\ncision trees. Machine Learning 1,\n81\u2013106.\nQuinlan, J. R. (1993). C4.5Programsfor\nMachine Learning. Morgan Kauff-\nman.\nQuinonero-Candela, J., C. Rasmussen,\nand C. Williams (2007). Approxi-mation methods for gaussian pro-cess regression. In L. Bottou,O. Chapelle, D. DeCoste, and J. We-\nston (Eds.),Large Scale Kernel Ma-\nchines, pp. 203\u2013223. MIT Press.\nRabiner, L. R. (1989). A tutorial on Hid-\nden Markov Models and selected\napplications in speech recognition.\nProc.oftheIEEE 77 (2), 257\u2013286.\nRai, P. and H. Daume (2009). Multi-\nlabel prediction via sparse in\ufb01nite\nCCA. InNIPS.Raiffa, H. (1968). DecisionAnalysis.A d -\ndison Wesley.\nRaina, R., A. Madhavan, and A. Ng\n(2009). Large-scale deep unsuper-vised learning using graphics pro-\ncessors. In Intl. Conf. on Machine\nLearning.\nRaina, R., A. Ng, and D. Koller (2005).\nTransfer learning by constructing\ninformative priors. In NIPS.\nRajaraman, A. and J. Ullman (2010).\nMining of massive datasets. Self-published.\nRajaraman, A. and J. Ullman (2011).\nMining of massive datasets. Cam-bridge.\nRakotomamonjy, A., F. Bach, S. Canu,\nand Y. Grandvalet (2008). Sim-\npleMKL.J.ofMachineLearningRe-\nsear\nch 9, 2491\u20132521.\nRamage, D., D. Hall, R. Nallapati,\nand C. Manning (2009). Labeled\nLDA: A supervised topic model forcredit attribution in multi-labeledcorpora. In EMNLP.\nRamage, D., C. Manning, and S. Du-\nmais (2011). Partially Labeled TopicModels for Interpretable Text Min-\ning. InProc. of the Int\u2019l Conf. on\nKnowledgeDiscoveryandDataMin-\ning.\nRamaswamy, S., P. Tamayo, R. Rifkin,\nS. Mukherjee, C. Yeang, M. Angelo,C. Ladd, M. Reich, E. Latulippe,J. Mesirov, T. Poggio, W. Gerald,M. Loda, E. Lander, and T. Golub(2001). Multiclass cancer diagno-sis using tumor gene expression\nsignature. Proc. of the National\nAcademyofScience,USA98, 15149\u2013\n15154.\nRanzato, M. and G. Hinton (2010).\nModeling pixel means and covari-ances using factored third-orderBoltzmann machines. In CVPR.\nRanzato, M., F.-J. Huang, Y.-L.\nBoureau, and Y. LeCun (2007). Un-supervised Learning of InvariantFeature Hierarchies with Applica-tions to Object Recognition. InCVPR.\nRanzato, M., C. Poultney, S. Chopra,\nand Y.LeCun (2006). Efficientlearning of sparse representationswith an energy-based model. InNIPS.Rao, A. and K. Rose (2001, February).\nDeterministically Annealed Designof Hidden Markov Model Speech\nRecognizers. IEEETrans.onSpeech\nandAudioProc. 9 (2), 111\u2013126.\nRasmussen, C. (2000). The in\ufb01nite\ngaussian mixture model. In NIPS.\nRasmussen, C. E. and J. Qui\u00f1onero-\nCandela (2005). Healing the rele-\nvance vector machine by augmen-\ntation. InIntl. Conf. on Machine\nLearning, pp. 689\u2013696.\nRasmussen, C. E. and C. K. I. Williams\n(2006).Gaussian Processes for Ma-\nchineLearning. MIT Press.\nRatsch, G., T. Onoda, and K. Muller\n(2001). Soft margins for adaboost.MachineLearning 42, 287\u2013320.\nRattray, M., O. Stegle, K. Sharp, and\nJ. Winn (2009). Inference algo-\nrithms and learning theory forBayesian sparse factor analysis. In\nProc. Intl. Workshop on Statistical-\nMechanicalInformatics.\nRauch, H. E., F. Tung, and C. T. Striebel\n(1965). Maximum likelihood esti-\nmates of linear dynamic systems.\nAIAAJournal 3 (8), 1445\u20131450.\nRavikumar, P., J. Lafferty, H. Liu, and\nL. Wasserman (2009). Sparse Ad-ditive Models. J. of Royal Stat. Soc.\nSeriesB 71 (5), 1009\u20131030.\nRaydan, M. (1997). The barzilai and\nborwein gradient method for the\nlarge scale unconstrained mini-mization problem. SIAMJ.onOpti-\nmization 7 (1), 26\u201333.\nRennie, J. (2004). Why sums are bad.\nTechnical report, MIT.\nRennie,J.,\nL. Shih, J. Teevan, and\nD. Karger (2003). Tackling the poorassumptions of naive Bayes text\nclassi\ufb01ers. In Intl.Conf.onMachine\nLearning.\nReshed, D., Y. Reshef, H. Finucane,\nS. Grossman, G. McVean, P. Turn-\nbaugh, E. Lander, M. Mitzen-macher, and P. Sabeti (2011, De-cember). Detecting novel associa-tions in large data sets. Science334,\n1518\u20131524.\nResnick, S. I. (1992). Adventures in\nStochasticProcesses. Birkhauser.\nRice, J. (1995). Mathematical statistics\nand data analysis. Duxbury. 2ndedition.", "1067": "BIBLIOGRAPHY 1037\nRichardson, S. and P. Green (1997). On\nBayesian Analysis of Mixtures With\nan Unknown Number of Compo-\nnents.J. of Royal Stat. Soc. Series\nB5 9, 731\u2013758.\nRiesenhuber, M. and T. Poggio\n(1999). Hierarchical models of ob-\nject recognition in cortex. Nature\nNeuroscience 2, 1019\u20131025.\nRish, I., G. Grabarnik, G. Cec-\nchi, F. Pereira, and G. Gordon(2008). Closed-form supervised di-mensionality reduction with gener-\nalized linear models. In Intl. Conf.\nonMachineLearning.\nRistic, B., S. Arulampalam, and N. Gor-\ndon (2004). BeyondtheKalmanFil-\nter: Particle Filters for Tracking Ap-\nplications. Artech House Radar Li-\nbrary.\nRobert, C. (1995). Simulation of trun-\ncated normal distributions. Statis-\nticsandcomputing 5, 121\u2013125.\nRobert, C. and G. Casella (2004).\nMonte Carlo Statisical Methods.Springer. 2nd edition.\nRoberts, G. and J. Rosenthal (2001).\nOptimal scaling for variousMetropolis-Hastings algorithms.\nStatisticalScience 16, 351\u2013367.\nRoberts, G. O. and S. K. Sahu\n(1997). Updating schemes, corre-\nlation structure, blocking and pa-rameterization for the gibbs sam-\npler.J. of Royal Stat. Soc. Series\nB5 9(2), 291\u2013317.\nRobinson, R. W. (1973). Counting la-\nbeled acyclic digraphs. In F. Harary(Ed.),New Directions in the Theory\nof Graphs, pp. 239\u2013273. Academic\nPress.\nRoch, S. (2006). A short proof\nthat phylogenetic tree reconstru-tion by maximum likelihood ishard.IEEE/ACM Trans. Comp. Bio.\nBioinformatics 31 (1).\nRodriguez, A. and K. Ghosh\n(2011). Modeling relationaldata through nested partition\nmodels.Biometrika . To appear.\nRose, K. (1998, November). Determin-\nistic annealing for clustering, com-\npression, classi\ufb01cation, regression,and related optimization problems.Proc.IEEE 80, 2210\u20132239.Rosenblatt, F. (1958). The percep-\ntron: A probabilistic model forinformation storage and organiza-\ntion in the brain. PsychologicalRe-\nview 65(6), 386\u00e2\u02d8A\u00b8S408.\nRoss, S. (1989). Introduction to Proba-\nbilityModels. Academic Press.\nRosset, S., J. Zhu, and T. Hastie (2004).\nBoosting as a regularized path toa maximum margin classi\ufb01er. J. of\nMachine Learning Research 5, 941\u2013\n973.\nRossi, P., G. Allenby, and R. McCulloch\n(2006).BayesianStatisticsandMar-\nketing. Wiley.\nRoth, D. (1996, Apr). On the hardness\nof approximate reasoning. Arti\ufb01cial\nIntelligence 82 (1-2),273\u2013302.\nRo\nther, C., P. Kohli, W. Feng, and J. Jia\n(2009). Minimizing sparse higherorder energy functions of discretevariables. In CVPR, pp. 1382\u20131389.\nRouder, J., P. Speckman, D. Sun, and\nR. Morey (2009). Bayesian t testsfor accepting and rejecting the null\nhypothesis. PyschonomicBulletin&\nReview 16 (2), 225\u2013237.\nRoverato, A. (2002). Hyper in-\nverse Wishart distribution for non-\ndecomposable graphs and its ap-plication to Bayesian inference for\nGaussian graphical models. Scand.\nJ.Statistics 29, 391\u2013411.\nRoweis, S. (1997). EM algorithms for\nPCA and SPCA. In NIPS.\nRubin, D. (1998). Using the SIR algo-\nrithm to simulate posterior distri-\nbutions. InBayesianStatistics3.\nRue, H. and L. Held (2005). Gaus-\nsian Markov Random Fields: The-ory and Applications, Volume 104\nofMonographsonStatisticsandAp-\nplied Probability. London: Chap-\nman & Hall.\nRue, H., S. Martino, and N. Chopin\n(2009). Approximate Bayesian In-ference for Latent Gaussian ModelsUsing Integrated Nested Laplace\nApproximations. J.ofRoyalStat.Soc.\nSeriesB 71, 319\u2013392.\nRumelhart, D., G. Hinton, and\nR. Williams (1986). Learning inter-\nnal representations by error propa-gation. In D. Rumelhart, J. McClel-land, and the PDD Research Group\n(Eds.),Parallel Distributed Process-\ning: Explorations in the Microstruc-\ntureofCognition. MIT Press.Ruppert, D., M. Wand, and R. Carroll\n(2003).Semiparametric Regression.\nCambridge University Press.\nRush, A. M. and M. Collins (2012). A\ntutorial on Lagrangian relaxation\nand dual decomposition for NLP.Technical report, Columbia U.\nRussell, S., J. Binder, D. Koller, and\nK. Kanazawa (1995). Local learningin probabilistic networks with hid-\nden variables. In Intl.JointConf.on\nAI.\nRussell, S. and P. Norvig (1995). Ar-\nti\ufb01cial Intelligence: A Modern Ap-\nproach. Englewood Cliffs, NJ: Pren-\ntice Hall.\nRussell, S. and P. Norvig (2002). Ar-\nti\ufb01cial Intelligence: A Modern Ap-\nproach. Prentice Hall. 2nd edition.\nRussell, S. and P. Norvig (2010). Ar-\nti\ufb01cial Intelligence: A Modern Ap-proach. Prentice Hall. 3rd edition.\nS. and M. Black (2009, April). Fields\nof experts. Intl. J. Computer Vi-\nsion 82(2), 205\u2013229.\nSachs, K., O. Perez, D. Pe\u2019er,\nD. Lauffenburger, and G. Nolan\n(2005). Causal protein-signalingnetworks derived from multipa-rameter single-cell data. Sci-\nence 308.\nSahami,M.\nand T. Heilman (2006).\nA Web-based Kernel Function forMeasuring the Similarity of Short\nText Snippets. In WWWconferenec.\nSalakhutdinov, R. (2009). Deep Gen-\nerative Models. Ph.D. thesis, U.\nToronto.\nSalakhutdinov, R. and G. Hinton\n(2009). Deep Boltzmann machines.InAI/Statistics, Volume 5, pp. 448\u2013\n455.\nSalakhutdinov, R. and G. Hinton\n(2010). Replicated Softmax: anUndirected Topic Model. In NIPS.\nSalakhutdinov, R. and H. Larochelle\n(2010). Efficient Learning of DeepBoltzmann Machines. In AI/Statis-\ntics.\nSalakhutdinov, R. and A. Mnih (2008).\nProbabilistic matrix factorization.InNIPS, Volume 20.", "1068": "1038 BIBLIOGRAPHY\nSalakhutdinov, R. and S. Roweis\n(2003). Adaptive overrelaxed bound\noptimization methods. In Proceed-\ningsoftheInternationalConference\non Machine Learning, Volume 20,\npp. 664\u2013671.\nSalakhutdinov, R., J. Tenenbaum, and\nA. Torralba (2011). Learning ToLearn with Compound HD Models.InNIPS.\nSalakhutdinov, R. R., A. Mnih, and\nG. E. Hinton (2007). Restrictedboltzmann machines for collabo-rative \ufb01ltering. In Intl. Conf. on\nMachine Learning, Volume 24, pp.791\u2013798.\nSalojarvi, J., K. Puolamaki, and\nS. Klaski (2005). On discriminativejoint density modeling. In Proc.Eu-\nropeanConf.onMachineLearning.\nSampson, F. (1968). A Novitiate in a\nPeriod of Change: An Experimental\nand Case Study of Social Relation-ships. Ph.D. thesis, Cornell.\nSantner, T., B. Williams, and W. Notz\n(2003).The Design and Analysis of\nComputerExperiments. Springer.\nSarkar, J. (1991). One-armed bandit\nproblems with covariates. The An-\nnalsofStatistics 19 (4), 1978\u20132002.\nSato, M. and S. Ishii (2000). On-line\nEM algorithm for the normalized\nGaussian network. Neural Compu-\ntation 12, 407\u2013432.\nSaul, L., T. Jaakkola, and M. Jordan\n(1996). Mean Field Theory for Sig-moid Belief Networks. J. of AI Re-\nsearch 4, 61\u201376.\nSaul, L. and M. Jordan (1995). Exploit-\ning tractable substructures in in-\ntractable networks. In NIPS, Vol-\nume 8.\nSaul, L. and M. Jordan (2000). Attrac-\ntor dynamics in feedforward neural\nnetworks.Neural Computation 12,\n1313\u20131335.\nSaunders, C., J. Shawe-Taylor, and\nA. Vinokourov (2003). String Ker-\nnels, FisherKernelsandFiniteStateAutomata. In NIPS.\nSavage, R., K. Heller, Y. Xi, Z. Ghahra-\nmani, W. Truman, M. Grant,K. Denby, and D. Wild (2009).R/BHC: fast Bayesian hierarchi-cal clustering for microarray data.\nBMCBioinformatics 10 (242).Schaefer, J. and K. Strimmer (2005).\nA shrinkage approach to large-\nscale covariance matrix estimationand implications for functional ge-\nnomics.Statist. Appl. Genet. Mol.\nBiol 4(32).\nSchapire, R. (1990). The strength of\nweak learnability. Machine Learn-\ning 5, 197\u2013227.\nSchapire, R. and Y. Freund (2012).\nBoosting: Foundations and Algo-\nrithms. MIT Press.\nSchapire, R., Y. Freund, P. Bartlett, and\nW. Lee (1998). Boosting the mar-\ngin: a new explanation for the ef-fectiveness of voting methods. An-\nnalsofStatistics 5, 1651\u20131686.\nSchar\nstein, D. and R. Szeliski (2002). A\ntaxonomy and evaluation of densetwo-frame stereo correspondence\nalgorithms. Intl. J. Computer Vi-\nsion 47(1), 7\u201342.\nSchaul, T., S. Zhang, and Y. LeCun\n(2012). No more pesky learning\nrates. Technical report, Courant In-stite of Mathematical Sciences.\nSchmee, J. and G. Hahn (1979). A sim-\nple method for regresssion analy-\nsis with censored data. Technomet-\nrics 21, 417\u2013432.\nSchmidt, M. (2010). Graphical model\nstructure learning with L1 regular-\nization. Ph.D. thesis, UBC.\nSchmidt, M., G. Fung, and R. Rosales\n(2009). Optimization methods for/lscript\u22121regularization. Technical re-\nport, U. British Columbia.\nSchmidt, M. and K. Murphy (2009).\nModeling Discrete InterventionalData using Directed Cyclic Graphi-cal Models. In UAI.\nSchmidt, M., K. Murphy, G. Fung, and\nR. Rosales (2008). Structure Learn-ing in Random Fields for HeartMotion Abnormality Detection. InCVPR.\nSchmidt, M., A. Niculescu-Mizil, and\nK. Murphy (2007). Learning Graph-ical Model Structure using L1-Regularization Paths. In AAAI.\nSchmidt, M., E. van den Berg,\nM. Friedlander, and K. Murphy(2009). Optimizing Costly Func-tions with Simple Constraints: ALimited-Memory Projected Quasi-Newton Algorithm. In AI & Statis-\ntics.Schniter, P., L. C. Potter, and J. Ziniel\n(2008). Fast Bayesian MatchingPursuit: Model Uncertainty andParameter Estimation for SparseLinear Models. Technical report, U.Ohio. Submitted to IEEE Trans. onSignal Processing.\nSchnitzspan, P., S. Roth, and B. Schiele\n(2010). Automatic discovery ofmeaningful object parts with latentCRFs. InCVPR.\nSchoelkopf, B. and A. Smola (2002).\nLearningwithKernels: SupportVec-\ntor Machines, Regularization, Opti-mization,andBeyond. MIT Press.\nSchoelkopf, B., A. Smola, and K.-R.\nMueller (1998). Nonlinear compo-\nnent analysis as a kernel eigen-\nvalue problem. Neural Computa-\ntion 10, 1299 \u2013 1319.\nSchraudolph, N. N., J. Yu, and S. G\u00fcn-\nter (2007). A Stochastic Quasi-\nNewton Method for Online ConvexOptimization. In AI/Statistics, pp.\n436\u2013443.\nSchwarz, G. (1978). Estimating the di-\nmension of a model. Annals of\nStatistics 6 (2), 461\u00e2\u02d8A\u00b8S464.\nSchwarz, R. and Y. Chow (1990). The\nn-best algorithm: an efficient andexact procedure for \ufb01nding the n\nmost likely hypotheses. In Intl.\nConf. on Acoustics, Speech and Sig-\nnalP\nroc.\nSchweikerta, G., A. Zien, G. Zeller,\nJ. Behr, C. Dieterich, C. Ong,\nP. Philips, F. D. Bona, L. Hartmann,\nA. Bohlen, N. Kr\u00c3ijger, S. Son-\nnenburg, and G. R\u00c3d\u2019tsch (2009).\nmGene: Accurate SVM-based GeneFinding with an Application to Ne-matode Genomes. Genome Re-\nsearch, 19, 2133\u20132143.\nScott, D. (1979). On optimal\nand data-based histograms.\nBiometrika 66 (3), 605\u2013610.\nScott, J. G. and C. M. Carvalho (2008).\nFeature-inclusion stochastic search\nfor gaussian graphical models. J.\nof Computational and GraphicalStatistics 17 (4), 790\u2013808.\nScott, S. (2009). Data augmenta-\ntion, frequentist estimation, andthe bayesian analysis of multino-\nmial logit models. StatisticalPapers .\nScott, S. (2010). A modern Bayesian\nlook at the multi-armed bandit.\nApplied Stochastic Models in Busi-\nnessandIndustry 26, 639\u2013658.", "1069": "BIBLIOGRAPHY 1039\nSedgewick, R. and K. Wayne (2011). Al-\ngorithms. Addison Wesley.\nSeeger, M. (2008). Bayesian Inference\nand Optimal Design in the Sparse\nLinear Model. J.ofMachineLearn-\ningResearch 9, 759\u2013813.\nSeeger, M. and H. Nickish (2008).\nCompressed sensing and bayesianexperimental design. In Intl. Conf.\nonMachineLearning.\nSegal, D. (2011, 12 February). The dirty\nlittle secrets of search. New York\nTimes.\nSeide, F., G. Li, and D. Yu (2011). Con-\nversational Speech Transcription\nUsing Context-Dependent Deep\nNeural Networks. In Interspeech.\nSejnowski, T. and C. Rosenberg (1987).\nParallel networks that learn to pro-nounce english text. Complex Sys-\ntems 1, 145\u2013168.\nSellke, T., M. J. Bayarri, and J. Berger\n(2001). Calibration of p Values\nfor Testing Precise Null Hypothe-\nses.TheAmericanStatistician 55 (1),\n62\u201371.\nSerre, T., L. Wolf, and T. Poggio (2005).\nObject recognition with features\ninspired by visual cortex. In CVPR,\npp. 994\u20131000.\nShachter, R. (1998). Bayes-ball: The\nrational pastime (for determiningirrelevance and requisite informa-tion in belief networks and in\ufb02u-ence diagrams). In UAI.\nShachter, R. and C. R. Kenley\n(1989). Gaussian in\ufb02uence dia-grams.Managment Science 35 (5),\n527\u2013550.\nShachter, R. D. and M. A. Peot (1989).\nSimulation approaches to generalprobabilistic inference on beliefnetworks. In UAI, Volume 5.\nShafer, G. R. and P. P. Shenoy (1990).\nProbability propagation. Annals of\nMathematicsandAI 2, 327\u2013352.\nShafto, P., C. Kemp, V. Mansinghka,\nM. Gordon, and J. B. Tenenbaum(2006). Learning cross-cutting sys-tems of categories. In CognitiveSci-\nenceConference.\nShahaf, D., A. Chechetka, and\nC. Guestrin (2009). Learning ThinJunction Trees via Graph Cuts. InAISTATS.Shalev-Shwartz, S., Y. Singer, and\nN. Srebro (2007). Pegasos: pri-mal estimated sub-gradient solver\nfor svm. In Intl. Conf. on Machine\nLearning.\nShalizi, C. (2009). Cs 36-350 lecture\n10: Principal components: mathe-\nmatics, example, interpretation.\nShan, H. and A. Banerjee (2010). Resid-\nual Bayesian co-clustering for ma-\ntrix approximation. In SIAM Intl.\nConf.onDataMining.\nShawe-Taylor, J. and N. Cristianini\n(2004).Kernel Methods for Pattern\nAnalysis. Cambridge.\nSheng, Q., Y. Moreau, and B. D. Moor\n(2003). BiclusteringMicroarraydata\nby Gibbs sampling. Bioinf\n ormat-\nics 19, ii196\u2013ii205.\nShi, J. and J. Malik (2000). Normal-\nized cuts and image segmentation.\nIEEETrans.onPatternAnalysisand\nMachineIntelligence .\nShoham, Y. and K. Leyton-Brown\n(2009).Multiagent Systems: Algo-\nrithmic, Game- Theoretic, and Log-ical Foundations. Cambridge Uni-\nversity Press.\nShotton, J., A. Fitzgibbon, M. Cook,\nT. Sharp, M. Finocchio, R. Moore,A. Kipman, and A. Blake (2011).Real-time human pose recognitionin parts from a single depth image.InCVPR.\nShwe, M., B. Middleton, D. Heck-\nerman, M. Henrion, E. Horvitz,H. Lehmann, and G. Cooper (1991).Probabilistic diagnosis using a re-formulation of the internist-1/qmr\nknowledge base. Methods. Inf.\nMed 30(4), 241\u2013255.\nSiddiqi, S., B. Boots, and G. Gordon\n(2007). A constraint generation ap-\nproach to learning stable linear dy-namical systems. In NIPS.\nSiepel, A. and D. Haussler (2003).\nCombining phylogenetic and hid-den markov models in biosequence\nanalysis. In Proc. 7th Intl. Conf.\non Computational Molecular Biol-\nogy(RECOMB).\nSilander, T., P. Kontkanen, and P. Myl-\nlym\u00c3d\u2019ki (2007). On Sensitivity ofthe MAP Bayesian Network Struc-ture to the Equivalent Sample SizeParameter. In UAI, pp. 360\u2013367.Silander, T. and P. Myllmaki (2006).\nA simple approach for \ufb01nding theglobally optimal Bayesian networkstructure. In UAI.\nSill, J., G. Takacs, L. Mackey, and\nD. Lin (2009). Feature-weighted lin-ear stacking. Technical report, .\nSilverman, B. W. (1984). Spline\nsmoothing: the equivalent variable\nkernel method. Annals of Statis-\ntics 12(3), 898\u2013916.\nSimard, P., D. Steinkraus, and J. Platt\n(2003). Best practices for convolu-\ntional neural networks applied to\nvisual document analysis. In Intl.\nConf. on Document Analysis and\nRecognition(ICDAR).\nSimon, D. (2006). Optimal State Es-\ntimation: Kalman, H In\ufb01nity, and\nNonlinearApproaches. Wiley.\nSingliar, T. and M. Hauskrecht (2006).\nNoisy-OR Component Analysis and\nits Application to Link Analysis. J.\nofMachineLearningResearch 7.\nSmidl, V. and A. Quinn (2005). The\nVariational Bayes Method in SignalProcessing. Springer.\nSmith, A. F. M. and A. E. Gelfand\n(1992). Bayesian statistics with-out tears: A sampling-resampling\nperspective. The American Statisti-\ncian 46(2), 84\u201388.\nSmith, R. and P. Cheeseman (1986).\nOn the representation and estima-tion of spatial uncertainty. Intl. J.\nRobo\nticsResearch 5 (4), 56\u201368.\nSmith, V., J. Yu, T. Smulders,\nA. Hartemink, and E. Jarvis (2006).\nComputational Inference of NeuralInformation Flow Networks. PLOS\nComputational Biology 2, 1436\u20131439.\nSmolensky, P. (1986). Information\nprocessing in dynamical systems:foundations of harmony theory.In D. Rumehart and J. McClel-land (Eds.),ParallelDistributedPro-\ncessing: Explorations in the Mi-\ncrostructure of Cognition. Volume 1.\nMcGraw-Hill.\nSmyth, P., D. Heckerman, and M. I. Jor-\ndan (1997). Probabilistic indepen-dence networks for hidden Markovprobability models. NeuralCompu-\ntation 9(2), 227\u2013269.\nSohl-Dickstein, J., P. Battaglino, and\nM. DeWeese (2011). In Intl. Conf.\nonMachineLearning.", "1070": "1040 BIBLIOGRAPHY\nSollich, P. (2002). Bayesian methods\nfor support vector machines: evi-\ndence and predictive class proba-\nbilities.Machine Learning 46, 21\u2013\n52.\nSontag, D., A. Globerson, and\nT. Jaakkola (2011). Introduction to\ndual decomposition for inference.In S. Sra, S. Nowozin, and S. J.Wright (Eds.), OptimizationforMa-\nchineLearning. MIT Press.\nSorenson, H. and D. Alspach (1971).\nRecursive Bayesian estimation us-ing Gaussian sums. Automatica 7,\n465\u00e2\u02d8A\u00b8S 479.\nSoussen, C., J. Iier, D. Brie, and\nJ. Duan (2010). From Bernoulli-Gaussian deconvolution to sparsesignal restoration. Technical report,Centre de Recherche en Automa-tique de Nancy.\nSpaan, M. and N. Vlassis (2005).\nPerseus: Randomized Point-basedValue Iteration for POMDPs. J.ofAI\nResearch 24, 195\u2013220.\nSpall, J. (2003). IntroductiontoStochas-\ntic Search and Optimization: Es-\ntimation, Simulation, and Control.\nWiley.\nSpeed, T. (2011, December). A cor-\nrelation for the 21st century. Sci-\nence 334, 152\u20131503.\nSpeed, T. and H. Kiiveri (1986). Gaus-\nsian Markov distributions over \ufb01-nite graphs. Annals of Statis-\ntics 14(1), 138\u2013150.\nSpiegelhalter, D. J. and S. L. Lauritzen\n(1990). Sequential updating of con-ditional probabilities on directed\ngraphical structures. Networks 20.\nSpirtes, P., C. Glymour, and\nR. Scheines (2000). Causa-\ntion, Prediction, and Search. MIT\nPress. 2nd edition.\nSrebro, N.(2001). MaximumLikelihood\nBounded Tree-Width Markov Net-works. InUAI.\nSrebro, N. and T. Jaakkola (2003).\nWeighted low-rank approxima-\ntions. InIntl. Conf. on Machine\nLearning.\nSteinbach, M., G. Karypis, and V. Ku-\nmar (2000). A comparison of doc-\nument clustering techniques. In\nKDDWorkshoponTextMining.Stephens, M. (2000). Dealing with\nlabel-switching in mixture mod-els.J.RoyalStatisticalSociety,Series\nB6 2, 795\u2013809.\nStern, D., R. Herbrich, and T. Grae-\npel (2009). Matchbox: Large Scale\nBayesian Recommendations. In\nProc. 18th. Intl. World Wide Web\nConference.\nSteyvers, M. and T. Griffiths (2007).\nProbabilistic topic models. InT. Landauer, D. McNamara, S. Den-nis, and W. Kintsch (Eds.), Latent\nSemanticAnalysis: ARoadtoMean-ing. Laurence Erlbaum.\nStigler, S. (1986). The history of statis-\ntics. Harvard University press.\nStolc\nke, A. and S. M. Omohundro\n(1992). Hidden Markov Model In-duction by Bayesian Model Merg-ing. InNIPS-5.\nStoyanov, V., A. Ropson, and J. Eis-\nner (2011). Empirical risk minimiza-tion of graphical model parametersgiven approximate inference, de-coding, and model structure. InAI/Statistics.\nSudderth, E. (2006). Graphical Models\nfor Visual Object Recognition and\nTracking. Ph.D. thesis, MIT.\nSudderth, E. and W. Freeman (2008,\nMarch). Signal and Image Process-\ning with Belief Propagation. IEEE\nSignalProcessingMagazine .\nSudderth, E., A. Ihler, W. Freeman, and\nA. Willsky (2003). NonparametricBelief Propagation. In CVPR.\nSudderth, E., A. Ihler, M. Isard,\nW. Freeman, and A. Willsky (2010).Nonparametric Belief Propagation.\nComm.oftheACM 53 (10).\nSudderth, E. and M. Jordan (2008).\nShared Segmentation of Natural\nScenes Using Dependent Pitman-\nYor Processes. In NIPS.\nSudderth, E., M. Wainwright, and\nA. Willsky (2008). Loop series andbethe variational bounds for attrac-tive graphical models. In NIPS.\nSun, J., N. Zheng, and H. Shum\n(2003). Stereo matching using be-lief propagation. IEEETrans.onPat-\ntern Analysis and Machine Intelli-gence 25(7), 787\u2013800.Sun, L., S. Ji, S. Yu, and J. Ye (2009). On\nthe equivalence between canonicalcorrelation analysis and orthonor-malized partial least squares. In\nIntl.JointConf.onAI.\nSunehag, P., J. Trumpf, S. V. N. Vish-\nwanathan, and N. N. Schraudolph\n(2009). Variable Metric StochasticApproximation Theory. In AI/Statis-\ntics, pp. 560\u2013566.\nSutton, C. and A. McCallum (2007).\nImproved Dynamic Schedules forBelief Propagation. In UAI.\nSutton, R. and A. Barto (1998). Rein-\nforcmentLearning: AnIntroduction.MIT Press.\nSwendsen, R. and J.-S. Wang (1987).\nNonuniversal critical dynamics in\nMonte Carlo simulations. Physical\nReviewLetters 58 , 86\u201388.\nSwersky, K., B. Chen, B. Marlin, and\nN. de Freitas (2010). A Tuto-\nrial on Stochastic ApproximationAlgorithms for Training RestrictedBoltzmann Machines and Deep Be-\nlief Nets. In InformationTheoryand\nApplications(ITA)Workshop.\nSzeliski, R. (2010). Computer Vi-\nsion: Algorithms and Applications.\nSpringer.\nSzeliski, R., R. Zabih, D. Scharstein,\nO. Veksler, V. Kolmogorov, A. Agar-wala, M. Tappen, and C. Rother(2008). A Comparative Studyof Energy Minimization Methodsfor Markov Random Fields withSmoothness-Based\nPriors.IEEE\nTrans. on Pattern Analysis and Ma-\nchineIntelligence 30 (6), 1068\u20131080.\nSzepesvari, C. (2010). Algorithms for\nReinforcement Learning . Morgan\nClaypool.\nTaleb, N. (2007). The Black Swan:\nThe Impact of the Highly Improba-ble. Random House.\nTalhouk, A., K. Murphy, and A. Doucet\n(2011). Efficient Bayesian Inference\nfor Multivariate Probit Models withSparse Inverse Correlation Matri-\nces.J.Comp.Graph.Statist..\nTanner, M. (1996). Tools for statistical\ninference. Springer.\nTanner, M. and W. Wong (1987). The\ncalculation of posterior distribu-tions by data augmentation. J. of\nthe Am. Stat. Assoc. 82 (398), 528\u2013\n540.", "1071": "BIBLIOGRAPHY 1041\nTarlow, D., I. Givoni, and R. Zemel\n(2010). Hop-map: efficient message\npassing with high order potentials.InAI/Statistics.\nTaskar, B., C. Guestrin, and D. Koller\n(2003). Max-margin markov net-works. InNIPS.\nTaskar, B., D. Klein, M. Collins,\nD. Koller, and C. Manning (2004).Max-margin parsing. In Proc. Em-\npirical Methods in Natural Lan-guageProcessing.\nTeh, Y. W. (2006). A hierarchical\nBayesian language model based on\nPitman-Yor processes. In Proc. of\nthe Assoc. for Computational Lin-\nguistics, pp. 985=992.\nTeh, Y.-W., M. Jordan, M. Beal, and\nD. Blei (2006). Hierarchical Dirich-let processes. J.oftheAm.Stat.As-\nsoc. 101(476), 1566\u20131581.\nTenenbaum, J. (1999). A Bayesian\nframework for concept learning.Ph.D. thesis, MIT.\nTenenbaum, J. B. and F. Xu (2000).\nWord learning as bayesian infer-\nence. InProc.22ndAnnualConf.of\ntheCognitiveScienceSociety.\nTheocharous, G., K. Murphy, and\nL. Kaelbling (2004). Representing\nhierarchical POMDPs as DBNs formulti-scale robot localization. InIEEEIntl.Conf.onRoboticsandAu-tomation.\nThiesson, B., C. Meek, D. Chickering,\nand D. Heckerman (1998). Learningmixtures of DAG models. In UAI.\nThomas, A. and P. Green (2009).\nEnumerating the decomposableneighbours of a decomposablegraph under a simple perturbation\nscheme.Comp. Statistics and Data\nAnalysis 53, 1232\u20131238.\nThrun, S., W. Burgard, and D. Fox\n(2006).Probabilistic Robotics. MIT\nPress.\nThrun, S., M. Montemerlo, D. Koller,\nB. Wegbreit, J. Nieto, and E. Nebot\n(2004). Fastslam: An efficient so-lution to the simultaneous local-ization and mapping problem with\nunknown data association. J. of\nMachineLearningResearch 2004.\nThrun, S. and L. Pratt (Eds.) (1997).\nLearningtolearn. Kluwer.Tibshirani, R. (1996). Regression\nshrinkage and selection via thelasso.J. Royal. Statist. Soc B 58 (1),\n267\u2013288.\nTibshirani, R., G. Walther, and\nT. Hastie (2001). Estimating the\nnumber of clusters in a dataset viathe gap statistic. J.ofRoyalStat.Soc.\nSeriesB 32 (2), 411\u2013423.\nTieleman, T. (2008). Training re-\nstricted Boltzmann machines us-ing approximations to the likeli-\nhood gradient. In Proceedings of\nthe25thinternationalconferenceon\nMachine learning, pp. 1064\u20131071.\nACM New York, NY, USA.\nTing, J., A. D\u2019Souza, S. Vijayakumar,\nand S. Schaal (2010). Efficientlearning and feature selection inhigh-dimensional regression. Neu-\nral\nComputation 22 (4), 831\u2013886.\nTipping, M. (1998). Probabilistic visual-\nization of high-dimensional binarydata. InNIPS.\nTipping, M. (2001). Sparse bayesian\nlearning and the relevance vector\nmachine.J. of Machine Learning\nResearch 1, 211\u2013244.\nTipping, M. and C. Bishop (1999).\nProbabilistic principal componentanalysis.J. of Royal Stat. Soc. Series\nB2 1(3), 611\u2013622.\nTipping, M. and A. Faul (2003). Fast\nmarginal likelihood maximisation\nfor sparse bayesian models. In AI/S-\ntats.\nTishby, N., F. Pereira, and W. Biale\n(1999). The information bottle-neck method. In The 37th an-\nnualAllertonConf.onCommunica-\ntion, Control, and Computing, pp.\n368\u00e2\u02d8A\u00b8S377.\nTomas, M., D. Anoop, K. Stefan,\nB. Lukas, and C. Jan (2011). Empir-\nical evaluation and combination ofadvanced language modeling tech-\nniques. InProc. 12th Annual Conf.\nof the Intl. Speech Communication\nAssociation(INTERSPEECH).\nTorralba, A., R. Fergus, and Y. Weiss\n(2008). Small codes and large im-age databases for recognition. InCVPR.\nTrain, K. (2009). Discrete choice meth-\nods with simulation. CambridgeUniversity Press. Second edition.Tseng, P. (2008). On Accelerated Proxi-\nmal Gradient Methods for Convex-Concave Optimization. Technicalreport, U. Washington.\nTsochantaridis, I., T. Joachims, T. Hof-\nmann, and Y. Altun (2005, Septem-ber). Large margin methodsfor structured and interdependent\noutput variables. J. of Machine\nLearningResearch 6, 1453\u20131484.\nTu, Z. and S. Zhu (2002). Image Seg-\nmentation by Data-Driven Markov\nChain Monte Carlo. IEEETrans.on\nPatternAnalysisandMachineIntel-\nligence 24 (5), 657\u2013673.\nTurian, J., L. Ratinov, and Y. Ben-\ngio (2010). Word representations:\na simple and general method forsemi-supervised learning. In Proc.\nACL.\nTurlach, B., W. Venables, and S. Wright\n(2005). Simultaneous variable se-lection.Technometrics 47 (3), 349\u2013\n363.\nTurner, R., P. Berkes, M. Sahani, and\nD. Mackay (2008). Counterexam-ples to variational free energy com-pactness folk theorems. Technicalreport, U. Cambridge.\nUeda, N. and R. Nakano (1998). Deter-\nministic annealing EM algorithm.\nNeuralNetworks 11, 271\u2013282.\nUsunier, N., D. Buffoni, and P. Galli-\nnari (2009). Ranking with ordered\nweighted pairwise classi\ufb01cation.\nVaithyanathan, S. and B. Dom (1999).\nModel selection in unsupervisedlearning with applications to doc-\nument clustering. In Intl. Conf. on\nMachineLearning.\nvan\nder Merwe, R., A. Doucet,\nN. de Freitas, and E. Wan (2000).\nThe unscented particle \ufb01lter. In\nNIPS-13.\nvan Dyk, D. and X.-L. Meng (2001).\nThe Art of Data Augmentation.\nJ. Computational and Graphical\nStatistics 10 (1), 1\u201350.\nVandenberghe, L. (2006). Applied nu-\nmerical computing: Lecture notes.\nVandenberghe, L. (2011). Ee236c - op-\ntimization methods for large-scalesystems.\nVanhatalo, J. (2010). Speeding up the\ninference in Gaussian process mod-\nels. Ph.D. thesis, Helsinki Univ.\nTechnology.", "1072": "1042 BIBLIOGRAPHY\nVanhatalo, J., V. Pietil\u00c3d\u2019inen, and\nA. Vehtari (2010). Approximate in-\nference for disease mapping withsparse gaussian processes. Statis-\nticsinMedicine 29 (15), 1580\u20131607.\nVapnik, V. (1998). Statistical Learning\nTheory. Wiley.\nVapnik, V., S. Golowich, and A. Smola\n(1997). Support vector method forfunction approximation, regressionestimation, and signal processing.InNIPS.\nVarian, H. (2011). Structural time series\nin R: a Tutorial. Technical report,Google.\nVerma, T. and J. Pearl (1990). Equiva-\nlence and synthesis of causal mod-els. InUAI.\nViinikanoja, J., A. Klami, and S. Kaski\n(2010). VariationalBayesianMixtureof Robust CCA Models. In Proc.Eu-\nropeanConf.onMachineLearning.\nVincent, P. (2011). A Connection be-\ntween Score Matching and Denois-\ning Autoencoders. Neural Compu-\ntation 23(7), 1661\u20131674.\nVincent, P., H. Larochelle, I. La-\njoie, Y. Bengio, and P.-A. Manzagol\n(2010). Stacked Denoising Autoen-coders: Learning Useful Represen-tations in a Deep Network with a\nLocal Denoising Criterion. J.ofMa-\nchine Learning Research 11, 3371\u2013\n3408.\nVinh, N., J. Epps, and J. Bailey (2009).\nInformation theoretic measures forclusterings comparison: Is a cor-rection for chance necessary? In\nIntl.Conf.onMachineLearning.\nVinyals, M., J. Cerquides, J. Rodriguez-\nAguilar, and A. Farinelli (2010).\nWorst-case bounds on the quality\nof max-product \ufb01xed-points. InNIPS.\nViola, P. and M. Jones (2001). Rapid\nobject detection using a boostedcascade of simple classi\ufb01ers. InCVPR.\nVirtanen, S. (2010). Bayesian expo-\nnential family projections. Master\u2019sthesis, Aalto University.\nVishwanathan, S. V. N. and A. Smola\n(2003). Fast kernels for string andtree matching. In NIPS.Viterbi, A. (1967). Error bounds\nfor convolutional codes and anasymptotically optimum decoding\nalgorithm.IEEE Trans. on Informa-\ntionTheory 13 (2), 260\u00e2\u02d8A\u00b8S269.\nvon Luxburg, U. (2007). A tutorial on\nspectral clustering. Statistics and\nComputing 17 (4), 395\u2013416.\nWagenmakers, E.-J., R. Wetzels,\nD. Borsboom, and H. van der\nMaas (2011). Why PsychologistsMust Change the Way They Ana-lyze Their Data: The Case of Psi.\nJournal of Personality and Social\nPsychology .\nWagner, D. and F. Wagner (1993). Be-\ntween min cut and graph bisec-\ntion. InProc. 18th Intl. Symp. on\nMath.Found.ofComp.Sci., pp.744\u2013\n750.\nWain\nwright, M., T. Jaakkola, and\nA. Willsky (2001). Tree-based repa-rameterization for approximate es-\ntimation on loopy graphs. In NIPS-\n14.\nWainwright, M., T. Jaakkola, and\nA. Willsky (2005). A new class\nof upper bounds on the log parti-\ntion function. IEEETrans.Info.The-\nory 51(7), 2313\u20132335.\nWainwright, M., P. Ravikumar, and\nJ. Lafferty (2006). Inferring graph-\nical model structure using /lscript\u22121-\nregularized pseudo-likelihood. InNIPS.\nWainwright, M. J., T. S. Jaakkola, and\nA. S. Willsky (2003). Tree-basedreparameterization framework foranalysis of sum-product and re-lated algorithms. IEEETrans.onIn-\nformationTheory 49 (5), 1120\u20131146.\nWainwright, M. J. and M. I. Jordan\n(2008a). Graphical models, expo-nential families, and variational in-ference.FoundationsandTrendsin\nMachineLearning 1\u20132, 1\u2013305.\nWainwright, M. J. and M. I. Jordan\n(2008b). Graphical models, expo-nential families, and variational in-ference.FoundationsandTrendsin\nMachineLearning 1\u20132, 1\u2013305.\nWallach, H., I. Murray, R. Salakhutdi-\nnov, and D. Mimno (2009). Evalua-tion methods for topic models. In\nIntl.Conf.onMachineLearning.\nWan, E. A. and R. V. der Merwe (2001).\nThe Unscented Kalman Filter. In\nS. Haykin (Ed.), Kalman Filtering\nandNeuralNetworks. Wiley.Wand, M. (2009). Semiparametric\nregression and graphical models.Aus t .N.Z.J.S t a t .51 (1), 9\u201341.\nWand, M. P., J. T. Ormerod, S. A.\nPadoan, and R. Fruhrwirth (2011).Mean Field Variational Bayes forElaborate Distributions. Bayesian\nAnalysis 6 (4), 847 \u2013 900.\nWang, C. (2007). Variational Bayesian\nApproach to Canonical Correlation\nAnalysis.IEEETrans.onNeuralNet-\nworks 18(3), 905\u2013910.\nWasserman, L. (2004). All of statistics.\nA concise course in statistical infer-\nence. Springer.\nWei, G. and M. Tanner (1990). A Monte\nCarloimplementationoftheEMal-gorithm and the poor man\u2019s data\naugmentation algorithms. J. of the\nAm.Stat.Assoc. 85 (411), 699\u2013704.\nWeinberger, K., A. Dasgupta, J. Atten-\nberg, J. Langford, and A. Smola\n(2009). Feature hashing for large\nscale multitask learning. In Intl.\nConf.onMachineLearning.\nWeiss, D., B. Sapp, and B. Taskar\n(2010). Sidestepping intractable in-\nference with structured ensemblecascades. In NIPS.\nWeiss, Y. (2000). Correctness of local\nprobability propagation in graph-\nical models with loops. Neural\nComputation 12 , 1\u201341.\nWeiss,\nY. (2001). Comparing the mean\n\ufb01eld method and belief propaga-\ntion for approximate inference in\nMRFs. In Saad and Opper (Eds.),\nAdvancedMeanFieldMethods. MIT\nPress.\nWeiss, Y. and W. T. Freeman (1999).\nCorrectness of belief propagationin Gaussian graphical models of ar-bitrary topology. In NIPS-12.\nWeiss, Y. and W. T. Freeman (2001a).\nCorrectness of belief propagationin Gaussian graphical models of ar-\nbitrary topology. Neural Computa-\ntion 13(10), 2173\u20132200.\nWeiss, Y. and W. T. Freeman (2001b).\nOn the optimality of solutions of\nthe max-product belief propaga-tion algorithm in arbitrary graphs.\nIEEE Trans. Information Theory,\nSpecial Issue on Codes on GraphsandIterativeAlgorithms 47 (2), 723\u2013\n735.\nWeiss, Y., A. Torralba, and R. Fergus\n(2008). Spectral hashing. In NIPS.", "1073": "BIBLIOGRAPHY 1043\nWelling, M., C. Chemudugunta, and\nN. Sutter (2008). Deterministic la-\ntent variable models and their pit-\nfalls. InIntl.Conf.onDataMining.\nWelling, M., T. Minka, and Y. W. Teh\n(2005). Structured region graphs:\nMorphing EP into GBP. In UAI.\nWelling, M., M. Rosen-Zvi, and G. Hin-\nton (2004). Exponential family har-moniums with an application toinformation retrieval. In NIPS-14.\nWelling, M. and C. Sutton (2005).\nLearning in Markov random \ufb01eldswith contrastive free energies. In\nTenth International Workshop on\nArti\ufb01cial Intelligence and Statistics\n(AISTATS).\nWelling, M. and Y.-W. Teh (2001). Belief\noptimization for binary networks:a stable alternative to loopy beliefpropagation. In UAI.\nWerbos, P. (1974). Beyond regression:\nNewToolsforPredictionandAnaly-\nsisintheBehavioralSciences. Ph.D.\nthesis, Harvard.\nWest, M. (1987). On scale mix-\ntures of normal distributions.Biometrika 74, 646\u2013648.\nWest, M. (2003). Bayesian Factor Re-\ngression Models in the \"Large p,Small n\" Paradigm. BayesianStatis-\ntics7.\nWest, M. and J. Harrison (1997).\nBayesian forecasting and dynamic\nmodels. Springer.\nWeston, J., S. Bengio, and N. Usunier\n(2010). Large Scale Image Annota-\ntion: Learning to Rank with JointWord-Image Embeddings. In Proc.\nEuropean Conf. on Machine Learn-ing.\nWeston, J., F. Ratle, and R. Collobert\n(2008). Deep Learning via Semi-\nSupervised Embedding. In Intl.\nConf.onMachineLearning.\nWeston, J. and C. Watkins (1999).\nMulti-lcass support vector ma-\nchines. InESANN.\nWiering, M. and M. van Otterlo\n(Eds.) (2012). Reinforcement learn-\ning: State-of-the-art. Springer.\nWilkinson, D. and S. Yeung (2002).\nConditional simulation from highlystructured gaussian systems withapplication to blocking-mcmc forthe bayesian analysis of very large\nlinear models. Statistics and Com-\nputing 12, 287\u2013300.Williams, C. (1998). Computation with\nin\ufb01nite networks. NeuralComputa-\ntion 10(5), 1203\u20131216.\nWilliams, C. (2000). A MCMC ap-\nproach to Hierarchical Mixture\nModelling . In S. A. Solla, T. K.Leen, and K.-R. M\u00fcller (Eds.), NIPS.\nMIT Press.\nWilliams, C. (2002). On a Connec-\ntion between Kernel PCA and Met-ric Multidimensional Scaling. Ma-\nchineLearningJ. 46 (1).\nWilliams, O. and A. Fitzgibbon (2006).\nGaussian process implicit surfaces.InGaussianprocessesinpractice.\nWilliamson, S. and Z. Ghahramani\n(2008). Pr\nobabilistic models for\ndata combination in recommendersystems. In NIPS Workshop on\nLearningfromMultipleSources.\nWinn, J. and C. Bishop (2005). Varia-\ntional message passing. J. of Ma-\nchine Learning Research 6, 661\u2013694.\nWipf, D. and S. Nagarajan (2007). A\nnew view of automatic relevancydetermination. In NIPS.\nWipf, D. and S. Nagarajan (2010, April).\nIterative Reweighted /lscript\u22121and/lscript\u22122\nMethods for Finding Sparse Solu-\ntions.J.ofSelectedTopicsinSignal\nProcessing (Special Issue on Com-\npressiveSensing) 4 (2).\nWipf, D., B. Rao, and S. Nagarajan\n(2010). Latent variable bayesianmodels for promoting sparsity.\nIEEE Transactions on Information\nTheory.\nWitten, D., R. Tibshirani, and T. Hastie\n(2009). A penalized matrix de-\ncomposition, with applications tosparse principal components andcanonical correlation analysis. Bio-\nstatistics 10 (3), 515\u2013534.\nWolpert, D. (1992). Stacked generaliza-\ntion.NeuralNetworks5 (2), 241\u2013259.\nWolpert, D. (1996). The lack of a priori\ndistinctions between learning algo-\nrithms.Neural Computation 8 (7),\n1341\u20131390.\nWong, F., C. Carter, and R. Kohn\n(2003). Efficient estimation of\ncovariance selection models.Biometrika 90 (4), 809\u2013830.Wood, F., C. Archambeau, J. Gasthaus,\nL. James, and Y. W. Teh (2009). Astochastic memoizer for sequence\ndata. InIntl. Conf. on Machine\nLearning.\nWright, S., R. Nowak, and\nM. Figueiredo (2009). Sparse\nreconstruction by separable ap-\nproximation. IEEE Trans. on Signal\nProcessing 57 (7), 2479\u20132493.\nWu, T. T. and K. Lange (2008). Coordi-\nnate descent algorithms for lassopenalized regression. Ann. Appl.\nStat 2(1), 224\u2013244.\nWu, Y., H. Tjelmeland, and M. West\n(2007). Bayesian CART: Prior struc-\nture and MCMC computations. J.\nof Computational and GraphicalStatistics 16 (1), 44\u201366.\nXu, F. and J. Tenenbaum (2007). Word\nlearning as Bayesian inference. Psy-\nchologicalReview 114 (2).\nXu, Z., V. Tresp, A. Rettinger, and\nK. Kersting (2008). Social net-work mining with nonparametric\nrelational models. In ACM Work-\nshoponSocialNetworkMiningand\nAnalysis(SNA-KDD2008).\nXu, Z., V. Tresp, K. Yu, and H.-P.\nKriegel\n(2006). In\ufb01nite hidden rela-\ntional models. In UAI.\nXu, Z., V. Tresp, S. Yu, K. Yu, and H.-P.\nKriegel (2007). Fast inference in in-\n\ufb01nite hidden relational models. InWorkshop on Mining and Learning\nwithGraphs.\nXue, Y., X. Liao, L. Carin, and B. Krish-\nnapuram (2007). Multi-task learn-\ning for classi\ufb01cation with dirichlet\nprocess priors. J.ofMachineLearn-\ningResearch 8 , 2007.\nYadollahpour, P., D. Batra, and\nG.Shakhnarovich(2011). DiverseM-\nbest Solutions in MRFs. In NIPS\nworkshop on Disrete Optimization\ninMachineLearning.\nYan, D., L. Huang, and M. I. Jordan\n(2009). Fast approximate spectral\nclustering. In 15th ACM Conf. on\nKnowledgeDiscoveryandDataMin-\ning.\nYang, A., A. Ganesh, S. Sastry, and\nY. Ma (2010, Feb). Fast l1-minimization algorithms and anapplication in robust face recog-nition: A review. Technical Re-port UCB/EECS-2010-13, EECS De-partment, University of California,Berkeley.", "1074": "1044 BIBLIOGRAPHY\nYang, C., R. Duraiswami, and L. David\n(2005). Efficient kernel machines\nusing the improved fast Gausstransform. In NIPS.\nYang, S., B. Long, A. Smola, H. Zha,\nand Z. Zheng (2011). Collaborativecompetitive \ufb01ltering: learning rec-ommender using context of user\nchoice. InProc. Annual Intl. ACM\nSIGIRConference.\nYanover, C., O. Schueler-Furman, and\nY. Weiss (2007). Minimizing and\nLearning Energy Functions for\nSide-Chain Prediction. In Recomb.\nYaun, G.-X., K.-W. Chang, C.-J. Hsieh,\nand C.-J. Lin (2010). A Comparison\nof Optimization Methods and Soft-ware for Large-scale L1-regularized\nLinear Classi\ufb01cation. J. of Machine\nLearningResearch 11, 3183\u20133234.\nYedidia, J., W. T. Freeman, and\nY. Weiss (2001). Understanding be-\nlief propagation and its generaliza-\ntions. InIntl.JointConf.onAI.\nYoshida, R. and M. West (2010).\nBayesian learning in sparse graphi-\ncal factor models via annealed en-tropy.J. of Machine Learning Re-\nsearch 11, 1771\u20131798.\nYounes, L. (1989). Parameter estima-\ntion for imperfectly observed Gibb-\nsian \ufb01elds.Probab.TheoryandRe-\nlatedFields 82, 625\u2013645.\nYu, C. and T. Joachims (2009). Learn-\ning structural SVMs with latentvariables. In Intl.Conf.onMachine\nLearning.\nYu, S., K. Yu, V. Tresp, K. H-P., and\nM. Wu (2006). Supervised proba-\nbilistic principal component anal-\nysis. InProc. of the Int\u2019l Conf. on\nKnowledgeDiscoveryandDataMin-\ning.\nYu, S.-Z. and H. Kobayashi (2006).\nPractical implementation of an ef-\ufb01cient forward-backward algorithmfor an explicit-duration hiddenMarkov model. IEEE Trans. on Sig-\nnalProcessing 54 (5), 1947\u2013 1951.\nYuan, M. and Y. Lin (2006). Model\nselection and estimation in re-gression with grouped variables.\nJ. Royal Statistical Society, Series\nB6 8(1), 49\u201367.\nYuan, M. and Y. Lin (2007).\nModel selection and estimationin the gaussian graphical model.\nBiometrika 94 (1), 19\u201335.Yuille, A. (2001). CCCP algorithms\nto minimze the Bethe and Kikuchi\nfree energies: convergent alterna-\ntives to belief propagation. Neural\nComputation 14, 1691\u20131722.\nYuille, A. and A. Rangarajan (2003).\nThe concave-convex procedure.NeuralComputation 15, 915.\nYuille, A. and S. Zheng (2009). Com-\npositional noisy-logical learning. InIntl.Conf.onMachineLearning.\nYuille, A. L. and X. He (2011). Proba-\nbilistic models of vision and max-\nmargin methods. Frontiers of\nElectrical and Electronic Engineer-ing 7(1).\nZellner, A. (1986). On assessing prior\ndistributions and bayesian regres-sion analysis with g-prior distri-\nbutions. In Ba\n yesian inference\nand decision techniques, Studies\nof Bayesian and Econometrics andStatisticsvolume6. North Holland.\nZhai, C. and J. Lafferty (2004). A\nstudy of smoothing methods for\nlanguage models applied to infor-mation retrieval. ACMTrans.onIn-\nformationSystems 22 (2), 179\u2013214.\nZhang, N. (2004). Hierarchical latnet\nclass models for cluster analysis. J.\nofMachineLearningResearch , 301\u2013\n308.\nZhang, N. and D. Poole (1996). Ex-\nploiting causal independence in\nBayesian network inference. J. of\nAIResearch , 301\u2013328.\nZhang, T. (2008). Adaptive Forward-\nBackward Greedy Algorithm for\nSparse Learning with Linear Mod-els. InNIPS.\nZhang, X., T. Graepel, and R. Herbrich\n(2010). Bayesian Online Learningfor Multi-label and Multi-variatePerformance Measures. In AI/Statis-\ntics.\nZhao, J.-H. and P. L. H. Yu (2008,\nNovember). Fast ML Estimation forthe Mixture of Factor Analyzers viaan ECM Algorithm. IEEE.Trans.on\nNeuralNetworks 19 (11).\nZhao, P., G. Rocha, and B. Yu (2005).\nGrouped and Hierarchical ModelSelection through Composite Abso-lute Penalties. Technical report, UCBerkeley.Zhao, P. and B. Yu (2007). Stagewise\nLasso.J. of Machine Learning Re-\nsearch 8, 2701\u20132726.\nZhou, H., D. Karakos, S. Khudanpur,\nA. Andreou, and C. Priebe (2009).On Projections of Gaussian Dis-tributions using Maximum Likeli-\nhood Criteria. In Proc.oftheWork-\nshoponInformationTheoryandits\nApplications.\nZhou, M., H. Chen, J. Paisley, L. Ren,\nG. Sapiro, and L. Carin (2009).\nNon-parametric Bayesian Dictio-nary Learning for Sparse ImageRepresentations. In NIPS.\nZhou, X. and X. Liu (2008). The\nEM algorithm for the extended \ufb01-nite mixture of the factor analyz-\ners model.ComputationalStatistics\nandDataAnalysis 52, 3939\u20133953.\nZhu, C. S., N. Y. Wu, and D. Mum-\nford (1997, November). Minimax\nentropy principle and its applica-\ntion to texture modeling. Neural\nComputation 9 (8).\nZhu, J. and E. Xing (2010). Conditional\ntopic random \ufb01elds. In Intl. Conf.\nonMachineLearning.\nZhu, L., Y. Chen, A.Yuille, and W. Free-\nman (2010). Latent hierarchical\nstructure learning for object detec-tion. InCVPR.\nZhu, M. and A. Ghodsi (2006). Au-\ntomatic dimensionality selectionfrom the scree plot via the use of\npro\ufb01le likelihood. Computational\nStatistics & Data Analysis 51, 918\u2013\n930.\nZhu, M.andA.Lu(2004). Thecounter-\nintuitive non-informative prior for\nthebe\nrnoulli family. J.StatisticsEd-\nucation.\nZinkevich, M. (2003). Online con-\nvex programming and generalized\nin\ufb01nitesimal gradient ascent. In\nIntl.Conf.onMachineLearning, pp.\n928\u00e2\u02d8A\u00b8S936.\nZobay, O. (2009). Mean \ufb01eld inference\nfor the Dirichlet process mixture\nmodel.Electronic J. of Statistics 3,\n507\u2013545.\nZoeter, O. (2007). Bayesian generalized\nlinear models in a terabyte world.InProc. 5th International Sympo-\nsium on image and Signal Process-\ningandAnalysis.", "1075": "BIBLIOGRAPHY 1045\nZou, H. (2006). The adaptive Lasso\nand its oracle properties. J. of the\nAm.Stat.Assoc., 1418\u20131429.\nZou, H. and T. Hastie (2005). Regular-\nization and variable selection via\nthe elastic net. J. of Royal Stat. Soc.\nSeriesB 67 (2), 301\u2013320.\nZou, H., T. Hastie, and R. Tibshirani\n(2006). Sparse principal compo-nent analysis. J. of Computational\nand Graphical Statistics 15 (2), 262\u2013\n286.\nZou, H., T. Hastie, and R. Tibshirani\n(2007). On the \"Degrees of Free-\ndom\" of the Lasso. AnnalsofStatis-\ntics 35(5), 2173\u20132192.\nZou, H. and R. Li (2008). One-\nstep sparse estimates in noncon-cave penalized likelihood models.AnnalsofStatistics36 (4), 1509\u20131533.\nZweig, G. and M. Padmanabhan\n(2000). Exact alpha-beta computa-\ntion in logarithmic space with ap-plication to map word graph con-\nstruction. In Proc.Intl.Conf.Spoken\nLang.", "1076": "", "1077": "Index to code\nagglomDemo, 894\namazonSellerDemo, 155arsDemo, 819arsEnvelope, 819\nbayesChangeOfVar, 151\nbayesLinRegDemo2d, 233bayesTtestDemo, 138beliefPropagation, 768bernoulliEntropyFig, 57besselk, 477betaBinomPostPredDemo, 79betaCredibleInt, 153betaHPD, 153, 154betaPlotDemo, 43biasVarModelComplexity3, 204bimodalDemo, 150binaryFaDemoTipping, 403binomDistPlot, 35binomialBetaPosteriorDemo, 75bleiLDAperplexityPlot, 955bolassoDemo, 440boostingDemo, 555, 558bootstrapDemoBer, 192\ncancerHighDimClassifDemo, 110\ncancerRatesEb, 172casinoDemo, 606, 607centralLimitDemo, 52changeOfVarsDemo1d, 53chowliuTreeDemo, 913coinsModelSelDemo, 164contoursSSEdemo, 219convexFnHand, 222curseDimensionality, 18\ndemard, 580\ndepnetFit, 909dirichlet3dPlot, 48dirichletHistogramDemo, 48discreteProbDistFig, 28discrimAnalysisDboundariesDemo, 103, 105discrimAnalysisFit, 106discrimAnalysisHeightWeightDemo, 145discrimAnalysisPredict, 106dpmGauss2dDemo, 888dpmSampleDemo, 881dt\ufb01t, 545dtreeDemoIris, 549, 550\nelasticDistortionsDemo, 567\nemLogLikelihoodMax, 365\nfaBiplotDemo, 383\n\ufb01sherDiscrimVowelDemo, 274\ufb01sheririsDemo, 6\ufb01sherLDAdemo, 272fmGibbs, 843\ngammaPlotDemo, 41, 150\ngammaRainfallDemo, 41gampdf, 41gaussCondition2Ddemo2, 112gaussHeightWeight, 102gaussImputationDemo, 115, 375gaussInferParamsMean1d, 121gaussInferParamsMean2d, 123gaussInterpDemo, 113gaussInterpNoisyDemo, 125gaussMissingFitEm, 374gaussMissingFitGibbs, 840gaussPlot2d, 142gaussPlot2Ddemo, 47gaussPlotDemo, 19gaussSeqUpdateSigma1D, 131generativeVsDiscrim, 269geomRidge, 229ggmFitDemo, 939ggmFitHtf, 939ggmFitMinfunc, 939ggmLassoDemo, 13, 940ggmLassoHtf, 940gibbsDemoIsing, 670, 873gibbsGaussDemo, 848giniDemo, 548gpcDemo2d, 529gpnnDemo, 536gprDemoArd, 520gprDemoChangeHparams, 519gprDemoMarglik, 522gprDemoNoiseFree, 517gpSpatialDemoLaplace, 532groupLassoDemo, 451\nhclustYeastDemo, 894, 896\nhingeLossPlot, 211, 556hmmFilter, 609hmmFwdBack, 611hmmLillypadDemo, 604hmmSelfLoopDist, 623hop\ufb01eldDemo, 670huberLossDemo, 223, 497\nicaBasisDemo, 471\nicaDemo, 408icaDemoUniform, 409IPFdemo2x2, 683isingImageDenoiseDemo, 739, 839\nkalmanFilter, 641\nkalmanTrackingDemo, 632kernelBinaryClassifDemo, 489kernelRegrDemo, 490, 491kernelRegressionDemo, 510KLfwdReverseMixGauss, 734KLpqGauss, 734kmeansHeightWeight, 10kmeansModelSel1d, 371kmeansYeastDemo, 341knnClassifyDemo, 17, 23\u201325knnVor\nonoi, 16\nkpcaDemo2, 495kpcaScholkopf, 493\nlassoPathProstate, 437, 438\nLassoShooting, 441leastSquaresProjection, 221linregAllsubsetsGraycodeDemo, 423linregBayesCaterpillar, 237, 238linregCensoredSchmeeHahnDemo, 379", "1078": "1048 INDEXES\nlinregDemo1, 241\nlinregEbModelSelVsN, 158, 159, 749linregFitL1Test, 447linregOnlineDemoKalman, 636linregPolyLassoDemo, 436linregPolyVsDegree, 9, 20, 436linregPolyVsN, 231linregPolyVsRegDemo, 208, 225, 226, 239linregPostPredDemo, 235linregRbfDemo, 487linregRobustDemoCombined, 223linregWedgeDemo2, 19LMSdemo, 265logregFit, 254logregLaplaceGirolamiDemo, 257, 258logregMultinomKernelDemo, 269logregSATdemo, 21logregSATdemoBayes, 259logregSatMhDemo, 852logregXorDemo, 486logsumexp, 86lossFunctionFig, 179lsiCode, 419\nmarsDemo, 554\nmcAccuracyDemo, 55mcEstimatePi, 54mcmcGmmDemo, 851, 860, 861mcQuantileDemo, 153mcStatDist, 598miMixedDemo, 59mixBerMnistEM, 341mixBetaDemo, 170mixexpDemo, 343mixexpDemoOneToMany, 344mixGaussDemoFaithful, 353mixGaussLikSurfaceDemo, 346mixGaussMLvsMAP, 356mixGaussOverRelaxedEmDemo, 369mixGaussPlotDemo, 339mixGaussSingularity, 356mixGaussVbDemoFaithful, 753, 755mixPpcaDemoNetlab, 386mixStudentBankruptcyDemo, 361mlpPriorsDemo, 574mlpRegEvidenceDemo, 579mlpRegHmcDemo, 579mnist1NNdemo, 25, 1002multilevelLinregDemo, 844mutualInfoAllPairsMixed, 59\nnaiveBayesBowDemo, 84, 88\nnaiveBayesFit, 83, 277naiveBayesPredict, 86, 277net\ufb02ixResultsPlot, 981newsgroupsVisualize, 5newtonsMethodMinQuad, 250newtonsMethodNonConvex, 250ngramPlot, 592NIXdemo2, 135normalGammaPenaltyPlotDemo, 460normalGammaThresholdPlotDemo, 461numbersGame, 69\u201371\npagerankDemo, 600, 603\npagerankDemoPmtk, 602paretoPlot, 44parzenWindowDemo2, 509pcaDemo2d, 388pcaDemo3d, 11pcaDemoHeightWeight, 389pcaEmStepByStep, 397pcaImageDemo, 12, 389pcaOver\ufb01tDemo, 400\u2013402pcaPmtk, 393pfColorTrackerDemo, 830poissonPlotDemo, 37postDensityIntervals, 154ppcaDemo2d, 388PRhand, 182probitPlot, 259probitRegDemo, 259, 294, 363prostateComparison, 436prostateSubsets, 427\nquantileDemo, 33randomWalk0to20Demo, 856\nrbpfManeuverDemo, 834, 835rbpfSlamDemo, 835rdaFit, 108regtreeSurfaceDemo, 545rejectionSamplingDemo, 818relevanceNetworkNewsgroupDemo, 908residualsDemo, 219ridgePathProstate, 437riskFnGauss, 198robustDemo, 40robustPriorDemo, 168\nsaDemoPeaks, 869, 870\nsampleCdf, 816samplingDistGaussShrinkage, 203sensorFusion2d, 123sensorFusionUnknownPrec, 141seqlogoDemo, 36shrinkageDemoBaseball, 175shrinkcov, 130shrinkcovDemo, 129shrunkenCentroidsFit, 109shrunkenCentroidsSRBCTdemo, 109, 110shuffledDigitsDemo, 7, 25sigmoidLowerBounds, 761sigmoidPlot, 21sigmoidplot2D, 246simpsonsParadoxGraph, 933sliceSamplingDemo1d, 865sliceSamplingDemo2d, 865smoothingKernelPlot, 507softmaxDemo2, 103SpaRSA, 445sparseDictDemo, 471sparseNnetDemo, 575sparsePostPlot, 459sparseSensingDemo, 438spectralClusteringDemo, 893splineBasisDemo, 125ssmTimeSeriesSimple, 638, 639steepestDescentDemo, 247, 248stickBreakingDemo, 883studentLaplacePdfPlo t,\n40\nsubgradientPlot, 432subSuperGaussPlot, 412surfaceFitDemo, 218svdImageDemo, 394svmCgammaDemo, 504\ntanhPlot, 570\ntrueskillDemo, 798", "1079": "INDEXTOCODE 1049\ntrueskillPlot, 797\nunigaussVbDemo, 745varEMbound, 368\nvariableElimination, 717visDirichletGui, 48visualizeAlarmNetwork, 314vqDemo, 354\nwiPlotDemo, 127", "1080": "Index to keywords\n#P-hard, 727\n0-1 loss,177\n3-SAT, 727\nA star search, 887\nabsorbing state, 598\naccept,848\naction, 176action nodes, 328\naction space, 176\nactions, 176activation, 563\nactive learning, 230, 234, 938Active set, 441\nactive set, 442\nActivity recognition, 605\nAdaboost.M1, 559\nadagrad,263\nadaline,569\nadaptive basis-function model, 543\nadaptive importance sampling, 821\nadaptive lasso, 460\nadaptive MCMC, 853\nadaptive rejection Metropolis sampling, 820\nadaptive rejection sampling, 820\nadd-one smoothing, 77, 593\nADF,653, 983\nadjacency matrix, 309,970\nadjust for, 934\nadjusted Rand index, 878\nadmissible, 197\nadmixture mixture, 950\nAdSense, 928AdWords, 928affinity propagation, 887\nagglomerative clustering, 893\nagglomerative hierarchical clustering, 927aha,68\nAI, 1007AIC,162, 557\nAkaike information criterion, 162\nalarm network, 313\nalignment, 701all pairs,503\nalleles,317\nalpha divergence, 735\nalpha expansion, 803\nalpha-beta swap, 804\nalternative hypothesis, 163\nanalysis view, 390\nanalysis-synthesis, 470\nancestors, 309\nancestral graph, 664\nancestral sampling, 822\nand-or graphs, 1007annealed importance sampling, 871, 923\nannealing, 853annealing importance sampling, 992ANO\nVA, 553\nanti-ferromagnets, 668\naperiodic, 598\napproximate inference, 727\napproximation error, 230\nARD, 238, 463, 520, 580\nARD kernel, 480\narea under the curve, 181ARMA,639, 674\narray CGH, 454association rules, 15associative, 931\nassociative Markov network, 668\nassociative memory, 568,669, 997\nassociative MRF, 802\nassumed density \ufb01lter, 267assumed density \ufb01ltering, 653, 787\nasymptotically normal, 194\nasymptotically optimal, 201\nasynchronous updates, 774\natom,469\natomic bomb, 52attractive MRF, 802\nattributes, 2,3\nAUC,181\naudio-visual speech recognition, 628\naugmented DAG, 932\nauto-encoder, 1000\nauto-encoders, 990auto-regressive HMM, 626\nautoclass, 11\nautocorrelation function, 862\nautomatic relevance determination, 463\nautomatic relevancy determination, 238, 398, 580, 747Automatic speech recognition, 605\nautomatic speech recognition, 624auxiliary function, 350\nauxiliary variables, 863, 868\naverage link clustering, 897\naverage precision, 303average precision at K, 183\naxis aligned, 47\naxis parallel splits, 544\nback-propagation, 999backdoor path, 934\nback\ufb01tting, 552, 563, 998\nbackground knowledge, 68backoff smoothing, 594\nbackpropagation, 570, 970\nbackpropagation algorithm, 569backslash operator, 228\nBackwards selection, 428\nbago\nf words,5,81,945\nbag-of-characters, 483bag-of-words, 483bagging,551\nbandwidth, 480,507\nbarren node removal, 334, 714\nBART,551, 586\nBarzilai-Borwein, 445\nbase distribution, 338\nbase learner, 554\nbase measure, 882\nbase rate fallacy, 30\nbasic feasible solution, 468basis function expansion, 20,217\nbasis functions, 421basis pursuit denoising, 430\nbatch,261\nBaum-Welch, 618\nBayes ball algorithm, 324\nBayes decision rule, 177,195\nBayes estimator, 177,195", "1081": "INDEXTOKEYWORDS 1051\nBayes factor, 137, 163, 921\nBayes model averaging, 71,581\nBayes point, 257\nBayes risk, 195\nBayes rule, 29, 340\nBayes Theorem, 29\nBayesian, xxvii, 27\nBayesian adaptive regression trees, 551\nBayesian factor regression, 405\nBayesian hierarchical clustering, 899\nBayesian information criterion, 161\nBayesian IPF, 683\nBayesian lasso, 448\nBayesian model selection, 156\nBayesian network structure learning, 914\nBayesian networks, 310\nBayesian Occam\u2019s razor, 156\nBayesian statistics, 149, 191\nBDe,917\nBDeu,918\nbeam search, 428, 887belief networks, 310\nbelief propagation, 611, 707, 767\nbelief state, 71,332,607,609\nbelief state MDP, 332\nbelief updating, 709\nbell curve, 20,38\nBerkson\u2019s paradox, 326\nBernoulli, 21,34\nBernoulli product model, 88\nBernoulli-Gaussian, 426\nBessel function, 483beta distribution, 42,7 4\nbeta function, 42beta process, 470beta-binomial, 78\nBethe,781\nBethe energy functional, 781\nBethe free energy, 781\nBFGS,251\nBhattacharya distance, 828\nbi-directed graph, 674\nbias,20,200, 457\nbias term, 669\nbias-variance\ntradeoff,202\nBIC,161, 256, 557, 920\nbiclustering, 903\nbig data,1\nbigram model, 591\nbinary classi\ufb01cation, 3,6 5\nbinary entropy function, 57\nbinary independence model, 88\nbinary mask, 426, 470\nbinary tree, 895\nBing, 302, 799, 983binomial, 34\nbinomial coefficient, 34\nbinomial distribution, 74binomial regression, 292BinomialBoost, 561\nBIO,687\nbiosequence analysis, 36, 170\nbipartite graph, 313\nbiplot,383\nbirth moves, 370bisecting K-means, 898\nbits,56\nbits-back, 733black swan paradox, 77,8 4\nblack-box, 340,585Blackwell-MacQueen, 884\nblank slate, 165\nblind signal separation, 407\nblind source separation, 407\nblocked Gibbs sampling, 848\nblocking Gibbs sampling, 848\nbloodtype, 317BN2O,315\nbolasso,439\nBoltzmann distribution, 104,869\nBoltzmann machine, 568, 669,983\nbond variables, 866\nBoosting, 554\nboosting, 553, 742bootstrap, 192\nbootstrap \ufb01lter, 827\nbootstrap lasso, 439\nbootstrap resampling, 439borrow statistical strength, 171, 231, 296, 845\nbottleneck, 205, 337,1000\nbottleneck layer, 970\nboundop\ntimization, 369\nbox constraints, 444Box-Muller, 817\nboxcar kernel, 508, 508\nBoyen-Koller, 654\nBP,707\nBPDN,430\nBradley Terry, 795branch and bound, 811branching factor, 954\nbridge regression, 458\nBrownian motion, 483bucket elimination, 715\nBUGS, 756, 847\nBuried Markov models, 627\nburn-in phase, 856\nburned in, 838\nburstiness, 88\nbursty, 480\nC4.5,545\ncalculus of variations, 289\ncalibration, 724\nCandidate method, 872\nCanonical correlation analysis, 407\ncanonical form, 282\ncanonical link function, 291\ncanonical parameters, 115,282\nCardinality constraints, 810CART,544, 545\nCartesian, 51cascade,776\ncase analysis, 260\ncategorical, 2,35\ncategorical PCA, 402,947, 961\ncategorical variables, 876Cauchy,40\ncausal Markov assumption, 931\nCausal models, 931\ncausal MRF, 661\ncausal networks, 310\ncausal sufficiency, 931\ncausality, 919, 929CCA,407\nCCCP,702\nCD,989\ncdf,32,3 8\nCensored regression, 379\ncensored regression, 380", "1082": "1052 INDEXES\ncentering matrix, 494\ncentral composite design, 523\ncentral interval, 152\ncentral limit theorem, 38, 51, 255\ncentral moment, 413\ncentral-limit theorem, 55\ncentroid, 341\ncentroids, 486\ncertainty factors, 675\nchain graph, 671\nchain rule, 29,307\nchance nodes, 328\nchange of variables, 50\nchannel coding, 56\nChapman-Kolmogorov, 590\ncharacteristic length scale, 480\nCheeseman-Stutz approximation, 923\nChi-squared distribution, 42\nchi-squared statistic, 163, 213\nchildren,309, 310\nChinese restaurant process, 884\nchip-Seq, 622Cholesky decomposition, 227, 817\nChomsky normal form, 689chordal,665\nchordal graph, 720\nChow-Liu algorithm, 312, 912\nCI,308\ncircuit complexity, 944\ncity block distance, 876\nclamped phase, 987\nclamped term, 677\nclamping, 319\nclass imbalance, 503\nclass-conditional density, 30,6 5\nclassical, 149classical statistics, 191\nclassi\ufb01cation, 2,3\nClassi\ufb01cation and regression trees, 544\nclausal form, 675\nclause, 727click-through rate, 4\nclique,310\ncliques, 719, 722closing the loop, 635\nclosure,662\ncluster variational method, 783\nClustering, 875\nclustering, 10, 340\nclusters, 487clutter problem, 788\nco-cluste\nring, 979\nco-occurrence matrix, 5co-parents, 327\ncoarse-to-\ufb01ne grid, 775cocktail party problem, 407\ncoclustering, 903\ncodebook, 354\ncollaborative \ufb01ltering, 14, 300, 387, 903, 979\ncollapsed Gibbs sampler, 841\ncollapsed Gibbs sampling, 956\ncollapsed particles, 831\ncollect evidence, 707\ncollect-to-root, 723\ncollider,324\nCOLT,210\ncommittee method, 580\ncommutative semi-ring, 717\ncommutative semiring, 726compactness, 897compelled edges, 915\ncomplementary prior, 997\ncomplete, 322\ncomplete data, 270, 349\ncomplete data assumption, 914\ncomplete data log likelihood, 348,350\ncomplete link clustering, 897\ncompleting the square, 143\ncomposite likelihood, 678\ncompressed sensing, 472\ncompressive sensing, 472\ncomputation tree, 772\ncomputational learning theory, 210\ncomputationalism, 569\nconcave,222, 286\nconcave-convex procedure, 702\nconcentration matrix, 46\nconcentration parameter, 882\nconcept, 65concept learning, 65\ncondensation, 827\nconditional entropy, 59\nconditional Gamma Poisson, 949conditional Gaussian, 920\nconditional independence, 308conditional likelihood, 620conditional logit model, 252\nconditional probability, 29\nconditional probability distribution, 308\nconditional probability tables, 308\nconditional random \ufb01eld, 684\nconditional random \ufb01elds, 606, 661conditional topic random \ufb01eld, 969\nconditionally conjugate, 132\nconditionally independent, 31\n,82\nconditioning, 319\nconditioning case, 322\nconductance, 858\ncon\ufb01dence interval, 212\ncon\ufb01dence intervals, 153confounder, 674\nconfounders, 931\nconfounding variable, 934\nconfusion matrix, 181\nconjoint analysis, 297\nconjugate gradients, 249, 524\nconjugate prior, 74\nconjugate priors, 281, 287conjunctive normal form, 675\nconnectionism, 569\nconsensus sequence, 36,606\nconservation of probability mass, 157\nconsistent, 200\nconsistent estimator, 233consistent estimators, 70constant symbols, 676\nconstraint satisfaction problems, 717, 726constraint-based approach, 924\ncontent addressable memory, 669\ncontext free grammar, 689\ncontext speci\ufb01c independence, 321\ncontext-speci\ufb01c independence, 944contextual bandit, 184, 254\ncontingency table, 682continuation method, 442,869\ncontrastive divergence, 569, 989\ncontrastive term, 677\ncontrol signal, 625, 631converge, 857convex, 58, 221, 247, 285, 677", "1083": "INDEXTOKEYWORDS 1053\nconvex belief propagation, 785, 943\nconvex combination, 76, 130, 338\nconvex hull, 777\nconvolutional DBNs, 1004\nconvolutional neural nets, 1004\nconvolutional neural network, 565\ncooling schedule, 870\ncorpus,953\ncorrelated topic model, 757, 961\ncorrelation coefficient, 45, 876\ncorrelation matrix, 45\ncorrespondence, 658\ncosine similarity, 480\ncost-bene\ufb01t analysis, 186\ncoupled HMM, 628\ncovariance, 44\ncovariance graph, 674, 908\ncovariance matrix, 45,4 6\ncovariance selection, 938\ncovariates, 2\nCPD,308\nCPTs,308\nCramer-Rao inequality, 201Cramer-Rao lower bound, 201\ncredible interval, 137,152, 212\nCRF, 661, 684\ncritical temperature, 868\ncritical value, 671\ncross entropy, 57, 571\ncross over rate, 181\ncross validation, 24,206\ncross-entropy, 246, 953\ncross-language information retrieval, 963crosscat,904\ncrowd sourcing, 10, 995\nCRP,884\nCTR,4\ncubic spline, 537\ncumulant function, 282,284\ncumulants, 284\ncumulative distribution function, 32,3 8\ncurse of dimensionality, 18, 487\ncurved exponential family, 282\ncutting plane, 698\nCV,24\ncycle,310\ncyclic\npermutation property, 99\nd-prime,106\nd-separated, 324\nDACE, 518DAG,310\ndamped updates, 739\ndamping, 773\nDasher, 591data association, 658, 810\ndata augmentation, 362, 847\ndata compression, 56\ndata fragmentation, 546\ndata fusion, 404\ndata overwhelms the prior, 69\ndata-driven MCMC, 853\ndata-driven proposals, 828\nDBM,996\nDBN,628,997\nDCM,89\nDCT, 469death moves, 370debiasing, 439\ndecision, 176decision boundary, 22decision diagram, 328\ndecision nodes, 328\ndecision problem, 176decision procedure, 177\ndecision rule, 22\ndecision trees, 544\ndecoding, 693\ndecomposable, 665,722, 941\ndecomposable graphs, 682decomposes, 322,917\nDeeBN,628\nDeeBNs,997\ndeep, 929deep auto-encoders, 1000\ndeep belief network, 997\ndeep Boltzmann machine, 996\ndeep directed networks, 996\ndeep learning, 479, 995\ndeep networks, 569defender\u2019s fallacy, 61\nde\ufb02ated matrix, 418\ndegeneracy problem, 825\ndegenerate, 532, 535\ndegree,310\ndegrees of freedom, 39,161, 206,229,534\ndelet\ned interpolation, 593\ndelta rule, 265\ndendrogram, 895\ndenoising auto-encoder, 1001\ndense stereo reconstruction, 690\ndensity estimation, 9\ndependency network, 909\ndependency networks, 679derivative free \ufb01lter, 651\ndescendants, 309\ndescriptive, 2\ndesign matrix, 3, 875detailed balance, 854\ndetailed balance equations, 599\ndeterminism, 944deterministic annealing, 367, 620\ndeviance, 547\nDGM,310\ndiagonal, 46diagonal covariance LDA, 107diagonal LDA, 108\ndiameter, 710,897\ndictionary, 469\ndigamma, 361, 752, 958digital cameras, 8dimensionality reduction, 11, 1000\nDirac delta function, 39\nDirac measure, 37,68\nDirchlet process, 903direct posterior probability approach, 184\ndirected,309\ndirected acyclic graph, 310\ndirected graphical model, 310\ndirected local Markov property, 327\ndirected mixed graph, 929directed mixed graphical model, 674\nDirichlet, 79Dirichlet Compound Multinomial, 89\nDirichlet distribution, 47\nDirichlet multinomial regression LDA, 969\nDirichlet process, 596, 879,882, 973, 976\nDirichlet process mixture models, 508, 755discontinuity preserving, 691\ndiscounted cumulative gain, 303", "1084": "1054 INDEXES\ndiscrete,35\ndiscrete AdaBoost, 559\ndiscrete choice modeling, 296\ndiscrete random variable, 28\ndiscrete with probability one, 884\ndiscretize, 59, 691\ndiscriminability, 106\ndiscriminant analysis, 101\ndiscriminant function, 500\ndiscriminative, 245\ndiscriminative classi\ufb01er, 30\ndiscriminative LDA, 968\ndiscriminative random \ufb01eld, 684\ndisease mapping, 531\ndisease transmission, 970disparity, 691\ndispersion parameter, 290\ndissimilarity analysis, 898\ndissimilarity matrix, 875\ndistance matrix, 875\ndistance transform, 775\ndistorted, 566\ndistortion, 354\ndistribute evidence, 707\ndistribute-from-root, 724\ndistributed encoding, 984\ndistributed representation, 569,627\ndistributional particles, 831\ndistributive law, 717\ndivisive clustering, 893\nDNA sequences, 36do calculus, 932\nDocument classi\ufb01cation, 87\ndocument classi\ufb01cation, 5\nDomain adaptation, 297\ndomain adaptation, 297dominates, 197\ndouble loop algorithms, 773\ndouble Pareto distribution, 461double sided exponential, 41\ndRUM,294\ndual decomposition, 808\ndual variables, 492, 499\ndummy encoding, 35\ndyadic, 976DyBN,628\nDyBNs,997\ndynamic Bayes net, 653dynamic Bayesian network, 628\ndynamic linear model, 636\ndynamic programming, 331, 920dynamic topic model, 962\nE step,350\ne-commer\nce, 11\nearly stopping, 263,557,572\nEB,173\nECM,369, 387\nECME,369\nECOC,581\neconometric forecasting, 660\neconomy sized SVD, 392\nedge appearance probability, 786\nedges,309\nedit distance, 479EER,181\neffective sample size, 75,825,862\nefficient IPF, 683\nefficiently PAC-learnable, 210\neigendecomposition, 98eigenfaces, 12eigengap, 857eigenvalue spectrum, 130\nEKF,648\nelastic net, 438, 456, 936\nelimination order, 718\nEM, 271,349, 618, 749\nemail spam \ufb01ltering, 5\nembedding, 575empirical Bayes, 157, 162,173, 300, 746\nempirical distribution, 37, 205\nempirical measure, 37\nempirical risk, 205, 697\nempirical risk minimization, 205, 261\nend effector, 344energy based models, 666\nenergy function, 255\nenergy functional, 732,778\nensemble, 980Ensemble learning, 580\nensemble learning, 742\nentanglement, 629\nentanglement problem, 635, 653Entropy,547\nentropy,56\nEP, 983Epanechnikov kernel, 508\nePCA,947\nepigraph, 222\nepistemological uncertainty, 973\nepoch,264,566\nepsilon\ninsensitive loss function, 497\nEPSR,859\nequal error rate, 181\nequilibrium distribution, 597\nequivalence class, 915\nequivalent kernel, 512,533\nequivalent sample size, 76,917\nerf,38\nergodic,599\nErlang distribution, 42\nERM,205, 261\nerror bar, 76\nerror correcting codes, 768error correction, 56\nerror function, 38\nerror signal, 265error-correcting output codes, 503, 581\nESS,862\nessential graph, 915\nestimated potential scale reduction, 859\nestimator, 191\nEuclidean distance, 18evidence, 156, 173\nevidence procedure, 173,238, 746\nevolutionary MCMC, 429exchangeable, 321, 963\nexclusive or, 486\nexpectation correction, 658\nexpectation maximization, 349\nexpectation proagation, 735Expectation propagation, 787\nexpectation propagation, 525expected complete data log likelihood, 350, 351\nexpected pro\ufb01t, 330\nexpected sufficient statistics, 350, 359, 619\nexpected value, 33\nexplaining away, 326\nexplicit duration HMM, 622\nexploration-exploitation, 184", "1085": "INDEXTOKEYWORDS 1055\nexploratory data analysis, 7\nexponential cooling schedule, 870\nExponential distribution, 42\nexponential family, 115, 253, 281, 282, 290, 347\nexponential family harmonium, 985\nexponential family PCA, 947\nexponential loss, 556\nexponential power distribution, 458\nextended Kalman \ufb01lter, 648\nextension, 67\nexternal \ufb01eld, 668\nF score,183\nF1 score,183, 699\nFA,381\nface detection, 8\nface detector, 555\nface recognition, 8\nFacebook, 974factor,665\nfactor analysis, 381, 402, 931, 947\nfactor analysis distance, 520\nfactor graph, 769, 769, 771, 888\nfactor loading matrix, 381\nfactorial HMM, 628\nfactorial prior, 463\nfactors,382\nfaithful,936\nfalse alarm, 30,180\nfalse alarm rate, 181\nfalse discovery rate, 184\nfalse negative, 180\nfalse positive, 30,180\nfalse positive rate, 181\nfamily,309\nfamily marginal, 359\nfan-in,313\nfantasy data, 990\nfarthest point clustering, 355\nfast Fourier transform, 717, 775fast Gauss transform, 524fast ICA,411\nfast iterative shrinkage thesholding algorithm, 446\nFastSLAM, 635, 835\nfat hand, 933\nfault diagnosis, 659\nfeature construction, 564\nfeature extraction, 6,564\nfeature function, 667\nfea\nture induction, 680\nfeature maps, 565\nfeature matrix, 875feature selection, 86\nfeature-based clustering, 875\nfeatures,2,3 ,412\nfeedback loops, 929\nfeedforward neural network, 563\nferro-magnets, 668\nFFT, 775\ufb01elds of experts, 473\n\ufb01ll-in edges, 719\nFiltering, 607\n\ufb01ltering,87\n\ufb01nite difference matrix, 113\n\ufb01nite mixture model, 879\ufb01rst-order logic, 674\nFisher information, 166\nFisher information matrix, 152, 193, 293\nFisher kernel, 485\nFisher scoring method, 293Fisher\u2019s linear discriminant analysis, 271\nFISTA,446\n\ufb01t-predict cycle, 206\n\ufb01xed effect, 298\nFixed lag smoothing, 608\n\ufb01xed point, 139\ufb02at clustering, 875\nFLDA,271\n\ufb02ow cytometry, 936folds,24\nforest,310,912\nforward stagewise additive modeling, 557\nforward stagewise linear regression, 562\nforwards KL, 733forwards model, 345\nforwards selection, 428\nforwards-backwards, 644, 688, 707, 720forwards-backwards algorithm, 428,611\nfounder model, 317\nfounder variables, 385\nFourier basis, 472fraction of variance explained, 400\nfree energy, 988\nfree-form optimization, 737\nfrequent itemset mining, 15\nfrequentist, 27, 149\nfrequentist statistics, 191\nFrobenius norm, 388\nfrustrated, 868\nfrustrated system, 668\nfull, 46full\nconditional, 328,838\nfunction approximation, 3\nfunctional data analysis, 124\nfunctional gradient descent, 561\nfurthest neighbor clustering, 897\nfused lasso, 454\nfuzzy clustering, 973\nfuzzy set theory, 65\ng-prior,236, 425\ngame against nature, 176game theory, 176Gamma, 623gamma distribution, 41\ngamma function, 42GaP,949\ngap statistic, 372\ngating function, 342\nGauss-Seidel, 710Gaussian, 20,3 8\nGaussian approximation, 255, 731\nGaussian Bayes net, 318\nGaussian copulas, 942\nGaussian graphical models, 725Gaussian kernel, 480,507, 517\nGaussian mixture model, 339\nGaussian MRF, 672\nGaussian process, 483, 505, 509, 512, 882Gaussian processes, 515\nGaussian random \ufb01elds, 938\nGaussian RBM, 986\nGaussian scale mixture, 359,447, 505\nGaussian sum \ufb01lter, 656\nGDA, 101GEE,300\nGEM,369\nGene \ufb01nding, 606\ngene \ufb01nding, 622gene knockout experiment, 931", "1086": "1056 INDEXES\ngene microarrays, 421\ngeneralization, 3\ngeneralization error, 23,180\ngeneralization gradient, 66\ngeneralize, 3\ngeneralized additive model, 552\ngeneralized belief propagation, 785\ngeneralized cross validation, 207\ngeneralized eigenvalue, 274\ngeneralized EM, 361,369\ngeneralized estimating equations, 300\ngeneralized linear mixed effects model, 298\ngeneralized linear model, 281, 290\ngeneralized linear models, 281generalized pseudo Bayes \ufb01lter, 657\ngeneralized t distribution, 461\ngenerate and test, 853\ngenerative approach, 245generative classi\ufb01er, 30\ngenerative pre-training, 999\ngenerative weights, 410,986\ngenetic algorithms, 348, 720, 921genetic linkage analysis, 315, 318\ngenome, 318genotype, 317\ngeometric distribution, 622\nGibbs distribution, 290,666\nGibbs sampler, 672Gibbs sampling, 328, 669, 736, 838\nGini index, 548\ngist, 963Gittins Indices, 184\nGlasso,940\nGlauber dynamics, 838\nGLM,290, 654\nGLMM,298\nglmnet, 442global balance equations, 597\nglobal convergence, 248\nglobal localization, 828\nglobal Markov property, 661\nglobal minimum, 222global prior parameter independence, 916\nglobally normalized, 686\nGM,308\nGMM,339\nGP-LVM,540\nGPs,515\nGPUs, 1006gradient boosting, 560\ngradient descent, 247, 445\nGram matrix, 481\ngr\nammars, 689, 1007\ngrandmother cells, 984, 1005\ngraph,309\ngraph cuts, 890\ngraph Laplacian, 891\ngraph surgery, 932\ngraph-guided fused lasso, 454\ngraphcuts, 801\ngraphical lasso, 940\ngraphical model, 308, 311\ngraphical models, xxviii, 13, 31, 32, 308, 337, 909Gray code, 422\ngreatest common divisor, 598\ngreedy equivalence search, 936\nground network, 676\nground states, 668\ngroup lasso, 450, 579, 942\ngrouping effect, 456Gumbel, 295\nHadamard product, 609\nHaldane prior, 166\nham, 5\nHamiltonian MCMC, 868\nHammersley-Clifford, 666\nhamming distance, 876\nhandwriting recognition, 7\nhaplotype, 317\nhard clustering, 340\nhard EM, 352\nhard thresholding, 434, 435\nharmonic mean, 183harmonium, 983\nHastings correction, 849\nhat matrix, 221\nHDI,154\nheat bath, 838\nheavy ball method, 249\nheavy tails, 43,223\nHellinger distance, 735\nHelmholtz free energy, 733\nHessian, 193, 852heteroscedastic LDA, 275\nheuristics, 727\nhidden,10, 349\nhidden layer, 563\nhidden Markov model, 312,603, 963\nhidden nodes, 313\nhidden semi-Markov model, 622\nhidden units, 564\nhidden variable, 312,924\nhidden\nvariables, 319, 914\nhierarchical adaptive lasso, 458\nhierarchical Bayesian model, 171\nhierarchical Bayesian models, 347hierarchical clustering, 875,893\nhierarchical Dirichlet process, 621hierarchical HMM, 624\nhierarchical latent class model, 926\nhierarchical mixture of experts, 344, 551\nhigh throughput, 184, 421high variance estimators, 550highest density interval, 154\nhighest posterior density, 153\nhill climbing, 920hindsight, 607\nhinge loss, 211, 477,499\nHinton diagram, 592Hinton diagrams, 399\nhistogram, 508\nhit rate,181\nHMM,312,603\nHMM \ufb01lter, 640HMMs, 685Hoeffding\u2019s inequality, 209\nhomogeneous, 589\nhomotopy, 442\nHop\ufb01eld network, 568,669\nhorizon,608\nHorn clauses, 676\nHPD,153\nHSMM,622\nHuber loss, 224, 561\nHugin,722\nHungarian algorithm, 659, 810\nhybrid MCMC, 868\nhybrid Monte Carlo, 584hybrid systems, 655", "1087": "INDEXTOKEYWORDS 1057\nhyper-parameters, 74\nhypothesis space, 66\nI-map,324\nI-projection, 733\nICA, 385, 409\nID3,545\nIDA,936\nidenti\ufb01able, 346\nidenti\ufb01able in the limit, 70\niff,68\niid,51,218, 320\nill-conditioned, 106, 129\nimage classi\ufb01cation, 7\nimage compression, 355image denoising, 473\nimage inpainting, 14,473\nimage segmentation, 671\nimage tagging, 968\nIMM,658\nimplicit feedback, 983\nimportance sampling, 820\nimportance weights, 821\nimpression log, 983\nimproper prior, 166, 168\nimputation, 14\nImputation Posterior, 847\nin-degree, 310\ninclusion probabilities, 423\nincremental EM, 365, 366\nindependence sampler, 848\nindependent and identically distributed, 51\nindependent component analysis, 409\nindicator function, 17,28, 976\ninduced width, 719\ninduction, 66,77\ninductive bias, 19, 582\ninfer.net, 799\ninference, 320in\ufb01nite hidden relational model, 977\nin\ufb01nite HMM, 621in\ufb01nite mixture models, 841, 879\nin\ufb01nite relational model, 903, 973, 976\nin\ufb02uence diagram, 328, 932\nin\ufb02uence model, 628\ninfomax, 416\ninformation, 27information arc, 329,331\ninf\normation bottleneck, 405\ninformation extraction, 688\ninformation \ufb01lter, 642\ninformation form, 115, 305,672, 711, 725\ninformation gain, 547\nInformation inequality, 58\ninformation projection, 733\ninformation retrieval, 183, 300, 953information theory, 56\ninheritance model, 317\ninner approximation, 779\ninnovation, 641\ninside outside, 624inside-outside algorithm, 689\ninstance-based learning, 17\nintegrate out, 156integrated likelihood, 156\nintegrated risk, 195\nintensive care unit, 313inter-causal reasoning, 326\ninteraction effects, 421interactive multiple models, 658interest point detector, 484interpolate, 112\ninterpolated Kneser-Ney, 595\ninterpolator, 517\ninterval censored, 379\ninterventional data, 936\ninterventions, 931intrinsic Gaussian random \ufb01eld, 113invariant, 8,854\ninvariant distribution, 597\ninvariant features, 1004\ninverse chi-squared distribution, 131\ninverse Gamma, 130inverse gamma, 42\ninverse Gaussian, 448\ninverse probability transform, 815\ninverse problem, 317\ninverse problems, 344\ninverse reinforcement learning, 186\ninverse Wishart, 126, 128\ninverted index, 600\ninverted indices, 1004IP, 847IPF,682\niris, 6, 548IRLS,251\nIRM, 976irreducible, 598\nIsing model, 668\nisotropic, 46\niterated EKF, 650\niterative conditional modes, 669,804,\n 929\niterative proportional \ufb01tting, 682, 939\niterative scaling, 683\niterative shrinkage and thresholding algorithm, 445iterative soft thresholding, 445\niteratively reweighted least squares, 251\nJacobi, 710, 773Jacobian, 151, 648, 649Jacobian matrix, 50\nJAGS,847\nJamBayes, 13James Stein estimator, 174James-Stein estimator, 173, 199JC Penney, 603Jeffreys prior, 166\nJeffreys-Lindley paradox, 165\nJensen\u2019s inequality, 58, 363\nJensen-Shannon divergence, 57\nJeopardy, 4jittered,486\nJJ bound, 761\njoint distribution, 29,307\njoint probability distribution, 44\nJTA,720\njump Markov linear system, 655\njunction tree, 722\njunction tree algorithm, 720, 731\njunction trees, 635\nK-centers, 887\nK-means algorithm, 352\nk-means++, 355\nK-medoids algorothm, 490\nk-spectrum kernel, 484\nK2 algorithm, 920\nKalman \ufb01lter, 122, 267, 632, 633, 640, 643\nKalman gain matrix, 637, 641\nKalman smoother, 633, 707", "1088": "1058 INDEXES\nKalman smoothing, 644, 712, 963\nKarhunen Loeve, 387\nKarl Popper, 77\nKDE,508, 510\nKendall\u2019s \u03c4,304\nkernel, 565, 600,848\nkernel density estimation, 127, 510kernel density estimator, 508\nkernel function, 479, 515\nkernel machine, 486\nkernel PCA, 494, 540, 892\nkernel regression, 511\nkernel smoothing, 511\nkernel trick, 488\nkernelised feature vector, 486\nKikuchi free energy, 784\nkinect, 551kinematic tracking, 344\nkink,372\nKL divergence, 57, 732\nKleene star, 483knee,372\nKNN,16\nknots,537\nknowledge base, 676\nknowledge discovery, 2,9\nknowledge engineering, 313\nKolmogorov Smirnov, 864kriging,516\nkronecker product, 253, 760\nKruskal\u2019s algorithm, 912Kullback-Leibler divergence, 57\nkurtosis,413, 415\nL-BFGS,252\n/lscript\n0pseudo-norm, 424\n/lscript0regularization, 426\n/lscript1loss, 179\n/lscript1regularization, 430\nL1-Adaboost, 563\nL1VM,488, 505\n/lscript2loss, 179\n/lscript2norm,218\n/lscript2regularization, 226\nL2boosting, 558\nL2VM,488\nlabel, 176label bias, 685\nlabel switching, 341,841\nlabel taxonomy, 689\nlabeled LDA, 953, 969\nlag, 608Lagrange multiplier, 80\nLagrange multipliers, 289Lagrangian, 80, 289\nLagrangian relaxation, 808\nLanczos algorithm, 398language model, 300, 953\nlanguage modeling, 81, 568\nlanguage models, 591\nLaplace, 223, 413, 429Laplace approximation, 255, 468\nLaplace distribution, 41\nLaplace\u2019s rule of succession, 77\nLAR,442, 562\nlarge margin classi\ufb01er, 501\nlarge margin principle, 259LARS,437,442, 558, 562\nlasso,431, 470, 562, 936\nlatent,11latent class model, 926\nlatent CRF, 701latent Dirichlet allocation, 949, 950\nlatent factors, 11\nlatent semantic analysis, 12, 947\nlatent semantic indexing, 418,947\nlatent SVMs, 702\nlatent variable models, 337\nlattice, 668Lauritzen-Spiegelhalter, 722\nLBP,767\nLDA,104, 927, 949, 950\nLDA-HMM, 963\nLDPC, 768LDS,631\nleaf,309\nleak node, 315\nleapfrog steps, 868\nlearning, 320learning curve, 230\nlearning rate, 247\nlearning to learn, 296\nlearning to rank, 300\nleastf\navorable prior, 197\nleast mean squares, 265,637\nleast squares, 219\nleast squares boosting, 428, 442, 558\nleave one out cross validation, 207\nleave-one out cross validation, 24\nleaves, 895left censored, 379\nleft-to-right, 612left-to-right transition matrix, 590\nLeNet5,566\nleptokurtic, 413\nLETOR,300\nlevel sets, 47Levenberg Marquardt, 250\nLevinson-Durbin, 627LG-SSM,631\nlikelihood, 319likelihood equivalence, 917\nlikelihood equivalent, 200likelihood principle, 214\nlikelihood ratio, 67,163\nlikelihood weighting, 822\nlimited memory BFGS, 252\nlimiting distribution, 598\nline minimization, 248\nline search, 248\nlinear discriminant analysis, 104\nlinear dynamical system, 631\nlinear Gaussian, 318\nlinear Gaussian system, 119\nlinear kernel, 482\nlinear program, 224\nlinear programming relaxtion, 800\nlinear regression, 19\nlinear smoother, 533\nlinear threshold unit, 252\nlinear trend, 660linear-Gaussian CPD, 673linear-Gaussian SSM, 631\nlinearity of expectation, 49\nlinearly separable, 22, 252, 266\nlink farms, 601\nlink function, 291\nLISREL,930\nListNet,302\nLMS,265,637", "1089": "INDEXTOKEYWORDS 1059\nlocal consistency, 780\nlocal evidence, 317,671\nlocal level model, 637\nlocal prior parameter independence, 917\nlocal variational approximation, 756\nlocalist encoding, 984\nlocally decodable, 811\nlocally normalized, 686,715\nlocally weighted regression, 512\nLOESS,512\nlog partition function, 282\nlog-linear, 667\nlog-loss,210\nlog-odds ratio, 283\nlog-sum-exp, 86,757\nlogic sampling, 822\nlogical reasoning problems, 726\nlogistic,21, 295\nlogistic distribution, 413, 863\nlogistic normal, 402, 961\nlogistic regression, 21, 106\nlogit,21\nlogitBoost, 560\nlong tail, 2, 296\nlong tails, 43\nLOOCV,24, 207\nlook-ahead RBPF, 832\nloop,310\nloopy belief propagation, 691, 767, 889\nLorentz,40\nloss,176\nloss function, 261loss matrix, 185\nloss-augmented decoding, 699\nloss-calibrated inference, 694\nlossy compression, 354low density parity check, 768Low-level vision, 690\nLOWESS, 512\nLSA,947, 1003\nlse,757\nLSI,947\nLVM,337\nM step,350\nM-projection, 733\nM3nets,693\nmachine learning, 1\nmacro-a\nveraged F1, 183\nMahalanobis distance, 98mammogram, 29\nmaneuvering target tracking, 832\nmanifest, 930\nMAP estimate, 4, 178\nMAR,270\nmargin, 563margin re-rescaling), 696\nmarginal distribution, 29\nmarginal likelihood, 156, 169\nmarginal polytope, 777\nmarginalizing out, 320\nmarginally independent, 30\nmarker,317\nmarket basket analysis, 15\nMarkov,324\nMarkov assumption, 308\nMarkov blanket, 327,662, 736, 838\nMarkov chain, 308,589\nMarkov Chain Monte Carlo, 815Markov chain Monte Carlo, 52, 600, 837Markov decision process, 331\nMarkov equivalence, 936Markov equivalent, 915, 917\nMarkov logic network, 675\nMarkov mesh, 661\nMarkov model, 589\nMarkov models, 32Markov network, 661\nMarkov random \ufb01eld, 661\nMarkov switching models, 604\nMARS, 538, 553, 562\nMART,562\nmaster, 810matching pursuit, 562matching pursuits, 428\nMatern kernel, 482\nMATLAB, xxviiimatrix completion, 14,939\nmatrix determinant lemma, 118\nmatrix factorization, 948matrix inversion lemma, 118, 144, 641\nmatrix permanent, 669matrix tree theorem, 914max \ufb02ow/min cut, 801\nmax margin Markov networks, 693\nmax pooling, 1005\nmax product linear programming, 810\nmax-product, 614,713\nmax-pr\noduct belief propagation, 800\nmaxent,289\nmaximal branching, 913\nmaximal clique, 310\nmaximal information coefficient, 60\nmaximal weight bipartite matching, 659maximizer of the posterior marginals, 612maximum a posteriori, 4\nmaximum entropy, 39, 104,289,667\nmaximum entropy classi\ufb01er, 252\nmaximum entropy Markov model, 685\nmaximum expected utility principle, 177\nmaximum likelihood estimate, 69\nmaximum risk, 196\nmaximum weight spanning tree, 912MCAR,270\nMCEM,368\nMCMC, 52, 596, 600, 815, 837\nMDL,162\nMDP,331\nMDS, 496mean,33\nmean absolute deviation, 511\nmean average precision, 303\nmean \ufb01eld, 735, 756, 767, 989\nmean \ufb01eld energy functional, 779\nmean function, 291\nmean precision, 182mean reciprocal rank, 303\nmean squared error, 205,218\nMechanical Turk, 10, 995median,33\nmedian model, 423\nMEMM,685\nmemory-based learning, 17\nMendelian inheritance, 317\nMercer kernel, 481\nMercer\u2019s theorem, 481, 539\nmessage passing, 644, 800metric, 691, 691, 803\nmetric CRF, 691\nmetric MRF, 803", "1090": "1060 INDEXES\nMetropolis Hastings, 848, 922\nMetropolis-Hastings algorithm, 869\nMFCC, 1005MH,848\nMI,59\nmicro-averaged F1, 183\nMicrosoft, 983mini-batch, 264, 571\nminimal, 282\nminimal I-map, 324\nminimax rule, 196\nminimum description length, 162\nminimum entropy prior, 621\nminimum mean squared error, 179\nminimum spanning tree, 897minorize-maximize, 369\nmisclassi\ufb01cation loss, 176Misclassi\ufb01cation rate, 547\nmisclassi\ufb01cation rate, 22,205\nmissed detection, 180\nmissing, 15missing at random, 270, 372, 982\nmissing completely at random, 270\nmissing data, 14, 914, 974missing data problem, 269\nmixed directed graphs, 931mixed membership model, 950\nmixed membership stochastic block model, 973\nmixed model, 298\nmixing matrix, 408\nmixing time, 857\nmixing weights, 169, 338\nmixture,72\nmixture density network, 344\nmixture model, 164, 338\nmixture of conjugate priors, 169\nmixture of experts, 342, 563, 973, 984\nmixture of factor analysers, 386\nmixture of Gaussians, 339\nmixture of Kalman \ufb01lters, 831\nmixture of trees, 914\nmixture proposal, 853\nMLE,69\nMLP,563\nMM,369\nMMSE,179\nMNIST,7, 341\nMobious numbers, 784\nmode,4\nmodel based clustering, 11\nmodel selection, 10, 24,156\nmodel selection consistent, 439\nmodel-based appr\noach, xxvii\nmodel-based clustering, 879\nmoderated output, 260\nmodularity, xxviiiMoE, 342moment matching, 176, 287,653, 658, 677\nmoment parameters, 115\nmoment projection, 733\nmomentum, 248\nmonks, 974Monte Carlo, 52, 151, 192, 258, 815\nMonte Carlo EM, 368\nMonte Carlo integration, 53Monte Carlo localization, 828\nmoralization, 663,715\nmotes, 218motif,36\nmPCA,948MPE,614\nMPM,612\nMRF,661\nMSE,218\nmulti label classi\ufb01cation, 970multi net, 627\nmulti-armed bandit, 184\nmulti-class logistic regression, 104\nmulti-clust, 904\nmulti-grid techniques, 775multi-information, 415\nmulti-label classi\ufb01cation, 3, 405\nmulti-layer perceptron, 563, 999\nmulti-level model, 171\nmulti-level modeling, 844multi-stage, 186multi-target tracking, 659\nmulti-task feature selection, 297\nmulti-task learning, 172, 231,296, 449, 757\nmulticlass classi\ufb01cation, 3\nmultidimensional scaling, 496multinomial, 35\nmultinomial coefficient, 35\nmultinomial logistic regression, 104,252\nmultinomial PCA, 948, 951\nmultinomial probit, 295\nmultinomial regression LDA, 968\nmultinomial resampling, 826\nmultinoulli distribution, 35\nmultiple hypothesis testing, 184\nmultiple hypothesis tracking, 656\nmultiple imputation, 115\nmultiplek\nernel learning, 524, 543\nmultiple LDA, 276\nmultiple output model, 3\nmultiple random restarts, 348, 921\nmultiple restarts, 620multivariate adaptive regression splines, 553\nmultivariate Bernoulli naive Bayes, 82\nmultivariate delta method, 763\nmultivariate Gamma function, 133multivariate gamma function, 126\nmultivariate Gaussian, 46,97, 339\nmultivariate normal, 46,97\nmultivariate probit, 295\nmultivariate Student t, 46\nmutual information, 46, 59, 87, 547, 912\nmutual inhibition, 564\nmutually independent, 62MVN,46,97\nN-best list, 616\nn-gram, 568n-gram models, 591\nNadaraya-Watson, 511\nnaive Bayes classi\ufb01er, 82, 88, 311\nnaive Bayes classi\ufb01ers, 32named entity extraction, 688\nNaN,14\nnats,56\nnatural exponential family, 282\nnatural gradient, 411natural parameters, 115,282\nNDCG,304\nnearest centroids classi\ufb01er, 102\nnearest medoid classi\ufb01cation, 491\nnearest neighbor, 16\nnearest neighbor clustering, 897\nnearest neighbor data association, 658\nnearest shrunken centroids, 109", "1091": "INDEXTOKEYWORDS 1061\nnegative binomial, 624\nnegative binomial distribution, 214\nnegative examples, 65negative log likelihood, 218,349\nnegative transfer, 297\nnegentropy, 415\nneighbors, 309\nneocognitron, 566\nnested plate, 321\nNesterov\u2019s method, 446\nNet\ufb02ix, 15, 580, 979, 981, 987, 993NETtalk,569\nneural network, 302, 969neural networks, 344, 535neutral process, 882\nNewton\u2019s algorithm, 249, 251\nNHST,213\nNIW,133\nNIX,136\nNLL,218,349\nNMAR,270\nNMF,470,949\nno forgetting, 331\nno free lunch theorem, 24, 582\nnodes,309\nnodes that \ufb01re together should wire together, 929\nnoise \ufb02oor, 230\nnoisy-OR, 313, 928\nnominal, 2\nnon-descendants, 327\nnon-factorial, 466\nnon-informative, 165\nnon-negative matrix factorization, 470,949\nnon-negative sparse coding, 470\nnon-null recurrent, 599\nnon-parametric Bayes, 879\nnon-parametric bootstrap, 192\nnon-parametric BP, 712\nnon-parametric model, 16\nnon-parametric prior, 879\nnon-serial dynamic programming, 717\nnon-smooth, 432\nnon-terminals, 689nonparanormal, 942\nnorm of a function, 539\nnormal,20,3 8\nnormal equation, 220\nnormal Gamma, 476\nnormal inverse chi-squared, 136\nNormal-in v\nerse-wishart, 133\nnormalized cut, 891\nnormalized discounted cumulative gain, 304\nnormalized mutual information, 879\nnot missing at random, 270\nnoun phrase chunking, 687\nNP-complete, 920NP-hard, 726\u03bd-SVM classi\ufb01er, 502\nnuisance variables, 320\nnull hypothesis, 163,213\nnull hypothesis signi\ufb01cance testing, 213\nnumber game, 65\nnumerical under\ufb02ow, 86\nobject detection, 8\nobject localization, 8\nobservation, 603\nobservation model, 312,631\nobserved data log likelihood, 348\nobserved information, 167observed information matrix, 193\nOccam factor, 255\nOccam\u2019s razor, 67,156, 399, 400\noccasionally dishonest casino, 606\noccupancy grid, 828\nOctave,xxviii\noffline,261\noil wild-catter, 328\nOLS,220\nOMP,428\none-armed bandit, 184\none-hot encoding, 35\none-of-C encoding, 252one-shot decision problem, 186\none-standard error rule, 208\none-step-ahead predictive density, 609\none-versus-one, 503\none-versus-the-rest, 503\none-vs-all, 503\nonline EM, 365\nonline gradient descent, 262\nonline learning, 75, 241,261\nontological uncertainty, 973\nontology, 977\nopen class, 596, 688\nOpen Directory Project, 600, 689open universe, 676\noptimal action, 177optimism of the training error, 206\noptimiza\ntion, 218\nordered Markov property, 310,327\nordinal, 295ordinal regression, 2,295, 301\nordinal variables, 876ordinary least squares, 220\nOrnstein-Uhlenbeck process, 483\northodox statistics, 191\northogonal least squares, 427\northogonal matching pursuits, 428\northogonal projection, 221\nout-degree, 310\nout-of-clique query, 722\nouter approximation, 780\noutliers, 179, 223\nover-complete, 282, 1001\novercomplete, 469\novercounting number, 784\noverdispersed, 859\nover\ufb01t,22\nover\ufb01tting, 72overrelaxed EM algorithm, 369\np-value, 138, 163, 163,213\nPAC,210\nPageRank, 301, 596, 600, 601\npaired t-test, 137\npairwise independent, 62pairwise Markov property, 662\npairwise MRF, 666\nparallel tempering, 858, 871, 922\nparameter, 176parameter expansion, 736parameter modularity, 918\nparameter sharing, 107\nparameter tying, 107,171,589\nparametric bootstrap, 192\nparametric model, 16,19\nparents,309, 310\nPareto distribution, 43\npart of speech, 605, 966", "1092": "1062 INDEXES\nPart of speech tagging, 605\npartial dependence plot, 586\npartial least squares, 406, 975\npartially directed acyclic graph, 915\npartially labeled LDA, 969\npartially observed Markov decision process, 331\npartially observed MRF, 672\nParticle \ufb01ltering, 823\nparticle \ufb01ltering, 267, 648, 823, 887\npartition function, 282,666\npartitional clustering, 875\npartitioned inverse formula, 116\npartitioning, 841partitions of the integers, 885\nParzen window density estimator, 508\npassing a \ufb02ow, 724\npath,310\npath diagrams, 929\npathologies, 211\npattern,915\npattern completion, 669\npattern recognition, 2\npattern search, 736, 783PCA,12,387, 493, 947\nPCFG,689\nPDAG, 936pdf,32\npedigree graph, 315\npeeling algorithm, 715\nPegasos,701\npenalized least squares, 226\npenalized log likelihood, 161\npenalized splines, 537\npenetrance model, 317\nperception-action, 331\nperceptron, 569perceptron algorithm, 266\nperceptual aliasing, 828\nperfect intervention, 931\nperfect map, 664\nperiod,598\npermanent, 942perplexity, 953, 953, 992\npersistent CD, 991\npersistent contrastive divergence, 680personalized recommendation, 77personalized spam \ufb01ltering, 296\nperturbation theory, 892phase,317\nphase transition, 671, 857\nphenotypes, 317phone, 624phonemes, 1005phylogenetic HMM, 317\nphylogenetic tree, 925piecewise polynomial, 537\npilo\nt runs,851\npipeline,687\nPitman-Koopman-Darmois theorem, 286\nPitman-Yor process, 885\nPlackett-Luce, 302\nplates,321\nplatykurtic, 413\nPLS,406\nPLSI,949\nplug-in, 147plug-in approximation, 72\nplutocracies, 43pmf,28\nPMTK,xxviiipoint estimate, 149, 150\npointwise approach, 301\npointwise marginal credibility intervals, 114\npointwise mutual information, 59\nPoisson,37\npoisson regression, 292\npolar, 51policy,177\nPolya urn, 89,884\nPolyak-Ruppert averaging, 263\npolynomial kernel, 481\npolynomial regression, 20\npolynomial time approximation schemes, 728polysemy, 951\npolytree,310\nPOMDP,331\npooled, 171pooled empirical variance, 108\npopulation minimizer, 556\npositive de\ufb01nite, 125, 222\npositive de\ufb01nite kernel, 481\npositive examples, 65posterior expected loss, 177\nposterior mean, 179posterior median, 179posterior mode, 178posterior predictive density, 608posterior predictive distribution, 66, 71, 234\npotential function, 665\nPotts model, 671, 856\npower law, 43\npower method, 603\nPPCA,381,387\nprecision, 38,182\nprecision at k, 303, 702\nprecision matrix, 46, 100\nprecision\nrecall curve, 182\npredict-update cycle, 609\npredict-update-project, 653\npredictive, 2\npreferences, 185\npreposterior risk, 195\nprevalence, 183\nPrim\u2019s algorithm, 912primal variables, 492, 499\nprincipal component, 388principal components, 1000principal components analysis, 12,387\nprincipal components regression, 230\nprinciple of insufficient reason, 58\nprobabilistic decision tree, 551probabilistic expert system, 313\nprobabilistic inference, 319\nprobabilistic latent semantic indexing, 949\nprobabilistic matrix factorization, 337, 980\nprobabilistic PCA, 387\nprobabilistic principal components analysis, 381\nprobabilistic relational modeling, 675,976\nprobability density function, 32\nprobability mass function, 28\nprobability of the evidence, 319,609,717\nprobability product kernel, 485\nprobability simplex, 47,7 9\nprobability theory, xxvii, 1probably approximately correct, 210\nprobe,583\nprobit,260, 655\nprobit regression, 293, 362, 380, 795, 864\nproduct of experts, 983\nproduct rule, 29", "1093": "INDEXTOKEYWORDS 1063\nproduction rules, 689\npro\ufb01le HMM, 606\npro\ufb01le log likelihood, 401\nprojected gradient descent, 444, 445\nprojection, 262projection pursuit, 415\nProlog, 676proposal distribution, 817,848, 869\npropose, 848prosecutor\u2019s fallacy, 61\nProtein sequence alignment, 606\nprotein-protein interaction networks, 970prototype, 341\nproximal operator, 443\npruning,549\npseudo counts, 75\npseudo likelihood, 678\npseudo marginals, 780\npseudo random number generator, 816\npseudo-likelihood, 943pure,546,548\npurity,877\npushing sums inside products, 715\npyramid match kernel, 484\nQALY,186\nQMR,313\nQP,431\nqq-plot,260\nQR decomposition, 228\nquadratic discriminant analysis, 102\nquadratic loss, 179\nquadratic program, 431, 498, 499\nquantile,33\nquantize, 59\nquartiles, 33\nQuasi-Newton, 251\nquery logs, 301query variables, 320\nquick medical reference, 313\nradar, 658radial basis function, 480\nRand index, 878\nrandom accelerations model, 633\nrandom effects, 298\nrandom effects mixture of experts, 969\nrandom forests, 551, 554\nrandom probability measure, 880\nrandom utility model, 294\nrandom walk Metropolis algorithm, 848\nrandom walk on the integers, 599random walk proposal, 869Rank correlation, 304\nrank one update, 118\nranking,87,601,702\nRankNe\nt,302\nRao-Blackwell, 841Rao-Blackwellisation, 841\nRao-Blackwellised particle \ufb01ltering, 831\nRao-Blackwellized particle \ufb01ltering, 659rare event, 182, 820\nrate,355\nrational behavior, 177\nRBF,480\nRBF kernel, 517RBF network, 486\nRBM,983, 996\nRBPF,831\nreal AdaBoost, 559recall,181, 182\nreceiver operating characteristic, 181\nreceptive \ufb01elds, 565\nrecognition weights, 410,986\nrecombination model, 317\nreconstruction error, 354,387\nrecurrent, 599\nrecurrent neural network, 568,669\nrecurrent neural networks, 591recursive, 929\nrecursive least squares, 265, 636\nre\ufb02ecting pair, 553\nregime switching, 660\nregime switching Markov model, 626\nregression, 2\nregression spline, 537\nregret,262\nregular,598\nregularization, 227\nregularization path, 436, 442, 562\nregularized discriminant analysis, 107\nregularized estimation, 130\nregularized particle \ufb01lter, 827\nregularized risk minimization, 206\nreinforcement learning, 2, 186\nreject action, 178\nrejection sampling, 817\nrejuvenation, 825\nrelation,975\nrelational probabilistic models, 676relational topic model, 974\nrelative entropy, 57\nrelative importance of predictor variables, 586\nrela\ntive risk,531\nrelevance network, 908\nrelevance vector machine, 463, 488\nRephil,928\nreplicated softmax model, 992\nrepresenter theorem, 539\nreproducing kernel Hilbert space, 539\nreproducing property, 539\nrerank, 616resample-move, 827\nresidual,641\nresidual analysis, 260\nresidual belief propagation, 774\nresidual error, 19\nresidual resampling, 826\nresidual sum of squares, 218\nresponse variable, 2\nresponsibility, 340,351\nrestricted Boltzmann machine, 983\nreverse KL, 733reversible jump MCMC, 370, 399, 855\nreward, 2Ricatti equations, 642\nrich get richer, 755,885\nridge regression, 203, 226\nright censored, 379\nrisk,195, 261\nrisk averse, 4, 178\nRJMCMC, 855\nRKHS,539\nRLS,636\nRobbins-Monro, 263, 366, 701\nrobust, 179robust priors, 168\nrobustness, 223\nROC,181\nrocking,261", "1094": "1064 INDEXES\nroot,309, 895\nroot mean square error, 979\nRosenblatt, 266rotamers, 690RTS smoother, 644\nrule of iterated expectation, 141\nrule of total probability, 29\nrules,550\nRUM,294\nrunning intersection property, 722\nRVM,488, 505\nsaddle point approximation, 255\nsample impoverishment, 826\nsample standard deviation, 136\nsamples, 52sampling distribution, 191, 191\nsampling importance resampling, 823\nsampling period, 633\nsatisfying assignment, 727saturated model, 428\nSBL,463\nscalar product, 19\nscale invariant prior, 168\nscale of evidence, 163scatter plot, 6\nSCFGs, 624schedule, 263\nSchur complement, 116\nscienti\ufb01c method, 71scope,328\nscore function, 167,193\nscore matching, 1001score vector, 485\nscores,382\nscree plot, 400\nscreening, 87\nsearch engine optimization, 603\nsecond order, 249\nsecond order Markov chain, 312\nsecond-order Markov model, 591self loops, 309\nsemantic hashing, 1003\nsemantic network, 977semantic role labeling, 576semi-conjugate, 132\nsemi-continuous HMM, 630\nsemi-Markov model, 622\nsemi-metric, 691\nsemi-parametric model, 298,524\nsemi-supervised, 405semi-supervised embedding, 576\nsemi-supervised learning, 268, 270\nsensible PCA, 387\nsensitivity, 29,181\nsensitivity analy\nsis,166\nsensor fusion, 122\nsentiment analysis, 967\nseparating set, 723\nseparation oracle, 699\nsequence logo, 36\nsequential, 186sequential minimal optimization, 499\nsequential TRBP, 801\nSGD,262\nShafer-Shenoy, 722\nshallow parsing, 687\nshared,103\nSherman-Morrison-Woodbury formula, 118\nshooting, 441, 940shrinkage, 122,174,230,557\nshrinkage estimation, 130\nshrinkage factor, 437\nside chains, 690side information, 982\nSIFT, 484sifting property, 39\nsigma points, 650, 651\nsigmoid,21, 105\nsigmoid belief net, 313,996\nsigmoid belief nets, 763sigmoid kernel, 482\nsignal detection theory, 106signal processing, 421signal-to-noise ratio, 122\nsignal-to-symbol, 1007\nsimilar, 66, 875similarity-based clustering, 875\nsimple cells, 413Simple linear regression, 241\nsimplex factor model, 949\nSimpon\u2019s paradox, 933\nSimulated annealing, 869\nsimulated annealing, 262, 348, 853, 921\nsimulation based, 823\nsimultaneous localization and mapping, 635\nsingle best replacement, 427\nsingle link clustering, 897\nsingle site updating, 847\nsingular value decomposition, 392\nsingular values, 392\nSIR, 823size principle, 67\nskewness, 413\nskip arcs, 568\nskip-chain CRF\n,688\nslack re-scaling, 696\nslack variables, 498\nSLAM,635, 834\nslaves, 810slice sampling, 865\nsliding window detector, 8\nslippage, 635slot machine, 184smallN, largeD,421\nSmartASS, 4SML,680\nSMO,499\nSmoothing, 607\nsmoothing kernel, 507, 507\nSmoothing splines, 536\nsocial networks, 970soft clustering, 340,973\nsoft margin constraints, 501\nsoft thresholding, 434, 435\nsoft weight sharing, 575\nsoftmax,104, 283\nsource coding, 56\nSpAM,553\nspam, 5spanning tree polytope, 786\nSpaRSA, 445sparse,15,421, 621, 945, 979\nsparse Bayesian learning, 463\nsparse boosting, 562\nsparse coding, 469\nsparse data problem, 77\nsparse kernel machine, 421sparse matrix factorization, 469, 470\nsparse PCA, 469", "1095": "INDEXTOKEYWORDS 1065\nsparse representation, 421\nsparse vector machine, 488\nsparsity, 41sparsity-promoting prior, 297spectral,445\nspectral clustering, 891\nspectral graph theory, 891\nspeech recognition, 590, 1005sphereing, 142\nspherical, 46\nspike and slab, 424\nspin, 668spline, 298split merge, 621\nsplit variable, 224\nsquare root \ufb01lter, 642\nsquared error, 179\nsquared exponential kernel, 480,517\nsquared loss, 176squashing function, 21\nSSM,631\nSSVMs,693\nstability selection, 439\nstable,936\nstacked denoising auto-encoder, 1001stacking, 580\nstandard deviation, 34\nstandard error, 56\nstandard error of the mean, 137, 208\nstandard errors, 194\nstandard model, 995\nstandard normal, 38\nstandard overcomplete representation, 776\nstandardized, 352Standardizing, 142\nstate, 176state estimation, 313\nstate space, 28\nstate space model, 631\nstate transition diagram, 590, 606\nstate transition matrix, 308\nstationary, 589,631\nstationary distribution, 596, 597\nstatistical learning theory, 209\nstatistical relational AI, 675\nstatistical relational learning, 976\nstatistically signi\ufb01cant, 213\nsteepest descent, 247,264\nStein\u2019s paradox, 199\nstemming, 81\nstep size, 247\nstepping out, 866\nstepwise EM, 365\nstick\n-breaking construction, 883\nsticky,850\nstochastic algorithm, 869stochastic approximation, 368stochastic approximation EM, 368\nstochastic automaton, 590\nstochastic block model, 972\nstochastic context free grammars, 624stochastic EM, 368\nstochastic gradient boosting, 584\nstochastic gradient descent, 262, 570, 868, 981, 987\nstochastic matrix, 307,589\nstochastic maximum likelihood, 680, 990\nstochastic optimization, 262\nstochastic process, 953stochastic processes, 589\nstochastic search, 429stochastic volatility, 831\nstop words, 81,480, 952\nstopping rule, 214\nstrati\ufb01ed CV, 206\nstrati\ufb01ed sampling, 826\nstreaming data, 261\nStreetView, 8strict, 197strictly convex, 222\nstring kernel, 483\nstrong local optimum, 804\nstrong sampling assumption, 67\nstructural EM, 925\nstructural equation model, 929\nstructural equation models, 674structural error, 230\nstructural risk minimization, 206\nstructural signatures, 926\nstructural support vector machines, 693\nstructural time series, 637\nstructural zeros, 672\nstructure learning, 621, 681\nstructured mean \ufb01eld, 740\nstructured output, 684\nstructured perceptron algorithm, 700\nstructured-output classi\ufb01cation problems, 266Student t, 359Studenttdistribution, 39\nsub-Gaussian, 413\nsubderivative, 432\nsubdifferential, 432\nsubgradient, 432, 432\nsubgraph, 310\nsubjective, 67\nsubjective\nprobability, 310\nsubmodular, 802\nsubsampling, 566\nsubspace method, 647\nsufficiency principle, 214\nsufficient statistics, 74, 79, 281, 282, 348\nsuffix trees, 483sum of squared errors, 218\nsum of squares, 220\nsum rule, 29\nsum-product, 614,709\nsum-product algorithm, 707\nsuper efficient, 820\nsuper-Gaussian, 413\nsupermodular, 802\nsupervised LDA, 967\nsupervised learning, 2\nsupervised PCA, 405\nsupport,426\nsupport vector machine, 488,496, 569\nsupport vector machines, 211support vectors, 496,498, 499\nsurrogate loss, 304surrogate loss function, 211\nsurrogate splits, 550\nsurvival of the \ufb01ttest, 825\nsuspicious coincidence, 164suspicious coincidences, 67\nSVD, 107, 228, 392, 980\nSVM, 211, 488,496\nSVMstruct, 698, 700\nSwendsen Wang, 866\nswitching linear dynamical system, 655, 831\nswitching state space model, 655\nsymbol grounding, 1007\nsymmetric, 849", "1096": "1066 INDEXES\nsynchronous updates, 773\nsyntactic sugar, 321\nsynthesis view, 387\nsystematic resampling, 826\nsystems biology, 13\nsystems identi\ufb01cation, 646\nsystolic array, 710\nt statistic, 137\nt-test,137\ntabula rasa, 165\ntail area probabilities, 33\ntail area probability, 213\nTAN,312, 914\nTASA, 951Taylor series, 255Taylor series expansion, 648Taylor\u2019s theorem, 248temperature, 104\ntemplate, 676\ntemplate matching, 543\ntensor product, 553tensor product basis, 538\nterminals, 689test statistic, 163,213\nTF-IDF,480\nthin junction tree \ufb01lter, 635thin junction trees, 944\nthin plate spline, 538\nthin SVD, 392\nthinning, 862\nThompson sampling, 185\ntied,103,565, 997\ntied-mixture HMM, 630\nTikhonov regularization, 124\ntime reversible, 599\ntime-invariant, 589\ntime-series forecasting, 637, 673Tobit model, 379\nToeplitz, 627tokens,945\ntopic,946,951\ntopic model, 757topological ordering, 310, 310\ntotal ordering, 310\ntrace,99\ntrace plot, 859\ntrace trick, 99\ntraceback, 614,717\ntracking, 823tracking by detection, 830\ntractable substructure, 739\ntrail,310\ntraining set, 2\ntr\nans-dimensional MCMC, 855\ntransfer function, 563,570\ntransfer learning, 296\ntransient, 599\ntransition matrix, 589, 590\ntransition model, 312,631\ntranslation invariance, 565,1004\ntranslation invariant, 472translation invariant prior, 167\nTRBP,787\nTRBP-S,801\ntree,310\ntree EP,793\ntree reparameterization, 774\ntree reweighted belief propagation, 786\ntree-augmented naive Bayes classi\ufb01er, 312treewidth, 320, 719, 800\ntrellis, 614trellis diagram, 612\ntri-cube kernel, 508\ntriangle inequality, 352, 875\ntriangulated, 722\ntridiagonal, 114trigram model, 591\ntrue positive rate, 181\nTrueSkill, 654, 793\ntruncated Gaussian, 362\ntruncated Gaussian potential, 691\ntruncated Newton, 250\ntruncated SVD, 393\nTRW,787\nTRW-S,801\ntube,497\ntuples, 975turbo codes, 768two-\ufb01lter smoothing, 646\ntwo-slice marginal, 611\ntype I, 213type I error rate, 181\ntype II maximum likelihood, 157\ntype-II maximum likelihood, 173\nU-shaped curve, 23\nUCB,185\nUGM,661\nUKF,650\nunbiased, 200\nuncertainty, 27\nunclamped phase, 988\nunclamped term, 677\nunconditionally independent, 30\nunder\n\ufb01ts,23\nundirected, 309\nundirected graphical model, 661\nundirected local Markov property, 662\nunfaithful, 663\nunidenti\ufb01able, 200, 278, 841Uni\ufb01ed Medical Language System, 977uniform distribution, 32\nunigram statistics, 591\nunigrams, 953uninformative, 165\nunion bound, 209\nunit information prior, 236\nuniversal approximator, 564\nunk,81,596\nunknown, 15, 81unrolled, 321\nunscented Kalman \ufb01lter, 523, 650\nunscented particle \ufb01lter, 828\nunscented transform, 650\nunstable, 550\nunsupervised learning, 2,9, 337\nup-down, 998\nuser rating pro\ufb01le, 949\nutilities, 294utility function, 177\nutility nodes, 328\nv-structure, 324, 326\nvalidation set, 23\nvalue nodes, 328\nvalue of perfect information, 331\nvanishing gradient, 999\nVapnik-Chervonenkis, 210\nVAR,673", "1097": "INDEXTOKEYWORDS 1067\nvariable duration HMM, 622\nvariable elimination, 318, 331, 715\nvariance, 33\nvariance stabilizing transform, 175\nvariation of information, 879\nvariational Bayes, 742\nvariational Bayes EM, 620, 750, 923\nvariational EM, 368\nvariational free energy, 733\nvariational inference, 281, 318, 731\nvariational message passing, 756\nvarimax,385, 410\nVB,742\nVBEM,750\nVC,210\nVC dimension, 206\nvector auto-regressive, 673\nvector quantization, 354\nversion space, 67\nvertices,309\nVIBES,756\nviews,904\nvisible, 349visible nodes, 313\nvisible variables, 319\nvisual words, 1007\nvisualizing, 12\nViterbi,612, 701\nViterbi decoding, 608\nViterbi training, 620\nVMP,756\nVoronoi tessellation, 18\nVQ,354\nWald, 448Wald interval, 212\nwarm starting, 442\nWARP,304\nWatson, 4wavelet, 469wavelet transforms, 413weak conditionality, 215\nweak learner, 554\nweak marginalization, 658\nweb crawling, 600\nweb spam, 603\nweight decay, 226,572, 987\nweight function, 533\nweight vector, 19\nweighted approximate-rank pairwise, 304\nw e i g h t e da v e r a g e ,7 1weighted least squares, 358weighted least squares problem, 251\nWhitening,142\nw\nhitening, 410\nWidrow-Hoff rule, 265\nWishart,125\nworking response, 250\nWorld Health Organization, 60wrapper method, 427\nXbox, 654, 795xor,486\nZellner\u2019s g-prior, 405zero avoiding, 733\nzero count problem, 77\nzero forcing, 733\nzero temperature limit, 800\nzig-zag,248Zipf\u2019s law, 43"}}